{
  "hash": "9a2a93e5cfb6bce17e9c5279d52ee603",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"What are we modeling? Using predictive fit to inform effect metric choice in meta-analysis\"\nauthor: \"James E. Pustejovsky\"\ndate: \"October 9, 2025\"\ntitle-slide-attributes:\n  data-background-image: images/ruler.jpg\n  data-background-size: cover\n  data-background-opacity: \"35%\"\nexecute:\n  echo: false\n  message: false\n  warning: false\nformat: \n  revealjs:\n    center-title-slide: true\n    math: true\n    html-math-method: mathjax\n    reference-location: document\n    slide-number: true\n    chalkboard: \n      buttons: false\n    css: styles.css\n    theme: [simple, mytheme.scss]\neditor: source\nfrom: markdown+emoji\nbibliography: \"references.bib\"\ncsl: nature.csl\n---\n\n## {.center}\n\n\n::: {.cell}\n\n:::\n\n\n### Research Synthesis\n\n> The systematic integration of empirical results across __multiple sources of evidence__, for purposes of drawing generalizations [@Cooper2009research]. \n\n### Meta-Analysis\n\n> Statistical models and methods to support quantitative research synthesis. \n\n## Fields that rely on research synthesis\n\n* Medicine (Cochrane Collaboration)\n* Education (What Works Clearinghouse)\n* Psychology\n* Social policy (justice, welfare, public health, etc.)\n* Economics, international development\n* Ecology\n* Physical sciences\n\n## {background=\"#43464B\" .center}\n\n- Some background on meta-analysis\n\n- The problem of effect metric choice\n\n- Proposal: Use predictive fit criteria to inform metric choice\n\n- Illustrations\n\n- Discussion\n\n## Canonical Meta-Analysis\n\n- We observe summary results from each of $k$ studies:\n\n    - Effect size estimate $T_i$\n    \n    - Standard error of ES estimate $se_i$\n    \n    - Study features $N_i$, $\\mathbf{x}_i$\n\n:::: {.columns}\n::: {.fragment .column width=\"50%\"}\n\n- A summary random effects model:\n    $$\n    \\begin{aligned}\n    T_i &\\sim N\\left(\\theta_i, \\ se_i^2 \\right)  \\\\\n    \\theta_i &\\sim N\\left(\\mu, \\ \\tau^2\\right)\n    \\end{aligned}\n    $$\n\n:::\n\n::: {.fragment .column width=\"50%\"}\n\n- A random effects meta-regression:\n    $$\n    \\begin{aligned}\n    T_i &\\sim N\\left(\\theta_i, \\ se_i^2 \\right)  \\\\\n    \\theta_i &\\sim N\\left(\\mathbf{x}_i \\boldsymbol\\beta,\\ \\tau^2\\right)\n    \\end{aligned}\n    $$\n:::\n\n::::\n\n:::: {.fragment}\n\n- \"Conceptual unity of statistical methods\" for meta-analysis [@Hedges2019statistical] suggests that most any effect size measure $\\theta_i$ can be used, as long as $T_i \\dot{\\sim} N\\left(\\theta_i, \\ se_i^2 \\right)$.\n\n::::\n\n## Prediction Interval\n\n- An approximate $1 - 2\\alpha$ prediction interval for a new study-specific parameter $\\theta_{new}$ [@Higgins2009reevaluation]:\n\n  $$\n  \\hat\\mu \\ \\pm \\ q_\\alpha \\times\\sqrt{\\hat\\tau^2 + \\mathbb{V}\\left(\\hat\\mu\\right)}\n  $$\n\n  - Largely used to characterize the extent of effect heterogeneity [@borenstein2017basics].\n  \n::: {.fragment}\n- Beyond this, \"predictive modeling\" culture [@Breiman2001statistical; @Donoho2017fifty]  has very little influence on meta-analysis. \n:::\n\n## Effect Metric Menagerie {.center background-image=\"images/menagerie.jpg\" background-size=\"cover\" background-opacity=\"50%\"}\n\n## Effect Metric Families\n\n::::: {.columns}\n\n:::: {.column width=\"48%\" .callout-tip icon=false .fragment}\n\n## Single-group summaries\n\n- Raw proportions $\\pi$\n- Arcsine-transformation $a = \\text{asin}\\left(\\sqrt{\\pi}\\right)$\n- Raw means $\\mu$\n\n:::: \n\n:::: {.column width=\"48%\" .callout-note icon=false .fragment}\n\n## Bivariate associations / psychometric\n\n- Pearson's correlation $\\rho$\n- Fisher's $z$-transformation $\\zeta = \\text{atanh}(\\rho)$\n- Cronbach's $\\alpha$ coefficients (or transformations thereof)\n\n::::\n\n::::: \n\n::::: {.columns}\n\n:::: {.column width=\"48%\"}\n\n::: {.callout-warning icon=false .fragment}\n\n## Group comparison of binary outcomes\n\n- Risk differences $\\pi_1 - \\pi_0$\n- Risk ratios (log-transformed) $\\log\\left(\\frac{\\pi_1}{\\pi_0}\\right)$\n- Odds ratios (log-transformed) $\\log\\left(\\frac{\\pi_1 / (1 - \\pi_1)}{\\pi_0 / (1 - \\pi_0)}\\right)$\n- Bivariate models for $\\pi_0, \\pi_1$\n\n:::\n::::\n\n:::: {.column width=\"48%\"}\n\n::: {.callout-important icon=false .fragment}\n\n## Group comparison of continuous outcomes\n\n- Raw mean differences $\\mu_1 - \\mu_0$\n- Standardized mean differences $\\delta = \\frac{\\mu_1 - \\mu_0}{\\sigma}$\n- Response ratios (log-transformed) $\\lambda = \\log\\left(\\frac{\\mu_1}{\\mu_0}\\right)$\n- Probability of superiority\n\n:::\n\n::::\n\n:::::\n\n## Effect Metric Choice\n\n- Choice of metric is constrained by\n\n    - Studies designs\n    \n    - Data availability, reporting conventions\n    \n    - Heterogeneity of study features (e.g., outcome scales)\n\n::: {.fragment}\n\n- In many applications, more than one metric could be used.\n\n    - Choice is often driven by disciplinary conventions.\n    \n:::\n\n## Metric choice studies\n\n- Large literature on choice of effect metrics for group comparison on binary outcomes.\n\n    - Theoretical arguments about interpretability, stability, non-collapsibility [@Poole2015risk; @Panagiotou2015commentary].\n    \n    - Risk differences tend to be more heterogeneous [@Engels2000heterogeneity; @Zhao2022empirical].\n\n::: {.fragment}\n- Strong opinions about effect metrics for group comparison on continuous outcomes [@Cummings2011arguments].\n\n    - Some novel alternatives to avoid standarization [@Ades2015simultaneous; @Lu2014simultaneous; @Davies2024mapping].\n    \n    - Various methods of standarization [e.g., @Hopkins2024standardization; @Fitzgerald2025using].\n\n:::\n\n::: {.fragment}\n- Choosing between standardized mean difference and response ratio metrics.\n\n    - Sensitivity analyses using both metrics [@Friedrich2011ratio].\n\n    - Model both metrics simultaneously [@Yang2024bivariate].\n\n:::\n\n## Can we choose based on predictive fit criteria?\n\n- Evaluate effect metrics by performance in __predicting data reported in a new study__.\n\n    - Data vector $\\mathbf{d}_i$ consisting of summary statistics used to compute effect size estimates.\n    \n- Use leave-one-out log-predictive density to measure predictive performance.\n  $$\n  LPD = \\sum_{i=1}^{k} \\log p_\\mathbf{D}\\left(\\mathbf{d}_i \\left| \\hat\\mu_{(-i)}, \\hat\\tau_{(-i)}, \\mathbf{X}_i, N_i\\right.\\right)\n  $$\n\n::: {.fragment}\n### Two challenges\n\n1. Polishing up models to generate predictions.\n\n2. Meta-analysis models one-dimensional $f(\\mathbf{d}_i)$, so we need auxiliary models for the rest of the data.\n\n:::\n\n## Bivariate associations\n\n\n\n## References\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}