{
  "hash": "172d2ccf9f328a2783e73b8a4e4713b0",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: Step-function selection models for meta-analysis\ndate: '2024-07-09'\ncategories:\n- effect size\n- distribution theory\n- selective reporting\nexecute:\n  echo: false\nbibliography: \"../selection-references.bib\"\ncsl: \"../apa.csl\"\nlink-citations: true\ncode-tools: true\ntoc: true\ncss: styles.css\ncrossref: \n  eq-prefix: \"\"\n---\n\n\nIn [a recent post](/posts/distribution-of-significant-effects/) I looked at the distribution of statistically significant effect sizes in studies that report multiple outcomes, when those studies are subject to a certain form of selective reporting. \nI considered a model where each effect size within the study is more likely to be reported when it is _affirmative_---or statistically significant and in the hypothesized direction---than when it is _non-affirmative_. \nBecause studies with multiple effect sizes are a very common occurrence in social science meta-analysis, it's interesting to think about how this form of selection leads to distortions of the results that are actually reported and available for meta-analysis. \n\nIn this post, I want to look a different scenario that is simpler in one respect but more complicated in another. \nSimpler, in that I'm going to ignore dependence issues and just think about the distribution of one effect size at a time. \nMore complicated, in that I'm going to look at a more general model for how selective reporting occurs, which I'll call the __*step-function selection model*__.\n\n# The step-function selection model \n\nThe step-function selection model was introduced by @hedges1992modeling and has been further expanded, tweaked, and studied in a bunch of subsequent work. \nThe model has two components: a set of assumptions about how effect size estimates are generated prior to selection (the _evidence-generation process_), and a set of assumptions about how selective reporting happens as a function of the effect size estimates (the _selective reporting process_). \n\nIn the original formulation, the evidence-generation process is a random effects model. Letting $T_i^*$ denote an effect size estimate prior to selective reporting and $\\sigma_i^*$ denote its (known) standard error, we assume that\n$$\nT_i^* | \\sigma_i^* \\sim N\\left(\\mu, \\tau^2 + \\left(\\sigma_i^*\\right)^2\\right),\n$$ {#eq-evidence-generation}\njust as in the conventional random effects model. Here $\\mu$ is the overall average effect size and $\\tau$ is the standard deviation of the effect size parameter distribution.\n\nFor the second component, we assume that the selective-reporting process is fully determined by the statistical significance and sign of the effect size estimates. We can therefore formalize the selective reporting process in terms of the one-sided p-values of the effect size estimates. Assuming that the degrees of freedom are large enough to not worry about, the one-sided p-value for effect size $i$ is a transformation of the effect size and its standard error:\n$$\np_i^* = 1 - \\Phi\\left(T_i^* / \\sigma_i^*\\right)\n$$ {#eq-p-onesided}\nIn the step-function model, we assume that the probability that an effect size estimate is reported (and thus available for meta-analysis) depends on where this one-sided p-value lies relative to a pre-specified set of significance thresholds, $\\alpha_1,...,\\alpha_H$. These thresholds define a set of intervals, each of which can have a different probability of selection.\nLet $O_i^*$ be a binary indicator equal to 1 if $T_i^*$ is reported and otherwise equal to zero. \nThe selective reporting process is then\n$$\n\\Pr\\left(O_i^* = 1| T_i^*, \\sigma_i^*\\right) = \\begin{cases}\n1 & \\text{if} \\quad p_i^* < \\alpha_1 \\\\ \n\\lambda_1 & \\text{if} \\quad \\alpha_h \\leq p_i^* < \\alpha_{h+1}, \\quad h = 1,...,H-1 \\\\ \n\\lambda_H & \\text{if} \\quad \\alpha_H \\leq p_i^* \\\\ \n\\end{cases}.\n$$ {#eq-selection-process}\nNote that the selection probability for the lowest interval $[0, \\alpha_1)$ is fixed to 1 because we can't estimate the absolute probability that an effect size estimate is reported. \nThe remaining parameters of the selection process therefore each represent a ratio of the probability of reporting an effect size estimate falling in a given interval to the probability of reporting an effect size estimate falling in the lowest interval.\n\nIn practice, the analyst will need to specify the thresholds of the significance intervals in order to estimate the model. One common choice is to use only a single threshold at $\\alpha_1 = .025$, which corresponds to a two-sided level of .05---Fisher's vaunted criteria for when a result should be considered significant. This is the so-called \"three-parameter\" selection model, where the parameters of interest are the average effect size $\\mu$, the heterogeneity SD $\\tau$, and the relative selection probability $\\lambda_1$. Other possible choices for thresholds might be:\n\n* A single threshold at $\\alpha_1 = .50$, so that negatively-signed effect size estimates have a different selection probability than positively-signed estimates;\n* A two-threshold model with $\\alpha_1 = .025$ and $\\alpha_2 = .50$ (I like to call this a four-parameter selection model); or\n* A model with thresholds for significant, positive results at $\\alpha_1 = .025$, for the sign of the estimate at $\\alpha_2 = .50$, and for statistically significant results in the opposite of the expected direction at $\\alpha_3 = .975$. \n\nMany other choices are possible, of course.\n\n# Distribution of observed effect sizes\n\nEquations (@eq-evidence-generation) and (@eq-selection-process) are sufficient to describe the distribution of effect sizes actually observed after selection. If we let $T_i$ and $\\sigma_i$ denote effect size estimates that are actually observed, then the distribution of $\\left(T_i | \\sigma_i\\right)$ is the same as that of $\\left(T_i^* | \\sigma_i^*, O_i^* = 1\\right)$. By Bayes Theorem, \n$$\n\\Pr(T = t | \\sigma_i) = \\frac{\\Pr\\left(O_i^* = 1| T_i^* = t, \\sigma_i^* = \\sigma_i\\right) \\times \\Pr(T_i^*  = t| \\sigma_i^* = \\sigma_i)}{\\Pr\\left(O_i^* = 1| \\sigma_i^* = \\sigma_i\\right)}\n$$ {#eq-observed-effect-distribution}\nFor the specific distributional assumptions of the step-function selection model, we can find an expression for the exact form of (@eq-observed-effect-distribution). \nIn doing so, it will be useful to define a further random variable---call it $S_i$---that is equal to the p-value interval into which effect size $T_i$ falls. For a given effect size with standard error $\\sigma_i^*$, these intervals are equivalent to intervals on the scale of the outcome, with thresholds $\\gamma_{hi} = \\sigma_i^* \\Phi^{-1}(1 - \\alpha_h)$. Now, let's define $S_i^*$ as\n$$\nS_i^* = \\begin{cases}\n0 & \\text{if} \\quad \\gamma_{1i} < T_i^* \\\\ \nh & \\text{if} \\quad \\gamma_{h+1,i} < T_i^* \\leq \\gamma_{hi}, \\quad h = 1,...,H-1 \\\\ \nH & \\text{if} \\quad T_i^* \\leq \\gamma_{Hi} \\\\ \n\\end{cases}\n$$\nand $S_i$ as the corresponding interval for the observed effect size $T_i$.\nNote that (@eq-selection-process) is equivalent to writing the relative selection probabilities as a function of $S_i$:\n$$\nw\\left(T_i^*, \\sigma_i^*\\right) = \\lambda_{S_i^*}\n$$ {#eq-selection-weights}\nAlso note that, prior to selection, the effect size estimate $T_i^*$ has marginal variance $\\eta_i^2 = \\tau^2 + \\left(\\sigma_i^*\\right)^2$, so we can write $\\Pr(T_i^*  = t| \\sigma_i^*) =  \\frac{1}{\\eta_i}\\phi\\left(\\frac{t - \\mu}{\\eta_i}\\right)$, where $\\phi()$ is the standard normal density.  We can then write the distribution of the observed effect size estimates as\n$$\n\\Pr(T_i = t | \\sigma_i) = \\frac{w\\left(t, \\sigma_i\\right) \\times \\frac{1}{\\eta_i}\\phi\\left(\\frac{t - \\mu}{\\eta_i}\\right)}{A_i},\n$$ {#eq-observed-effect-density}\nwhere\n$$\n\\begin{aligned}\nA_i &= \\int w\\left(t, \\sigma_i^*\\right) \\times \\frac{1}{\\eta_i}\\phi\\left(\\frac{t - \\mu}{\\eta_i}\\right) dt \\\\\n&= \\sum_{h=0}^H \\lambda_h B_{hi},\n\\end{aligned}\n$$ {#eq-Ai}\nwith \n$$\nB_{hi} = \\Phi\\left(\\frac{\\gamma_{hi} - \\mu}{\\eta_i}\\right) - \\Phi\\left(\\frac{\\gamma_{h+1,i} - \\mu}{\\eta_i}\\right)\n$$\nand where we take $\\lambda_0 = 1$, $\\alpha_0 = 0$, and $\\alpha_{H+1} = 1$ [@hedges2005selection]. Note that $A_i  = \\Pr(O^* = 1 | \\sigma_i^*)$, the probability that an effect size estimate with precision $\\sigma_i^*$ will be observed.\n\nThe observed effect size estimates follow what we might call a \"piece-wise normal\" distribution. \nFor a given $\\sigma_i$ and given the interval $S_i$ into which the effect size falls, the effect size follows a truncated normal distribution. Formally, \n$$\n\\left(T_i | S_i = h, \\sigma_i \\right) \\sim TN(\\mu,\\eta_i^2, \\gamma_{h+1,i},\\gamma_{h i}).\n$$\nFurthermore, the distribution of $S_i$ is given by\n$$\n\\Pr(S_i = h | \\sigma_i) = \\frac{\\lambda_h B_{hi}}{\\sum_{g=0}^H \\lambda_g B_{gi}},\n$$\nwhich will be useful for deriving moments of the distribution of $T_i$.\n\nHere is an interactive graph showing the distribution of the effects prior to selection (in grey) and the distribution of observed effect sizes (in blue) based on a four-parameter selection model with selection thresholds of $\\alpha_1 = .025$ and $\\alpha_2 = .50$. Initially, the selection parameters are set to $\\lambda_1 = 0.6$ and $\\lambda_2 = 0.3$, but you can change these however you like.\n\n\n```{ojs}\nmath = require(\"mathjs\")\nnorm = import('https://unpkg.com/norm-dist@3.1.0/index.js?module')\n\neta = math.sqrt(tau**2 + sigma**2)\nH = 2\nalpha = [.025, .500]\nlambda = [1, lambda1, lambda2]\n\nfunction findlambda(p, alp, lam) {\n  var m = 0;\n  while (p >= alp[m]) {\n    m += 1;\n  }\n  return lam[m];\n}\n\nfunction findMoments(mu, tau, sigma, alp, lam) {\n  let H = alp.length;\n  let eta = math.sqrt(tau**2 + sigma**2);\n  \n  let gamma_h = Array(H+2).fill(null).map((x,i) => {\n    if (i==0) {\n      return Infinity;\n    } else if (i==H+1) {\n      return -Infinity;\n    } else {\n      return sigma * norm.icdf(1 - alp[i-1]);\n    }\n  });\n  \n  let c_h = Array(H+2).fill(null).map((x,i) => {\n    return (gamma_h[i] - mu) / eta;\n  });\n\n  let B_h = Array(H+1).fill(null).map((x,i) => {\n    return norm.cdf(c_h[i]) - norm.cdf(c_h[i+1])\n  });\n\n  let Ai = 0;\n  for (let i = 0; i <= H; i++) {\n  \tAi += lam[i] * B_h[i];\n  }\n  \n  let psi_h = Array(H+1).fill(null).map((x,i) => {\n    return (norm.pdf(c_h[i+1]) - norm.pdf(c_h[i])) / B_h[i]\n  }); \n\n  let psi_top = 0;\n  for (let i = 0; i <= H; i++) {\n  \tpsi_top += lam[i] * B_h[i] * psi_h[i];\n  }\n  \n  let psi_bar = psi_top / Ai;\n\n  let ET = mu + eta * psi_bar;\n\n  let dc_h = c_h.map((c_val) => {\n    if (math.abs(c_val) == Infinity) {\n      return 0;\n    } else {\n      return c_val * norm.pdf(c_val);\n    }\n  });\n\n  let kappa_h = Array(H+1).fill(null).map((x,i) => {\n    return (dc_h[i] - dc_h[i+1]) / B_h[i];\n  });\n\n  let kappa_top = 0;\n  for (let i = 0; i <= H; i++) {\n  \tkappa_top += lam[i] * B_h[i] * kappa_h[i];\n  }\n  let kappa_bar = kappa_top / Ai;\n  let SDT = eta * math.sqrt(1 - kappa_bar - psi_bar**2);\n  \n  return ({Ai: Ai, ET: ET, SDT: SDT});\n}\n\nmoments = findMoments(mu, tau, sigma, alpha, lambda)\nAi_toprint = moments.Ai.toFixed(3)\nET_toprint = moments.ET.toFixed(3)\neta_toprint = eta.toFixed(3)\nSDT_toprint = moments.SDT.toFixed(3)\n\n```\n\n```{ojs}\npts = 201\n\ndat = Array(pts).fill().map((element, index) => {\n  let t = mu - 3 * eta + index * eta * 6 / (pts - 1);\n  let p = 1 - norm.cdf(t / sigma);\n  let dt = norm.pdf((t - mu) / eta) / eta;\n  let lambda_val = findlambda(p, alpha, lambda);\n  return ({\n    t: t,\n    d_unselected: dt,\n    d_selected: lambda_val * dt\n  })\n})\n\n```\n\n\n::::: {.grid .column-page}\n\n:::: {.g-col-8 .center}\n\n\n```{ojs}\nPlot.plot({\n  height: 300,\n  y: {\n    grid: false,\n    label: \"Density\"\n  },\n  x: {\n    label: \"Effect size estimate (Ti)\"\n  },   \n  marks: [\n    Plot.ruleY([0]),\n    Plot.ruleX([0]),\n    Plot.areaY(dat, {x: \"t\", y: \"d_unselected\", fillOpacity: 0.3}),\n    Plot.areaY(dat, {x: \"t\", y: \"d_selected\", fill: \"blue\", fillOpacity: 0.5}),\n    Plot.lineY(dat, {x: \"t\", y: \"d_selected\", stroke: \"blue\"})\n  ]\n})\n```\n\n\n:::{.moments}\n\n\n```{ojs}\ntex`\n\\begin{aligned}\n\\mu &= ${mu} & \\qquad \\eta &= ${eta_toprint} \\\\\n\\mathbb{E}\\left(T_i\\right) &= ${ET_toprint}\n& \\qquad \\sqrt{\\mathbb{V}\\left(T_i\\right)} &= ${SDT_toprint} \\\\ \n\\Pr(O_i^* = 1) &= ${Ai_toprint}\n\\end{aligned}\n`\n```\n\n:::\n\n::::\n\n:::: {.g-col-4}\n\n\n```{ojs}\n//| panel: input\n\nviewof mu = Inputs.range(\n  [-2, 2], \n  {value: 0.15, step: 0.01, label: \"Average effect size (mu):\"}\n)\n\nviewof tau = Inputs.range(\n  [0, 2], \n  {value: 0.10, step: 0.01, label: \"Heterogeneity SD (tau):\"}\n)\n\nviewof sigma = Inputs.range(\n  [0, 1], \n  {value: 0.20, step: 0.01, label: \"Standard error (sigma):\"}\n)\n\nviewof lambda1 = Inputs.range(\n  [0, 2],\n  {value: 0.60, step: 0.01, label: \"Selection probability over .025 < p <= .500 (lambda1)\"}\n)\n\nviewof lambda2 = Inputs.range(\n  [0, 2],\n  {value: 0.30, step: 0.01, label: \"Selection probability over .500 < p (lambda2)\"}\n)\n\n```\n\n\n::::\n\n\n:::::\n\n# Moments of $T_i | \\sigma_i$\n\nUsing the auxiliary random variable $S_i$ makes it pretty straight-forward to find the moments of $T_i | \\sigma_i$. Let me denote\n$$\n\\psi_{hi} = \\frac{\\phi\\left(c_{h+1,i}\\right) - \\phi\\left(c_{hi}\\right)}{B_{hi}},\n$$\nwhere $c_{hi} = \\left(\\gamma_{hi} - \\mu\\right) / \\eta_i$ for $h=0,...,H$.\nThen from the properties of truncated normal distributions, \n$$\n\\mathbb{E}(T_i | S_i = h, \\sigma_i) = \\mu + \\eta_i \\times \\psi_{hi},\n$$\nIt follows immediately that\n$$\n\\begin{aligned}\n\\mathbb{E}(T_i | \\sigma_i) &= \\sum_{h=0}^H \\Pr(S_i = h | \\sigma_i) \\times \\mathbb{E}(T_i | S_i = h, \\sigma_i) \\\\\n&= \\mu + \\eta_i \\frac{\\sum_{h=0}^H \\lambda_h B_{hi} \\psi_{hi}}{\\sum_{h=0}^H \\lambda_h B_{hi}}.\n\\end{aligned}\n$$ {#eq-Ti-expectation}\nThe second term of (@eq-Ti-expectation) is the bias of $T_i$ relative to the overall mean effect $\\mu$. Generally, it will depend on all the parameters of the evidence-generating process, including $\\sigma_i$, $\\mu$, and $\\tau$ (through the $c_{hi}$ and $B_{hi}$ terms) and on the selection weights $\\lambda_1,...,\\lambda_H$.\n\nJust for giggles, let me chug through and get the variance of $T_i$ as well. Letting \n$$\n\\bar\\psi_i = \\frac{\\sum_{h=0}^H \\lambda_h B_{hi} \\psi_{hi}}{\\sum_{h=0}^H \\lambda_h B_{hi}}\n$$\nand\n$$\n\\kappa_{hi} = \\frac{c_{hi} \\phi\\left(c_{hi}\\right) - c_{h+1,i} \\phi\\left(c_{h+1,i}\\right)}{B_{hi}},\n$$\nwe can write the variance of the truncated normal conditional distribution\n$$\n\\mathbb{V}(T_i | S_i = h, \\sigma_i) = \\eta_i^2 \\left(1 - \\kappa_{hi} - \\psi_{hi}^2\\right).\n$$\nUsing variance decomposition, we then have\n$$\n\\begin{aligned}\n\\mathbb{V}(T_i | \\sigma_i) &= \\mathbb{E}\\left[\\mathbb{V}(T_i | S_i, \\sigma_i)\\right] + \\mathbb{V}\\left[\\mathbb{E}(T_i | S_i, \\sigma_i)\\right] \\\\\n&= \\mathbb{E}\\left[\\mathbb{V}(T_i | S_i, \\sigma_i) + \\left[\\mathbb{E}(T_i | S_i, \\sigma_i) - \\mathbb{E}(T_i | \\sigma_i)\\right]^2\\right] \\\\\n&= \\frac{1}{\\sum_{h=0}^H \\lambda_h B_{hi}} \\sum_{h=0}^H \\lambda_h B_{hi} \\left( \\eta_i^2 \\left(1 - \\kappa_{hi} - \\psi_{hi}^2\\right) + \\eta_i^2  \\left[\\psi_{hi} - \\bar\\psi_i \\right]^2\\right) \\\\\n&= \\eta_i^2 \\left(1 - \\bar\\kappa_i - \\bar\\psi_i^2\\right),\n\\end{aligned}\n$$ {#eq-Ti-variance}\nwhere \n$$\n\\bar\\kappa_i = \\frac{\\sum_{h=0}^H \\lambda_h B_{hi} \\kappa_{hi}}{\\sum_{h=0}^H \\lambda_h B_{hi}}\n$$\nJust as with the expectation, the variance is a complicated function of all the model parameters. \n\n# Funnel density\n\nThe graph above shows the distribution of observed effect size estimates with a given sampling standard error $\\sigma_i$. \nIn practice, meta-analysis datasets include many effect sizes with a range of different standard errors. \nFunnel plots are a commonly used graphical representation the distribution of effects in a meta-analysis.\nThey are simply scatterplots, showing effect size estimates on the horizontal axis and standard errors (or some measure of precision) on the vertical axis.\nUsually, they are arranged so that effects from larger studies appear closer to the top of the plot.\nFunnel plots are often used a diagnostic for selective reporting because they will tend to be asymmetric when non-affirmative effects are less likely to be reported than affirmative effects.\n\nI think it's pretty useful to use the layout of a funnel plot to understand how meta-analytic models work. A basic random effects model implies a certain distribution of population effects, which can be represented by the density of points in a funnel plot.[^technically-conditional] \nThat density will have a shape kind of like an upside down funnel: narrow near the top (where studies are large and $\\sigma_i$ is small), getting wider and wider as $\\sigma_i$ increases (i.e., as studies get smaller and smaller). \n\n[^technically-conditional]: To be precise, the density will depend on the marginal distribution of $\\sigma_i^*$'s in the population of effects. I'm going to side-step this problem by using the funnel plot layout to show the _conditional_ distribution of the effect size estimates, given $\\sigma_i^* = \\sigma_i$. \n\nHere's an illustration of this density, using darker color to indicate areas of the plot where effect sizes are more likely. \nI've used $\\mu = 0.15$ (represented by the vertical red line) and $\\tau = 0.10$ to calculate the density. \nThe vertical gray line corresponds to $\\mu = 0$, which is also the threshold where effect size estimates will have $p$-values of $\\alpha = .50$. \nThe sloped gray line corresponds to the treshold where effect size estimates have $p$-values of $\\alpha = .025$; to the right of this line, effects will be statistically significant and affirmative; to the left, effects are non-affirmative.\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-6-1.png){width=90%}\n:::\n:::\n\n\nThe above graph shows the (conditional) distribution of effect size estimates under the random effects model, without any selective reporting. \nSelective reporting of study results will distort this distribution, shrinking the density for effects that are not affirmative.\n\nHere is an interactive funnel plot showing the distribution of effect size estimates under a four-parameter selection model. Just as in the interactive graph above, I use fixed selection thresholds of $\\alpha_1 = .025$ and $\\alpha_2 = .50$. Initially, I set $\\mu = 0.15$ and $\\tau = 0.10$ and selection parameters of $\\lambda_1 = 0.6$ and $\\lambda_2 = 0.3$, but you can change these however you like.\n\n\n```{ojs}\nSE_pts = 100\nt_pts = 181\nlambda_f = [1, lambda1_f, lambda2_f]\nalpha_f = [.025, .500]\nsigma_max = 0.5\neta_max_f = math.sqrt(tau_f**2 + sigma_max**2)\n\nfunnel_dat = Array(t_pts * SE_pts).fill(null).map((x,row) => {\n  let i = row % SE_pts;\n  let j = (row - i) / SE_pts;\n  let sigma = (i + 1) * sigma_max / SE_pts;\n  let t = mu_f - 3 * eta_max_f + j * eta_max_f * 6 / (t_pts - 1);\n  let eta = math.sqrt(tau_f**2 + sigma**2);\n  let p = 1 - norm.cdf(t / sigma);\n  let dt = norm.pdf((t - mu_f) / eta) / eta;\n  let lambda_val = findlambda(p, alpha_f, lambda_f);\n  return ({i: i, j: j, t: t, sigma: sigma, eta: eta, p: p, dt: dt, lambda: lambda_val, d_selected: lambda_val * dt});\n})\n\nsigline_dat = [\n  ({t: 0, sigma: 0}),\n  ({t: sigma_max * norm.icdf(0.975), sigma: sigma_max})\n]\n\n```\n\n\n::::: {.grid .column-page}\n\n:::: {.g-col-8 .center}\n\n\n```{ojs}\nPlot.plot({\n  height: 400,\n  width: 700,\n  padding: 0,\n  grid: false,\n  x: {axis: \"top\", label: \"Effect size estimate (Ti)\"},\n  y: {label: \"Standard error (sigma_i)\", reverse: true},\n  color: {\n    scheme: \"pubugn\",\n    type: \"sqrt\",\n    label: \"Density\"\n  },\n  marks: [\n    Plot.dot(funnel_dat, {x: \"t\", y: \"sigma\", fill: \"d_selected\", r:2, symbol: \"square\"}),\n    Plot.ruleY([0], {stroke: \"black\"}),\n    Plot.ruleX([0], {stroke: \"grey\"}),\n    Plot.line(sigline_dat, {x: \"t\", y: \"sigma\", stroke: \"gray\", strokeWidth: 1})\n  ]\n})\n```\n\n```{ojs}\nPlot.legend({color: {scheme: \"pubugn\", type: \"sqrt\", label: \"Density\"}})\n```\n\n\n::::\n\n:::: {.g-col-4}\n\n\n```{ojs}\n//| panel: input\n\nviewof mu_f = Inputs.range(\n  [-2, 2], \n  {value: 0.15, step: 0.01, label: \"Average effect size (mu):\"}\n)\n\nviewof tau_f = Inputs.range(\n  [0, 2], \n  {value: 0.10, step: 0.01, label: \"Heterogeneity SD (tau):\"}\n)\n\nviewof lambda1_f = Inputs.range(\n  [0, 2],\n  {value: 0.60, step: 0.01, label: \"Selection probability over .025 < p <= .500 (lambda1)\"}\n)\n\nviewof lambda2_f = Inputs.range(\n  [0, 2],\n  {value: 0.30, step: 0.01, label: \"Selection probability over .500 < p (lambda2)\"}\n)\n```\n\n\n::::\n\n:::::\n\nIf you fiddle with the selection parameters, you will see that the density of certain areas of the plot changes.\nFor instance, lowering $\\lambda_2$ will reduce the density of negative effect size estimates; lowering $\\lambda_1$ will reduce the density of positive but non-affirmative effect size estimates, which fall between the vertical axis and the diagonal line corresponding to $\\alpha = .025$. \n\n# Comment\n\nIn this post, I've given expressions for the density of effect size estimates under the step-function selection model, as well as expressions for the mean and variance of effect size estimates of a given precision (i.e., for $T_i | \\sigma_i$). \nAlthough these expressions are pretty complex, it seems like they could be useful for studying the properties of different estimators that have been proposed for dealing with selective reporting, such as the \"unrestricted weighted least squares\" method, which is just the idea of using fixed effects weights even though the effects are heterogeneous [@henmi2010confidence; @stanley2014metaregression]; the PET and PEESE estimators [@stanley2008metaregression]; the endogenous kink meta-regression [@bom2019kinked]; and perhaps other estimators in the literature. \nGraphical depictions of the step function density (as in the funnel plot above) also seem potentially useful for understanding the properties of these estimators.\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}