{
  "hash": "452d9dafc2d3969c2423e6254314eb43",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: Variance stabilization of Cohen's d\ndate: '2025-07-14'\ncategories:\n- effect-size\n- standardized-mean-difference\n- delta-method\ncode-fold: true\ncode-tools: true\ntoc: true\nbibliography: d-r-z-refs.bib\ndraft: true\ncsl: \"../apa.csl\"\n---\n\nIn a recent _Psychological Methods_ paper, @Haaf2023does suggest conducting meta-analysis of standardized mean differences after transforming them to point biserial correlations and then applying Fisher's $z$-transformation. \nThey argue that this leads to a variance-stabilized effect size, which has sampling variance that is constant across varying values of the true parameter $\\delta$. \n@Bartos2023robust cites @Haaf2023does as justification for conducting meta-analysis of standardized mean differences after applying this $d$-to-$r$-to-$z$ transformation.\nAll this surprised me a bit because the transformation was unfamiliar, although I've written about these sorts of effect size conversions before [cf. @Pustejovsky2014converting].\nIn other work [@Pustejovsky2018testing], I've used the variance-stabilizing transformation given in @Hedges1985statistical, which is different than this approach. \nWhat gives? Is this $d$-to-$r$-to-$z$ business _really_ variance stabilizing?\n\nThe first step in the transformation proposed by @Haaf2023does depends on the relative sample sizes of the two groups, which I will parameterize as $R = N_1 : N_2$.\nLetting $a = (R + 1)^2 / R$, the transformation function is given by \n$$\nz(d; a) = \\frac{1}{2}\\left[\\log\\left(\\sqrt{d^2 + a} + d\\right) - \\log\\left(\\sqrt{d^2 + a} - d\\right)\\right]\n$$\n\nThe shape of the transformation is depicted in @fig-Haaf-transform.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\n\nd_to_z <- function(d, R) {\n  r <- d / sqrt(d^2 + (R + 1)^2 / R)\n  atanh(r)\n}\n\nd_to_h <- function(d, R) {\n  a <- (R + 1)^2 / R\n  sqrt(2) * sign(d) * (log(abs(d) + sqrt(d^2 + 2 * a)) - log(2 * a) / 2)\n}\n\ntransform_dat <- \n  expand_grid(\n    d = seq(-5,5,0.02),\n    R = 1:3\n  ) |>\n  mutate(\n    z = d_to_z(d, R),\n    h = d_to_h(d, R),\n    R_fac = factor(paste(R,\"1\", sep = \":\"))\n  )\n\nggplot(transform_dat) +\n  aes(d, z, color = R_fac) + \n  geom_abline(slope = 1 / 2, intercept = 0, linetype = \"dashed\") + \n  geom_line() + \n  theme_minimal() + \n  theme(legend.position = \"inside\", legend.position.inside = c(0.9, 0.2)) + \n  labs(color = \"R\") + \n  scale_x_continuous(breaks = seq(-5,5,1), minor_breaks = NULL)\n```\n\n::: {.cell-output-display}\n![d-to-r-to-z transformation for various allocation fractions $p$. The dashed line is y = x / 2.](index_files/figure-html/fig-Haaf-transform-1.png){#fig-Haaf-transform width=768}\n:::\n:::\n\n\nThe variance-stabilizing transformation given in @Hedges1985statistical is \n$$\nh(d; a) = \\sqrt{2} \\left(\\frac{d}{|d|}\\right) \\left[\\log\\left(|d| + \\sqrt{d^2 + 2a}\\right) - \\frac{1}{2}\\log\\left(2a\\right)\\right]\n$$\n\n@fig-transformation-comparison compares the two transformation functions, with $h(z)$ in red and $z(d)$ in blue. The functions are very similar over the range [-2, 2] and only begin to diverge when $|d| > 3$, the realm of what would typically be considered implausibly large effect sizes.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntransform_dat |>\n  mutate(R_fac = fct_rev(R_fac) |> fct_relabel(\\(x) paste(\"R ==\", x))) |>\n  pivot_longer(cols = c(z, h), names_to = \"stat\", values_to = \"val\") |>\n  ggplot() + \n  aes(d, val, color = stat) + \n  geom_line() + \n  facet_wrap(~ R_fac, labeller = \"label_parsed\") + \n  theme_minimal() + \n  theme(legend.position = \"inside\", legend.position.inside = c(0.05, 0.9)) + \n  scale_x_continuous(breaks = seq(-4,4,2)) + \n  labs(y = \"\", color = \"\")\n```\n\n::: {.cell-output-display}\n![Comparison of transformation functions given by Haaf and Rouder (2023) and by Hedges and Olkin (1985)](index_files/figure-html/fig-transformation-comparison-1.png){#fig-transformation-comparison width=768}\n:::\n:::\n\nThis suggests that any differences in the performance of the transformations will be driven by behavior in the extremes of the distribution and will thus be less pronounced when $N_1$ and $N_2$ are large.\n\n# Two-sample simulations\n\nI ran some quick simulations to look at how the sampling variance of $d$, $z(d)$, and $h(d)$ change as a function of the true average effect size $\\delta$. \nThese simulations are based on a data-generating process in which two independent samples are drawn from normally distributed populations with a standardized mean difference of $\\delta$, with total sample size $N$ and allocation ratio of $R = N_1:N_2$.\nA variance-stabilizing transformation should produce an estimator with true standard error (i.e., square root of the sampling variance) that is constant, not depending on $\\delta$. \n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(simhelpers)\nlibrary(future)\nplan(multisession)\n\n# Generate standardized mean differences d, z(d), h(d)\n\nr_smd <- function(reps, delta = 0, N = 20, R = 1) {\n  a <- sqrt(N * R / (R + 1)^2)\n  ncp <- delta * a\n  tstats <- rt(reps, df = N - 2, ncp = ncp)\n  d <- tstats / a\n  z <- d_to_z(d, R = R)\n  h <- d_to_h(d, R = R)\n  data.frame(d = d, z = z, h = h)\n}\n\n# Summarize mean and variance\ncalc_M_SE <- function(dat) {\n  M <- colMeans(dat, na.rm = TRUE)\n  SE <- apply(dat, 2, sd, na.rm = TRUE)\n  data.frame(stat = names(dat), M = M, SE = SE)\n}\n\n# Simulation driver\nsim_d_twosample <- compose(calc_M_SE, r_smd)\n\n# Parameter grid\nsim_grid <-\n  expand_grid(\n    N = c(10, 30, 50),\n    R = 1:3,\n    delta = seq(0, 3, 0.1)\n  ) |>\n  mutate(\n    reps = 2e4\n  )\n\n# Execute simulations\nd_twosample_res <- \n  sim_grid |>\n  evaluate_by_row(\n    sim_d_twosample, \n    system_time = FALSE,\n    verbose = FALSE\n  ) |>\n  mutate(\n    R_fac = factor(R) |> fct_relabel(\\(x) paste0(\"R == \", x, \":1\")),\n    N_fac = factor(N) |> fct_relabel(\\(x) paste(\"N ==\", x)),\n    SE_scaled = if_else(stat == \"d\", SE * sqrt((N - 2) * R / (R + 1)^2), sqrt(N - 2) * SE)\n  )\n```\n:::\n\n\n\n@fig-true-SE-twosample shows the relationship between the parameter $\\delta$ and the true standard error of the standardized mean difference estimator $d$, along with the true standard errors of the transformed effect size estimators $z(d)$ and $h(d)$. \nTo facilitate comparison between the raw and transformed estimators, I have re-scaled the standard errors according to their values when $\\delta = 0$ (multiplying by $\\sqrt{(N - 2) R / (R + 1)^2}$ for $d$ and by $\\sqrt{N - 2}$ for $z(d)$ and $h(d)$).\n\n\n::: {.cell .column-body-outset lightbox='true'}\n\n```{.r .cell-code}\nggplot(d_twosample_res) + \n  aes(delta, SE_scaled, color = stat) + \n  geom_hline(yintercept = 0) + \n  geom_line() + \n  facet_grid(N_fac ~ R_fac, scales = \"free_y\", labeller = \"label_parsed\") + \n  theme_minimal() +\n  labs(\n    x = expression(delta), \n    y = \"True Standard Error (rescaled)\", \n    color = \"\"\n  )\n```\n\n::: {.cell-output-display}\n![True standard errors of untransformed $d$ and transformed ($z(d)$ and $h(d)$) effect size estimates under a two-sample homoskedastic normal model. Standard errors are rescaled so that SE = 1 when $\\delta$ = 0.](index_files/figure-html/fig-true-SE-twosample-1.png){#fig-true-SE-twosample width=100%}\n:::\n:::\n\n\nClearly, the variance of $z(d)$ is not constant, but rather is _decreasing_ in $\\delta$.\nFor instance, when $R = 1:1$ and $N = 50$, the true SE of $z(d)$ decreases to 86% of its null value as $\\delta$ increases to 2. \nThis pattern is the opposite of what we see for the raw effect size $d$, which has standard error that increases to 124% of its null value under the same conditions.\nBetween these two, the transformed $h(d)$ effect sizes have stable SEs across the range of $\\delta$.\nThus, $h(d)$ is variance stabilizing but $z(d)$ is not.\n\n## Dichotomized bivariate normal simulations\n\nIn @Pustejovsky2014converting, I argued that the appropriateness of $d$-to-$r$-to-$z$ transformations and the specific form of transformation to use both depend on features of the study's design. \nThe two-sample normal model would be relevant for experimental studies, and it seems that the $z(d)$ transformation is not variance-stabilizing there.\nBut perhaps it would work for other study designs and data-generating processes?\nIn @Pustejovsky2014converting, I also looked at extreme group designs and \"dichotomization\" designs, where a researcher is interested in the correlation between two continuous variables, one of which has been dichotomized based on quantiles of its distribution (either sample quantiles or population quantiles). \n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Generate standardized mean differences d, z(d), h(d)\n\nr_bivariate <- function(\n  reps, delta = 0, N = 20, R = 1, fixed = TRUE\n) {\n  require(mvtnorm)\n  rho <- delta / sqrt(delta^2 + (R + 1)^2 / R)\n  Sigma <- rho + diag(1 - rho, nrow = 2L)\n  d <- replicate(reps, {\n    Z <- rmvnorm(n = N, mean = c(0,0), sigma = Sigma)\n    \n    if (fixed) {\n      X <- ifelse(Z[,1] <= qnorm(R / (R + 1)), \"A\",\"B\")\n      Xtb <- table(X)\n      if (any(Xtb == 0)) return(NA_real_)\n    } else {\n      X <- ifelse(rank(Z[,1]) <= N * R / (R + 1), \"A\",\"B\")\n      Xtb <- table(X)\n      R <- Xtb[1] / Xtb[2]\n    }\n    \n    M <- tapply(Z[,2], X, mean)\n    V <- tapply(Z[,2], X, var)\n    Vpool <- sum((Xtb - 1) * V) / (N - 2)\n    as.numeric(diff(M)) / sqrt(Vpool)\n  }, simplify = FALSE) |>\n    unlist()\n  z <- d_to_z(d, R = R)\n  h <- d_to_h(d, R = R)\n  data.frame(d = d, z = z, h = h)\n}\n\n# Simulation driver\nsim_bivariate <- compose(calc_M_SE, r_bivariate)\n\n# Execute simulations\nd_bivariate <- \n  sim_grid |>\n  mutate(reps = 1e4) |> \n  cross_join(tibble(fixed = c(TRUE, FALSE))) |> \n  evaluate_by_row(\n    sim_bivariate,\n    # system_time = FALSE,\n    verbose = FALSE\n  ) |>\n  mutate(\n    R_fac = factor(R) |> fct_relabel(\\(x) paste0(\"R == \", x, \":1\")),\n    N_fac = factor(N) |> fct_relabel(\\(x) paste(\"N ==\", x)),\n    SE_scaled = if_else(stat == \"d\", SE * sqrt((N - 2) * R / (R + 1)^2), sqrt(N - 2) * SE)\n  )\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   user  system elapsed \n1236.23   33.87 2661.12 \n```\n\n\n:::\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nd_bivariate |>\n  filter(fixed) |>\nggplot() + \n  aes(delta, SE_scaled, color = stat) + \n  geom_hline(yintercept = 0) + \n  geom_line() + \n  facet_grid(N_fac ~ R_fac, scales = \"free_y\", labeller = \"label_parsed\") + \n  theme_minimal() +\n  labs(\n    x = expression(delta), \n    y = \"True Standard Error (rescaled)\", \n    color = \"\"\n  )\n```\n\n::: {.cell-output-display}\n![True standard errors of untransformed $d$ and transformed ($z(d)$ and $h(d)$) effect size estimates under a dichotomized bivariate normal model with a fixed threshold. Standard errors are rescaled so that SE = 1 when $\\delta$ = 0.](index_files/figure-html/fig-true-SE-bivariate-fixed-1.png){#fig-true-SE-bivariate-fixed width=768}\n:::\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nd_bivariate |>\n  filter(!fixed) |>\nggplot() + \n  aes(delta, SE_scaled, color = stat) + \n  geom_hline(yintercept = 0) + \n  geom_line() + \n  facet_grid(N_fac ~ R_fac, scales = \"free_y\", labeller = \"label_parsed\") + \n  theme_minimal() +\n  labs(\n    x = expression(delta), \n    y = \"True Standard Error (rescaled)\", \n    color = \"\"\n  )\n```\n\n::: {.cell-output-display}\n![True standard errors of untransformed $d$ and transformed ($z(d)$ and $h(d)$) effect size estimates under a dichotomized bivariate normal model with a sample threshold. Standard errors are rescaled so that SE = 1 when $\\delta$ = 0.](index_files/figure-html/fig-true-SE-bivariate-sampled-1.png){#fig-true-SE-bivariate-sampled width=768}\n:::\n:::\n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}