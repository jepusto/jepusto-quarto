{
  "hash": "f97927ab0041b990366d52a4e21b15e4",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: A handmade clubSandwich for multi-site trials\ndate: '2019-03-09'\ncategories:\n- sandwiches\n- robust variance estimation\n- econometrics\n- weighting\ncode-tools: true\n---\n\n\nI'm just back from the [Society for Research on Educational Effectiveness](https://www.sree.org/assets/conferences/2019s/program.pdf) meetings, where I presented work on small-sample corrections for cluster-robust variance estimators in two-stage least squares models, which I've implemented in the [`clubSandwich`](/software/clubSandwich/) R package. [Here's my presentation](/files/SREE-2019-2SLS-CRVE.pdf). So I had \"clubSandwich\" estimators on the brain when a colleague asked me about whether the methods were implemented in SAS. \n\nThe short answer is \"no.\" \n\nThe moderately longer answer is \"not unless we can find funding to pay someone who knows how to program properly in SAS.\" However, for the specific model that my colleague was interested in, it turns out that the small-sample corrections implemented in clubSandwich can be expressed in closed form, and they're simple enough that they could easily be hand-calculated. I'll sketch out the calculations in the remainder of this post. \n\n## A multi-site trial\n\nConsider a multi-site trial conducted across $J$ sites, which we take as a sample from a larger super-population of sites. Each site consists of $n_j$ units, of which $p_j n_j$ are randomized to treatment and the remainder $(1 - p_j) n_j$ are randomized to control. For each unit $i$ in each site $j$, we have an outcome $y_{ij}$ and a treatment indicator $t_{ij}$. \n\nA conventional approach to estimating the overall average impact in this setting is to use a model with a treatment indicator and fixed effects for each site:\n$$\ny_{ij} = \\beta_j + \\delta t_{ij} + e_{ij}\n$$\nand then to cluster the standard errors by site. Clustering by site makes sense here if (and only if) we're interested in generalizing to the super-population of sites. \n\nLet $\\hat\\delta_j$ denote the impact estimate from site $j$, calculated as the difference in means between treated and untreated units at site $j$:\n$$\n\\hat\\delta_j = \\frac{1}{n_j p_j} \\left(\\sum_{i=1}^{n_j} t_{ij} y_{ij}\\right) - \\frac{1}{n_j (1 - p_j)} \\left(\\sum_{i=1}^{n_j} (1 - t_{ij}) y_{ij}\\right).\n$$\nfor $j = 1,..,J$. The overall impact estimate here is a precision-weighted average of the site-specific impacts:\n$$\n\\hat\\delta = \\frac{1}{W} \\sum_{j=1}^J w_j \\hat\\delta_j,\n$$\nwhere $w_j = n_j p_j (1 - p_j)$ and $W = \\sum_j w_j$.\n\n## Sandwich estimators\n\nThe conventional clustered variance estimator (or sandwich estimator) for $\\hat\\delta$ is a simple function of the (weighted) sample variance of the site-specific effects. It can be calculated directly as:\n$$\nV^{CR0} = \\frac{1}{W^2} \\sum_{j=1}^J w_j^2 \\left(\\hat\\delta_j - \\hat\\delta\\right)^2.\n$$\nUnder a conventional random effects model for the $\\delta_j$s, this estimator has a downward bias in finite samples.\n\nThe clubSandwich variance estimator here uses an estimator for the sample variance of site-specific effects that is unbiased under a certain working model. It is only slightly more complicated to calculate:\n$$\nV^{CR2} = \\frac{1}{W^2} \\sum_{j=1}^J \\frac{w_j^2 \\left(\\hat\\delta_j - \\hat\\delta\\right)^2}{1 - w_j / W}.\n$$\n\nThe other difference between conventional methods and the clubSandwich approach is in the reference distribution used to calculate hypothesis tests and confidence intervals. The conventional approach uses a standard normal reference distribution (i.e., a z-test) that is asymptotically justified. The clubSandwich approach uses a $t$ reference distribution, with degrees of freedom estimated using a Satterthwaite approximation. In the present context, the degrees of freedom are a little bit ugly but still not hard to calculate:\n$$\ndf = \\left[\\sum_{j=1}^J \\frac{w_j^2}{(W - w_j)^2} - \\frac{2}{W}\\sum_{j=1}^J \\frac{w_j^3}{(W - w_j)^2} + \\frac{1}{W^2} \\left(\\sum_{j=1}^J \\frac{w_j^2}{W - w_j} \\right)^2 \\right]^{-1}.\n$$\n\nIn the special case that all sites are of the same size and use a constant treatment allocation, the weights become equal. The clubSandwich variance estimator then reduces to \n$$\nV^{CR2} = \\frac{S_\\delta^2}{J} \\qquad \\text{where} \\qquad S_\\delta^2 = \\frac{1}{J - 1}\\sum_{j=1}^J \\left(\\hat\\delta_j - \\hat\\delta\\right)^2,\n$$\nand the degrees of freedom reduce to simply $df = J - 1$.\n\n## Tennessee STAR\n\nHere is a worked example of the calculations (using R of course, because my SAS programming skills atrophied years ago). I'll use data from the famous Tennessee STAR class size experiment, which was a multi-site trial in which students were randomized to small or regular-sized kindergarten classes within each of several dozen schools. To make the small-sample issues more pronounced, I'll limit the sample to urban schools and look at impacts of small class-size on reading and math scores at the end of kindergarten. STAR was actually a three-arm trial---the third arm being a regular-sized class but with an additional teacher aide. For simplicity (and following convention), I'll collapse the teacher-aide condition and the regular-sized class condition into a single arm and also limit the sample to students with complete outcome data on both tests. \n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\ndata(STAR, package = \"AER\")\n\nSTAR_urban <-\n  STAR %>%\n  filter(\n    # limit to urban/inner city schools\n    schoolk %in% c(\"urban\",\"inner-city\"),\n    # limit to complete outcome data\n    !is.na(readk), !is.na(mathk)\n  ) %>%\n  droplevels() %>%\n  # collapse control conditions\n  mutate(stark = fct_collapse(stark, regular = c(\"regular\",\"regular+aide\"))) %>%\n  select(schoolidk, stark, readk, mathk)\n\nSTAR_summary <- \n  STAR_urban %>%\n  count(schoolidk)\n```\n:::\n\nAfter these exclusions, the data include a total of 1810 students from 23 schools, ranging in size from 34 to 134 students.\n\nFor starters, let's get the average impacts using a seeming unrelated regression specification, with both conventional and clubSandwich standard errors. \n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(clubSandwich)\nSTAR_fit <- lm(cbind(readk, mathk) ~ 0 + schoolidk + stark, data = STAR_urban)\n\n# conventional SEs\nCR0 <- \n  coef_test(STAR_fit, vcov = \"CR0\", \n            cluster = STAR_urban$schoolidk, \n            test = \"z\",\n            coefs = c(\"readk:starksmall\",\"mathk:starksmall\"))\n\nCR0\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n            Coef. Estimate   SE t-stat d.f. (z) p-val (z) Sig.\n readk:starksmall     6.16 2.73   2.25      Inf    0.0241    *\n mathk:starksmall    12.13 4.79   2.53      Inf    0.0113    *\n```\n\n\n:::\n\n```{.r .cell-code}\n# clubSandwich SEs\nCR2 <- \n  coef_test(STAR_fit, vcov = \"CR2\", \n            cluster = STAR_urban$schoolidk, \n            coefs = c(\"readk:starksmall\",\"mathk:starksmall\"))\n\nCR2\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n            Coef. Estimate   SE t-stat d.f. (Satt) p-val (Satt) Sig.\n readk:starksmall     6.16 2.81   2.19          19       0.0409    *\n mathk:starksmall    12.13 4.92   2.47          19       0.0234    *\n```\n\n\n:::\n:::\n\n\nNow I'll do it \"by hand\"---or rather, with a bit of `dplyr`:\n\n::: {.cell}\n\n```{.r .cell-code}\n# summary statistics by site\n\nschool_summaries <- \n  STAR_urban %>%\n  group_by(schoolidk, stark) %>%\n  summarise(\n    # means by arm and site\n    readk = mean(readk),\n    mathk = mean(mathk),\n    n_arm = n()\n  ) %>%\n  summarise(\n    # impact estimates by site\n    readk = diff(readk),\n    mathk = diff(mathk),\n    n = sum(n_arm),\n    p = n_arm[stark==\"small\"] / n\n  ) %>%\n  mutate(w = n * p * (1 - p))\n\n# overall impacts\n\nschool_summaries %>%\n  gather(\"subject\",\"impact_j\", readk, mathk) %>%\n  group_by(subject) %>%\n  summarise(\n    impact = weighted.mean(impact_j, w = w),\n    SE_CR0 = sqrt(sum(w^2 * (impact_j - impact)^2) / sum(w)^2),\n    SE_CR2 = sqrt(sum(w^2 * (impact_j - impact)^2 / (1 - w / sum(w))) / sum(w)^2),\n    df_CR2 = 1 / (sum(w^2 / (sum(w) - w)^2) - \n                    2 * sum(w^3 / (sum(w) - w)^2) / sum(w) + \n                    sum(w^2 / (sum(w) - w))^2 / sum(w)^2)\n  ) %>%\n  knitr::kable(digits = 2)\n```\n\n::: {.cell-output-display}\n\n\n|subject | impact| SE_CR0| SE_CR2| df_CR2|\n|:-------|------:|------:|------:|------:|\n|mathk   |  12.13|   4.79|   4.92|  18.99|\n|readk   |   6.16|   2.73|   2.81|  18.99|\n\n\n:::\n:::\n\n\nThe CR0 and CR2 standard errors match the results from `coef_test`, as do the Satterthwaite degrees of freedom. Note that the degrees of freedom are equal to 19 in this case, a bit less than $J - 1 = 22$ due to variation in the weight assigned to each school. \n\n## Other weights\n\nSome analysts might not like the approach of using precision-weighted average of the site-specific impacts, as I've examined here. Instead, one might choose to weight the site-specific effects by the site-specific sample sizes, or to use some sort of random effects weighting that allows for random heterogeneity across sites. The formulas given above for conventional and clubSandwich clustered variance estimators apply directly to other weighting schemes too. Just substitute your favorite weights in place of $w_j$. When doing so, the clubSandwich estimator will be exactly unbiased under the assumption that your preferred weighting scheme corresponds to inverse-variance weighting, and the Satterthwaite degrees of freedom approximation will be derived under the same model. \n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}