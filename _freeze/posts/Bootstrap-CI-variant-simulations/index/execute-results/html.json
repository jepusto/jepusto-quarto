{
  "hash": "506085468dd8c2ee3a290ba7b17da8ef",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: A bigger simulation of bootstrap confidence interval variations\ndate: '2025-01-18'\ncategories:\n- programming\n- Rstats\n- bootstrap\ncode-fold: show\ncode-tools: true\ntoc: true\nbibliography: \"references.bib\"\ncsl: \"../apa.csl\"\n---\n\n\n\nIn my [previous post](/posts/Bootstrap-CI-variations/), I demonstrated some new brand-new additions to the [`simhelpers` package](https://meghapsimatrix.github.io/simhelpers/): utilities for calculating bootstrap confidence intervals. That post walked through validating the functions against the results of other packages and showing how they can be used to extrapolate confidence interval coverage rates using fewer bootstraps than one would want to see for a real data analysis.\nIn the course of illustrating the functions, I set up a small simulation study (with just one set of parameters) to show how the extrapolation technique works.\nIn this post, I'm going to expand these simulations across a bigger set of conditions in a multi-factor simulation.\nThis exercise is mostly an excuse to showcase some of the other useful features of `simhelpers`---especially the `bundle_sim()` function for creating simulation drivers and the `evaluate_by_row()` function for executing simulations across a grid of parameter values. \n\nThe goal of the simulation is to evaluate how the bootstrap CIs work for estimating the Pearson correlation from a bivariate $t$ data-generating process.\nI will compare the performance of four different confidence intervals for the correlation coefficient: 1) the Fisher-z interval, which is derived assuming bivariate normality of the measurements; 2) a percentile bootstrap CI; 3) a studentized bootstrap CI; and 4) Efron's bias-corrected-and-accelerated bootstrap CI.\nFor the bootstrap intervals, I will use my implementation of the @boos2000MonteCarloEvaluation extrapolation technique (which [I described in much greater detail here](/posts/Simulating-bootstrap-CIs/)).\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\nlibrary(simhelpers)\n```\n:::\n\n\n\n# Data-generating process\n\nAs a more-or-less arbitrary data-generating process, I'll look at a bivariate central $t$ distribution with scale matrix $\\boldsymbol\\Sigma = \\left[\\begin{array}{cc} 1 \\ \\rho \\\\ \\rho \\ 1 \\end{array}\\right]$ and degrees of freedom $\\nu$:\n$$\n\\left(\\begin{array}{c} X_1 \\\\ X_2 \\end{array}\\right) \\sim t_{MV} \\left( \\left[\\begin{array}{c} 0 \\\\ 0 \\end{array}\\right], \\left[\\begin{array}{cc} 1 \\ \\rho \\\\ \\rho \\ 1 \\end{array}\\right], \\nu  \\right).\n$$\nThe scale matrix of the multivariate $t$ is not exactly the same as its covariance, but $\\rho$ nonetheless corresponds to the correlation between $X_1$ and $X_2$. For purposes of simulating data, I've fixed the means to zero and the diagonals of the scale matrix to 1 because they should not affect the distribution of the sample correlation. \n\nHere's a data-generating function that returns randomly generated samples based on the bivariate $t$ model:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nr_t_bi <- function(n, rho = 0, df = 8) {\n  Sigma <- rho + diag(1 - rho, nrow = 2)\n  mvtnorm::rmvt(n = n, sigma = Sigma, df = df)\n}\n\ndat <- r_t_bi(6, rho = 0.8, df = 10)\ndat\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n           [,1]       [,2]\n[1,] -1.3127111 -1.9011068\n[2,] -0.5437800 -0.3590287\n[3,] -0.9715497 -0.4678720\n[4,] -0.8036683 -1.4880140\n[5,] -0.4911120 -0.2064628\n[6,] -0.5032190 -0.7707798\n```\n\n\n:::\n:::\n\n\n\n# Estimation methods \n\nFollowing along the same lines as my previous post, I will look at several different confidence intervals. First, I'll use an interval based on Fisher's z-transformation, which is both normalizing and variance-stabilizing (so that the standard error of the point estimator does not depend on the parameter) when the data come from a bivariate normal distribution. Let $r$ denote the usual sample correlation based on a sample of $n$ observations and let $z(r) = \\frac{1}{2}\\ln\\left(\\frac{1 + r}{1 - r}\\right) = \\text{atan}^{-1}(r)$ be the Fisher transformation function, with inverse $r(z) = \\text{atan}(z) = \\frac{e^{2z} - 1}{e^{2z} + 1}$. The $1 - 2\\alpha$-level Fisher-z interval is given by \n$$\n\\left[\\text{atan}\\left(z(r) - \\frac{\\Phi^{-1}(1 - \\alpha)}{\\sqrt{n - 3}}\\right), \\ \\text{atan}\\left(z(r) + \\frac{\\Phi^{-1}(1 - \\alpha)}{\\sqrt{n - 3}}\\right) \\right].\n$$\nThis interval is based on a bivariate normality assumption that isn't actually consistent with the data-generating process, so I would not expect it to have the correct coverage unless the degrees of freedom are large enough that the bivariate $t$ approaches the bivariate normal. \n\nIn addition to this interval, I'll look at several different bootstrap intervals, including the usual percentile interval, a studentized interval, and a bias-corrected-and-accelerated interval. \nThe percentile interval is approximate so will not necessarily have exactly nominal coverage, but its coverage rate should approach the nominal level as the sample size gets bigger.\nThe BCa interval should have second-order accurate coverage [@efron1987better] and so one would expect its coverage rate to approach the nominal level more quickly than the percentile interval.\nWith all these intervals, I'll use the the @boos2000MonteCarloEvaluation extrapolation technique to estimate the coverage level, for which I'll need to calculate and keep track of intervals based on $B = 49, 99, 199, 299$, and $399$ bootstrap replicates.\n\nThe BCa interval involves making small tweaks to the percentile interval based on an estimate of bias and an estimate of an acceleration constant that is a function of the empirical influence values.\nI'm going to keep track of the bias correction estimate and the acceleration correction estimates, so that I can understand how noisy they are. \n\nHere's a function that calculates the point estimator $r$, the Fisher-z interval, the empirical influence values, and then the bootstrap CIs. \n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncor_CI <- function(\n  dat, \n  CI_type = c(\"percentile\",\"student\",\"BCa\"),\n  B_vals = c(49, 99, 199, 299, 399)\n) {\n  \n  # point estimate\n  r_est <- cor(dat[,1], dat[,2])\n  N <- nrow(dat)\n  SE_r <- sqrt((1 - r_est^2)^2 / (N - 1))\n  \n  # Fisher z CI\n  z <- atanh(r_est)\n  SE_z <- 1 / sqrt(N - 3)\n  CI_z <- z + c(-1, 1) * qnorm(0.975) * SE_z\n\n  \n  # empirical influence if needed\n  if (\"BCa\" %in% CI_type) {\n    jacks <- sapply(1:N, \\(x) cor(dat[-x,1], dat[-x,2]))\n    inf_vals <- r_est - jacks\n  } else {\n    inf_vals <- NULL\n  }\n  \n  # bootstrap samples\n  r_boot <- replicate(max(B_vals), {\n    i <- sample(1:N, replace = TRUE, size = N)\n    r <- cor(dat[i,1], dat[i,2])\n    c(r, sqrt((1 - r^2)^2 / (N - 1)))\n  })\n  \n  bs_CIs <- simhelpers::bootstrap_CIs(\n    boot_est = r_boot[1,],\n    boot_se = r_boot[2,],\n    est = r_est,\n    se = SE_r,\n    influence = inf_vals,\n    CI_type = CI_type,\n    B_vals = B_vals,\n    format = \"wide-list\"\n  )\n\n  tibble::tibble(\n    r = r_est,\n    SE = SE_r,\n    lo = tanh(CI_z[1]),\n    hi = tanh(CI_z[2]),\n    w = qnorm(mean(r_boot < r_est)),\n    a = sum(inf_vals^3) / (6 * sum(inf_vals^2)^1.5),\n    bs_CIs = bs_CIs\n  )\n}\n\nres <- cor_CI(dat)\nres\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 × 7\n      r    SE     lo    hi     w     a bs_CIs      \n  <dbl> <dbl>  <dbl> <dbl> <dbl> <dbl> <btstr_CI>  \n1 0.741 0.202 -0.177 0.970 0.581 0.111 <df [5 × 7]>\n```\n\n\n:::\n:::\n\n\nThe `bundle_sim` function is a convenient way to combine the data-generating function and the estimation function into one. \nIt wraps the composition of both functions in a call to `replicate()`, so that I can repeat the steps of generating data and analyzing the data to my heart's content: \n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsim_cor <- bundle_sim(\n  f_generate = r_t_bi, \n  f_analyze = cor_CI\n)\n```\n:::\n\n\n\nThe result is one function, `sim_cor()`, with the all of the arguments from the data-generating function and analysis function combined and with an additional argument (called `reps`) to specify the number of times to repeat the whole thing:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nargs(sim_cor)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nfunction (reps, n, rho = 0, df = 8, CI_type = c(\"percentile\", \n    \"student\", \"BCa\"), B_vals = c(49, 99, 199, 299, 399), seed = NA_integer_) \nNULL\n```\n\n\n:::\n:::\n\n\nTo illustrate how it works, I can call `sim_cor()` to generate five replications of the generate-and-analyze steps:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfive_reps <- sim_cor(5, n = 20, rho = 0.8, df = 8)\nfive_reps\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 5 × 7\n      r    SE    lo    hi     w        a bs_CIs      \n  <dbl> <dbl> <dbl> <dbl> <dbl>    <dbl> <btstr_CI>  \n1 0.543 0.162 0.133 0.795 0.680 -0.00590 <df [5 × 7]>\n2 0.735 0.106 0.433 0.888 0.529  0.0188  <df [5 × 7]>\n3 0.623 0.140 0.250 0.835 0.676  0.0284  <df [5 × 7]>\n4 0.672 0.126 0.326 0.859 0.688  0.0675  <df [5 × 7]>\n5 0.723 0.110 0.412 0.883 0.615  0.0284  <df [5 × 7]>\n```\n\n\n:::\n:::\n\n\n\n\n# Performance measures\n\nAfter generating many sets of results, I'll need to summarize across replications to evaluate the coverage rates of each set of confidence intervals. In my previous post, I did this with a bit of `dplyr` code. \nBecause I'm going to run the simulation across multiple conditions with different parameter values, it will be helpful to have a function so that I can re-use it later. \nHere's such a function, which has basically the same content as the code chunk from my previous post:\n\n\n::: {.cell}\n\n```{.r .cell-code}\neval_performance <- function(dat, rho = 0, B_target = 1999) {\n  require(simhelpers, quietly = TRUE)\n  \n  dat |>\n    dplyr::summarize(\n      calc_absolute(estimates = r, true_param = rho, criteria = \"bias\"),\n      calc_coverage(lower_bound = lo, upper_bound = hi, true_param = rho),\n      extrapolate_coverage(\n        CI_subsamples = bs_CIs, true_param = rho, \n        B_target = B_target,\n        nested = TRUE, format = \"long\"\n      ),\n      across(c(w, a), list(M = ~ mean(.x), SD = ~ sd(.x)))\n    )\n}\n\neval_performance(five_reps, rho = 0.8)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 × 18\n  K_absolute   bias bias_mcse K_coverage coverage coverage_mcse width width_mcse\n       <int>  <dbl>     <dbl>      <int>    <dbl>         <dbl> <dbl>      <dbl>\n1          5 -0.141    0.0351          5      0.8         0.179 0.541     0.0380\n# ℹ 10 more variables: K_boot_coverage <int>, bootstraps <list>,\n#   boot_coverage <list>, boot_coverage_mcse <list>, boot_width <list>,\n#   boot_width_mcse <list>, w_M <dbl>, w_SD <dbl>, a_M <dbl>, a_SD <dbl>\n```\n\n\n:::\n:::\n\n\nThe function that I created earlier with `bundle_sim()` can be further enhanced by wrapping in `eval_performance()`.\nI'll just revise my earlier `sim_cor()` to add it:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsim_cor <- bundle_sim(\n  f_generate = r_t_bi, \n  f_analyze = cor_CI,\n  f_summarize = eval_performance\n)\n```\n:::\n\n\nThe result is a \"simulation driver\" function, which takes model parameters as inputs, runs a full simulation, and returns a dataset with performance measures:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsim_cor(\n  reps = 10,\n  n = 25, \n  rho = 0.75, \n  df = 8,\n  CI_type = \"BCa\"\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 × 18\n  K_absolute     bias bias_mcse K_coverage coverage coverage_mcse width\n       <int>    <dbl>     <dbl>      <int>    <dbl>         <dbl> <dbl>\n1         10 -0.00667    0.0364         10      0.9        0.0949 0.374\n# ℹ 11 more variables: width_mcse <dbl>, K_boot_coverage <int>,\n#   bootstraps <list>, boot_coverage <list>, boot_coverage_mcse <list>,\n#   boot_width <list>, boot_width_mcse <list>, w_M <dbl>, w_SD <dbl>,\n#   a_M <dbl>, a_SD <dbl>\n```\n\n\n:::\n:::\n\n\nTo examine the raw results without running the performance summaries, just set the `summarize` argument to `FALSE`:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsim_cor(\n  reps = 10,\n  n = 25, \n  rho = 0.75, \n  df = 8,\n  CI_type = \"BCa\",\n  summarize = FALSE\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 10 × 7\n       r     SE    lo    hi     w        a bs_CIs      \n   <dbl>  <dbl> <dbl> <dbl> <dbl>    <dbl> <btstr_CI>  \n 1 0.780 0.0800 0.556 0.898 0.645  0.0129  <df [5 × 3]>\n 2 0.526 0.148  0.165 0.763 0.645 -0.0389  <df [5 × 3]>\n 3 0.762 0.0855 0.525 0.889 0.661  0.0626  <df [5 × 3]>\n 4 0.825 0.0652 0.638 0.920 0.665  0.0120  <df [5 × 3]>\n 5 0.925 0.0294 0.836 0.967 0.716  0.105   <df [5 × 3]>\n 6 0.775 0.0816 0.547 0.896 0.573 -0.0524  <df [5 × 3]>\n 7 0.769 0.0835 0.537 0.893 0.676 -0.0401  <df [5 × 3]>\n 8 0.878 0.0467 0.740 0.945 0.669  0.0384  <df [5 × 3]>\n 9 0.831 0.0631 0.650 0.923 0.634  0.00181 <df [5 × 3]>\n10 0.738 0.0930 0.483 0.877 0.661 -0.0162  <df [5 × 3]>\n```\n\n\n:::\n:::\n\n\n\n# Parameters to examine\n\nNow that I've got a simulation driver, I'm in position to run simulations across a wider set of conditions. \nFor multifactor simulations, I like to instantiate the full set of conditions in a dataset, which I then process to actually do the computation. \nFor these simulations, the model parameters are the true correlation, $\\rho$, and the degrees of freedom, $\\nu$;\nthe only design parameter is $n$, the sample size. \nFor $\\rho$, I'll look at values between 0.0 and 0.8 just to cover most of the range of possibilities. \n(I don't need to look at negative correlations because the statistics involved here are all symmetric with respect to the sign of $\\rho$.)\nFor degrees of freedom, I'll look at a fairly small value of $\\nu = 8$, an intermediate value of $\\nu = 16$, and a larger value of $\\nu = 32$, the last of which should be fairly close to a bivariate normal distribution.\nFor sake of complete overkill, I guess I'll also throw in $\\nu =48$, which should be almost identical to a bivariate normal distribution. \nIt's useful to evaluate such a condition because the the Fisher z interval is known to be exact under bivariate normality, so we can validate the simulation set-up by verifying that the coverage levels are consistent with theory here.\nFor the choice of sample sizes to examine, I'm interested in understanding how the coverage of the bootstrap intervals changes as sample size increases, so I will look at a fairly fine grid of values between a very small sample size of $n = 10$ and a more moderate sample size of $n = 100$.\nHere's how I create a dataset of simulation conditions:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nparams <- expand_grid(\n  n = seq(10,100,10),\n  rho = seq(0.0,0.8,0.2),\n  df = c(8,16,32,48)\n)\n\nparams\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 200 × 3\n       n   rho    df\n   <dbl> <dbl> <dbl>\n 1    10   0       8\n 2    10   0      16\n 3    10   0      32\n 4    10   0      48\n 5    10   0.2     8\n 6    10   0.2    16\n 7    10   0.2    32\n 8    10   0.2    48\n 9    10   0.4     8\n10    10   0.4    16\n# ℹ 190 more rows\n```\n\n\n:::\n:::\n\n\n\n# Execute \n\nWith all these components in place, it's time to start computing.\nThe task here is to run the `sim_cor()` simulation driver function on the set of parameters in each row of `params`. In the tidyverse idiom, one would normally do this with `purrr::pmap()` or one of its variants. \nThe `simhelpers` package includes a function `evaluate_by_row()` that accomplishes the same thing.\nIt's more or less a wrapper to `pmap()`, with a few little extra touches.\nInstead of `purrr::pmap()`, it uses `furrr::future_pmap()` to enable parallel computing via `future` the package.\nIt also keeps track of total compute time and does a little bit of tidying up the simulation results after running everything.\nHere is how I execute the simulation with `evaluate_by_row()`:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# enable parallel computing\nlibrary(future)\nlibrary(furrr)\nplan(multisession, workers = 8L)\n\nset.seed(20250117)\n\nres <- evaluate_by_row(\n  params = params,\n  sim_function = sim_cor,\n  reps = 4000,\n  CI_type = c(\"percentile\",\"student\",\"BCa\"),\n  B_vals = c(49,99,199,299,399),\n  B_target = 1999,\n  .options = furrr_options(seed = TRUE),\n  .progress = TRUE\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   user  system elapsed \n   4.13    0.57 4869.19 \n```\n\n\n:::\n:::\n\n\n\n# Results\n\nAfter a little further data-cleaning on the back end, we can see how the coverage rates of different intervals compare to each other across the full range of parameter values and sample sizes. \nHere's a graph of the simulated coverage rates as a function of $n$:\n\n\n\n::: {.cell .column-body-outset lightbox='true'}\n\n```{.r .cell-code  code-fold=\"true\"}\nFisher_res <- \n  res %>%\n  select(n, rho, df, coverage, width) %>%\n  mutate(CI_type = \"Fisher-z\")\n\nboot_res <- \n  res %>%\n  select(n, rho, df, bootstraps, coverage = boot_coverage, width = boot_width) %>%\n  unnest(c(bootstraps, coverage, width)) %>%\n  filter(bootstraps == 1999) %>%\n  select(-bootstraps)\n\nCI_res <- \n  bind_rows(Fisher_res, boot_res) %>%\n  mutate(\n    rho_lab = paste(\"rho ==\", rho),\n    df_lab = factor(df, levels = c(8,16,32,48), labels = paste(\"nu ==\", c(8,16,32,48))),\n    CI_type = factor(CI_type, levels = c(\"Fisher-z\",\"percentile\",\"student\",\"BCa\"))\n  )\n\nggplot(CI_res) + \n  aes(n, coverage, color = CI_type) + \n  facet_grid(df_lab ~ rho_lab, labeller = \"label_parsed\") + \n  scale_x_continuous(breaks = seq(20,100,20)) + \n  scale_y_continuous(limits = c(0.87,1.0), breaks = seq(0.9,1.0,0.05), expand = expansion(0, 0)) + \n  geom_hline(yintercept = 0.95, linetype = \"dashed\") + \n  geom_point() + geom_line() + \n  theme_minimal() + \n  theme(legend.position = \"top\") + \n  labs(x = \"n\", y = \"Coverage rate\", color = \"\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/coverage-graph-1.png){width=100%}\n:::\n:::\n\n\n\nA few observations here:\n\n* For small degrees of freedom, the Fisher-z interval has below-nominal coverage and appears to degrade further as sample size gets better. This makes sense because the interval is derived under a bivariate normal model that is not consistent with the actual data-generating process unless the degrees of freedom are large. With larger degrees of freedom $(\\nu = 32)$, the Fisher-z has almost exactly nominal coverage.\n* Across degrees of freedom and true correlations, the percentile interval has below-nominal coverage for the smaller sample sizes but appears to get steadily better as sample size increases. \n* The studentized interval performs remarkably well, with coverage very close to nominal even at very small sample sizes.\n* Rather curiously, the BCa interval pretty consistently has worse coverage than the (simpler) percentile interval, even for larger sample sizes. This is counter-intuitive to me because the BCa interval is supposed to have second-order accurate coverage [@efron1987better] whereas the percentile interval is only first-order accurate. \nThe upshot of this is that one would expect the coverage rate of the BCa interval to improve more quickly than the percentile interval, but that doesn't seem to be the case here. \n\nFor sake of completeness, here is a graph of the average width of each type of CI: \n\n\n\n::: {.cell .column-body-outset lightbox='true'}\n\n```{.r .cell-code  code-fold=\"true\"}\nggplot(CI_res) + \n  aes(n, width, color = CI_type) + \n  facet_grid(df_lab ~ rho_lab, labeller = \"label_parsed\") + \n  scale_x_continuous(breaks = seq(20,100,20)) + \n  scale_y_continuous(breaks = seq(0,0.8,0.2), expand = expansion(0, 0)) + \n  expand_limits(y = 0) + \n  coord_cartesian(ylim = c(0,1)) + \n  geom_point() + geom_line() + \n  theme_minimal() + \n  theme(legend.position = \"top\") + \n  labs(x = \"n\", y = \"Average interval width\", color = \"\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/width-graph-1.png){width=100%}\n:::\n:::\n\n\nAt the smallest sample sizes, all of the intervals are quite wide and the studentized interval is extremely so (I've truncated the graph at a width of 1.0, so the studentized interval goes off the scale when $n = 10$). The other intervals are all pretty similar in width, and nearly identically so when the degrees of freedom get large enough.\n\n# Thoughts\n\nI think the most intriguing thing about these simulation results is the BCa interval seems to generally be _worse_ than a regular percentile interval---not the refinement that one might expect. \nI'm not sure why that is.\nIt could have something to do with how the acceleration constant is calculated. \nI've computed it with a jackknife approximation, but there are alternatives (such as using an infinitesimal jackknife, or just direct calculation if one is willing to impose a distributional assumption) which might work better.\nThere are also some other interval construction methods that I haven't bothered to look at here.\nFor instance, @hu2020Interval proposed two intervals based on non-parametric empirical likelihood methods. \nThey reported their own set of simulations to evaluate their proposed intervals against the Fisher-z interval, but they didn't look at bootstrap-based intervals.\nTheir work could be the basis for a range of extensions to the simulation reported here, such as \n\n* Adding their empirical likelihood intervals to the set of estimators examined here---how do they do compared to nonparametric bootstraps?\n* Evaluating the Fisher-z and bootstrap intervals under the non-normal data-generating processes that  @hu2020Interval examined. Does the performance of the bootstrap intervals change in meaningful ways under a bivariate exponential distribution?\n\nThe main point of this post isn't so much to get into the weeds of interval estimators for the correlation coefficient. \nRather, it was to demonstrate an approach to programming simulations of bootstrap methods and showcase some of the tools available in `simhelpers` that can help with this process.\nThe schematic I've followed for programming the simulations is discussed in much greater detail in [my book with Luke Miratrix](https://jepusto.github.io/Designing-Simulations-in-R/) on writing simulations.\n\n\n\n\n\n\n\n::: {.callout-note icon=false appearance=\"simple\" title=\"Session Information\" collapse=true}\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"false\"}\nsessioninfo::session_info()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n─ Session info ───────────────────────────────────────────────────────────────\n setting  value\n version  R version 4.3.1 (2023-06-16 ucrt)\n os       Windows 10 x64 (build 19045)\n system   x86_64, mingw32\n ui       RTerm\n language (EN)\n collate  English_United States.utf8\n ctype    English_United States.utf8\n tz       America/Chicago\n date     2025-01-18\n pandoc   3.2 @ C:/Program Files/RStudio/resources/app/bin/quarto/bin/tools/ (via rmarkdown)\n\n─ Packages ───────────────────────────────────────────────────────────────────\n ! package     * version date (UTC) lib source\n P cli           3.6.3   2024-06-21 [?] RSPM (R 4.3.0)\n P colorspace    2.1-1   2024-07-26 [?] RSPM (R 4.3.0)\n P digest        0.6.37  2024-08-19 [?] RSPM (R 4.3.0)\n P dplyr       * 1.1.4   2023-11-17 [?] RSPM\n P evaluate      0.23    2023-11-01 [?] RSPM (R 4.3.0)\n P farver        2.1.2   2024-05-13 [?] RSPM\n P fastmap       1.1.1   2023-02-24 [?] RSPM\n P forcats     * 1.0.0   2023-01-29 [?] RSPM\n P generics      0.1.3   2022-07-05 [?] RSPM\n P ggplot2     * 3.5.1   2024-04-23 [?] RSPM (R 4.3.0)\n P glue          1.8.0   2024-09-30 [?] RSPM (R 4.3.0)\n P gtable        0.3.6   2024-10-25 [?] RSPM (R 4.3.0)\n P hms           1.1.3   2023-03-21 [?] RSPM\n P htmltools     0.5.7   2023-11-03 [?] RSPM (R 4.3.0)\n P htmlwidgets   1.6.4   2023-12-06 [?] RSPM\n P jsonlite      1.8.8   2023-12-04 [?] RSPM\n P knitr         1.47    2024-05-29 [?] RSPM\n P lifecycle     1.0.4   2023-11-07 [?] RSPM\n P lubridate   * 1.9.3   2023-09-27 [?] RSPM\n P magrittr      2.0.3   2022-03-30 [?] RSPM\n P munsell       0.5.1   2024-04-01 [?] RSPM\n P mvtnorm       1.3-2   2024-11-04 [?] RSPM (R 4.3.0)\n   pillar        1.10.1  2025-01-07 [1] RSPM (R 4.3.0)\n P pkgconfig     2.0.3   2019-09-22 [?] RSPM\n P purrr       * 1.0.2   2023-08-10 [?] RSPM\n P R6            2.5.1   2021-08-19 [?] RSPM\n P rbibutils     2.3     2024-10-04 [?] RSPM (R 4.3.0)\n   Rdpack        2.6.2   2024-11-15 [1] RSPM (R 4.3.0)\n P readr       * 2.1.5   2024-01-10 [?] RSPM\n   renv          1.0.5   2024-02-29 [1] RSPM (R 4.3.1)\n P rlang         1.1.4   2024-06-04 [?] RSPM\n P rmarkdown     2.27    2024-05-17 [?] RSPM\n P rstudioapi    0.17.1  2024-10-22 [?] RSPM (R 4.3.0)\n P scales        1.3.0   2023-11-28 [?] RSPM\n P sessioninfo   1.2.2   2021-12-06 [?] RSPM\n   simhelpers  * 0.3.1   2025-01-14 [1] Github (meghapsimatrix/simhelpers@3a7dda2)\n P stringi       1.8.4   2024-05-06 [?] RSPM (R 4.3.0)\n P stringr     * 1.5.1   2023-11-14 [?] RSPM\n P tibble      * 3.2.1   2023-03-20 [?] RSPM\n P tidyr       * 1.3.1   2024-01-24 [?] RSPM\n P tidyselect    1.2.1   2024-03-11 [?] RSPM\n P tidyverse   * 2.0.0   2023-02-22 [?] RSPM\n P timechange    0.3.0   2024-01-18 [?] RSPM\n P tzdb          0.4.0   2023-05-12 [?] RSPM\n P vctrs         0.6.5   2023-12-01 [?] RSPM\n P withr         3.0.2   2024-10-28 [?] RSPM (R 4.3.0)\n P xfun          0.45    2024-06-16 [?] RSPM\n P yaml          2.3.8   2023-12-11 [?] RSPM\n\n [1] C:/Users/James/Documents/jepusto-quarto/renv/library/R-4.3/x86_64-w64-mingw32\n [2] C:/Users/James/AppData/Local/R/cache/R/renv/sandbox/R-4.3/x86_64-w64-mingw32/bd3f13aa\n\n P ── Loaded and on-disk path mismatch.\n\n──────────────────────────────────────────────────────────────────────────────\n```\n\n\n:::\n:::\n\n\n:::\n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}