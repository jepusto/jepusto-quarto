{
  "hash": "e131a645cd89f630bf539004c2153049",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: Bootstrap confidence interval variations\ndate: '2025-01-15'\ncategories:\n- programming\n- Rstats\n- bootstrap\ncode-fold: show\ncode-tools: true\ndraft: true\ntoc: true\nbibliography: \"references.bib\"\ncsl: \"../apa.csl\"\n---\n\n\n\nIn my [previous post](/posts/Bootstrap-CI-variations/) I demonstrated some new utilities for calculating bootstrap confidence intervals that are brand-new additions to the [`simhelpers` package](https://meghapsimatrix.github.io/simhelpers/), validating that the functions are consistent with the results of other packages and walking through how they can be used to extrapolate confidence interval coverage rates using fewer bootstraps than one would want to see for a real data analysis.\nIn the course of illustrating the functions, I set up a small simulation study evaluating how the bootstrap CIs work for estimating the Pearson correlation from a bivariate t data-generating process.\nIn this post, I'm going to expand these simulations to a bigger set of conditions in a multi-factor simulation.\nI will compare the performance of four different confidence intervals for the correlation coefficient: 1) the Fisher-z interval, which is derived assuming bivariate normality of the measurements; 2) a percentile bootstrap CI; 3) a studentized bootstrap CI; and 4) Efron's bias-corrected-and-accelerated bootstrap CI.\nThis exercise is mostly an excuse to showcase some of the other useful features of `simhelpers`---especially the `bundle_sim()` function for creating simulation drivers and the `evaluate_by_row()` function for executing simulations across a grid of parameter values. \n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\nlibrary(simhelpers)\n```\n:::\n\n\n\n# Data-generating process\n\n\nHere's the data-generating function:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nr_t_bi <- function(n, rho = 0, df = 8) {\n  Sigma <- rho + diag(1 - rho, nrow = 2)\n  mvtnorm::rmvt(n = n, sigma = Sigma, df = df)\n}\n\nrho <- 0.75\ndat <- r_t_bi(25, rho = rho, df = 8)\n```\n:::\n\n\n\n# Estimation methods \n\nAnd the estimation function:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncor_CI <- function(\n  dat, \n  CI_type = \"percentile\", \n  B_vals = c(49, 99, 199, 299, 399)\n) {\n  \n  # point estimate\n  r_est <- cor(dat[,1], dat[,2])\n  N <- nrow(dat)\n  SE_r <- sqrt((1 - r_est^2)^2 / (N - 1))\n  \n  # Fisher z CI\n  z <- atanh(r_est)\n  SE_z <- 1 / sqrt(N - 3)\n  CI_z <- z + c(-1, 1) * qnorm(0.975) * SE_z\n\n  \n  # empirical influence if needed\n  if (\"BCa\" %in% CI_type) {\n    jacks <- sapply(1:N, \\(x) cor(dat[-x,1], dat[-x,2]))\n    inf_vals <- r_est - jacks\n  } else {\n    inf_vals <- NULL\n  }\n  \n  # bootstrap samples\n  r_boot <- replicate(max(B_vals), {\n    i <- sample(1:N, replace = TRUE, size = N)\n    r <- cor(dat[i,1], dat[i,2])\n    c(r, sqrt((1 - r^2)^2 / (N - 1)))\n  })\n  \n  bs_CIs <- simhelpers::bootstrap_CIs(\n    boot_est = r_boot[1,],\n    boot_se = r_boot[2,],\n    est = r_est,\n    se = SE_r,\n    influence = inf_vals,\n    CI_type = CI_type,\n    B_vals = B_vals,\n    format = \"wide-list\"\n  )\n  \n  tibble::tibble(\n    r = r_est,\n    SE = SE_r,\n    lo = tanh(CI_z[1]),\n    hi = tanh(CI_z[2]),\n    w = mean(r_boot < r_est),\n    a = sum(inf_vals^3) / (6 * sum(inf_vals^2)^1.5),\n    bs_CIs = bs_CIs\n  )\n}\n\nres <- cor_CI(dat, CI_type = c(\"percentile\",\"student\"))\nres\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 × 7\n      r    SE    lo    hi     w     a bs_CIs      \n  <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <btstr_CI>  \n1 0.590 0.133 0.254 0.799 0.699   NaN <df [5 × 5]>\n```\n\n\n:::\n:::\n\n\n# Performance measures\n\nNow I'll summarize across replications to evaluate the coverage rates of each set of confidence intervals. For the Fisher intervals, `simhelpers::calc_coverage()` does the trick, but for the bootstrap intervals I need to use `simhelpers::extrapolate_coverage()` because each replication is a specially structured _set_ of multiple confidence intervals with different values of $B$.\nI will implement these summary calculations with a function so that I can re-use it later:\n\n\n::: {.cell}\n\n```{.r .cell-code}\neval_performance <- function(dat, rho = 0, B_target = 1999) {\n  require(simhelpers, quietly = TRUE)\n  \n  dat |>\n    dplyr::summarize(\n      calc_absolute(estimates = r, true_param = rho, criteria = \"bias\"),\n      calc_coverage(lower_bound = lo, upper_bound = hi, true_param = rho),\n      extrapolate_coverage(\n        CI_subsamples = bs_CIs, true_param = rho, \n        B_target = B_target,\n        nested = TRUE, format = \"long\"\n      )\n    )\n}\n```\n:::\n\n\n\n# Simulation driver\n\nFirst, I'll revise `sim_cor()` to also wrap in the `eval_performance()` function:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsim_cor <- bundle_sim(\n  f_generate = r_t_bi, \n  f_analyze = cor_CI,\n  f_summarize = eval_performance\n)\n```\n:::\n\n\nThis results in a \"simulation driver\" function, which takes model parameters as inputs, runs a full simulation, and returns a dataset with performance measures.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsim_cor(\n  reps = 10,\n  n = 25, \n  rho = 0.75, \n  df = 8,\n  CI_type = \"percentile\",\n  B_vals = c(49,99,199,299,399),\n  B_target = 1999\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 × 14\n  K_absolute   bias bias_mcse K_coverage coverage coverage_mcse width width_mcse\n       <int>  <dbl>     <dbl>      <int>    <dbl>         <dbl> <dbl>      <dbl>\n1         10 0.0169    0.0346         10        1             0 0.347     0.0397\n# ℹ 6 more variables: K_boot_coverage <int>, bootstraps <list>,\n#   boot_coverage <list>, boot_coverage_mcse <list>, boot_width <list>,\n#   boot_width_mcse <list>\n```\n\n\n:::\n:::\n\n\n\n# Parameters to examine\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nparams <- expand_grid(\n  n = seq(10,100,10),\n  rho = seq(0.15,0.75,0.15),\n  df = c(8,16,32)\n)\n\nparams\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 150 × 3\n       n   rho    df\n   <dbl> <dbl> <dbl>\n 1    10  0.15     8\n 2    10  0.15    16\n 3    10  0.15    32\n 4    10  0.3      8\n 5    10  0.3     16\n 6    10  0.3     32\n 7    10  0.45     8\n 8    10  0.45    16\n 9    10  0.45    32\n10    10  0.6      8\n# ℹ 140 more rows\n```\n\n\n:::\n:::\n\n\n\n# Execute \n\nAfter enabling parallel computing, I can execute the simulation using `evaluate_by_row()`:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(future)\nlibrary(furrr)\nplan(multisession)\n\nres <- evaluate_by_row(\n  params = params,\n  sim_function = sim_cor,\n  reps = 4000,\n  CI_type = c(\"percentile\",\"student\",\"BCa\"),\n  B_vals = c(49,99,199,299,399),\n  B_target = 1999,\n  .options = furrr_options(seed = TRUE),\n  .progress = TRUE\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   user  system elapsed \n  12.84    0.83 5716.27 \n```\n\n\n:::\n:::\n\n\n\n# Results\n\nAfter a little further data-cleaning on the back end, we can see how the coverage rates of different intervals compare to each other across a range of parameter values and sample sizes. \nHere's a graph of the simulated coverage rates as a function of $n$:\n\n\n\n::: {.cell .column-page}\n\n```{.r .cell-code  code-fold=\"true\"}\nFisher_res <- \n  res %>%\n  select(n, rho, df, coverage, width) %>%\n  mutate(CI_type = \"Fisher-z\")\n\nboot_res <- \n  res %>%\n  select(n, rho, df, bootstraps, coverage = boot_coverage, width = boot_width) %>%\n  unnest(c(bootstraps, coverage, width)) %>%\n  filter(bootstraps == 1999) %>%\n  select(-bootstraps)\n\nCI_res <- \n  bind_rows(Fisher_res, boot_res) %>%\n  mutate(\n    rho_lab = paste(\"rho ==\", rho),\n    df_lab = factor(df, levels = c(8,16,32), labels = paste(\"nu ==\", c(8,16,32))),\n    CI_type = factor(CI_type, levels = c(\"Fisher-z\",\"percentile\",\"student\",\"BCa\"))\n  )\n\nggplot(CI_res) + \n  aes(n, coverage, color = CI_type) + \n  facet_grid(df_lab ~ rho_lab, labeller = \"label_parsed\") + \n  scale_x_continuous(breaks = seq(20,100,20)) + \n  scale_y_continuous(limits = c(0.85,1.0), breaks = seq(0.9,1.0,0.05), expand = expansion(0, 0)) + \n  geom_hline(yintercept = 0.95, linetype = \"dashed\") + \n  geom_point() + geom_line() + \n  theme_minimal() + \n  theme(legend.position = \"top\") + \n  labs(x = \"n\", y = \"Coverage rate\", color = \"\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-9-1.png){width=100%}\n:::\n:::\n\n\n\nFor small degrees of freedom, the Fisher-z interval has below-nominal coverage and does not appear to improve as sample size gets better. \nThis makes sense because the interval is derived under a bivariate normal model that is not consistent with the actual data-generating process unless the degrees of freedom are large.\nWith larger degrees of freedom $(\\nu = 32)$, the Fisher-z has almost exactly nominal coverage.\nAcross degrees of freedom and true correlations, the percentile interval has below-nominal coverage for the smaller sample sizes but appears to get steadily better as sample size increases. \nThe studentized interval performs remarkably well, with coverage very close to nominal even at very small sample sizes.\nRather curiously, the BCa interval pretty consistently has worse coverage than the (simpler) percentile interval, even for larger sample sizes. \nThis is counter-intuitive to me because the BCa interval is supposed to have second-order accurate coverage [@efron1987better] whereas the percentile interval is only first-order accurate. \nThe upshot of this is that one would expect the coverage rate of the BCa interval to improve more quickly than the percentile interval, but that doesn't seem to be the case here. ",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}