{
  "hash": "15fa53ee55be575f937bd12a5561e5eb",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: Imputing covariance matrices for meta-analysis of correlated effects\ndate: '2017-08-10'\ncategories:\n- meta-analysis\n- sandwiches\n- robust variance estimation\n- Rstats\ncode-tools: true\n---\n\n\nIn many systematic reviews, it is common for eligible studies to contribute effect size estimates from not just one, but _multiple_ relevant outcome measures, for a common sample of participants. If those outcomes are correlated, then [so too will be the effect size estimates](/posts/Correlations-between-SMDs/). To estimate the degree of correlation, you would need the sample correlation among the outcomes---information that is woefully uncommon for primary studies to report (and best of luck to you if you try to follow up with author queries). Thus, the meta-analyst is often left in a situation where the sampling _variances_ of the effect size estimates can be reasonably well approximated, but the sampling _covariances_ are unknown for some or all studies. \n\nSeveral solutions to this conundrum have been proposed in the meta-analysis methodology literature. One possible strategy is to just impute a correlation based on subject-matter knowledge (or at least feigned expertise), and assume that this correlation is constant across studies. This analysis could be supplemented with sensitivity analyses to examine the extent to which the parameter estimates and inferences are sensitive to alternative assumptions about the inter-correlation of effects within studies. A related strategy, described by [Wei and Higgins (2013)](https://dx.doi.org/10.1002/sim.5679), is to meta-analyze any available correlation estimates and then use the results to impute correlations for any studies with missing correlations. \n\nBoth of these approaches require the meta-analyst to calculate block-diagonal sampling covariance matrices for the effect size estimates, which can be a bit unwieldy. I often use the impute-the-correlation strategy in my meta-analysis work and have written a helper function to compute covariance matrices, given known sampling variances and imputed correlations for each study. In the interest of not repeating myself, I've added the function to the latest version of my clubSandwich package. In this post, I'll explain the function and demonstrate how to use it for conducting meta-analysis of correlated effect size estimates. \n\n## An R function for block-diagonal covariance matrices\n\nHere is the function: \n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(clubSandwich)\nimpute_covariance_matrix\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nfunction (vi, cluster, r, ti, ar1, smooth_vi = FALSE, subgroup = NULL, \n    return_list = identical(as.factor(cluster), sort(as.factor(cluster))), \n    check_PD = TRUE) \n{\n    cluster <- droplevels(as.factor(cluster))\n    vi_list <- split(vi, cluster)\n    if (smooth_vi) \n        vi_list <- lapply(vi_list, function(x) rep(mean(x, na.rm = TRUE), \n            length(x)))\n    if (missing(r) & missing(ar1)) \n        stop(\"You must specify a value for r or for ar1.\")\n    if (!missing(r)) {\n        r_list <- rep_len(r, length(vi_list))\n        if (missing(ar1)) {\n            vcov_list <- Map(function(V, rho) (rho + diag(1 - \n                rho, nrow = length(V))) * tcrossprod(sqrt(V)), \n                V = vi_list, rho = r_list)\n        }\n    }\n    if (!missing(ar1)) {\n        if (missing(ti)) \n            stop(\"If you specify a value for ar1, you must provide a vector for ti.\")\n        ti_list <- split(ti, cluster)\n        ar_list <- rep_len(ar1, length(vi_list))\n        if (missing(r)) {\n            vcov_list <- Map(function(V, time, phi) (phi^as.matrix(stats::dist(time))) * \n                tcrossprod(sqrt(V)), V = vi_list, time = ti_list, \n                phi = ar_list)\n        }\n        else {\n            vcov_list <- Map(function(V, rho, time, phi) (rho + \n                (1 - rho) * phi^as.matrix(stats::dist(time))) * \n                tcrossprod(sqrt(V)), V = vi_list, rho = r_list, \n                time = ti_list, phi = ar_list)\n        }\n        vcov_list <- lapply(vcov_list, function(x) {\n            attr(x, \"dimnames\") <- NULL\n            x\n        })\n    }\n    if (!is.null(subgroup)) {\n        si_list <- split(subgroup, cluster)\n        subgroup_list <- lapply(si_list, function(x) sapply(x, \n            function(y) y == x))\n        vcov_list <- Map(function(V, S) V * S, V = vcov_list, \n            S = subgroup_list)\n    }\n    if (check_PD) \n        check_PD(vcov_list)\n    if (return_list) {\n        return(vcov_list)\n    }\n    else {\n        vcov_mat <- unblock(vcov_list)\n        cluster_index <- order(order(cluster))\n        return(vcov_mat[cluster_index, cluster_index])\n    }\n}\n<bytecode: 0x000001d425d53050>\n<environment: namespace:clubSandwich>\n```\n\n\n:::\n:::\n\n\nThe function takes three required arguments: \n\n* `vi` is a vector of sampling variances.\n* `cluster` is a vector identifying the study from which effect size estimates are drawn. Effects with the same value of `cluster` will be treated as correlated.\n* `r` is the assumed value(s) of the correlation between effect size estimates from each study. Note that `r` can also be a vector with separate values for each study. \n\nHere is a simple example to demonstrate how the function works. Say that there are just three studies, contributing 2, 3, and 4 effects, respectively. I'll just make up some values for the effect sizes and variances:\n\n::: {.cell}\n\n```{.r .cell-code}\ndat <- data.frame(study = rep(LETTERS[1:3], 2:4), \n                  yi = rnorm(9), \n                  vi = 4:12)\ndat\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  study         yi vi\n1     A  2.5441763  4\n2     A  1.5374848  5\n3     B -0.3374318  6\n4     B  0.3957611  7\n5     B -0.4592493  8\n6     C  0.4457232  9\n7     C -0.6421787 10\n8     C -1.9565539 11\n9     C -1.1833011 12\n```\n\n\n:::\n:::\n\n\nI'll assume that effect size estimates from a given study are correlated at 0.7:\n\n::: {.cell}\n\n```{.r .cell-code}\nV_list <- impute_covariance_matrix(vi = dat$vi, cluster = dat$study, r = 0.7)\nV_list\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n$A\n         [,1]     [,2]\n[1,] 4.000000 3.130495\n[2,] 3.130495 5.000000\n\n$B\n         [,1]     [,2]     [,3]\n[1,] 6.000000 4.536518 4.849742\n[2,] 4.536518 7.000000 5.238320\n[3,] 4.849742 5.238320 8.000000\n\n$C\n         [,1]      [,2]      [,3]      [,4]\n[1,] 9.000000  6.640783  6.964912  7.274613\n[2,] 6.640783 10.000000  7.341662  7.668116\n[3,] 6.964912  7.341662 11.000000  8.042388\n[4,] 7.274613  7.668116  8.042388 12.000000\n```\n\n\n:::\n:::\n\n\nThe result is a list of matrices, where each entry corresponds to the variance-covariance matrix of effects from a given study. To see that the results are correct, let's examine the correlation matrix implied by these correlation matrices:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncov2cor(V_list$A)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     [,1] [,2]\n[1,]  1.0  0.7\n[2,]  0.7  1.0\n```\n\n\n:::\n\n```{.r .cell-code}\ncov2cor(V_list$B)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     [,1] [,2] [,3]\n[1,]  1.0  0.7  0.7\n[2,]  0.7  1.0  0.7\n[3,]  0.7  0.7  1.0\n```\n\n\n:::\n\n```{.r .cell-code}\ncov2cor(V_list$C)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     [,1] [,2] [,3] [,4]\n[1,]  1.0  0.7  0.7  0.7\n[2,]  0.7  1.0  0.7  0.7\n[3,]  0.7  0.7  1.0  0.7\n[4,]  0.7  0.7  0.7  1.0\n```\n\n\n:::\n:::\n\nAs requested, effects are assumed to be equi-correlated with r = 0.7.\n\nIf the data are sorted in order of the cluster IDs, then the list of matrices returned by `impute_covariance_matrix()` can be fed directly into the `rma.mv` function in metafor (as I demonstrate below). However, if the data are not sorted by `cluster`, then feeding in the list of matrices will not work correctly. Instead, the full $N \\times N$ variance-covariance matrix (where $N$ is the total number of effect size estimates) will need to be calculated so that the rows and columns appear in the correct order. To address this possibility, the function includes an optional argument, `return_list`, which determines whether to output a list of matrices (one matrix per study/cluster) or a single matrix corresponding to the full variance-covariance matrix across all studies. By default, `return_list` tests for whether the `cluster` argument is sorted and returns the appropriate form. The argument can also be set directly by the user. \n\nHere's what happens if we feed in the data in a different order:\n\n::: {.cell}\n\n```{.r .cell-code}\ndat_scramble <- dat[sample(nrow(dat)),]\ndat_scramble\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  study         yi vi\n1     A  2.5441763  4\n3     B -0.3374318  6\n2     A  1.5374848  5\n8     C -1.9565539 11\n6     C  0.4457232  9\n7     C -0.6421787 10\n4     B  0.3957611  7\n9     C -1.1833011 12\n5     B -0.4592493  8\n```\n\n\n:::\n\n```{.r .cell-code}\nV_mat <- round(impute_covariance_matrix(vi = dat_scramble$vi, cluster = dat_scramble$study, r = 0.7), 3)\nV_mat\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n      [,1]  [,2] [,3]   [,4]  [,5]   [,6]  [,7]   [,8]  [,9]\n [1,] 4.00 0.000 3.13  0.000 0.000  0.000 0.000  0.000 0.000\n [2,] 0.00 6.000 0.00  0.000 0.000  0.000 4.537  0.000 4.850\n [3,] 3.13 0.000 5.00  0.000 0.000  0.000 0.000  0.000 0.000\n [4,] 0.00 0.000 0.00 11.000 6.965  7.342 0.000  8.042 0.000\n [5,] 0.00 0.000 0.00  6.965 9.000  6.641 0.000  7.275 0.000\n [6,] 0.00 0.000 0.00  7.342 6.641 10.000 0.000  7.668 0.000\n [7,] 0.00 4.537 0.00  0.000 0.000  0.000 7.000  0.000 5.238\n [8,] 0.00 0.000 0.00  8.042 7.275  7.668 0.000 12.000 0.000\n [9,] 0.00 4.850 0.00  0.000 0.000  0.000 5.238  0.000 8.000\n```\n\n\n:::\n:::\n\n\nTo see that this is correct, check that the diagonal entries of `V_mat` are the same as `vi`:\n\n::: {.cell}\n\n```{.r .cell-code}\nall.equal(dat_scramble$vi, diag(V_mat))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] TRUE\n```\n\n\n:::\n:::\n\n\n## An example with real data\n\n[Kalaian and Raudenbush (1996)](https://dx.doi.org/10.1037/1082-989X.1.3.227) introduced a multi-variate random effects model, which can be used to perform a joint meta-analysis of studies that contribute effect sizes on distinct, related outcome constructs. They demonstrate the model using data from a synthesis on the effects of SAT coaching, where many studies reported effects on both the math and verbal portions of the SAT. The data are available in the `clubSandwich` package:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(dplyr, warn.conflicts=FALSE)\ndata(SATcoaching)\n\n# calculate the mean of log of coaching hours\nmean_hrs_ln <- \n  SATcoaching %>% \n  group_by(study) %>%\n  summarise(hrs_ln = mean(log(hrs))) %>%\n  summarise(hrs_ln = mean(hrs_ln, na.rm = TRUE))\n\n# clean variables, sort by study ID\nSATcoaching <- \n  SATcoaching %>%\n  mutate(\n    study = as.factor(study),\n    hrs_ln = log(hrs) - mean_hrs_ln$hrs_ln\n  ) %>%\n  arrange(study, test)\n\nSATcoaching %>%\n  select(study, year, test, d, V, hrs_ln) %>%\n  head(n = 20)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n                   study year   test     d      V      hrs_ln\n1  Alderman & Powers (A) 1980 Verbal  0.22 0.0817 -0.54918009\n2  Alderman & Powers (B) 1980 Verbal  0.09 0.0507 -0.19250515\n3  Alderman & Powers (C) 1980 Verbal  0.14 0.1045 -0.14371499\n4  Alderman & Powers (D) 1980 Verbal  0.14 0.0442 -0.19250515\n5  Alderman & Powers (E) 1980 Verbal -0.01 0.0535 -0.70333077\n6  Alderman & Powers (F) 1980 Verbal  0.14 0.0557 -0.88565233\n7  Alderman & Powers (G) 1980 Verbal  0.18 0.0561 -0.09719497\n8  Alderman & Powers (H) 1980 Verbal  0.01 0.1151  1.31157225\n9              Burke (A) 1986 Verbal  0.50 0.0825  1.41693276\n10             Burke (B) 1986 Verbal  0.74 0.0855  1.41693276\n11                Coffin 1987   Math  0.33 0.2534  0.39528152\n12                Coffin 1987 Verbal -0.23 0.2517  0.39528152\n13            Curran (A) 1988   Math -0.08 0.1065 -0.70333077\n14            Curran (A) 1988 Verbal -0.10 0.1066 -0.70333077\n15            Curran (B) 1988   Math -0.29 0.1015 -0.70333077\n16            Curran (B) 1988 Verbal -0.14 0.1007 -0.70333077\n17            Curran (C) 1988   Math -0.34 0.1104 -0.70333077\n18            Curran (C) 1988 Verbal -0.16 0.1092 -0.70333077\n19            Curran (D) 1988   Math -0.06 0.1089 -0.70333077\n20            Curran (D) 1988 Verbal -0.07 0.1089 -0.70333077\n```\n\n\n:::\n:::\n\n\nThe correlation betwen math and verbal test scores are not available, but it seems reasonable to use a correlation of r = 0.66, as reported in the SAT technical information. To synthesize these effects, I'll first compute the required variance-covariances:\n\n::: {.cell}\n\n```{.r .cell-code}\nV_list <- impute_covariance_matrix(vi = SATcoaching$V, \n                                   cluster = SATcoaching$study, \n                                   r = 0.66)\n```\n:::\n\n\nThis can then be fed into `metafor` to estimate a fixed effect or random effects meta-analysis or meta-regression models:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(metafor, quietly = TRUE)\n\n# bivariate fixed effect meta-analysis\nMVFE_null <- rma.mv(d ~ 0 + test, V = V_list, data = SATcoaching)\nMVFE_null\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nMultivariate Meta-Analysis Model (k = 67; method: REML)\n\nVariance Components: none\n\nTest for Residual Heterogeneity:\nQE(df = 65) = 72.1630, p-val = 0.2532\n\nTest of Moderators (coefficients 1:2):\nQM(df = 2) = 19.8687, p-val < .0001\n\nModel Results:\n\n            estimate      se    zval    pval   ci.lb   ci.ub      \ntestMath      0.1316  0.0331  3.9783  <.0001  0.0668  0.1965  *** \ntestVerbal    0.1215  0.0313  3.8783  0.0001  0.0601  0.1829  *** \n\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n\n\n:::\n\n```{.r .cell-code}\n# bivariate fixed effect meta-regression\nMVFE_hrs <- rma.mv(d ~ 0 + test + test:hrs_ln, V = V_list, \n                   data = SATcoaching)\nMVFE_hrs\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nMultivariate Meta-Analysis Model (k = 65; method: REML)\n\nVariance Components: none\n\nTest for Residual Heterogeneity:\nQE(df = 61) = 67.9575, p-val = 0.2523\n\nTest of Moderators (coefficients 1:4):\nQM(df = 4) = 23.7181, p-val < .0001\n\nModel Results:\n\n                   estimate      se    zval    pval    ci.lb   ci.ub     \ntestMath             0.0946  0.0402  2.3547  0.0185   0.0159  0.1734   * \ntestVerbal           0.1119  0.0341  3.2762  0.0011   0.0449  0.1788  ** \ntestMath:hrs_ln      0.1034  0.0546  1.8946  0.0581  -0.0036  0.2103   . \ntestVerbal:hrs_ln    0.0601  0.0442  1.3592  0.1741  -0.0266  0.1467     \n\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n\n\n:::\n\n```{.r .cell-code}\n# bivariate random effects meta-analysis\nMVRE_null <- rma.mv(d ~ 0 + test, V = V_list, data = SATcoaching, \n                 random = ~ test | study, struct = \"UN\")\nMVRE_null\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nMultivariate Meta-Analysis Model (k = 67; method: REML)\n\nVariance Components:\n\nouter factor: study (nlvls = 47)\ninner factor: test  (nlvls = 2)\n\n            estim    sqrt  k.lvl  fixed   level \ntau^2.1    0.0122  0.1102     29     no    Math \ntau^2.2    0.0026  0.0507     38     no  Verbal \n\n        rho.Math  rho.Vrbl    Math  Vrbl \nMath           1                 -    20 \nVerbal   -1.0000         1      no     - \n\nTest for Residual Heterogeneity:\nQE(df = 65) = 72.1630, p-val = 0.2532\n\nTest of Moderators (coefficients 1:2):\nQM(df = 2) = 18.1285, p-val = 0.0001\n\nModel Results:\n\n            estimate      se    zval    pval   ci.lb   ci.ub      \ntestMath      0.1379  0.0434  3.1783  0.0015  0.0528  0.2229   ** \ntestVerbal    0.1168  0.0337  3.4603  0.0005  0.0506  0.1829  *** \n\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n\n\n:::\n\n```{.r .cell-code}\n# bivariate random effects meta-regression\nMVRE_hrs <- rma.mv(d ~ 0 + test + test:hrs_ln, V = V_list, \n                   data = SATcoaching,\n                   random = ~ test | study, struct = \"UN\")\nMVRE_hrs\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nMultivariate Meta-Analysis Model (k = 65; method: REML)\n\nVariance Components:\n\nouter factor: study (nlvls = 46)\ninner factor: test  (nlvls = 2)\n\n            estim    sqrt  k.lvl  fixed   level \ntau^2.1    0.0152  0.1234     28     no    Math \ntau^2.2    0.0014  0.0373     37     no  Verbal \n\n        rho.Math  rho.Vrbl    Math  Vrbl \nMath           1                 -    19 \nVerbal   -1.0000         1      no     - \n\nTest for Residual Heterogeneity:\nQE(df = 61) = 67.9575, p-val = 0.2523\n\nTest of Moderators (coefficients 1:4):\nQM(df = 4) = 23.6459, p-val < .0001\n\nModel Results:\n\n                   estimate      se    zval    pval    ci.lb   ci.ub     \ntestMath             0.0893  0.0507  1.7631  0.0779  -0.0100  0.1887   . \ntestVerbal           0.1062  0.0357  2.9738  0.0029   0.0362  0.1762  ** \ntestMath:hrs_ln      0.1694  0.0725  2.3354  0.0195   0.0272  0.3116   * \ntestVerbal:hrs_ln    0.0490  0.0459  1.0681  0.2855  -0.0409  0.1389     \n\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n\n\n:::\n:::\n\n\nThe results of fitting this model using restricted maximum likelihood with metafor are actually a bit different from the estimates reported in the original paper, potentially because Kalaian and Raudenbush use a Cholesky decomposition of the sampling covariances, which alters the interpretation of the random effects variance components. The metafor fit is also a bit goofy because the correlation between the random effects for math and verbal scores is very close to -1, although evidently it is not uncommon to obtain such degenerate estimates of the random effects structure. \n\n## Robust variance estimation.\n\nExperienced meta-analysts will no doubt point out that a further, alternative analytic strategy to the one described above would be to use robust variance estimation methods (RVE; [Hedges, Tipton, & Johnson](https://dx.doi.org/10.1002/jrsm.5)). However, RVE is not so much an alternative strategy as it is a complementary technique, which can be used in combination with any of the models estimated above. Robust standard errors and hypothesis tests can readily be obtained with the [clubSandwich package](https://cran.r-project.org/package=clubSandwich). Here's how to do it for the random effects meta-regression model:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(clubSandwich)\ncoef_test(MVRE_hrs, vcov = \"CR2\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n             Coef. Estimate     SE t-stat d.f. (Satt) p-val (Satt) Sig.\n          testMath   0.0893 0.0360   2.48       20.75       0.0218    *\n        testVerbal   0.1062 0.0215   4.94       16.45       <0.001  ***\n   testMath:hrs_ln   0.1694 0.1010   1.68        7.90       0.1325     \n testVerbal:hrs_ln   0.0490 0.0414   1.18        7.57       0.2725     \n```\n\n\n:::\n:::\n\n\nRVE is also available in the  [robumeta R package](https://CRAN.R-project.org/package=robumeta), but there are several differences between the implementation there and the method I've demonstrated here. From the user's perspective, an advantage of robumeta is that it does all of the covariance imputation calculations \"under the hood,\" whereas with metafor the calculations need to be done prior to fitting the model. Beyond this, differences include:\n\n* robumeta uses a specific random effects structure that can't be controlled by the user, whereas metafor can be used to estimate a variety of different random effects structures;\n* robumeta uses a moment estimator for the between-study variance, whereas metafor provides FML or REML estimation;\n* robumeta uses semi-efficient, diagonal weights when fitting the meta-regression, whereas metafor uses weights that are fully efficient (exactly inverse-variance) under the working model. \n\nThe advantages and disadvantages of these two approaches involve some subtleties that I'll get into in a future post. \n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}