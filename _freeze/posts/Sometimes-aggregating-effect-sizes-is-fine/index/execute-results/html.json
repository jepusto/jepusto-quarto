{
  "hash": "f0afb8d96ee66dfd4cb77fb5dc1fb77c",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: Sometimes, aggregating effect sizes is fine\ndate: '2019-07-02'\nbibliography: \"../meta-references.bib\"\ncsl: \"../apa.csl\"\nlink-citations: true\ncategories:\n- effect size\n- meta-analysis\n- dependent effect sizes\ncode-tools: true\ntoc: true\ntoc-title: Contents\n---\n\n\nIn meta-analyses of psychology, education, and other social science research, it is very common that some of the included studies report more than one relevant effect size. \nFor example, in a meta-analysis of intervention effects on reading outcomes, some studies may have used multiple measures of reading outcomes (each of which meets inclusion criteria), or may have measured outcomes at multiple follow-up times; some studies might have also investigated more than one version of an intervention, and it might be of interest to include effect sizes comparing each version to the no-intervention control condition;\nand it's even possible that some studies may have _all_ of these features, potentially contributing _lots_ of effect size estimates.\n\nThese situations create a technical challenge for conducting a meta-analysis. \nBecause effect size estimates from the same study are correlated, it's not usually reasonable to use methods that are premised on each effect size estimate being independent (i.e., univariate methods). \nInstead, the analyst needs to apply methods that take into account the dependencies among estimates coming from the same study. \nIt used to be common to use ad hoc approaches for handling dependence, such as averaging the estimates together or selecting one estimate per study and then using univariate methods [cf. @Becker2000multivariate]. \nMore sophisticated, multivariate meta-analysis (MVMA) models that directly account for correlations among the effect size estimates had been developed [@Kalaian1996multivariate] but were challenging to implement and so rarely used (at least, that's my impression). \nMore recently, techniques such as multi-level meta-analysis [MLMA,  @VandenNoortgate2013threelevel; @VandenNoortgate2015metaanalysis] and robust variance estimation [RVE, @Hedges2010robust] have emerged, which account for dependencies while using all available effect size estimates and still being feasible to implement. \nThese new techniques of MLMA and RVE are starting to be more widely adopted in practice, and it is not implausible that they will become the standard approach in psychological and educational meta-analysis within a few years. \n\nGiven the extent of interest in MLMA and RVE, one might wonder: are the older ad hoc approaches _ever_ reasonable or appropriate? \nI think that some are, under certain circumstances. \nIn this post I'll highlight one such circumstance, where aggregating effect size estimates is not only reasonable but leads to _exactly the same results_ as a multivariate model. This occurs when two conditions are met:\n\n1. We are not interested in within-study heterogeneity of effects and\n2. Any predictors included in the model vary between studies but not within a given study (i.e., effect sizes from the same study all have the same values of the predictors).\n\nIn short, if all we care about is understanding between-study variation in effect sizes, then it is fine to aggregate them up to the study level.\n\n# A model that's okay to average\n\nTo make this argument precise, let me lay out a model where it applies. \nFor full generality, I'll consider a meta-regression model for a collection of $K$ studies, where study $k$ contributes $J_k \\geq 1$ effect size estimates. \nLet $T_{jk}$ denote effect size estimate $j$ in study $k$, with sampling variance $S_{jk}^2$. \nEffect size estimates from study $k$ maybe be correlated at the sampling level, with correlation $\\rho_{ijk}$ between effect size estimates $i$ and $j$ from study $k$. \nI will assume that the correlations are known, although in practice one might need to just take a guess about the degree of correlation, such as by assuming $\\rho_{ijk} = 0.7$ for all pairs of estimates from each included study. \nLet $\\mathbf{x}_k$ be a row vector of predictor variables for study $k$. \nNote that the predictors do not have a subscript $j$ because I'm assuming here that they are constant within a study. \n\nA multivariate meta-regression model for these data might be:\n$$\nT_{jk} = \\mathbf{x}_k \\boldsymbol\\beta + u_k + e_{jk},\n$$\nwhere $u_k$ is a between-study random effect with variance $\\tau^2$ and $e_{jk}$ is the sampling error for effect size $j$ from study $k$, assumed to have known variance $S_{jk}^2$. \nErrors from the same study are correlated, so $\\text{Cov}(e_{ik}, e_{jk}) = \\rho_{ijk} S_{ik} S_{jk}$. \nThis is a commonly considered model for dependent effect size estimates. \nIn the paper that introduced RVE, @Hedges2010robust termed it the \"correlated effects\" model (implemented in `robumeta` as `model = \"CORR\"`, which is the default). \nNote that it also satisfies the conditions I outlined above: no within-study random effects, predictors that vary only between study. \nWe can fit it using the `rma.mv()` function in the `metafor` package, as I will demonstrate below.\n\nAn alternative to this multivariate model would be to first average the effects within each study, then fit a univariate random effects model. \nJust how we do the averaging will matter: we'll need to use inverse-variance weighting. \nLet $\\mathbf{T}_k$ be the $J_k \\times 1$ vector of effect size estimates from study $k$. Let $\\mathbf{S}_k$ be the $J_k \\times J_k$ sampling covariance matrix for $\\mathbf{T}_k$, and let $\\mathbf{1}_k$ be a $J_k \\times 1$ vector of 1s. The inverse-variance weighted average of the effects from study k can then be written as\n$$\n\\bar{T}_k = V_k \\mathbf{1}_k’ \\mathbf{S}_k^{-1} \\mathbf{T}_k, \n$$\nwhere $V_k = 1 / (\\mathbf{1}_k’ \\mathbf{S}_k^{-1} \\mathbf{1}_k)$. The quantity $V_k$ is also the sampling variance of $\\bar{T}_k$.[^specialcase] \n\n[^specialcase]: A common special case is that the sampling variances for effect sizes within a given study $k$ are _all equal_, so that $S_{ik} = s_{jk} = S_k$ for $i,j = 1,...,J_ik$ and $k = 1,...,K$. We might further posit that there is a constant sampling correlation between every pair of effect sizes within a given study, so that $\\rho_{ijk} = \\rho_k$ for $i,j = 1,...,J_ik$ and $k = 1,...,K$. If both of these conditions hold, then the inverse-variance weighted average effect size simplifies to the arithmetic average\n$$\n\\bar{T}_k = \\frac{1}{J_k} \\sum_{j=1}^{J_k} T_{jk}\n$$\nwith sampling variance \n$$\nV_k = \\frac{(J_k - 1)\\rho_k + 1}{J} \\times S_k^2\n$$\n[cf. @borenstein2009introduction, Eq. (24.6), p. 230].\n\nA conventional, univariate random effects model for the averaged effect sizes is\n$$\n\\bar{T}_k = \\mathbf{x}_k \\boldsymbol\\beta + u_k + \\bar{e}_k, \n$$\nwhere $\\text{Var}(u_k) = \\tau^2$ and $\\text{Var}(\\bar{e}_k) = V_k$. \nThis model can be fit using `rma.uni` from `metafor`. \nIn fact, doing so will yield the same estimates of model parameters as fitting the multivariate model---for all intents and purposes, they are equivalent models. \nThere are at several different ways to see that this equivalence holds. \nI'll offer three, from most practical to most theoretical.\n(If you'd rather just take my word that this claim is true, feel free to skip down to the [last section](#so-what), where I comment on implications.)\n\n# Computational equivalence\n\nOne good way to check the equivalence of the univariate and multivariate models is to apply both to a dataset. I'll use the data from a stylized example described in @TannerSmith2013robust, looking at the effects of alcohol abuse interventions on alcohol consumption among adolescents and young adults. (The data are simulated for teaching purposes, so don't infer anything about real life from the results below!) The data are included in the `robumeta` package:\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\n\ndata(corrdat, package = \"robumeta\")\n\n# sort by study\ncorrdat <- arrange(corrdat, studyid, esid)\n```\n:::\n\nThe data consist of 172 effect sizes from 39 studies. Some studies report effects at multiple follow-up times and/or for multiple programs compared to a common control condition, leading to dependent effect size estimates.The data also include variables encoding a variety of sample and study characteristics, such as whether the study was conducted with a college student sample and the gender composition of the sample: \n\n::: {.cell}\n\n```{.r .cell-code}\nhead(corrdat)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  esid studyid effectsize        var binge followup males college\n3 4006       1  0.2086383 0.03246468     1 51.42857    67       0\n1 4016       1  0.2244635 0.03244931     1 51.42857    67       0\n2 4026       1  0.3151743 0.03278697     1 51.42857    67       0\n5 3513       2  0.2220929 0.01972874     0 17.14286    81       1\n9 3514       2 -0.1922628 0.02031393     0 17.14286    86       1\n8 3556       2  0.3273109 0.01987042     0 17.14286    81       1\n```\n\n\n:::\n:::\n\nSuppose that we are interested in estimating the differences in average effects by type of sample (college versus adolescent), controlling for the proportion of males in the study. For some reason, there is within-study variation in the percentage of males, so I'll take the study-level average for this covariate:\n\n::: {.cell}\n\n```{.r .cell-code}\ncorrdat <-\n  corrdat %>%\n  group_by(studyid) %>%\n  mutate(males = mean(males))\n```\n:::\n\n\nWe can then fit this model using a multi-variate meta-regression in metafor. \n\nIn order to estimate the model, we'll first need to create a variance-covariance matrix for the effect size estimates in each study, which can be accomplished using `impute_covariance_matrix` from `clubSandwich` ([further details here](/posts/imputing-covariance-matrices-for-multi-variate-meta-analysis/)). I'll assume a correlation of 0.6 between pairs of effect sizes within a given study:\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(clubSandwich)\nlibrary(metafor)\n\nV_list <- impute_covariance_matrix(vi = corrdat$var, cluster = corrdat$studyid, r = 0.6)\n\nMV_fit <- rma.mv(effectsize ~ college + males, V = V_list, \n                 random = ~ 1 | studyid,\n                 data = corrdat, method = \"REML\")\nMV_fit\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nMultivariate Meta-Analysis Model (k = 172; method: REML)\n\nVariance Components:\n\n            estim    sqrt  nlvls  fixed   factor \nsigma^2    0.0590  0.2429     39     no  studyid \n\nTest for Residual Heterogeneity:\nQE(df = 169) = 815.2448, p-val < .0001\n\nTest of Moderators (coefficients 2:3):\nQM(df = 2) = 9.9016, p-val = 0.0071\n\nModel Results:\n\n         estimate      se     zval    pval    ci.lb    ci.ub     \nintrcpt    0.6466  0.2693   2.4007  0.0164   0.1187   1.1744   * \ncollege    0.3703  0.1317   2.8123  0.0049   0.1122   0.6283  ** \nmales     -0.0076  0.0038  -1.9832  0.0473  -0.0152  -0.0001   * \n\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n\n\n:::\n:::\n\nAlternately, we could aggregate the effects up to the study level and then fit a univariate meta-regression using the same moderators. Here is a function to calculate the aggregated effect size estimates and variances:\n\n::: {.cell}\n\n```{.r .cell-code}\nagg_effects <- function(yi, vi, r = 0.6) {\n  corr_mat <- r + diag(1 - r, nrow = length(vi))\n  sd_mat <- tcrossprod(sqrt(vi))\n  V_inv_mat <- chol2inv(chol(sd_mat * corr_mat))\n  V <- 1 / sum(V_inv_mat)\n  data.frame(es = V * sum(yi * V_inv_mat), var = V)\n}\n```\n:::\n\n\nHere's the data-munging:\n\n::: {.cell}\n\n```{.r .cell-code}\ncorrdat_agg <-\n  corrdat %>%\n  group_by(studyid) %>%\n  summarise(\n    es = list(agg_effects(yi = effectsize, vi = var, r = 0.6)),\n    males = mean(males),\n    college = mean(college)\n  ) %>%\n  unnest(es)\n\nhead(corrdat_agg)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 6 × 5\n  studyid      es    var males college\n    <dbl>   <dbl>  <dbl> <dbl>   <dbl>\n1       1  0.249  0.0239  67         0\n2       2 -0.0210 0.0129  81         1\n3       3  0.726  0.0819  76.2       0\n4       4  0.370  0.0431  80         1\n5       5 -0.0911 0.0281  79         0\n6       6 -0.416  0.0111  74         0\n```\n\n\n:::\n:::\n\nAnd here's the meta-regression:\n\n::: {.cell}\n\n```{.r .cell-code}\nuni_fit <- rma.uni(es ~ college + males, vi = var, \n                   data = corrdat_agg, method = \"REML\")\nuni_fit\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nMixed-Effects Model (k = 39; tau^2 estimator: REML)\n\ntau^2 (estimated amount of residual heterogeneity):     0.0590 (SE = 0.0242)\ntau (square root of estimated tau^2 value):             0.2429\nI^2 (residual heterogeneity / unaccounted variability): 61.42%\nH^2 (unaccounted variability / sampling variability):   2.59\nR^2 (amount of heterogeneity accounted for):            19.12%\n\nTest for Residual Heterogeneity:\nQE(df = 36) = 96.7794, p-val < .0001\n\nTest of Moderators (coefficients 2:3):\nQM(df = 2) = 9.9016, p-val = 0.0071\n\nModel Results:\n\n         estimate      se     zval    pval    ci.lb    ci.ub     \nintrcpt    0.6466  0.2693   2.4007  0.0164   0.1187   1.1744   * \ncollege    0.3703  0.1317   2.8123  0.0049   0.1122   0.6283  ** \nmales     -0.0076  0.0038  -1.9832  0.0473  -0.0152  -0.0001   * \n\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n\n\n:::\n:::\n\nThe heterogeneity estimates are nearly equal (the difference is due to using numerical optimization):\n\n::: {.cell}\n\n```{.r .cell-code}\nMV_fit$sigma2\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.0589972\n```\n\n\n:::\n\n```{.r .cell-code}\nuni_fit$tau2\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.05899673\n```\n\n\n:::\n:::\n\nAnd the meta-regression coefficient estimates are identical to six decimal places:\n\n::: {.cell}\n\n```{.r .cell-code}\ncoef(MV_fit)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     intrcpt      college        males \n 0.646561371  0.370274721 -0.007633517 \n```\n\n\n:::\n\n```{.r .cell-code}\ncoef(uni_fit)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     intrcpt      college        males \n 0.646561352  0.370274307 -0.007633519 \n```\n\n\n:::\n\n```{.r .cell-code}\nall.equal(coef(MV_fit), coef(uni_fit))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"Mean relative difference: 4.243578e-07\"\n```\n\n\n:::\n:::\n\nFor this example we arrive at the same results using either multivariate meta-analysis or univariate meta-analysis of aggregated effect size estimates.[^FML] The main limitation of this illustration is generality---how can we be sure that these results aren't just a quirk of this particular dataset? Would we get the same results for _any_ dataset? \n\n[^FML]: The same thing holds if we use FML rather than RML estimation---try it for yourself and see!\n\n# From multivariate to univariate model \n\nHere's another, somewhat more general perspective on the relationship between the models: the univariate model can be _derived_ directly from the multivariate one. Start with the multivariate model in matrix form:\n$$\n\\mathbf{T}_k = \\mathbf{x}_k \\boldsymbol\\beta \\mathbf{1}_k + u_k \\mathbf{1}_k + \\mathbf{e}_k,\n$$\nwhere $\\mathbf{e}_k$ is the vector of sampling errors for study $k$, with $\\text{Var}(\\mathbf{e}_k) = \\mathbf{S}_k$. Pre-multiply both sides by $V_k \\mathbf{1}_k’ \\mathbf{S}_k^{-1}$ to get\n$$\n\\begin{aligned}\nV_k \\mathbf{1}_k’ \\mathbf{S}_k^{-1} \\mathbf{T}_k &= V_k \\left(\\mathbf{1}_k’ \\mathbf{S}_k^{-1} \\mathbf{1}_k\\right) \\mathbf{x}_k \\boldsymbol\\beta + u_k V_k \\left(\\mathbf{1}_k’ \\mathbf{S}_k^{-1} \\mathbf{1}_k\\right) + V_k \\mathbf{1}_k’ \\mathbf{S}_k^{-1} \\mathbf{e}_k \\\\\n\\bar{T}_k &= \\mathbf{x}_k \\boldsymbol\\beta + u_k + \\bar{e}_k,\n\\end{aligned}\n$$\nwhere $\\text{Var}(\\bar{e}_k) = V_k \\mathbf{1}_k’ \\mathbf{S}_k^{-1} \\mathbf{S}_k \\mathbf{S}_k^{-1} \\mathbf{1}_k V_k = V_k$, just as in the univariate model. \n\nThis demonstrates that the parameters of the two models are the same quantities—that is, both models are estimating the same thing. But that would also hold if we used _any_ weighted average of $\\mathbf{T}_k$---it needn't be inverse-variance. The only thing that would be different is $\\text{Var}(\\bar{e}_k)$. To fully establish the equivalence of the two models, I'll examine the likelihoods of each model.\n\n# Equivalence of likelihoods \n\nMultivariate meta-analysis models are typically estimated by full maximum likelihood (FML) or restricted maximum likelihood methods. FML and RML are also commonly used for univariate meta-analysis. With these methods, estimates are obtained as the parameter values that maximize the log likelihood of the model, given the data (or the restricted likelihood for RML). Therefore, we can establish the exact equivalence of parameter estimates by showing that the log likelihood of the univariate and multivariate models differ by a constant value (so that the location of the maxima are identical). \n\n## Full likelihood \n\nFor the univariate model, the log-likelihood contribution of study $k$:\n$$\nl^{U}_k\\left(\\boldsymbol\\beta, \\tau^2\\right) = -\\frac{1}{2} \\log\\left(\\tau^2 + V_k\\right) - \\frac{1}{2} \\frac{\\left(\\bar{T}_k - \\mathbf{x}_k \\boldsymbol\\beta\\right)^2}{\\tau^2 + V_k}.\n$$\nFor the multivariate model, the log-likelihood contribution of study $k$ is:\n$$\nl^{MV}_k\\left(\\boldsymbol\\beta, \\tau^2\\right) = -\\frac{1}{2} A -\\frac{1}{2} B\n$$\nwhere \n$$\nA = \\log\\left|\\tau^2\\mathbf{1}_k\\mathbf{1}_k' + \\mathbf{S}_k\\right| \n$$\nand \n$$\nB = \\left(\\mathbf{T}_k - \\mathbf{x}_k \\boldsymbol\\beta \\mathbf{1}_k\\right)' \\left(\\tau^2\\mathbf{1}_k\\mathbf{1}_k' + \\mathbf{S}_k\\right)^{-1} \\left(\\mathbf{T}_k - \\mathbf{x}_k \\boldsymbol\\beta \\mathbf{1}_k\\right).\n$$\nThe term $A$ can be rearranged as\n$$\nA = \\log\\left|\\left(\\tau^2\\mathbf{1}_k\\mathbf{1}_k'\\mathbf{S}_k^{-1} + \\mathbf{I}_k\\right) \\mathbf{S}_k\\right|\n$$\nwhere $\\mathbf{I}_k$ is a $J_k \\times J_k$ identity matrix. One of the properties of determinants is that the determinant of a product of two matrices is equal to the product of the determinants. Another is that, for two vectors $\\mathbf{u}$ and $\\mathbf{v}$, $\\left|\\mathbf{I} + \\mathbf{u}\\mathbf{v}'\\right| = 1 + \\mathbf{v}'\\mathbf{u}$. Applying both of these properties, it follows that \n$$\n\\begin{aligned}\nA &= \\log\\left|\\left(\\tau^2\\mathbf{1}_k\\mathbf{1}_k'\\mathbf{S}_k^{-1} + \\mathbf{I}_k\\right) \\mathbf{S}_k\\right| \\\\\n&= \\log \\left( \\left|\\mathbf{I}_k + \\tau^2\\mathbf{1}_k\\mathbf{1}_k'\\mathbf{S}_k^{-1}\\right| \\left|\\mathbf{S}_k\\right|\\right) \\\\\n&= \\log \\left(1 + \\frac{\\tau^2}{V_k}\\right) + \\log \\left|\\mathbf{S}_k\\right| \\\\\n&= \\log(\\tau^2 + V_k) - \\log(V_k) + \\log \\left|\\mathbf{S}_k\\right|.\n\\end{aligned}\n$$\nThe $B$ term takes a little more work. \nFrom [the Sherman-Morrison identity](https://en.wikipedia.org/wiki/Sherman%E2%80%93Morrison_formula), we have that: \n$$\n\\left(\\tau^2\\mathbf{1}_k\\mathbf{1}_k' + \\mathbf{S}_k\\right)^{-1} = \\mathbf{S}_k^{-1} - \\mathbf{S}_k^{-1} \\mathbf{1}_k \\left(\\frac{1}{\\tau^2} + \\frac{1}{V_k}\\right)^{-1} \\mathbf{1}_k'\\mathbf{S}_k^{-1},\n$$ {#eq-Sherman}\nby which it follows that\n$$\n\\mathbf{1}_k'\\left(\\tau^2\\mathbf{1}_k\\mathbf{1}_k' + \\mathbf{S}_k\\right)^{-1}\\mathbf{1}_k = \\frac{1}{\\tau^2 + V_k}.\n$$ {#eq-inversevariance}\nNow, rearrange the $B$ term to get\n$$\n\\begin{aligned}\nB &= \\left[\\mathbf{T}_k - \\bar{T}_k \\mathbf{1}_k + \\left(\\bar{T}_k - \\mathbf{x}_k \\boldsymbol\\beta\\right) \\mathbf{1}_k\\right]' \\left(\\tau^2\\mathbf{1}_k\\mathbf{1}_k' + \\mathbf{S}_k\\right)^{-1} \\left[\\mathbf{T}_k - \\bar{T}_k \\mathbf{1}_k + \\left(\\bar{T}_k - \\mathbf{x}_k \\boldsymbol\\beta\\right) \\mathbf{1}_k\\right] \\\\\n&= B_1 + 2 B_2 + B_3\n\\end{aligned}\n$$\nwhere\n$$\n\\begin{aligned}\nB_1 &= \\left(\\mathbf{T}_k - \\bar{T}_k \\mathbf{1}_k\\right)' \\left(\\tau^2\\mathbf{1}_k\\mathbf{1}_k' + \\mathbf{S}_k\\right)^{-1} \\left(\\mathbf{T}_k - \\bar{T}_k \\mathbf{1}_k\\right) \\\\\nB_2 &= \\left(\\mathbf{T}_k - \\bar{T}_k \\mathbf{1}_k\\right)' \\left(\\tau^2\\mathbf{1}_k\\mathbf{1}_k' + \\mathbf{S}_k\\right)^{-1} \\mathbf{1}_k \\left(\\bar{T}_k - \\mathbf{x}_k \\boldsymbol\\beta\\right) \\\\\nB_3 &= \\left(\\bar{T}_k - \\mathbf{x}_k \\boldsymbol\\beta\\right) \\mathbf{1}_k' \\left(\\tau^2\\mathbf{1}_k\\mathbf{1}_k' + \\mathbf{S}_k\\right)^{-1} \\mathbf{1}_k \\left(\\bar{T}_k - \\mathbf{x}_k \\boldsymbol\\beta\\right)\n\\end{aligned}\n$$\nApplying @eq-Sherman to $B_1$,\n$$\n\\begin{aligned}\nB_1 &= \\left(\\mathbf{T}_k - \\bar{T}_k \\mathbf{1}_k\\right)' \\left[\\mathbf{S}_k^{-1} - \\mathbf{S}_k^{-1} \\mathbf{1}_k \\left(\\frac{1}{\\tau^2} + \\frac{1}{V_k}\\right)^{-1} \\mathbf{1}_k'\\mathbf{S}_k^{-1}\\right] \\left(\\mathbf{T}_k - \\bar{T}_k \\mathbf{1}_k\\right) \\\\ \n&= \\left(\\mathbf{T}_k - \\bar{T}_k \\mathbf{1}_k\\right)' \\mathbf{S}_k^{-1}\\left(\\mathbf{T}_k - \\bar{T}_k \\mathbf{1}_k\\right) - \\left(\\mathbf{T}_k - \\bar{T}_k \\mathbf{1}_k\\right)' \\mathbf{S}_k^{-1} \\mathbf{1}_k \\left(\\frac{1}{\\tau^2} + \\frac{1}{V_k}\\right)^{-1} \\mathbf{1}_k'\\mathbf{S}_k^{-1}\\left(\\mathbf{T}_k - \\bar{T}_k \\mathbf{1}_k\\right) \\\\\n&= \\left(\\mathbf{T}_k - \\bar{T}_k \\mathbf{1}_k\\right)' \\mathbf{S}_k^{-1}\\left(\\mathbf{T}_k - \\bar{T}_k \\mathbf{1}_k\\right).\n\\end{aligned}\n$$\nThe second term drops out because $\\left(\\mathbf{T}_k - \\bar{T}_k \\mathbf{1}_k\\right)' \\mathbf{S}_k^{-1} \\mathbf{1}_k = \\bar{T}_k / V_k - \\bar{T}_k / V_k = 0$. Along similar lines,\n$$\n\\begin{aligned}\nB_2 &= \\left(\\mathbf{T}_k - \\bar{T}_k \\mathbf{1}_k\\right)' \\left[\\mathbf{S}_k^{-1} - \\mathbf{S}_k^{-1} \\mathbf{1}_k \\left(\\frac{1}{\\tau^2} + \\frac{1}{V_k}\\right)^{-1} \\mathbf{1}_k'\\mathbf{S}_k^{-1}\\right] \\mathbf{1}_k \\left(\\bar{T}_k - \\mathbf{x}_k \\boldsymbol\\beta\\right) \\\\ \n&= \\left(\\mathbf{T}_k - \\bar{T}_k \\mathbf{1}_k\\right)' \\mathbf{S}_k^{-1}\\mathbf{1}_k \\left(\\bar{T}_k - \\mathbf{x}_k \\boldsymbol\\beta\\right) - \\left(\\mathbf{T}_k - \\bar{T}_k \\mathbf{1}_k\\right)' \\mathbf{S}_k^{-1} \\mathbf{1}_k \\left(\\frac{1}{\\tau^2} + \\frac{1}{V_k}\\right)^{-1} \\mathbf{1}_k'\\mathbf{S}_k^{-1}\\mathbf{1}_k \\left(\\bar{T}_k - \\mathbf{x}_k \\boldsymbol\\beta\\right) \\\\\n&= 0.\n\\end{aligned}\n$$\nFinally, the third term simplifies using @eq-inversevariance:\n$$\nB_3 = \\frac{\\left(\\bar{T}_k - \\mathbf{x}_k \\boldsymbol\\beta\\right)^2}{\\tau^2 + V_k}.\n$$\nThus, the full $B$ term reduces to\n$$\nB = \\left(\\mathbf{T}_k - \\bar{T}_k \\mathbf{1}_k\\right)' \\mathbf{S}_k^{-1}\\left(\\mathbf{T}_k - \\bar{T}_k \\mathbf{1}_k\\right) + \\frac{\\left(\\bar{T}_k - \\mathbf{x}_k \\boldsymbol\\beta\\right)^2}{\\tau^2 + V_k}\n$$\nand the multivariate log likelihood contribution is\n$$\n\\begin{aligned}\nl^{MV}_k\\left(\\boldsymbol\\beta, \\tau^2\\right) &= -\\frac{1}{2} \\log(\\tau^2 + V_k) + \\frac{1}{2} \\log(V_k) - \\frac{1}{2}\\log \\left|\\mathbf{S}_k\\right| - \\frac{1}{2} \\left(\\mathbf{T}_k - \\bar{T}_k \\mathbf{1}_k\\right)' \\mathbf{S}_k^{-1}\\left(\\mathbf{T}_k - \\bar{T}_k \\mathbf{1}_k\\right) -\\frac{1}{2} \\frac{\\left(\\bar{T}_k - \\mathbf{x}_k \\boldsymbol\\beta\\right)^2}{\\tau^2 + V_k} \\\\\n&= l^U_k\\left(\\boldsymbol\\beta, \\tau^2\\right) + \\frac{1}{2} \\log(V_k) - \\frac{1}{2}\\log \\left|\\mathbf{S}_k\\right| - \\frac{1}{2} \\left(\\mathbf{T}_k - \\bar{T}_k \\mathbf{1}_k\\right)' \\mathbf{S}_k^{-1}\\left(\\mathbf{T}_k - \\bar{T}_k \\mathbf{1}_k\\right).\n\\end{aligned}\n$$\nThe last three terms depend on the data ($\\mathbf{T}_k$ and $\\mathbf{S}_k$) but not on the parameters $\\boldsymbol\\beta$ or $\\tau^2$. Therefore, the univariate and multivariate likelihoods will be maximized at the same parameter values, i.e., the FML estimators are identical.\n\n## Restricted likelihood \n\nIn practice, it is more common to use RML estimation rather than FML.\nThe RML estimators maximize a different objective function that includes the full likelihood, plus an additional term. The RML objective function for the univariate model is\n$$\n\\sum_{k=1}^K l^U_k(\\boldsymbol\\beta, \\tau^2) - \\frac{1}{2} R^U(\\tau^2)\n$$\nwhere \n$$\nR^U(\\tau^2) = \\log \\left|\\sum_{k=1}^k\\frac{\\mathbf{x}_k' \\mathbf{x}_k}{\\tau^2 + V_k} \\right|.\n$$\nFor the multivariate model, the RML objective is\n$$\n\\sum_{k=1}^K l^{MV}_k(\\boldsymbol\\beta, \\tau^2) - \\frac{1}{2} R^{MV}(\\tau^2).\n$$\nwhere\n$$\n\\begin{aligned}\nR^{MV}(\\tau^2) &= \\log \\left|\\sum_{k=1}^k \\mathbf{x}_k'\\mathbf{1}_k'\\left(\\tau^2\\mathbf{1}_k\\mathbf{1}_k' + \\mathbf{S}_k\\right)^{-1}\\mathbf{1}_k \\mathbf{x}_k \\right|\\\\\n&= \\log \\left|\\sum_{k=1}^k\\frac{\\mathbf{x}_k' \\mathbf{x}_k}{\\tau^2 + V_k} \\right| \\\\\n&= R^U(\\tau^2)\n\\end{aligned}\n$$\nbecause of @eq-inversevariance. Thus, the univariate and multivariate models also have the same RML estimators.\n\n# So what?\n\nBeyond being a good excuse to write a bunch of matrix algebra, why does any of this matter? I think there are two main implications. First, it is useful to recognize the equivalence of these models in order to understand when the multivariate model is _necessary_. If both of the conditions that I've described hold, then it is entirely acceptable to use aggregation rather than the more complicated multivariate model.[^reviewer2] Using the simpler univariate model might be desirable in practice because it makes the analysis easier to follow, because it makes it easier to run diagnostics or create illustrations of the results, or because of software limitations. Conversely, if either of the conditions does not hold, then there may be differences between the two approaches and the analyst will need to think carefully about which method better addresses their research questions.\n\n[^reviewer2]: As RVE and MLMA become more wide-spread, I could imagine it happening that a meta-analyst who uses aggregation and a univariate model might get push-back from a reviewer, who uncritically recommends using a \"more advanced\" method to handle dependence. The results in this post provide a way for the meta-analyst to establish that doing so would be unnecessary. \n\nA second implication is computational: because it gives the same results, the univariate model could be used as a short-cut for fitting the multivariate model. Compare the differences in computational time:\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(microbenchmark)\nmicrobenchmark(\n  uni = rma.uni(es ~ college + males, vi = var, \n                data = corrdat_agg, method = \"REML\"),\n  multi = rma.mv(effectsize ~ college + males, V = V_list, \n                 random = ~ 1 | studyid,\n                 data = corrdat, method = \"REML\")\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nUnit: milliseconds\n  expr     min      lq      mean  median      uq      max neval\n   uni  6.2403  6.8699  8.666934  7.5398  9.6606  18.2778   100\n multi 55.9095 62.5514 72.086832 68.8157 73.4212 208.4049   100\n```\n\n\n:::\n:::\n\nIf the aggregation is done in advance, it is _way_ quicker to fit the univariate model. The short-cut would be useful if we needed to estimate _lots_ of multi-variate meta-regressions (as long as the equivalence conditions hold). For example, if we needed to bootstrap the multivariate model, we could pre-compute the aggregated effects and then just bootstrap the much simpler, much quicker univariate model. \n\nI suspect that the results I've presented here can be further generalized, but this will need a bit of further investigation. For one, there are also equivalences between variance estimators: using the CR2 cluster-robust variance estimator for the multivariate model is equivalent to using the HC2 heteroskedasticity-robust variance estimator for the univariate model with aggregated effects.[^RVE]\nFor another, the same sort of equivalence relationships hold even if there are additional random effects in the model, so long as the random effects are at the study level or higher levels of aggregation (e.g., lab effects, where labs are nested within studies).\nI'll leave these generalizations as exercises for a future rainy day.\n\n[^RVE]: Here's verification with the computational example from above:\n\n    ::: {.cell}\n    \n    ```{.r .cell-code}\n    # multivariate CR2\n    coef_test(MV_fit, vcov = \"CR2\")\n    ```\n    \n    ::: {.cell-output .cell-output-stdout}\n    \n    ```\n       Coef. Estimate      SE t-stat d.f. (Satt) p-val (Satt) Sig.\n     intrcpt  0.64656 0.17647   3.66        11.5      0.00345   **\n     college  0.37027 0.18648   1.99        11.9      0.07053    .\n       males -0.00763 0.00287  -2.66        14.5      0.01826    *\n    ```\n    \n    \n    :::\n    \n    ```{.r .cell-code}\n    # univariate HC2\n    coef_test(uni_fit, vcov = \"CR2\", cluster = corrdat_agg$studyid)\n    ```\n    \n    ::: {.cell-output .cell-output-stdout}\n    \n    ```\n       Coef. Estimate      SE t-stat d.f. (Satt) p-val (Satt) Sig.\n     intrcpt  0.64656 0.17622   3.67        11.5      0.00342   **\n     college  0.37027 0.18597   1.99        11.9      0.06985    .\n       males -0.00763 0.00287  -2.66        14.5      0.01808    *\n    ```\n    \n    \n    :::\n    :::\n\n\n\n# References\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}