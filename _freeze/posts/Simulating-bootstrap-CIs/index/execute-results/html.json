{
  "hash": "1e38561fa70c2d8e964052ddda8dde50",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: Simulating bootstrap confidence intervals\nsubtitle: Save your booties by extrapolating!\ndate: '2024-07-01'\ncategories:\n- programming\n- Rstats\n- bootstrap\ncode-fold: show\ncode-tools: true\ntoc: true\nbibliography: \"references.bib\"\ncsl: \"../apa.csl\"\n---\n\n\nBootstrapping is a brilliant statistical idea.\nI realize that it's become pretty routine, these days, and maybe has lost some of the magical veneer it once had, but I will fess up that I am still a fanboy.\nI teach my students to think of bootstrapping as your back-pocket tool for inference: if you're not sure how to get a standard error or a confidence interval for some quantity that you've estimated, bootstrapping is probably a good place to start---and possibly good enough.\n\nThe core idea behind bootstrap inference is to use simulation to emulate the process of collecting a sample, and to take the variation across simulation results as a proxy for the sampling uncertainty in your real data.\nA sample of $B$ simulated results (which we might call _booties_) form the basis of bootstrap standard errors, confidence intervals, hypothesis tests, and the like.\nThe simulation process can be as simple as re-sampling observations from your original data, in which case the original sample is a stand-in for the population and each re-sample is a replication of the process of collecting a dataset for analysis.\nOther simulation processes are possible too, which can account for more complex study designs or make stronger use of parametric modeling assumptions.\n\nAcross all its variations, bootstrapping involves using brute-force computation to do something that would otherwise take a bunch of math (like [delta-method](/posts/Multivariate-delta-method/) calculations).\nThis same feature makes it a royal pain to study bootstrap techniques using Monte Carlo simulation.\nIn a Monte Carlo simulation, one has to generate hypothetical data, apply an estimator to each simulated dataset, and then repeat the whole process $R$ times (for some large $R$) to see the sampling distribution of the estimator.\nBut if the estimator itself requires doing a bunch of re-sampling (or some other form of simulation) just to get one confidence interval, then the computation involved in the whole process can be pretty demanding.\nIn this post, I'll look at a trick that can make the whole process a bit more efficient.\n\n@boos2000MonteCarloEvaluation proposed a technique for approximating the power of a $B = \\infty$ bootstrap test, while using only a finite, feasibly small number of replicates. \nTheir work focused on hypothesis tests, but the approach applies directly to confidence interval simulations as well. \nThe trick is to calculate the bootstrap test several times for each replication, each time with a different number of booties, and then extrapolate to $B = \\infty$.\n\n# Simulating a bootstrap\n\n\n::: {.cell}\n\n:::\n\n\nSuppose we're doing a simulation to evaluate the properties of a some sort of bootstrap confidence interval, where the main performance characteristics of interest are the coverage rate and the average width of the intervals. \nThe simulation process will need to be repeated $R$ times, and $R$ will need to be fairly large to control the Monte Carlo error in the simulation results (e.g., to ensure that Monte Carlo standard errors for coverage rates will be less than 1%, we'll need to run at least 2500 replications). \nFor each of those replications, we'll need to do $B$ bootstrap replications (ahem,  _booties_!) to obtain a confidence interval.\nOn top of all that, we will also often want to repeat the whole process across a number of different conditions.\nFor example, the first simulation in  @joshiClusterWildBootstrapping2022 involved a $4 \\times 2 \\times 2 \\times 11$ factorial design (176 total conditions), with $R = 2400$ replications per condition and $B = 399$ booties per replication.\nIt quickly adds up---or rather, it quickly multiplies out!\n\nA challenge here is that the power of a bootstrap hypothesis test depends on $B$ [as discussed in @davidson2000BootstrapTestsHow]; by extension, so too does the coverage rate and width of a bootstrap confidence interval. \nIf we use only a small number of booties, then the simulation results will under-state coverage relative to what we might obtain when applying the method in practice (where we might use something more like $B = 1999$ or $3999$)\nBut if we use a realistically large number of booties, then our simulations will take forever to finish...or we'll have to cut down on the number of conditions include...or both...or maybe we'll decide that the delta-method math isn't so bad after all.\n\n# Extrapolating\n\nNow imagine that we generate $B$ booties, but then take several simple random sub-samples from the booties, of size $B_1 < B_2 <... < B_P = B$.\nEach of these is a valid bootstrap sample, but using a smaller number of booties.\nWe can calculate a confidence interval for each one, so we end up with a total of $P \\times R$ confidence intervals. \nLet $C_{br}$ be an indicator for whether replication $r$ of the confidence interval with $b$ booties covers the true parameter, for $b = B_1,B_2,...,B_P$ and $r = 1,...,R$. \nWe can extrapolate the coverage rate using a linear regression[^functional-form] of $C_{br}$ on $1 / b$:\n$$\nC_{br} = \\alpha_0 + \\alpha_1 \\frac{1}{b} + \\epsilon_{br}.\n$$\nThen $\\alpha_0$ is the coverage rate when $b = \\infty$.\nThe outcomes $C_{B_1r},...,C_{B_P r}$ will be dependent because they're based on the same set of booties. \nHowever, the replicates are independent, so we can just use cluster-robust variance estimators to get Monte Carlo standard errors for an estimate of $\\alpha_0$.\nEssentially the same thing works for extrapolating the width of a $B = \\infty$ confidence interval: just replace $C_{br}$ with $W_{br}$, the width of the confidence interval based on the $b^{th}$ bootstrap subsample from replication $r$.\n\n[^functional-form]: @boos2000MonteCarloEvaluation provided some technical justification for using linear extrapolation rather than some other type of curve. They also reported that linear extrapolation worked better than a quadratic model in their numerical experiments.\n\nIn the context of hypothesis testing problems, @boos2000MonteCarloEvaluation actually proposed something a little bit more complicated than what I've described above.\nInstead of just taking a single sub-sample of each size $B_1,...,B_P$, they note that that expected rejection rate for each size sub-sample can be computed using hypergeometric probabilities.\nThe same thing works for computing confidence interval coverage, but I don't see any way to make it work for confidence interval width.\nHowever, instead of computing just one sub-sample of size $b$, we could compute several and then average across them to approximate the expected value of $C_{br}$ or $W_{br}$ given the full set of $B$ booties. \n\n# An example\n\nHere's an example of how this all works. For sake of simplicity (and compute time), I'm going to use an overly simple, not particularly well-motivated example of bootstrapping: using a percentile bootstrap for the population mean, where the estimator is a trimmed sample mean. As a data-generating process, I'll use a shifted $t$ distribution with population mean $\\mu$ and degrees of freedom $\\nu$:\n$$\nY_1,...,Y_N \\stackrel{iid}{\\sim} \\mu + t(\\nu)\n$$\nOr in R code:\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(20240629)\nN <- 50\nmu <- 2\nnu <- 5\ndat <- mu + rt(N, df = nu)\n```\n:::\n\nAs an estimator, I'll use the 10% trimmed mean:\n\n::: {.cell}\n\n```{.r .cell-code}\ny_bar_trim <- mean(dat, trim = 0.1)\n```\n:::\n\nA bootstrap confidence interval for $\\mu$ can be computed as follows:\n\n::: {.cell}\n\n```{.r .cell-code}\nB <- 399\nbooties <- replicate(B, {\n  sample(dat, size = N, replace = TRUE) |> \n  mean(trim = 0.1)\n})\nquantile(booties, c(.025, .975))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n    2.5%    97.5% \n1.491885 2.255334 \n```\n\n\n:::\n:::\n\nNow, instead of computing just the one CI based on all $B$ booties, I'll also compute confidence intervals for several smaller subsamples from the booties:\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\n\nB_vals <- c(39, 59, 79, 99, 199, 399) # sub-sample sizes\nm <- 10 # number of CIs per B_val\n\nsub_boots <- map(B_vals, \\(x) {\n  if (x == length(booties)) m <- 1L\n  replicate(m , {\n    sample(booties, size = x, replace = TRUE) |>\n    quantile(c(.025, .975))\n  }, simplify = FALSE)\n})\n```\n:::\n\nThe result in `sub_boots` is a list with one entry per sub-sample size, where each entry includes $m = 10$ bootstrap confidence intervals. \n\nNow I'll compute the expected coverage rate and interval width for each of the `B_vals`:\n\n::: {.cell}\n\n```{.r .cell-code}\nboot_coverage <- map_dbl(sub_boots, \\(x) {\n  map_dbl(x, \\(y) y[1] < mu & mu < y[2]) |>\n    mean()\n})\nboot_width <- map_dbl(sub_boots, \\(x) {\n  map_dbl(x, diff) |>\n    mean()\n})\nboot_performance <- data.frame(\n  B = B_vals,\n  coverage = boot_coverage,\n  width = boot_width\n)\nboot_performance\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n    B coverage     width\n1  39        1 0.7400180\n2  59        1 0.7680832\n3  79        1 0.7214260\n4  99        1 0.7424530\n5 199        1 0.7519438\n6 399        1 0.7809795\n```\n\n\n:::\n:::\n\n\n## Across replications\n\nThe above is just for a single realization of the data-generating process, so it's not particularly revealing. More interesting is if we replicate the whole process a bunch of times:\n\n::: {.cell}\n\n```{.r .cell-code}\ndgp <- function(N, mu, nu) {\n  mu + rt(N, df = nu)\n}\n\nestimator <- function(\n    dat, # data\n    B_vals = 399, # number of booties to evaluate\n    m = 1, # CIs to replicate per sub-sample size\n    trim = 0.1 # trimming percentage\n  ) {\n  \n  # compute booties\n  N <- length(dat)\n  booties <- replicate(max(B_vals), {\n    sample(dat, size = N, replace = TRUE) |> \n    mean(trim = 0.1)\n  })\n\n  # confidence intervals for each B_vals\n  sub_boots <- map(B_vals, \\(x) {\n    if (x == length(booties)) m <- 1L\n    replicate(m , {\n      sample(booties, size = x, replace = TRUE) |>\n      quantile(c(.025, .975))\n    }, simplify = FALSE)\n  })\n\n  # coverage rates\n  boot_coverage <- map_dbl(sub_boots, \\(x) {\n    map_dbl(x, \\(y) y[1] < mu & mu < y[2]) |>\n      mean()\n  })\n  \n  # CI widths\n  boot_width <- map_dbl(sub_boots, \\(x) {\n    map_dbl(x, diff) |>\n      mean()\n  })\n  \n  # put it all together\n  data.frame(\n    B = B_vals,\n    coverage = boot_coverage,\n    width = boot_width\n  )\n}\n\n# build a simulation driver function\nlibrary(simhelpers)\nsimulate_bootCIs <- bundle_sim(\n  f_generate = dgp,\n  f_analyze = estimator\n)\n\n# run the simulation\nres <- simulate_bootCIs(\n  reps = 2500,\n  N = 50,\n  mu = 2,\n  nu = 5,\n  B_vals = B_vals,\n  m = 10\n)\n```\n:::\n\nThe object `res` has 2500 replications of the bootstrapping process, where each replication has averaged coverage rates for $B_1 = 39,...,B_6 = 399$. \n\nSummarizing across replications results in the following:\n\n::: {.cell .column-body-outset}\n\n```{.r .cell-code  code-fold=\"true\"}\nsim_summary <- \n  res %>%\n  group_by(B) %>%\n  summarize(\n    coverage_M = mean(coverage),\n    coverage_SE = sd(coverage) / sqrt(n()),\n    width_M = mean(width),\n    width_SE = sd(width) / sqrt(n()),\n    .groups = \"drop\"\n  )\n\nqn <- qnorm(0.975)\n\nsim_summary %>%\n  pivot_longer(\n    c(ends_with(\"_M\"), ends_with(\"_SE\")), \n    names_to = c(\"metric\", \".value\"), \n    names_pattern = \"(.+)_(.+)\"\n  ) %>%\n  ggplot() + \n  aes(x = B, y = M) + \n  scale_x_continuous(transform = \"reciprocal\", breaks = c(B_vals, 9999)) + \n  expand_limits(x = 9999) + \n  geom_smooth(method = \"lm\", formula = y ~ x, fullrange = TRUE, se = FALSE) + \n  geom_pointrange(aes(ymin = M - qn * SE, ymax = M + qn * SE)) + \n  facet_wrap(~ metric, scales = \"free\") + \n  theme_minimal() + \n  labs(x = \"Bootstraps (B)\", y = \"\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-8-1.png){width=100%}\n:::\n:::\n\nThe graphs above suggest that linear extrapolation is quite reasonable, both for coverage and interval width. \nThe graphs also show that both metrics are meaningfully affected by the number of bootstraps. \n\n## Computing the extrapolation\n\nA literal approach to computing the extrapolated coverage rate and interval width is to fit a regression across all the replications and values of $b$:\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(clubSandwich)\nres$rep <- rep(1:2500, each = length(B_vals))\n\n# coverage extrapolation\nlm(coverage ~ I(1 / B), data = res) |>\n  conf_int(vcov = \"CR1\", cluster = res$rep, test = \"z\", coefs = \"(Intercept)\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n       Coef. Estimate      SE d.f. Lower 95% CI Upper 95% CI\n (Intercept)    0.946 0.00433  Inf        0.938        0.955\n```\n\n\n:::\n\n```{.r .cell-code}\n# width extrapolation\nlm(width ~ I(1 / B), data = res) |>\n  conf_int(vcov = \"CR1\", cluster = res$rep, test = \"z\", coefs = \"(Intercept)\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n       Coef. Estimate      SE d.f. Lower 95% CI Upper 95% CI\n (Intercept)    0.654 0.00182  Inf        0.651        0.658\n```\n\n\n:::\n:::\n\nThe calculation can be simplified a bit (or at least made a little more transparent?) by recognizing that $\\hat\\alpha_0$ is a simple average of the intercepts from each block-specific regression (call these $\\hat\\alpha_{0r}$ for $r = 1,..,R$) and that $\\hat\\alpha_{0r}$ is just a weighted average of the outcomes, with weights determined by $B_1,...,B_P$. \nThe weights are given by\n$$\nw_{br} = \\frac{1}{p} - \\frac{\\tilde{B}}{S_B} \\times \\left(\\frac{1}{b} - \\tilde{B}\\right), \n$$\nwhere $\\displaystyle{\\tilde{B} = \\frac{1}{p} \\sum_{b \\in \\{B_1,...,B_P\\}} \\frac{1}{b}}$ and $\\displaystyle{S_B = \\sum_{b \\in \\{B_1,...,B_P\\}} \\left(\\frac{1}{b} - \\tilde{B}\\right)^2}$. \nWith these weights, \n$$\n\\hat\\alpha_{0r} = \\sum_{b \\in \\{B_1,...,B_P\\}} w_{br} C_{br}\n$$\nand\n$$\n\\hat\\alpha_0 = \\frac{1}{R} \\sum_{r=1}^R \\hat\\alpha_{0r}\n$$\nwith (cluster-robust) standard error\n$$\nSE = \\sqrt{\\frac{1}{R} \\times \\frac{\\sum_{r=1}^R \\left(\\hat\\alpha_{0r} - \\hat\\alpha_0\\right)^2}{R - 1}}.\n$$\nHere's a function that does the above calculations:\n\n::: {.cell}\n\n```{.r .cell-code}\ncalc_CI_coverage_width <- function(res, B_vals) {\n  \n  # calculate wts for each replication\n  p <- length(B_vals)\n  Btilde <- mean(1 / B_vals)\n  x <- 1 / B_vals - Btilde\n  S_B <- as.numeric(crossprod(x))\n  B_wts <- 1 / p - x * Btilde / S_B\n  \n  res %>%\n    group_by(rep) %>%\n    # calculate alpha_0r per replication\n    summarize(\n      across(c(coverage, width), ~ sum(.x * B_wts)),\n      .groups = \"drop\"\n    ) %>%\n    # average over replications\n    summarize(\n      across(c(coverage, width), list(\n        M = ~ mean(.x),\n        SE = ~ sd(.x) / sqrt(n())\n      ))\n    )\n  \n}\n\ncalc_CI_coverage_width(res, B_vals = B_vals)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 Ã— 4\n  coverage_M coverage_SE width_M width_SE\n       <dbl>       <dbl>   <dbl>    <dbl>\n1      0.946     0.00433   0.654  0.00182\n```\n\n\n:::\n:::\n\n\n## Validation\n\nLet's see how the extrapolated coverage rate and interval width compare to the brute-force approach. I'll do this by re-running the simulations using $B = 2399$ booties, but not bothering with the extrapolation.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbig_booties <- simulate_bootCIs(\n  reps = 2500,\n  N = 50,\n  mu = 2,\n  nu = 5,\n  B_vals = 2399\n)\n\nbig_sim_summary <- \n  big_booties %>%\n    summarize(\n      across(c(coverage, width), list(\n        M = ~ mean(.x),\n        SE = ~ sd(.x) / sqrt(n())\n      ))\n    )\n\nbig_sim_summary\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  coverage_M coverage_SE  width_M    width_SE\n1     0.9516 0.004293058 0.655017 0.001779658\n```\n\n\n:::\n:::\n\n::: {.cell .column-body-outset}\n\n```{.r .cell-code  code-fold=\"true\"}\nbig_sim_summary %>%\n  mutate(B = 1999L) %>%\n  bind_rows(sim_summary) %>%\n  mutate(\n    est = if_else(B == 1999L, \"Brute-force\", \"Extrapolation\")\n  ) %>%\n  pivot_longer(\n    c(ends_with(\"_M\"), ends_with(\"_SE\")), \n    names_to = c(\"metric\", \".value\"), \n    names_pattern = \"(.+)_(.+)\"\n  ) %>%\n  ggplot() + \n  aes(x = B, y = M) + \n  scale_x_continuous(transform = \"reciprocal\", breaks = c(B_vals, 9999)) + \n  scale_color_manual(values = c(`Brute-force` = \"red\", `Extrapolation` = \"black\")) + \n  expand_limits(x = 9999) + \n  geom_smooth(aes(group = est), method = \"lm\", formula = y ~ x, fullrange = TRUE, se = FALSE, color = \"grey\") + \n  geom_pointrange(aes(ymin = M - qn * SE, ymax = M + qn * SE, color = est)) + \n  facet_wrap(~ metric, scales = \"free\") + \n  theme_minimal() + \n  theme(legend.position = c(0.9, 0.9)) + \n  labs(x = \"Bootstraps (B)\", y = \"\", color = \"\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-12-1.png){width=100%}\n:::\n:::\n\n::: {.cell}\n\n:::\n\n\nThe extrapolated coverage rate and interval width are consistent with the brute-force calculations. \nThe brute-force approach requires calculating $2399 \\times R$ booties, whereas the extrapolation requires only $399 \\times R$ booties. However, the number of confidence interval calculations is one per bootstrap, or $2399 \\times R$ in all for the brute-force calculation (in this example, that comes out to be 5\\,997\\,500), but $399 \\times R \\times \\left(m \\times (P - 1) + 1\\right)$ for the extrapolation approach (in this example, that comes out to 50\\,872\\,500). \nThus, this technique is only going to be worth the trouble if computing the booties takes much longer than doing the confidence interval calculations.\n\n# Notes and loose ends\n\nThe @boos2000MonteCarloEvaluation technique seems quite useful and worth knowing about if you're doing any sort of Monte Carlo simulations involving bootstrapping.\nI've provided a proof-of-concept code-through, but there's a certainly a few loose ends here. \nFor one, what's the right set of sub-sample bootie sizes? I've followed Boos and Zhang's suggestion, but it's not based on much of any theory. \nI've deviated a little bit from the original paper by using $m$ repeated sub-samples instead of implementing the probability calculations proposed in the original paper. Is there a cost to this? Not sure. \nIs $m = 10$ the right number to use? Again, not sure. \nSome further fiddling seems warranted. \n\nThis code-through also has me wondering about how the approach could be abstracted to make it easier to apply to new problems. It seems like the sort of thing that would fit well in [`simhelpers`](/software/simhelpers/), but this will take a bit more thought. ",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}