---
title: Power approximations for overall average effects in meta-analysis of dependent effect sizes
author:
- Mikkel H. Vembye
- James E. Pustejovsky
- Terri D. Pigott
date: 2022-10-17T00:00:00
publish_date: 2022-10-17T00:00:00
publication_type: Journal article
publication: Journal of Educational and Behavioral Statistics
volume: 48
issue: 1
pages: 70-102
categories:
- power
- meta-analysis
- dependent effect sizes
- robust variance estimation
image: Power difference between approximated and true (simulated) power versus approximated
  power for the C(H)E working models, across different methods of sampling study characteristics.
links:
- text: Journal
  url: https://doi.org/10.3102/10769986221127379
  fa-icon: newspaper
- text: Supplementary materials
  url: https://osf.io/q84dv/
- text: R package
  url: https://mikkelvembye.github.io/POMADE/
  fa-icon: gift
- fa-icon: lock-open
  text: Pre-Print
  url: https://osf.io/preprints/metaarxiv/6tp9y/
- fa-icon: code
  text: Code
  url: https://osf.io/yhkq4/

---


Meta-analytic models for dependent effect sizes have grown increasingly sophisticated over the last few decades, which has created challenges for a priori power calculations. We introduce power approximations for tests of average effect sizes based upon several common approaches for handling dependent effect sizes. In a Monte Carlo simulation, we show that the new power formulas can accurately approximate the true power of meta-analytic models for dependent effect sizes. Lastly, we investigate the Type I error rate and power for several common models, finding that tests using robust variance estimation provide better Type I error calibration than tests with model-based variance estimation. We consider implications for practice with respect to selecting a working model and an inferential approach.

