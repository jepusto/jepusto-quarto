
@article{Bartos2023robust,
	title = {Robust {Bayesian} meta-analysis: {Model}-averaging across complementary publication bias adjustment methods},
	volume = {14},
	doi = {10.1002/jrsm.1594},
	abstract = {Publication bias is a ubiquitous threat to the validity of meta-analysis and the accumulation of scientific evidence. In order to estimate and counteract the impact of publication bias, multiple methods have been developed; however, recent simulation studies have shown the methods' performance to depend on the true data generating process, and no method consistently outperforms the others across a wide range of conditions. Unfortunately, when different methods lead to contradicting conclusions, researchers can choose those methods that lead to a desired outcome. To avoid the condition-dependent, all-or-none choice between competing methods and conflicting results, we extend robust Bayesian meta-analysis and model-average across two prominent approaches of adjusting for publication bias: (1) selection models of p-values and (2) models adjusting for small-study effects. The resulting model ensemble weights the estimates and the evidence for the absence/presence of the effect from the competing approaches with the support they receive from the data. Applications, simulations, and comparisons to preregistered, multi-lab replications demonstrate the benefits of Bayesian model-averaging of complementary publication bias adjustment methods.},
	language = {en},
	number = {1},
	journal = {Research Synthesis Methods},
	author = {Bartoš, František and Maier, Maximilian and Wagenmakers, Eric-Jan and Doucouliagos, Hristos and Stanley, T. D.},
	year = {2023},
	pages = {99--116},
}


@article{Haaf2023does,
	title = {Does every study? {Implementing} ordinal constraint in meta-analysis},
	volume = {28},
	doi = {10.1037/met0000428},
	abstract = {The most prominent goal when conducting a meta-analysis is to estimate the true effect size across a set of studies. This approach is problematic whenever the analyzed studies have qualitatively different results; that is, some studies show an effect in the predicted direction while others show no effect and still others show an effect in the opposite direction. In case of such qualitative differences, the average effect may be a product of different mechanisms, and therefore uninterpretable. The first question in any meta-analysis should therefore be whether all studies show an effect in the same, expected direction. To tackle this question a model with ordinal constraints is proposed where the ordinal constraint holds each study in the set. This “every study” model is compared with a set of alternative models, such as an unconstrained model that predicts effects in both directions. If the ordinal constraints hold, one underlying mechanism may suffice to explain the results from all studies, and this result could be supported by reduced between-study heterogeneity. A major implication is then that average effects become interpretable. We illustrate the model comparison approach using Carbajal et al.’s (2021) meta-analysis on the familiar-word-recognition effect, show how predictor analyses can be incorporated in the approach, and provide R-code for interested researchers. As common in meta-analysis, only surface statistics (such as effect size and sample size) are provided from each study, and the modeling approach can be adapted to suit these conditions. (PsycInfo Database Record (c) 2023 APA, all rights reserved)},
	number = {2},
	journal = {Psychological Methods},
	author = {Haaf, Julia M. and Rouder, Jeffrey N.},
	year = {2023},
	pages = {472--487},
}


@book{Hedges1985statistical,
	address = {Orlando, FL},
	title = {Statistical Methods for Meta-Analysis},
	publisher = {Academic Press},
	author = {Hedges, Larry V and Olkin, Ingram},
	year = {1985},
}



@article{Pustejovsky2014converting,
	title = {Converting from d to r to z when the design uses extreme groups, dichotomization, or experimental control.},
	volume = {19},
	doi = {10.1037/a0033788},
	number = {1},
	journal = {Psychological Methods},
	author = {Pustejovsky, James E.},
	year = {2014},
	pages = {92--112},
}

@article{Pustejovsky2018testing,
	title = {Testing for funnel plot asymmetry of standardized mean differences},
	url = {http://doi.wiley.com/10.1002/jrsm.1332},
	doi = {10.1002/jrsm.1332},
	urldate = {2019-01-07},
	journal = {Research Synthesis Methods},
	author = {Pustejovsky, James E. and Rodgers, Melissa},
	year = {2018},
}
