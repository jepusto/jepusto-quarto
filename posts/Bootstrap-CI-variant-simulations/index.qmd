---
title: Bootstrap confidence interval variations
date: '2025-01-15'
categories:
- programming
- Rstats
- bootstrap
code-fold: show
code-tools: true
draft: true
toc: true
bibliography: "references.bib"
csl: "../apa.csl"
---

In my [previous post](/posts/Bootstrap-CI-variations/), I demonstrated some new brand-new additions to the [`simhelpers` package](https://meghapsimatrix.github.io/simhelpers/): utilities for calculating bootstrap confidence intervals. That post walked through validating that the functions are consistent with the results of other packages and walking through how they can be used to extrapolate confidence interval coverage rates using fewer bootstraps than one would want to see for a real data analysis.
In the course of illustrating the functions, I set up a small simulation study (with just a single set of parameters) to show how the extrapolation technique works.
In this post, I'm going to expand these simulations across a bigger set of conditions in a multi-factor simulation.
This exercise is mostly an excuse to showcase some of the other useful features of `simhelpers`---especially the `bundle_sim()` function for creating simulation drivers and the `evaluate_by_row()` function for executing simulations across a grid of parameter values. 

The goal of the simulation is to evaluate how the bootstrap CIs work for estimating the Pearson correlation from a bivariate t data-generating process.
I will compare the performance of four different confidence intervals for the correlation coefficient: 1) the Fisher-z interval, which is derived assuming bivariate normality of the measurements; 2) a percentile bootstrap CI; 3) a studentized bootstrap CI; and 4) Efron's bias-corrected-and-accelerated bootstrap CI.
For the bootstrap intervals, I will use my implementation of the @boos2000MonteCarloEvaluation extrapolation technique (which [I described in much greater detail here](/posts/Simulating-bootstrap-CIs/)).

```{r}
library(tidyverse)
library(simhelpers)
```

# Data-generating process

As a more-or-less arbitrary data-generating process, I'll look at a bivariate central $t$ distribution with scale matrix $\boldsymbol\Sigma = \left[\begin{array}{cc} 1 \ \rho \\ \rho \ 1 \end{array}\right]$ and degrees of freedom $\nu$:
$$
\left(\begin{array}{c} X_1 \\ X_2 \end{array}\right) \sim t_{MV} \left( \left[\begin{array}{c} 0 \\ 0 \end{array}\right], \left[\begin{array}{cc} 1 \ \rho \\ \rho \ 1 \end{array}\right], \nu  \right).
$$
The scale matrix of the multivariate $t$ is not exactly the same as its covariance, but $\rho$ nonetheless corresponds to the correlation between $X_1$ and $X_2$.

Here's a data-generating function that returns randomly generated samples based on the bivariate $t$ model:
```{r}
r_t_bi <- function(n, rho = 0, df = 8) {
  Sigma <- rho + diag(1 - rho, nrow = 2)
  mvtnorm::rmvt(n = n, sigma = Sigma, df = df)
}

dat <- r_t_bi(6, rho = 0.8, df = 10)
dat
```

# Estimation methods 

And the estimation function:
```{r}
cor_CI <- function(
  dat, 
  CI_type = "percentile", 
  B_vals = c(49, 99, 199, 299, 399)
) {
  
  # point estimate
  r_est <- cor(dat[,1], dat[,2])
  N <- nrow(dat)
  SE_r <- sqrt((1 - r_est^2)^2 / (N - 1))
  
  # Fisher z CI
  z <- atanh(r_est)
  SE_z <- 1 / sqrt(N - 3)
  CI_z <- z + c(-1, 1) * qnorm(0.975) * SE_z

  
  # empirical influence if needed
  if ("BCa" %in% CI_type) {
    jacks <- sapply(1:N, \(x) cor(dat[-x,1], dat[-x,2]))
    inf_vals <- r_est - jacks
  } else {
    inf_vals <- NULL
  }
  
  # bootstrap samples
  r_boot <- replicate(max(B_vals), {
    i <- sample(1:N, replace = TRUE, size = N)
    r <- cor(dat[i,1], dat[i,2])
    c(r, sqrt((1 - r^2)^2 / (N - 1)))
  })
  
  bs_CIs <- simhelpers::bootstrap_CIs(
    boot_est = r_boot[1,],
    boot_se = r_boot[2,],
    est = r_est,
    se = SE_r,
    influence = inf_vals,
    CI_type = CI_type,
    B_vals = B_vals,
    format = "wide-list"
  )
  
  tibble::tibble(
    r = r_est,
    SE = SE_r,
    lo = tanh(CI_z[1]),
    hi = tanh(CI_z[2]),
    w = mean(r_boot < r_est),
    a = sum(inf_vals^3) / (6 * sum(inf_vals^2)^1.5),
    bs_CIs = bs_CIs
  )
}

res <- cor_CI(dat, CI_type = c("percentile","student"))
res
```

# Performance measures

Now I'll summarize across replications to evaluate the coverage rates of each set of confidence intervals. For the Fisher intervals, `simhelpers::calc_coverage()` does the trick, but for the bootstrap intervals I need to use `simhelpers::extrapolate_coverage()` because each replication is a specially structured _set_ of multiple confidence intervals with different values of $B$.
I will implement these summary calculations with a function so that I can re-use it later:
```{r}
#| cache: true

eval_performance <- function(dat, rho = 0, B_target = 1999) {
  require(simhelpers, quietly = TRUE)
  
  dat |>
    dplyr::summarize(
      calc_absolute(estimates = r, true_param = rho, criteria = "bias"),
      calc_coverage(lower_bound = lo, upper_bound = hi, true_param = rho),
      extrapolate_coverage(
        CI_subsamples = bs_CIs, true_param = rho, 
        B_target = B_target,
        nested = TRUE, format = "long"
      )
    )
}

```

# Simulation driver

First, I'll revise `sim_cor()` to also wrap in the `eval_performance()` function:
```{r}
sim_cor <- bundle_sim(
  f_generate = r_t_bi, 
  f_analyze = cor_CI,
  f_summarize = eval_performance
)
```
This results in a "simulation driver" function, which takes model parameters as inputs, runs a full simulation, and returns a dataset with performance measures.
```{r}
sim_cor(
  reps = 10,
  n = 25, 
  rho = 0.75, 
  df = 8,
  CI_type = "percentile",
  B_vals = c(49,99,199,299,399),
  B_target = 1999
)
```

# Parameters to examine

```{r}

params <- expand_grid(
  n = seq(10,100,10),
  rho = seq(0.15,0.75,0.15),
  df = c(8,16,32)
)

params
```

# Execute 

After enabling parallel computing, I can execute the simulation using `evaluate_by_row()`:
```{r}
#| cache: true
library(future)
library(furrr)
plan(multisession)

res <- evaluate_by_row(
  params = params,
  sim_function = sim_cor,
  reps = 4000,
  CI_type = c("percentile","student","BCa"),
  B_vals = c(49,99,199,299,399),
  B_target = 1999,
  .options = furrr_options(seed = TRUE),
  .progress = TRUE
)
```

# Results

After a little further data-cleaning on the back end, we can see how the coverage rates of different intervals compare to each other across a range of parameter values and sample sizes. 
Here's a graph of the simulated coverage rates as a function of $n$:

```{r}
#| code-fold: true
#| fig-width: 10
#| fig-height: 6
#| out-width: 100%
#| column: page

Fisher_res <- 
  res %>%
  select(n, rho, df, coverage, width) %>%
  mutate(CI_type = "Fisher-z")

boot_res <- 
  res %>%
  select(n, rho, df, bootstraps, coverage = boot_coverage, width = boot_width) %>%
  unnest(c(bootstraps, coverage, width)) %>%
  filter(bootstraps == 1999) %>%
  select(-bootstraps)

CI_res <- 
  bind_rows(Fisher_res, boot_res) %>%
  mutate(
    rho_lab = paste("rho ==", rho),
    df_lab = factor(df, levels = c(8,16,32), labels = paste("nu ==", c(8,16,32))),
    CI_type = factor(CI_type, levels = c("Fisher-z","percentile","student","BCa"))
  )

ggplot(CI_res) + 
  aes(n, coverage, color = CI_type) + 
  facet_grid(df_lab ~ rho_lab, labeller = "label_parsed") + 
  scale_x_continuous(breaks = seq(20,100,20)) + 
  scale_y_continuous(limits = c(0.85,1.0), breaks = seq(0.9,1.0,0.05), expand = expansion(0, 0)) + 
  geom_hline(yintercept = 0.95, linetype = "dashed") + 
  geom_point() + geom_line() + 
  theme_minimal() + 
  theme(legend.position = "top") + 
  labs(x = "n", y = "Coverage rate", color = "")
```

For small degrees of freedom, the Fisher-z interval has below-nominal coverage and does not appear to improve as sample size gets better. 
This makes sense because the interval is derived under a bivariate normal model that is not consistent with the actual data-generating process unless the degrees of freedom are large.
With larger degrees of freedom $(\nu = 32)$, the Fisher-z has almost exactly nominal coverage.
Across degrees of freedom and true correlations, the percentile interval has below-nominal coverage for the smaller sample sizes but appears to get steadily better as sample size increases. 
The studentized interval performs remarkably well, with coverage very close to nominal even at very small sample sizes.
Rather curiously, the BCa interval pretty consistently has worse coverage than the (simpler) percentile interval, even for larger sample sizes. 
This is counter-intuitive to me because the BCa interval is supposed to have second-order accurate coverage [@efron1987better] whereas the percentile interval is only first-order accurate. 
The upshot of this is that one would expect the coverage rate of the BCa interval to improve more quickly than the percentile interval, but that doesn't seem to be the case here. 