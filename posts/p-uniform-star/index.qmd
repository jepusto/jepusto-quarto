---
title: p-uniform* is a three-parameter selection model
date: '2025-11-11'
categories:
- meta-analysis
- selective reporting
- estimation
code-fold: true
code-tools: true
toc: true
bibliography: "../selection-references.bib"
draft: true
csl: "../apa.csl"
---

[Robbie van Aert](http://robbievanaert.com/) and [Marcel van Assen](https://www.tilburguniversity.edu/staff/m-a-l-m-vanassen) have a new paper out on a method for estimating average effect size in meta-analyses subject to publication bias, called p-uniform\*. The method has been around for a while [the first version of their paper is from 2018, @VanAert2018correcting] but the paper is only now appearing in print.
The authors frame the p-uniform\* method as a generalization of an earlier proposal of theirs, called p-uniform, which was limited in that it applies only to meta-analytic data where all effect size estimates are statistically significant at the conventional $\alpha = .05$ level.
The new method improves upon p-uniform by using all of the data (including effect size estimates that are not statistically significant) and by allowing for heterogeneity in the distribution of effect size parameters from different samples.

In their new paper, the authors contrast p-uniform* with the well-known three-parameter selection model (3PSM), which is a special case of the [step-function selection model](/posts/step-function-selection-models/) introduced by @hedges1992modeling.
They note that both methods can be understood as selection models, in that they are based upon explicit assumptions about the process that determines whether an effect size estimate from a study is reported and available for meta-analysis.
Further, they argue that the new p-uniform* method is preferable to the 3PSM because it does not involve estimation of a selection weight parameter, and thus is more parsimonious:

> P-uniform* has the advantage over 3PSM that the weight in the selection model does not need to be estimated. These weights are known to be imprecisely estimated in more complex selection models than the selection model of 3PSM where multiple weights need to be estimated (Hedges & Vevea, 1996, 2005; Vevea & Woods, 2005). P-uniform\* only assumes that these weights are the same for the statistically significant and the same for the nonsignificant primary studiesâ€™ effect sizes, without requiring estimation of the weights. This makes p-uniform* a more parsimonious model than 3PSM with similar statistical properties as shown in our simulation studies [@VanAert2018correcting, p.25].

In this post, I offer a different interpretation of p-uniform\*.
Rather than being a "more parsimonious model" than the 3PSM, I think it is useful to understand p-uniform\* as _a different estimator_ of the average effect size and heterogeneity parameters in the 3PSM.
I will demonstrate that the maximum likelihood estimator of p-uniform\* is _exactly_ equivalent to the maximum likelihood estimator of 3PSM under the (admittedly artificial) condition that all effect sizes in the meta-analytic data have the same sampling variance. 
Outside of this condition, the methods are not exactly equivalent but will tend to produce very similar point estimates of average effect size and between-study heterogeneity because both are based on unbiased estimating equations. 
However, confidence intervals produced by each method will differ except when based on the same method of construction and when all effect sizes have the same sampling variance. 

# Notation

This discussion is going to get a little bit statistical, so buckle up, y'all. 
Consider a meta-analysis that includes $k$ independent effect size estimates, where $T_i$ and $\sigma_i$ denote the effect size estimate and corresponding standard error from study $i = 1,...,k$.
Let's say that statistical significance is defined based on the level $2 \times \alpha$ (which will usually be .05, so $\alpha = .025$).
I will assume that the effect sizes are defined so that positive effects are what would be theoretically expected, and I will call an effect size estimate _affirmative_ if it is statistically significant and in the expected direction.
Let $A_i$ be an indicator for whether effect size estimate $i$ is affirmative, so $A_i = I\left(\frac{T_i}{\sigma_i} > \Phi^{-1}(1 - \alpha)\right)$, where $\Phi^{-1}()$ is the standard normal quantile function.
I will also use $\Phi()$ to denote the cumulative function and $\phi()$ to denote the density function of the standard normal distribution.

# The three-parameter selection model

Before considering how p-uniform\* relates to the 3PSM, let me explain the latter model.
The three-parameter selection model makes two sets of assumptions.
First, it is assumed that if we could observe every effect size estimate ever produced, then those effect size estimates would follow a conventional random effects model with mean $\mu$ and between-study standard deviation $\tau$.
Second, it is assumed that the probability of observing a given estimate depends only on its sign and statistical significance. 
Specifically, the probability of observing a non-affirmative estimate differs from the probability of observing an affirmative estimate by a factor of $\lambda > 0$. 
If $\lambda < 1$, then there is selection in favor of affirmative (statistically significant) results, which produces upward bias in average effect size estimates from conventional meta-analytic methods. 

Under these assumptions, the distribution of an observed effect size estimate is what I call a "piece-wise normal" distribution, with density
$$
d_T(t, a_i | \sigma_i) = \frac{ \lambda^{1 - a_i}}{1 - (1 - \lambda)\beta_i} \times \frac{1}{\sqrt{\tau^2 + \sigma_i^2}}\phi\left(\frac{t - \mu}{\sqrt{\tau^2 + \sigma_i^2}}\right),
$$ {#eq-observed-effect-density}
where $a_i = I\left(\frac{t}{\sigma_i} > \Phi^{-1}(1 - \alpha)\right)$ and $\beta_i = \Phi\left(\frac{\sigma_i \Phi^{-1}(1 - \alpha) - \mu}{\sqrt{\tau^2 + \sigma_i^2}}\right)$.
Note that $\beta_i$ depends on $\mu$ and $\tau$ but not on $\lambda$.
From the density, the log-likelihood of the 3PSM given the observed effect size estimates is 
$$
\begin{aligned}
\mathcal{l}(\mu, \tau, \lambda) &= \sum_{i=1}^k \log d_T(t = T_i, a_i = A_i | \sigma_i) \\
&= - \frac{1}{2} \sum_{i=1}^k\left(\frac{(T_i - \mu)^2}{\tau^2 + \sigma_i^2} + \log\left(\tau^2 + \sigma_i^2\right)\right) \\
& \qquad \qquad \qquad + k_N \log \lambda - \sum_{i=1}^k \log\left(1 - (1 - \lambda) \beta_i\right)
\end{aligned}
$$ {#eq-3PSM-log-lik}
where $k_N = \sum_{i=1}^k (1 - A_i)$ is the number of non-affirmative effect size estimates observed.
The maximum likelihood estimators $\hat\mu$, $\hat\tau$, $\hat\lambda$ are the values that maximize $\mathcal{l}(\mu, \tau, \lambda)$. 

One way to compute maximum likelihood estimators is by _profiling_ in one or more of the variables. 
For instance, we can consider the profile log-likelihood of $\mu$ and $\tau$, which is the log-likelihood function for given values of $\mu$ and $\tau$, but maximized over $\lambda$.
With profiling, the maximum likelihood estimator of $\lambda$ is treated as a function of $\mu$ and $\lambda$, as
$$
\hat\lambda(\mu, \tau) = \arg \max_{\lambda > 0} \ \mathcal{l}(\mu, \tau, \lambda).
$$
The profile log-likelihood is then 
$$
\mathcal{l}_P(\mu, \tau) = \mathcal{l}(\mu, \tau, \hat\lambda(\mu, \tau))
$$ {#eq-profile-log-likelihood}
and the maximum likelihood estimators $\hat\mu$ and $\hat\tau$ are the values that maximize $l_P(\mu, \tau)$, with $\hat\lambda = \hat\lambda(\hat\mu, \hat\tau)$. 
The end result is the same as maximizing the whole log-likelihood all at once---it's just that in some cases, it's easier to maximize in pieces (especially if there is a simple way to compute $\hat\lambda(\mu, \tau)$).

For the three-parameter selection model, the maximum likelihood estimator of $\lambda$ can also be defined as the solution to the score equation (the derivative of $\mathcal{l}(\mu,\tau,\lambda)$ with respect to $\lambda$)
$$
k_A = \sum_{i=1}^k \frac{1 - \beta_i}{1 - (1 - \lambda) \beta_i}.
$$ {#eq-lambda-score-equation}
The quantity on the left is the observed number of affirmative effects, equal to $k_A = k - k_N = \sum_{i=1}^k A_i$.
The quantity on the right is the expected number of affirmative results in the observed sample based on the three-parameter selection model.
@eq-lambda-score-equation does not have a closed-form solution except in some simple special cases, but it can be solved readily with numerical algorithms.

# p-uniform\*

Now let me consider van Aert and van Assen's proposed method, p-uniform*.
The method is an estimator of $\mu$ and $\tau$, but defined by a different objective function that involves truncated normal distributions [Equation 3 of @VanAert2018correcting].
For the affirmative effects, the objective function involves the density of each effect size estimate, given that it is affirmative;
for the non-affirmative effects, it involves the density of each effect size estimate, given that it is not affirmative.
Taking the log of their objective function [Equation 4 of @VanAert2018correcting] gives
$$
\begin{aligned}
\mathcal{u}(\mu, \tau) &= - \frac{1}{2} \sum_{i=1}^k\left(\frac{(T_i - \mu)^2}{\tau^2 + \sigma_i^2} + \log\left(\tau^2 + \sigma_i^2\right)\right) \\
& \qquad \qquad \qquad - \sum_{i=1}^k \left[(1 - A_i) \log \beta_i +A_i \log(1 - \beta_i) \right].
\end{aligned}
$$ {#eq-pUs-objective}
The p-uniform estimators $\ddot\mu$ and $\ddot\tau$ are the quantities that maximize $\mathcal{u}(\mu, \tau)$.

The objective function $\mathcal{u}(\mu, \tau)$ can be derived directly from the three-parameter selection model. 
The truncated densities are the _conditional_ densities of the effect size estimates, given their affirmative or non-affirmative status.
Thus, $\mathcal{u}(\mu, \tau) = \sum_{i=1}^k \log d_T(t = T_i | a_i = A_i, \sigma_i)$, whereas the 3PSM log-likelihood uses the _unconditional_ density of the observed effect size estimates.
The p-uniform\* method is essentially the maximum likelihood estimator of $\mu$ and $\tau$, conditional on $A_1,...,A_k$.
Conditioning removes all of the information in the likelihood that relates to the selection parameter $\lambda$, so this parameter does not appear in $\mathcal{u}(\mu, \tau)$.

One might well ask why we should use estimators that condition on $A_1,...,A_k$....


# Equal standard errors

The simulations reported in @VanAert2018correcting are based on a data-generating process where all the effect size estimates have equal standard errors, so $\sigma_1 = \sigma_2 = \cdots = \sigma$.
This is a fairly artificial scenario that does not arise frequently in real meta-analysis, as far as I know.
However, it is nonetheless a useful case to consider because it makes the math much easier and cleaner.

In this scenario with equal standard errors, the estimators of $\mu$ and $\tau$ from p-uniform\* are exactly equal to the corresponding maximum likelihood estimators from the 3PSM.
To see this, observe that when all $\sigma_i$ are equal, then 
all the $\beta_i$'s become constant, with $\beta_i = \beta = \Phi\left(\frac{\sigma \Phi^{-1}(1 - \alpha) - \mu}{\sqrt{\tau^2 + \sigma^2}}\right)$ for $i = 1,...,k$.
The maximum likelihood estimator of $\lambda$ (@eq-lambda-score-equation) simplifies to
$$
\hat\lambda(\mu, \tau) = \frac{k_N (1 - \beta)}{k_A \beta}.
$$
The difference between the profile log-likelihood of the 3PSM (@eq-profile-log-likelihood) and the objective function of p-uniform\* (@eq-pUs-objective) can then be written as
$$
\begin{aligned}
\mathcal{l}_P(\mu, \tau) - \mathcal{l}_U(\mu, \tau) &= k_N \log \hat\lambda - k \log\left(1 - \beta + \hat\lambda \beta\right) \\
& \qquad \qquad + k_N \log(\beta) + k_A \log(1 - \beta) \\
&= k_N \log k_N - k_N \log k_A \\
& \qquad  + k_N \log(1 - \beta) - k_N \log(\beta) \\
& \qquad \qquad - k \log k - k \log(1 - \beta) + k \log(k_A) \\
& \qquad \qquad \qquad + k_N \log(\beta) + k_A \log(1 - \beta) \\
&= k_N \log k_N + k_A \log k_A - k \log k.
\end{aligned}
$$ {#eq-diff-SEeq}
The difference between the objective functions of the 3PSM and p-uniform\* is a _constant_ that does not depend on the model parameters. 
Consequently, the maxima of the functions must be equal, and the profile likelihood confidence intervals for $\mu$ and $\tau$ must be identical.

Several implications follow from this.
First, the equivalence of the p-uniform\* and 3PSM estimators explains why the performance of the point estimators for $\mu$ is so similar (in terms of bias and root mean-squared error) in the simulations of @VanAert2018correcting.
Second, the equivalence of the estimators runs counter to van Aert and van Assen's argument that p-uniform\* is more parsimonious and has fewer limitations.
If the estimators are the same, then clearly one cannot have advantages over the other.
Third, @VanAert2018correcting reported differences between p-uniform\* and 3PSM in the performance of the confidence intervals for $\mu$ in the main simulations.
However, given the equivalence of the objective functions, differences in the behavior of the confidence intervals must be attributable to differences in how they are constructed 
(profile likelihood confidence intervals for p-uniform\* but Wald-type confidence intervals for the 3PSM), rather than differences between the models.

# Varying standard errors

Except when the standard errors are all equal, the objective function of p-uniform\* is not exactly identical to the profile log-likelihood of the 3PSM. However, objective function of p-uniform\* produces unbiased estimating equations for $\mu$ and $\tau$. 
To see this, observe that another way to express the p-uniform\* estimator is as the solution to the system of equations that set the derivative of $\mathcal{u}(\mu, \tau)$ with respect to the parameters equal to zero. 
For p-uniform\*, these derivatives are
$$
\begin{aligned}
\frac{\partial \mathcal{u}}{\partial \mu} &=  \sum_{i=1}^k \left[ \frac{(T_i - \mu)}{\tau^2 + \sigma_i^2} + \frac{\phi(c_i)}{\sqrt{\tau^2 + \sigma_i^2}}\left(\frac{1 - \beta_i - A_i}{\beta_i (1 - \beta_i)}\right) \right] \\
\frac{\partial \mathcal{u}}{\partial \tau^2} &=  \frac{1}{2} \sum_{i=1}^k \left[ \left(\frac{(T_i - \mu)^2 - \left(\tau^2 + \sigma_i^2\right)}{\left(\tau^2 + \sigma_i^2\right)^2}\right) + \frac{c_i \phi(c_i)}{(\tau^2 + \sigma_i^2)} \left(\frac{1 - \beta_i - A_i}{\beta_i (1 - \beta_i)}\right)\right] 
\end{aligned}
$$ {#eq-pUs-scores}
where $c_i = \frac{\sigma_i \Phi^{-1}(1 - \alpha) - \mu}{\sqrt{\tau^2 + \sigma_i^2}}$.

[My previous post](/posts/step-function-selection-models/) gave expressions for the mean and variance of an observed effect size estimate under the step-function selection model.
For the case of a single-step model, these results imply that
$$
\begin{aligned}
\mathbb{E}(T_i - \mu) &= \sqrt{\tau^2 + \sigma_i^2} \times \frac{(1 - \lambda) \phi(c_i)}{1 - (1 - \lambda) \beta_i} \\
\mathbb{E}\left[(T_i - \mu)^2\right] &= \left(\tau^2 + \sigma_i^2\right) \times \left(1 + \frac{(1 - \lambda) c_i \phi(c_i)}{1 - (1 - \lambda) \beta_i}\right).
\end{aligned}
$$
Furthermore, 
$$
\mathbb{E}(A_i) = \frac{1 - \beta_i}{1 - (1 - \lambda)\beta_i}.
$$
Substituting these expressions into the expectations of @eq-pUs-scores leads to
$$
\begin{aligned}
\mathbb{E}\left(\frac{\partial \mathcal{u}}{\partial \mu}\right) &=  \sum_{i=1}^k \left[ \frac{\mathbb{E}(T_i - \mu)}{\tau^2 + \sigma_i^2} + \frac{\phi(c_i)}{\sqrt{\tau^2 + \sigma_i^2}}\left(\frac{1 - \beta_i - \mathbb{E}(A_i)}{\beta_i (1 - \beta_i)}\right) \right] \\
&= \sum_{i=1}^k \frac{\phi(c_i)}{\sqrt{\tau^2 + \sigma_i^2}}\left[ \frac{1 - \lambda}{1 - (1 - \lambda) \beta_i} - \frac{1 - \lambda}{\left(1 - (1 - \lambda)\beta_i\right)} \right] \\
&= 0
\end{aligned}
$$
and 
$$
\begin{aligned}
\mathbb{E}\left(\frac{\partial \mathcal{u}}{\partial \tau^2} \right) &=  \frac{1}{2} \sum_{i=1}^k \left[ \left(\frac{\mathbb{E}\left[(T_i - \mu)^2\right] - \left(\tau^2 + \sigma_i^2\right)}{\left(\tau^2 + \sigma_i^2\right)^2}\right) + \frac{c_i \phi(c_i)}{(\tau^2 + \sigma_i^2)} \left(\frac{1 - \beta_i - \mathbb{E}(A_i)}{\beta_i (1 - \beta_i)}\right)\right] \\
&= \frac{1}{2} \sum_{i=1}^k \frac{1}{(\tau^2 + \sigma_i^2)} \left[1 + \frac{(1 - \lambda)c_i \phi(c_i) }{1 - (1 - \lambda) \beta_i} - 1 - \frac{c_i \phi(c_i) (1 - \lambda)}{\left(1 - (1 - \lambda)\beta_i\right)}\right] \\
&= 0.
\end{aligned}
$$
Thus, the p-uniform\* estimator is based on unbiased estimating equations for $\mu$ and $\tau$.

van Aert and van Assen argue that p-uniform\* is advantageous because it does not entail estimation of the selection weight $\lambda$.
However, one could also take this as a deficiency, insofar as the p-uniform\* estimator does not yield information about the strength of selection. 
This deficiency could be remedied by borrowing the estimator of $\lambda$ from the step-function model, as $\ddot\lambda = \hat\lambda(\ddot\mu, \ddot\tau)$. 
This estimator can be computed by solving @eq-lambda-score-equation for $\lambda$ with $\beta_i$ evaluated at $\ddot\mu$ and $\ddot\tau$. 

# References
