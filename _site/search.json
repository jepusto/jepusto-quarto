[
  {
    "objectID": "posts.html",
    "href": "posts.html",
    "title": "Posts",
    "section": "",
    "text": "Distribution of the number of significant effect sizes\n\n\nIn a study reporting multiple outcomes\n\n\n\neffect size\n\n\ndistribution theory\n\n\n\n\n\n\n\n\n\nMar 28, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nApproximating the distribution of cluster-robust Wald statistics\n\n\n\n\n\n\nrobust variance estimation\n\n\ndistribution theory\n\n\n\n\n\n\n\n\n\nMar 24, 2024\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/Approximating-cluster-robust-Wald-tests/index.html",
    "href": "posts/Approximating-cluster-robust-Wald-tests/index.html",
    "title": "Approximating the distribution of cluster-robust Wald statistics",
    "section": "",
    "text": "In Tipton and Pustejovsky (2015), we examined several different small-sample approximations for cluster-robust Wald test statistics, which are like \\(F\\) statistics but based on cluster-robust variance estimators. These statistics are, frankly, kind of weird and awkward to work with, and the approximations that we examined were far from perfect. In this post, I will look in detail at the robust Wald statistic for a simple but common scenario: a one-way ANOVA problem with clusters of dependent observations. \\(\\def\\Pr{{\\text{Pr}}}\n\\def\\E{{\\text{E}}}\n\\def\\Var{{\\text{Var}}}\n\\def\\Cov{{\\text{Cov}}}\n\\def\\cor{{\\text{cor}}}\n\\def\\bm{\\mathbf}\n\\def\\bs{\\boldsymbol}\\)\nConsider a setup where clusters can be classified into one of \\(C\\) categories, with each cluster of observations falling into a single category. Let \\(\\bs\\mu = \\left[\\mu_c \\right]_{c=1}^C\\) denote the means of these categories. Suppose we have an estimator of those means \\(\\bs{\\hat\\mu} = \\left[\\hat\\mu_c\\right]_{c=1}^C\\) and a corresponding cluster-robust variance estimator \\(\\bm{V}^R = \\bigoplus_{c=1}^C V^R_c\\). Note that \\(\\bm{V}^R\\) is diagonal because the estimators for each category are independent. Assume that the robust variance estimator is unbiased so \\(\\E\\left(V^R_c\\right) = \\Var\\left( \\hat\\mu_c \\right) = \\psi_c\\) for \\(c = 1,...,C\\). Let \\(\\bs\\Psi = \\bigoplus_{c=1}^C \\psi_c\\).\nSuppose that we want to test the null hypothesis that that means of the categories are all equal, \\(H_0: \\mu_1 = \\mu_2 = \\cdots = \\mu_C\\). We can express this null using a \\(q \\times C\\) contrast matrix \\(\\bm{C} = \\left[-\\bm{1}_q \\ \\bm{I}_q \\right]\\), where \\(q = C - 1\\). The null hypothesis is then \\(\\bm{C} \\bs\\mu = \\bm{0}_q\\). The corresponding cluster-robust Wald statistic is \\[\nQ = \\bs{\\hat\\mu}' \\bm{C}' \\left(\\bm{C} \\bm{V}^R \\bm{C}'\\right)^{-1} \\bm{C} \\bs{\\hat\\mu}.\n\\] Under the null hypothesis, the distribution of \\(Q\\) will converge to a \\(\\chi^2_q\\) as the number of clusters in each category grows large. However, with a limited number of clusters in some of the categories, this approximate reference distribution is not very accurate and tests based on it can have wildly inflated type I error rates.\nIn the paper, we considered several different ways of approximating the distribution of \\(Q\\) that work at smaller sample sizes. One class of approaches to approximating the sampling distribution of \\(Q\\) is to use a Hotelling’s \\(T^2\\) distribution with degrees of freedom \\(\\eta\\). Given the degrees of freedom, Hotelling’s \\(T^2\\) is a multiple of an \\(F\\) distribution: \\[\n\\frac{\\eta - q + 1}{\\eta q} Q \\sim F(q, \\eta - q + 1).\n\\] The question is then how to determine \\(\\eta\\).\nSeveral of the approaches that we considered are based on representing the \\(Q\\) statistic as \\[\nQ = \\bm{z}' \\bm{D}^{-1} \\bm{z},\n\\] where \\(\\bs\\Omega = \\bm{C} \\bs\\Psi \\bm{C}'\\), \\(\\bm{z} = \\bs\\Omega^{-1/2}\\bm{C}\\hat\\mu_c\\), \\(\\bm{G} = \\bs\\Omega^{-1/2} \\bm{C}\\), and \\[\n\\bm{D} = \\bm{G} \\bm{V}^R \\bm{G}'.\n\\] The various approaches we considered involve different ways of approximating the sampling distribution of \\(\\bm{D}\\).\nOne of the approximations involves finding degrees of freedom \\(\\eta\\) by following a strategy suggested by Zhang (2012, 2013). These degrees of freedom are given by \\[\n\\eta_Z = \\frac{q(q + 1)}{\\sum_{s=1}^q \\sum_{t = 1}^q \\Var(d_{st})},\n\\] where \\(d_{st}\\) is the entry in row \\(s\\), column \\(t\\) of \\(\\bm{D}\\). To find \\(\\eta_Z\\), we can compute the denominator using general formulas given in the paper. However, with a bit of analysis we can find a much simpler expression for the special case of one-way ANOVA.\nBefore going further, it’s useful to observe that \\(\\bm{D}\\) is invariant to linear transformations of \\(\\bm{C}\\). In particular, an equivalent way to write the null hypothesis is as \\(H_0: \\bs\\Psi_{\\circ}^{-1/2} \\bm{C} = \\bm{0}_q\\), where \\(\\bs\\Psi_{\\circ} = \\bigoplus_{c=2}^C \\psi_c\\) is the diagonal of the true sampling variances of categories 2 through \\(C\\), omitting the first category. Thus, let me redefine \\[\n\\bs\\Omega = \\bs\\Psi_{\\circ}^{-1/2} \\bm{C} \\bs\\Psi \\bm{C}'\\bs\\Psi_{\\circ}^{-1/2},\n\\] \\(\\bm{z} = \\bs\\Omega^{-1/2}\\bs\\Psi_{\\circ}^{-1/2}\\bm{C}\\hat\\mu_c\\), and \\[\n\\bm{G} = \\bs\\Omega^{-1/2} \\bs\\Psi_{\\circ}^{-1/2} \\bm{C}.\n\\] This transformation of the constraint matrix will make it possible to find a closed-form expression for \\(\\bs\\Omega^{-1/2}\\).\nNow, observe that \\[\n\\begin{aligned}\n\\bs\\Omega &= \\bs\\Psi_{\\circ}^{-1/2} \\bm{C} \\bs\\Psi \\bm{C}'\\bs\\Psi_{\\circ}^{-1/2} \\\\\n&= \\bs\\Psi_{\\circ}^{-1/2} \\left(\\bs\\Psi_{\\circ} + \\psi_1 \\bm{1}_q \\bm{1}_q'\\right)\\bs\\Psi_{\\circ}^{-1/2} \\\\\n&= \\bm{I}_q + \\psi_1 \\bm{f} \\bm{f}',\n\\end{aligned}\n\\] where \\(\\bm{f} = \\bs\\Psi_{\\circ}^{-1/2} \\bm{1}_q = \\left[ \\psi_c^{-1/2}\\right]_{c = 2}^C\\). From the Woodbury identity, \\[\n\\bs\\Omega^{-1} = \\bm{I} - \\frac{1}{W} \\bm{f} \\bm{f}',\n\\] where \\(W = \\sum_{c=1}^C \\frac{1}{\\psi_c}\\).\nFasi, Higham, and Liu (2023) provide formulas for \\(p^{th}\\) roots of low-rank updates to scaled identity matrices. Their results provide a neat closed-form expression for \\(\\bs\\Omega^{-1/2}\\). From their Equation (1.9), \\[\n\\bs\\Omega^{-1/2} = \\mathbf{I}_q - \\kappa \\ \\bm{f} \\bm{f}',\n\\] where \\(\\kappa = \\frac{\\sqrt{\\psi_1}}{W \\sqrt{\\psi_1} + \\sqrt{W}}\\). Further, we can write the \\(q \\times C\\) matrix \\(\\bm{G}\\) as \\[\n\\begin{aligned}\n\\bm{G} &= \\bs\\Omega^{-1/2} \\bs\\Psi_{\\circ}^{-1/2} \\bm{C} \\\\\n&= \\left( \\mathbf{I}_q - \\kappa \\ \\bm{f} \\bm{f}' \\right) \\bs\\Psi_{\\circ}^{-1/2} \\left[-\\bm{1}_q, \\ \\bm{I}_q \\right] \\\\\n&= \\left[\\frac{\\kappa(W \\psi_1 - 1) - \\psi_1}{\\psi_1} \\bm{f},  \\left( \\mathbf{I}_q - \\kappa \\ \\bm{f} \\bm{f}' \\right) \\bs\\Psi_{\\circ}^{-1/2}\\right],\n\\end{aligned}\n\\] with entries given by \\[\ng_{sc} = \\begin{cases}\n\\frac{\\kappa(W \\psi_1 - 1) - \\psi_1}{\\psi_1 \\sqrt{\\psi_{s+1}}} & \\text{if} \\quad c = 1 \\\\\n\\frac{I(s+1 = c)}{\\sqrt{\\psi_{c}}} - \\frac{\\kappa}{\\psi_c \\sqrt{\\psi_{s+1}}} & \\text{if} \\quad c &gt; 1.\n\\end{cases}\n\\] Because \\(\\bm{D} = \\bm{G} \\bm{V}^R \\bm{G}'\\) and \\(\\bm{V}^R\\) is diagonal, we can write the entries of \\(\\bm{D}\\) as \\[\nd_{st} = \\sum_{c=1}^C g_{sc} g_{tc} V^R_c.\n\\] And because the variance estimators for each category are independent, \\[\n\\Var(d_{st}) = \\sum_{c=1}^C g_{sc}^2 g_{tc}^2 \\Var(V^R_c).\n\\] In prior work, we derived expressions for the Satterthwaite degrees of freedom for variances of average effect sizes, and the same formulas can be applied here with the category-specific \\(V^R_c\\). Let me write \\(\\nu_c = 2\\left[\\E(V^R_c)\\right]^2 / \\Var(V^R_c)\\) for the degrees of freedom corresponding to category \\(c\\). Then \\[\n\\Var(d_{st}) = 2 \\sum_{c=1}^C g_{sc}^2 g_{tc}^2 \\frac{\\psi_c^2}{\\nu_c}.\n\\] We can use this to obtain an expression for Zhang’s approximate degrees of freedom: \\[\n\\begin{aligned}\nq(q + 1)\\eta_Z^{-1} &= \\sum_{s=1}^q \\sum_{t = 1}^q \\Var(d_{st}) \\\\\n&= 2\\sum_{s=1}^q \\sum_{t = 1}^q \\sum_{c=1}^C g_{sc}^2 g_{tc}^2 \\frac{\\psi_c^2}{\\nu_c} \\\\\n&= 2\\sum_{c=1}^C \\frac{\\psi_c^2}{\\nu_c} \\left(\\sum_{s=1}^q g_{sc}^2\\right)^2.\n\\end{aligned}\n\\] Now, all we need to do is simplify… \\[\n\\begin{aligned}\n\\sum_{s=1}^q g_{s1}^2 &= \\sum_{s=1}^q \\frac{\\left(\\kappa(W \\psi_1 - 1) - \\psi_1\\right)^2}{\\psi_1^2 \\psi_{s+1}} \\\\\n&= \\frac{\\left(\\kappa(W \\psi_1 - 1) - \\psi_1\\right)^2}{\\psi_1^2} \\sum_{c=2}^C \\frac{1}{\\psi_{s+1}} \\\\\n&= \\frac{\\left(\\kappa(W \\psi_1 - 1) - \\psi_1\\right)^2}{\\psi_1^2} \\frac{(W \\psi_1 - 1)}{\\psi_1} \\\\\n&= \\text{...a bunch of tedious algebra...} \\\\\n&= \\frac{1}{\\psi_1^2} \\left(\\psi_1 - \\frac{1}{W}\\right)\n\\end{aligned}\n\\] and, for \\(c = 2,...,C\\), \\[\n\\begin{aligned}\n\\sum_{s=1}^q g_{sc}^2 &= \\sum_{s=1}^q \\left(\\frac{I(s+1 = c)}{\\sqrt{\\psi_{c}}} - \\frac{\\kappa}{\\psi_c \\sqrt{\\psi_{s+1}}}\\right)^2 \\\\\n&= \\frac{1}{\\psi_c} - \\frac{2 \\kappa}{\\psi_c^2} + \\frac{\\kappa^2}{\\psi_c^2}\\sum_{s=1}^q \\frac{1}{\\psi_{s+1}} \\\\\n&= \\frac{1}{\\psi_c} - \\frac{2 \\kappa}{\\psi_c^2} + \\frac{\\kappa^2}{\\psi_c^2}\\frac{(W \\psi_1 - 1)}{\\psi_1} \\\\\n&= \\text{...a bunch of tedious algebra...} \\\\\n&= \\frac{1}{\\psi_c^2} \\left(\\psi_c - \\frac{1}{W}\\right)\n\\end{aligned}\n\\] Thus, \\[\n\\begin{aligned}\nq(q + 1)\\eta_Z^{-1} &= 2\\sum_{c=1}^C \\frac{\\psi_c^2}{\\nu_c} \\left(\\sum_{s=1}^q g_{sc}^2\\right)^2 \\\\\n&= 2\\sum_{c=1}^C \\frac{1}{\\nu_c \\psi_c^2}\\left(\\psi_c - \\frac{1}{W}\\right)^2 \\\\\n&= 2\\sum_{c=1}^C \\frac{1}{\\nu_c}\\left(1 - \\frac{1}{\\psi_c W}\\right)^2\n\\end{aligned}\n\\] or, rearranging, \\[\n\\eta_Z = \\frac{C(C - 1)}{2 \\sum_{c=1}^C \\frac{1}{\\nu_c}\\left(1 - \\frac{1}{\\psi_c W}\\right)^2}.\n\\] It’s a surprisingly clean formula! Once these degrees of freedom are calculated, the degrees of freedom for the reference \\(F\\) distribution would be \\(q\\) and \\(\\eta_Z - q + 1\\).\nIn the paper, we also considered two other degrees of freedom approximations, which involve not only the variances of \\(d_{st}\\) but also the covariances between entries. In principle, one could follow similar algebra to get expressions for these other degrees of freedom as well. However, our simulations indicated that the other degrees of freedom approximations tend to be overly conservative and produce type-I error rates way below the nominal level (essentially, hardly ever rejecting the null) and less accurate than HTZ. So, there’s not much reason to work through them unless you find algebra enjoyable for its own sake.\nA further question about this cluster-robust Wald statistic is how to approximate its sampling distribution under specific alternative hypotheses. In other words, given a vector of means \\(\\mu_1,...,\\mu_C\\) where the null does not hold, plus some information to determine \\(\\psi_c\\) and \\(\\nu_c\\) for \\(c = 1,...,C\\), how could we approximate the distribution of \\(Q\\)? We need something like a non-central Hotelling’s \\(T^2\\) distribution…"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "James E. Pustejovsky",
    "section": "",
    "text": "I am a statistician and associate professor in the School of Education at the University of Wisconsin-Madison, where I teach in the Educational Psychology Department and the graduate program in Quantitative Methods. My research involves developing statistical methods for problems in education, psychology, and other areas of social science research, with a focus on methods related to research synthesis and meta-analysis."
  },
  {
    "objectID": "index.html#education",
    "href": "index.html#education",
    "title": "James E. Pustejovsky",
    "section": "Education",
    "text": "Education\nNorthwestern University | Evanston, IL\nPhD in Statistics | 2013\nBoston College | Chestnut Hill, MA\nBA in Economics | 2003"
  },
  {
    "objectID": "index.html#interests",
    "href": "index.html#interests",
    "title": "James E. Pustejovsky",
    "section": "Interests",
    "text": "Interests\n\nMeta-analysis\nCausal inference\nRobust statistical methods\nEducation statistics\nSingle case experimental designs"
  },
  {
    "objectID": "posts/Distribution-of-signficant-effects/index.html",
    "href": "posts/Distribution-of-signficant-effects/index.html",
    "title": "Distribution of the number of significant effect sizes",
    "section": "",
    "text": "A while back, I posted the outline of a problem about the number of significant effect size estimates in a study that reports multiple outcomes. This problem interests me because it connects to the issue of selective reporting of study results, which creates problems for meta-analysis. Here, I’ll re-state the problem in slightly more general terms and then make some notes about what’s going on.\nConsider a study that assesses some effect size across \\(m\\) different outcomes. (We’ll be thinking about one study at a time here, so no need to index the study as we would in a meta-analysis problem.) Let \\(T_i\\) denote the effect size estimate for outcome \\(i\\), let \\(V_i\\) denote the sampling variance of the effect size estimate for outcome \\(i\\), and let \\(\\theta_i\\) denote the true effect size parameter for corresponding to outcome \\(i\\). Assume that the study outcomes \\(\\left[T_i\\right]_{i=1}^m\\) follow a correlated-and-hierarchical effects model, in which \\[T_i = \\mu + u + v_i + e_i,\\] where the study-level error \\(u \\sim N\\left(0,\\tau^2\\right)\\), the effect-specific error \\(v_i \\stackrel{iid}{\\sim} N\\left(0, \\omega^2\\right)\\), and the vector of sampling errors \\(\\left[e_i\\right]_{i=1}^m\\) is multivariate normal with mean \\(\\mathbf{0}\\), known variances \\(\\text{Var}(e_i) = \\sigma^2\\), and compound symmetric correlation structure \\(\\text{cor}(e_h, e_i) = \\rho\\).\nDefine \\(A_i\\) as an indicator that is equal to one if \\(T_i\\) is statistically significant at level \\(\\alpha\\) based on a one-sided test, and otherwise equal to zero. (Equivalently, let \\(A_i\\) be equal to one if the effect is statistically significant at level \\(2 \\alpha\\) and in the theoretically expected direction.) Formally, \\[A_i = I\\left(\\frac{T_i}{\\sigma} &gt; q_\\alpha \\right)\\] where \\(q_\\alpha = \\Phi^{-1}(1 - \\alpha)\\) is the critical value from a standard normal distribution (e.g., \\(q_{.05} = 1.645\\), \\(q_{.025} = 1.96\\)). Let \\(N_A = \\sum_{i=1}^m A_i\\) denote the total number of statistically significant effect sizes in the study. The question is: what is the distribution of \\(N_A\\)."
  },
  {
    "objectID": "posts/Distribution-of-signficant-effects/index.html#compound-symmetry-to-the-rescue",
    "href": "posts/Distribution-of-signficant-effects/index.html#compound-symmetry-to-the-rescue",
    "title": "Distribution of the number of significant effect sizes",
    "section": "Compound symmetry to the rescue",
    "text": "Compound symmetry to the rescue\nAs I noted in the previous post, this set-up means that the effect size estimates have a compound symmetric distribution. We can make this a bit more explicit by writing the sampling errors in terms of the sum of a component that’s common acrosss outcomes and a component that’s specific to each outcome. Thus, let \\(e_i = f + g_i\\), where \\(f \\sim N\\left(0, \\rho \\sigma^2 \\right)\\) and \\(g_i \\stackrel{iid}{\\sim} N \\left(0, (1 - \\rho) \\sigma^2\\right)\\). Let me also define \\(\\zeta = \\mu + u + f\\) as the conditional mean of the effects. It then follows that the effect size estimates are conditionally independent, given the common components: \\[\n\\left(T_i | \\zeta \\right) \\stackrel{iid}{\\sim} N\\left(\\zeta, \\omega^2 + (1 - \\rho) \\sigma^2\\right)\n\\] Furthermore, the conditional probability of a significant effect is \\[\n\\text{Pr}(A_i = 1 | \\zeta) = \\Phi\\left(\\frac{\\zeta - q_{\\alpha} \\sigma}{\\sqrt{\\omega^2 + (1 - \\rho)\\sigma^2}}\\right)\n\\] and \\(A_1,...,A_m\\) are mutually independent, conditional on \\(\\zeta\\). Therefore, the conditional distribution of \\(N_A\\) is binomial, \\[\n\\left(N_A | \\zeta\\right) \\sim Bin(m, \\pi)\n\\] where \\[\n\\pi = \\Phi\\left(\\frac{\\zeta - q_{\\alpha} \\sigma}{\\sqrt{\\omega^2 + (1 - \\rho)\\sigma^2}}\\right).\n\\] What about the unconditional distribution?\nTo get rid of the \\(\\zeta\\), we need to integrate over its distribution, which leads to \\[\n\\text{Pr}(N_A = a) = \\text{E}\\left[\\text{Pr}\\left(N_A | \\zeta\\right)\\right] = \\int f_{N_A}\\left(a | \\zeta, \\omega, \\sigma, \\rho, m\\right) \\times f_\\zeta(\\zeta | \\mu, \\tau, \\sigma, \\rho) \\ d \\zeta,\n\\] where \\(f_{N_A}\\left(a | \\zeta, \\omega, \\sigma, \\rho \\right)\\) is a binomial density with size \\(m\\) and probability \\(\\pi = \\pi(\\zeta, \\omega, \\sigma, \\rho)\\) and \\(f_\\zeta(\\zeta | \\mu, \\tau, \\sigma, \\rho)\\) is a normal density with mean \\(\\mu\\) and variance \\(\\tau^2 + \\rho \\sigma^2\\).\nThis distribution is what you might call a binomial-normal convolution or a random-intercept probit model (where the random intercept is \\(\\zeta\\)). As far as I know, the distribution cannot be evaluated analytically but instead must be calculated using some sort of numerical integration routine. Here is an interactive graph of the probability mass function (the probability points are calculated using Gaussian quadrature):\n\nmath = require(\"mathjs\")\nnorm = import('https://unpkg.com/norm-dist@3.1.0/index.js?module')\n\nquad_points = JSON.parse(all_quad_points).at(qp - 1)\n\nsigma = 2 / math.sqrt(ESS)\n\nzeta_sd = math.sqrt(tau**2 + rho * sigma**2)\n\ncrit = norm.icdf(1 - alpha)\n\nbinomial_coefs = Array(m+1).fill(null).map((x,index) =&gt; {\n  return math.combinations(m, index);\n})\n\nprobs = quad_points.map(zeta =&gt; {\n  let Z = (zeta[0] * zeta_sd + mu - crit * sigma) / math.sqrt(omega**2 + (1 - rho) * sigma**2);\n  return [norm.cdf(Z), zeta[1]];\n})\n\np_binom_norm = binomial_coefs.map((coef, a) =&gt; {\n  let p = probs.map((x) =&gt; {\n    return (x[0]**a) * ((1 - x[0])**(m - a)) * x[1];\n  });\n  return coef * math.sum(p);\n})\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPlot.plot({\n  x: {\n    label: \"Number of significant effect sizes\"\n  },\n  y: {\n    domain: [0, 1],\n    label: \"Probability\"\n  },\n  marks: [\n    Plot.ruleY(0),\n    Plot.barY(p_binom_norm, {\n      fill: \"steelblue\"\n    }),\n  ]\n})\n\n\n\n\n\n\n\n\n\nviewof m = Inputs.range(\n  [1, 30], \n  {value: 6, step: 1, label: \"Number of effect sizes (m):\"}\n)\n\nviewof ESS = Inputs.range(\n  [4, 300], \n  {value: 80, step: 1, label: \"Effective sample size:\"}\n)\n\nviewof mu = Inputs.range(\n  [-2, 2], \n  {value: 0.3, step: 0.01, label: \"Average effect size (mu):\"}\n)\n\nviewof tau = Inputs.range(\n  [0, 1], \n  {value: 0.1, step: 0.01, label: \"Between-study SD (tau):\"}\n)\n\nviewof omega = Inputs.range(\n  [0, 1], \n  {value: 0.1, step: 0.01, label: \"Within-study SD (omega):\"}\n)\n\nviewof rho = Inputs.range(\n  [0, 1], \n  {value: 0.6, step: 0.01, label: \"Sampling error correlation (rho):\"}\n)\n\nviewof alpha = Inputs.range(\n  [0.005, 0.995], \n  {value: 0.025, step: .005, label: \"One-sided significance threshold (alpha):\"}\n)\n\nviewof qp = Inputs.range(\n  [1, 30], \n  {value: 21, step: 1, label: \"Number of quadrature points:\"}\n)"
  },
  {
    "objectID": "posts/Distribution-of-signficant-effects/index.html#just-the-moments-please",
    "href": "posts/Distribution-of-signficant-effects/index.html#just-the-moments-please",
    "title": "Distribution of the number of significant effect sizes",
    "section": "Just the moments, please",
    "text": "Just the moments, please\nIf all we care about is the expectation of \\(N_A\\), we don’t need to bother with all the conditioning business and can just look at the marginal distribution of the effect size estimates taken individually. Marginally, \\(T_i\\) is normally distributed with mean \\(\\mu\\) and variance \\(\\tau^2 + \\omega^2 + \\sigma^2\\), so \\(\\text{Pr}(A_i = 1) = \\psi\\), where \\[\n\\psi = \\Phi\\left(\\frac{\\mu - q_{\\alpha} \\sigma}{\\sqrt{\\tau^2 + \\omega^2 + \\sigma^2}}\\right).\n\\] By the linearity of expectations, \\[\n\\text{E}(N_A) = \\sum_{i=1}^m \\text{E}(A_i) = m \\psi.\n\\]\nWe can also get an approximation for the variance of \\(N_A\\) by working with its conditional distribution above. By the rule of variance decomposition, \\[\n\\begin{aligned}\n\\text{Var}(N_A) &= \\text{E}\\left[\\text{Var}\\left(N_A | \\zeta\\right)\\right] + \\text{Var}\\left[\\text{E}\\left(N_A | \\zeta\\right)\\right] \\\\\n&= m \\times \\text{E}\\left[\\pi (1 - \\pi)\\right] + m^2 \\times \\text{Var}\\left[\\pi\\right]\\\\\n&= m \\times \\text{E}\\left[\\pi\\right] \\left(1 - \\text{E}\\left[\\pi\\right]\\right) + m (m - 1) \\times \\text{Var}\\left[\\pi\\right],\n\\end{aligned}\n\\] where \\(\\pi\\) is, as defined above, a function of \\(\\zeta\\) and thus a random variable. Now, \\(\\text{E}(\\pi) = \\psi\\) and we can get something close to \\(\\text{Var}(\\pi)\\) using a first-order approximation: \\[\n\\text{Var}\\left(\\pi\\right) \\approx \\left(\\left.\\frac{\\delta \\pi}{\\delta \\zeta}\\right|_{\\zeta = \\mu}\\right)^2 \\times \\text{Var}\\left(\\zeta\\right) = \\left[\\phi\\left(\\frac{\\mu - q_{\\alpha} \\sigma}{\\sqrt{\\omega^2 + (1 - \\rho)\\sigma^2}}\\right)\\right]^2 \\times \\frac{\\tau^2 + \\rho \\sigma^2}{\\omega^2 + (1 - \\rho)\\sigma^2}.\n\\] Thus, \\[\n\\begin{aligned}\n\\text{Var}(N_A) \\approx m \\times \\psi \\left(1 - \\psi\\right) + m (m - 1) \\times \\left[\\phi\\left(\\frac{\\mu - q_{\\alpha} \\sigma}{\\sqrt{\\omega^2 + (1 - \\rho)\\sigma^2}}\\right)\\right]^2 \\times \\frac{\\tau^2 + \\rho \\sigma^2}{\\omega^2 + (1 - \\rho)\\sigma^2}.\n\\end{aligned}\n\\] If the amount of common variation is small, so \\(\\tau^2\\) is near zero and \\(\\rho\\) is near zero, then the contribution of the second term will be small, and \\(N_A\\) will act more or less like a binomial random variable with size \\(m\\) and probability \\(\\psi\\). On the other hand, if the amount of independent variation in the effect sizes is small, so \\(\\omega^2\\) is near zero and \\(\\rho\\) is near 1, then the term on the right will approach \\(m(m - 1)\\psi(1 - \\psi)\\) and \\(\\text{Var}\\left(N_A\\right)\\) will approach \\(m^2 \\psi(1 - \\psi)\\), or the variance of \\(m\\) times a single Bernoulli variate. So you could say that \\(N_A\\) has anywhere between \\(1\\) and \\(m\\) variate’s worth of information in it, depending on the degree of correlation between the effect size estimates."
  }
]