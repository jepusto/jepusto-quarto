[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "James E. Pustejovsky",
    "section": "",
    "text": "BlueSky\n  \n  \n     \n  \n  \n    \n     \n  \n\n  \n  \nI am a statistician and associate professor in the School of Education at the University of Wisconsin-Madison, where I teach in the Educational Psychology Department and the graduate program in Quantitative Methods. My research involves developing statistical methods for problems in education, psychology, and other areas of social science research, with a focus on methods related to research synthesis and meta-analysis.\n\n\n\n\nPhD in Statistics | 2013  Northwestern University\nBA in Economics | 2003  Boston College\n\n\n\n\n\nMeta-analysis\nCausal inference\nRobust statistical methods\nEducation statistics\nSingle case experimental designs"
  },
  {
    "objectID": "index.html#education",
    "href": "index.html#education",
    "title": "James E. Pustejovsky",
    "section": "",
    "text": "PhD in Statistics | 2013  Northwestern University\nBA in Economics | 2003  Boston College"
  },
  {
    "objectID": "index.html#interests",
    "href": "index.html#interests",
    "title": "James E. Pustejovsky",
    "section": "",
    "text": "Meta-analysis\nCausal inference\nRobust statistical methods\nEducation statistics\nSingle case experimental designs"
  },
  {
    "objectID": "working-papers.html",
    "href": "working-papers.html",
    "title": "Working Papers",
    "section": "",
    "text": "Conducting power analysis for meta-analysis of dependent effect sizes: Common guidelines and an introduction to the POMADE R package\n\n\n\n\n\n\n\n\n\n\n\nJul 21, 2023\n\n\n\n\n\n\nNo matching items\n\n\n\n(All publications)"
  },
  {
    "objectID": "contact.html",
    "href": "contact.html",
    "title": "Contact",
    "section": "",
    "text": "pustejovsky@wisc.edu\n 608-262-0842\n 1082C Education Sciences, 1025 West Johnson St., Madison, WI 53706-1706"
  },
  {
    "objectID": "posts/index.html",
    "href": "posts/index.html",
    "title": "Posts",
    "section": "",
    "text": "Distribution of the number of significant effect sizes\n\n\nIn a study reporting multiple outcomes\n\n\n\neffect size\n\n\ndistribution theory\n\n\n\n\n\n\n\n\n\nMar 28, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nApproximating the distribution of cluster-robust Wald statistics\n\n\n\n\n\n\nrobust variance estimation\n\n\ndistribution theory\n\n\n\n\n\n\n\n\n\nMar 24, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nImplementing Consul’s generalized Poisson distribution in Stan\n\n\n\n\n\n\nBayes\n\n\nsimulation\n\n\ndistribution-theory\n\n\ngeneralized linear model\n\n\nprogramming\n\n\nRstats\n\n\n\n\n\n\n\n\n\nDec 6, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nImplementing Efron’s double Poisson distribution in Stan\n\n\n\n\n\n\nBayes\n\n\nsimulation\n\n\ndistribution-theory\n\n\ngeneralized linear model\n\n\nprogramming\n\n\nRstats\n\n\n\n\n\n\n\n\n\nSep 15, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nCluster-Bootstrapping a meta-analytic selection model\n\n\n\n\n\n\nbootstrap\n\n\ndependent effect sizes\n\n\nmeta-analysis\n\n\npublication bias\n\n\nprogramming\n\n\nRstats\n\n\n\nIn this post, we will sketch out what we think is a promising and pragmatic method for examining selective reporting while also accounting for effect size dependency. The method is to use a cluster-level bootstrap, which involves re-sampling clusters of observations to approximate the sampling distribution of an estimator. To illustrate this technique, we will demonstrate how to bootstrap a Vevea-Hedges selection model.\n\n\n\n\n\nMar 30, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nCohen’s \\(d_z\\) makes me dizzy when considering measurement error\n\n\n\n\n\n\neffect size\n\n\nstandardized mean difference\n\n\ndesign-comparable SMD\n\n\nmeasurement-error\n\n\n\nMeta-analyses in education, psychology, and related fields rely heavily of Cohen’s \\(d\\), or the standardized mean difference effect size, for quantitatively describing the magnitude and direction of intervention effects. In these fields, Cohen’s \\(d\\) is so pervasive that its use is nearly automatic, and analysts rarely question its utility or consider alternatives (response ratios, anyone? POMP?). Despite this state of affairs, working with Cohen’s \\(d\\) is theoretically challenging because the standardized mean difference metric does not have a singular definition. Rather, its definition depends on the choice of the standardizing variance used in the denominator.\n\n\n\n\n\nFeb 17, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nCorrigendum to Pustejovsky and Tipton (2018), redux\n\n\nA revised version of Theorem 2\n\n\n\nrobust variance estimation\n\n\neconometrics\n\n\nmatrix algebra\n\n\n\nIn my 2018 paper with Beth Tipton, published in the Journal of Business and Economic Statistics, we considered how to do cluster-robust variance estimation in fixed effects models estimated by weighted (or unweighted) least squares. We were recently alerted that Theorem 2 in the paper is incorrect as stated. It turns out, the conditions in the original version of the theorem are too general. A more limited version of the Theorem does actually hold, but only for models estimated using ordinary (unweighted) least squares, under a working model that assumes independent, homoskedastic errors. In this post, I’ll give the revised theorem, following the notation and setup of the previous post (so better read that first, or what follows won’t make much sense!).\n\n\n\n\n\nNov 7, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nCorrigendum to Pustejovsky and Tipton (2018)\n\n\nTheorem 2 is incorrect as stated\n\n\n\nrobust variance estimation\n\n\neconometrics\n\n\nmatrix algebra\n\n\n\nIn my 2018 paper with Beth Tipton, published in the Journal of Business and Economic Statistics, we considered how to do cluster-robust variance estimation in fixed effects models estimated by weighted (or unweighted) least squares. A careful reader recently alerted us to a problem with Theorem 2 in the paper, which concerns a computational short cut for a certain cluster-robust variance estimator in models with cluster-specific fixed effects. The theorem is incorrect as stated, and we are currently working on issuing a correction for the published version of the paper. In the interim, this post details the problem with Theorem 2. I’ll first review the CR2 variance estimator, then describe the assertion of the theorem, and then provide a numerical counter-example demonstrating that the assertion is not correct as stated.\n\n\n\n\n\nSep 28, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nVariance component estimates in meta-analysis with mis-specified sampling correlation\n\n\n\n\n\n\nmeta-analysis\n\n\ndependent effect sizes\n\n\ndistribution theory\n\n\nhierarchical models\n\n\n\n\n\n\n\n\n\nNov 28, 2021\n\n\n\n\n\n\n\n\n\n\n\n\nImplications of mean-variance relationships for standardized mean differences\n\n\n\n\n\n\nstandardized mean difference\n\n\nresponse ratio\n\n\ndistribution theory\n\n\nmeta-analysis\n\n\n\nA question came up on the R-SIG-meta-analysis listserv about whether it was reasonable to use the standardized mean difference metric for synthesizing studies where the outcomes are measured as proportions. I think this is an interesting question because, while the SMD could work perfectly fine as an effect size metric for proportions, there are also other alternatives that could be considered, such as odds ratios or response ratios or raw differences in proportions. Further, there are some situations where the SMD has disadvantages for synthesizing contrasts between proportions. Thus, it’s a situation where one has to make a choice about the effect size metric, and where the most common metric (the SMD) might not be the right answer. In this post, I want to provide a bit more detail regarding why I think mean-variance relationships in raw data can signal that the standardized mean differences might be less useful as an effect size metric compared to alternatives.\n\n\n\n\n\nNov 2, 2021\n\n\n\n\n\n\n\n\n\n\n\n\nInverting partitioned matrices\n\n\n\n\n\n\nmatrix algebra\n\n\n\nThere’s lots of linear algebra out there that’s quite useful for statistics, but that I never learned in school or never had cause to study in depth. In the same spirit as my previous post on the Woodbury identity, I thought I would share my notes on another helpful bit of math about matrices. At some point in high school or college, you might have learned how to invert a small matrix by hand. It turns out that there’s a straight-forward generalization of this formula to matrices of arbitrary size, but that are partitioned into four pieces.\n\n\n\n\n\nOct 20, 2021\n\n\n\n\n\n\n\n\n\n\n\n\nStandardized mean differences in single-group, repeated measures designs\n\n\n\n\n\n\neffect size\n\n\nstandardized mean difference\n\n\ndistribution theory\n\n\n\n\n\n\n\n\n\nOct 6, 2021\n\n\n\n\n\n\n\n\n\n\n\n\nFinding the distribution of significant effect sizes\n\n\nIn a study reporting multiple outcomes\n\n\n\neffect size\n\n\ndistribution theory\n\n\n\n\n\n\n\n\n\nApr 27, 2021\n\n\n\n\n\n\n\n\n\n\n\n\nThe Woodbury identity\n\n\nA life-hack for analyzing hierarchical models\n\n\n\nhierarchical models\n\n\nmatrix algebra\n\n\n\n\n\n\n\n\n\nDec 4, 2020\n\n\n\n\n\n\n\n\n\n\n\n\nAn ANCOVA puzzler\n\n\n\n\n\n\nmeta-analysis\n\n\neffect size\n\n\nstandardized mean difference\n\n\n\n\n\n\n\n\n\nNov 24, 2020\n\n\n\n\n\n\n\n\n\n\n\n\nFrom Longhorn to Badger\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAug 28, 2020\n\n\n\n\n\n\n\n\n\n\n\n\nWhat do meta-analysts mean by ‘multivariate’ meta-analysis?\n\n\n\n\n\n\nmeta-analysis\n\n\nmultivariate\n\n\ndependent effect sizes\n\n\n\n\n\n\n\n\n\nJun 27, 2020\n\n\n\n\n\n\n\n\n\n\n\n\nWeighting in multivariate meta-analysis\n\n\n\n\n\n\nmeta-analysis\n\n\nweighting\n\n\n\nOne common question about multivariate/multi-level meta-analysis is how such models assign weight to individual effect size estimates. When a version of the question came up recently on the R-sig-meta-analysis listserv, Dr. Wolfgang Viechtbauer offered a whole blog post in reply, demonstrating how weights work in simpler fixed effect and random effects meta-analysis and then how things get more complicated in multivariate models. In this post, I’ll try to add some further intuition on how weights work in certain multivariate meta-analysis models. Most of the discussion will apply to models that include multiple level of random effects, but no predictors. I’ll also comment briefly on meta-regression models with only study-level predictor variables, and finally give some pointers to work on more complicated models.\n\n\n\n\n\nJun 9, 2020\n\n\n\n\n\n\n\n\n\n\n\n\nAn update on code folding with blogdown + Academic theme\n\n\n\n\n\n\nprogramming\n\n\nRstats\n\n\n\n\n\n\n\n\n\nMay 3, 2020\n\n\n\n\n\n\n\n\n\n\n\n\nSimulating correlated standardized mean differences for meta-analysis\n\n\n\n\n\n\neffect size\n\n\nstandardized mean difference\n\n\nmeta-analysis\n\n\nsimulation\n\n\nprogramming\n\n\ndistribution theory\n\n\n\n\n\n\n\n\n\nSep 30, 2019\n\n\n\n\n\n\n\n\n\n\n\n\nSometimes, aggregating effect sizes is fine\n\n\n\n\n\n\neffect size\n\n\nmeta-analysis\n\n\ndependent effect sizes\n\n\n\n\n\n\n\n\n\nJul 2, 2019\n\n\n\n\n\n\n\n\n\n\n\n\nCode folding with blogdown + Academic theme\n\n\n\n\n\n\nprogramming\n\n\nRstats\n\n\n\n\n\n\n\n\n\nApr 14, 2019\n\n\n\n\n\n\n\n\n\n\n\n\nCRAN downloads of my packages\n\n\n\n\n\n\nprogramming\n\n\nRstats\n\n\nmeta-analysis\n\n\n\n\n\n\n\n\n\nApr 9, 2019\n\n\n\n\n\n\n\n\n\n\n\n\nSystematic Reviews and Meta-analysis SIG at AERA 2019\n\n\n\n\n\n\nmeta-analysis\n\n\nAERA\n\n\n\n\n\n\n\n\n\nMar 26, 2019\n\n\n\n\n\n\n\n\n\n\n\n\nA handmade clubSandwich for multi-site trials\n\n\n\n\n\n\nsandwiches\n\n\nrobust variance estimation\n\n\neconometrics\n\n\nweighting\n\n\n\n\n\n\n\n\n\nMar 9, 2019\n\n\n\n\n\n\n\n\n\n\n\n\nEffective sample size aggregation\n\n\n\n\n\n\neconometrics\n\n\ncausal inference\n\n\nweighting\n\n\n\n\n\n\n\n\n\nJan 22, 2019\n\n\n\n\n\n\n\n\n\n\n\n\nEasily simulate thousands of single-case designs\n\n\n\n\n\n\nbehavioral observation\n\n\nsimulation\n\n\nsingle-case design\n\n\n\n\n\n\n\n\n\nJun 21, 2018\n\n\n\n\n\n\n\n\n\n\n\n\nNew paper: A gradual effects model for single-case designs\n\n\n\n\n\n\nsingle-case design\n\n\neffect size\n\n\ngeneralized linear model\n\n\n\n\n\n\n\n\n\nMay 14, 2018\n\n\n\n\n\n\n\n\n\n\n\n\nclubSandwich at the Austin R User Group Meetup\n\n\n\n\n\n\nRstats\n\n\nrobust variance estimation\n\n\nsandwiches\n\n\n\n\n\n\n\n\n\nApr 26, 2018\n\n\n\n\n\n\n\n\n\n\n\n\nSampling variance of Pearson r in a two-level design\n\n\n\n\n\n\neffect size\n\n\ncorrelation\n\n\nmeta-analysis\n\n\ndelta method\n\n\ndistribution theory\n\n\n\n\n\n\n\n\n\nApr 19, 2018\n\n\n\n\n\n\n\n\n\n\n\n\nThe multivariate delta method\n\n\n\n\n\n\ndelta method\n\n\ndistribution theory\n\n\n\n\n\n\n\n\n\nApr 11, 2018\n\n\n\n\n\n\n\n\n\n\n\n\nNew paper: Using response ratios for meta-analyzing SCDs with behavioral outcomes\n\n\n\n\n\n\nsingle-case design\n\n\neffect size\n\n\nresponse ratio\n\n\nmeta-analysis\n\n\n\n\n\n\n\n\n\nMar 16, 2018\n\n\n\n\n\n\n\n\n\n\n\n\nNew paper: procedural sensitivities of effect size measures for SCDs\n\n\n\n\n\n\nsingle-case design\n\n\neffect size\n\n\nresponse ratio\n\n\nnon-overlap measures\n\n\nsimulation\n\n\n\n\n\n\n\n\n\nJan 11, 2018\n\n\n\n\n\n\n\n\n\n\n\n\nBack from the IES PI meeting\n\n\n\n\n\n\nsingle-case design\n\n\nhypothesis testing\n\n\n\n\n\n\n\n\n\nJan 10, 2018\n\n\n\n\n\n\n\n\n\n\n\n\n2SLS standard errors and the delta-method\n\n\n\n\n\n\ninstrumental variables\n\n\ncausal inference\n\n\ndelta method\n\n\ndistribution theory\n\n\n\n\n\n\n\n\n\nOct 7, 2017\n\n\n\n\n\n\n\n\n\n\n\n\nPooling clubSandwich results across multiple imputations\n\n\n\n\n\n\nmissing data\n\n\nsandwiches\n\n\nsmall-sample\n\n\nRstats\n\n\n\n\n\n\n\n\n\nSep 27, 2017\n\n\n\n\n\n\n\n\n\n\n\n\nImputing covariance matrices for meta-analysis of correlated effects\n\n\n\n\n\n\nmeta-analysis\n\n\nsandwiches\n\n\nrobust variance estimation\n\n\nRstats\n\n\n\n\n\n\n\n\n\nAug 10, 2017\n\n\n\n\n\n\n\n\n\n\n\n\nThe siren song of significance\n\n\n\n\n\n\npre-registration\n\n\nhypothesis testing\n\n\n\n\n\n\n\n\n\nJun 19, 2017\n\n\n\n\n\n\n\n\n\n\n\n\nYou wanna PEESE of d’s?\n\n\n\n\n\n\nmeta-analysis\n\n\npublication bias\n\n\nstandardized mean difference\n\n\nsimulation\n\n\n\n\n\n\n\n\n\nApr 27, 2017\n\n\n\n\n\n\n\n\n\n\n\n\nNew working paper: Using log response ratios for meta-analyzing SCDs with behavioral outcomes\n\n\n\n\n\n\nsingle-case design\n\n\neffect size\n\n\nresponse ratio\n\n\nmeta-analysis\n\n\n\n\n\n\n\n\n\nApr 26, 2017\n\n\n\n\n\n\n\n\n\n\n\n\nPresentation at IES 2016 PI meeting\n\n\n\n\n\n\nsingle-case design\n\n\neffect size\n\n\n\n\n\n\n\n\n\nDec 19, 2016\n\n\n\n\n\n\n\n\n\n\n\n\nNew tutorial paper on BC-SMD effect sizes\n\n\n\n\n\n\nsingle-case design\n\n\neffect size\n\n\ndesign-comparable SMD\n\n\n\n\n\n\n\n\n\nDec 19, 2016\n\n\n\n\n\n\n\n\n\n\n\n\nBug in nlme::lme with fixed sigma and REML estimation\n\n\n\n\n\n\nRstats\n\n\nprogramming\n\n\nhierarchical models\n\n\nnlme\n\n\n\n\n\n\n\n\n\nNov 7, 2016\n\n\n\n\n\n\n\n\n\n\n\n\nWhat is Tau-U?\n\n\n\n\n\n\neffect size\n\n\nsingle-case design\n\n\nnon-overlap measures\n\n\n\n\n\n\n\n\n\nNov 3, 2016\n\n\n\n\n\n\n\n\n\n\n\n\nNew working paper: Procedural sensitivities of SCD effect sizes\n\n\n\n\n\n\neffect size\n\n\nsingle-case design\n\n\nresponse ratio\n\n\nnon-overlap measures\n\n\nsimulation\n\n\n\n\n\n\n\n\n\nOct 17, 2016\n\n\n\n\n\n\n\n\n\n\n\n\nSimulation studies in R (Fall, 2016 version)\n\n\n\n\n\n\nRstats\n\n\nsimulation\n\n\nprogramming\n\n\n\n\n\n\n\n\n\nSep 28, 2016\n\n\n\n\n\n\n\n\n\n\n\n\nBug in nlme::getVarCov\n\n\n\n\n\n\nRstats\n\n\nprogramming\n\n\nhierarchical models\n\n\nnlme\n\n\n\n\n\n\n\n\n\nAug 10, 2016\n\n\n\n\n\n\n\n\n\n\n\n\nAlternative formulas for the standardized mean difference\n\n\n\n\n\n\nmeta-analysis\n\n\neffect size\n\n\ndistribution theory\n\n\nstandardized mean difference\n\n\n\n\n\n\n\n\n\nJun 3, 2016\n\n\n\n\n\n\n\n\n\n\n\n\nAssigning after dplyr\n\n\n\n\n\n\nRstats\n\n\nprogramming\n\n\n\n\n\n\n\n\n\nMay 13, 2016\n\n\n\n\n\n\n\n\n\n\n\n\nUnlucky randomization\n\n\n\n\n\n\nexperimental design\n\n\n\n\n\n\n\n\n\nMay 11, 2016\n\n\n\n\n\n\n\n\n\n\n\n\nThe sampling distribution of sample variances\n\n\n\n\n\n\ndistribution theory\n\n\n\n\n\n\n\n\n\nApr 25, 2016\n\n\n\n\n\n\n\n\n\n\n\n\nTau-U\n\n\n\n\n\n\neffect size\n\n\nsingle-case design\n\n\nnon-overlap measures\n\n\n\n\n\n\n\n\n\nMar 23, 2016\n\n\n\n\n\n\n\n\n\n\n\n\nStandard errors and confidence intervals for NAP\n\n\n\n\n\n\neffect size\n\n\nsingle-case design\n\n\nnon-overlap measures\n\n\ndistribution theory\n\n\n\n\n\n\n\n\n\nFeb 28, 2016\n\n\n\n\n\n\n\n\n\n\n\n\nEstimating average effects in regression discontinuities with covariate interactions\n\n\n\n\n\n\neconometrics\n\n\nRstats\n\n\ncausal inference\n\n\nregression discontinuity\n\n\n\n\n\n\n\n\n\nJan 27, 2016\n\n\n\n\n\n\n\n\n\n\n\n\nRegression discontinuities with covariate interactions in the rdd package\n\n\n\n\n\n\neconometrics\n\n\nRstats\n\n\ncausal inference\n\n\nregression discontinuity\n\n\n\n\n\n\n\n\n\nJan 25, 2016\n\n\n\n\n\n\n\n\n\n\n\n\nClustered standard errors and hypothesis tests in fixed effects models\n\n\n\n\n\n\neconometrics\n\n\nfixed effects\n\n\nsandwiches\n\n\nRstats\n\n\n\n\n\n\n\n\n\nJan 10, 2016\n\n\n\n\n\n\n\n\n\n\n\n\nSpecial Education Pro-Sem\n\n\n\n\n\n\nmeta-analysis\n\n\nsingle-case design\n\n\neffect size\n\n\n\n\n\n\n\n\n\nNov 24, 2015\n\n\n\n\n\n\n\n\n\n\n\n\nCorrelations between standardized mean differences\n\n\n\n\n\n\nmeta-analysis\n\n\neffect size\n\n\nstandardized mean difference\n\n\ndistribution theory\n\n\n\n\n\n\n\n\n\nSep 17, 2015\n\n\n\n\n\n\n\n\n\n\n\n\nFatal crashes in Austin/Travis County\n\n\n\n\n\n\ntransportation\n\n\n\n\n\n\n\n\n\nAug 20, 2015\n\n\n\n\n\n\n\n\n\n\n\n\nThe clubSandwich package for meta-analysis with RVE\n\n\n\n\n\n\nmeta-analysis\n\n\nrobust variance estimation\n\n\nsandwiches\n\n\nRstats\n\n\n\n\n\n\n\n\n\nJul 10, 2015\n\n\n\n\n\n\n\n\n\n\n\n\nNew article: Four methods for analyzing PIR data\n\n\n\n\n\n\nbehavioral observation\n\n\nsingle-case design\n\n\n\n\n\n\n\n\n\nFeb 11, 2015\n\n\n\n\n\n\n\n\n\n\n\n\nGetting started with scdhlm\n\n\n\n\n\n\nsingle-case design\n\n\ndesign-comparable SMD\n\n\nRstats\n\n\n\n\n\n\n\n\n\nOct 19, 2014\n\n\n\n\n\n\n\n\n\n\n\n\nWanted: PIR data\n\n\n\n\n\n\nbehavioral observation\n\n\n\n\n\n\n\n\n\nSep 3, 2014\n\n\n\n\n\n\n\n\n\n\n\n\nNew article: Design-comparable effect sizes in multiple baseline designs: A general modeling framework\n\n\n\n\n\n\nsingle-case design\n\n\neffect size\n\n\nhierarchical models\n\n\ndesign-comparable SMD\n\n\n\n\n\n\n\n\n\nJul 20, 2014\n\n\n\n\n\n\n\n\n\n\n\n\nARPobservation now on CRAN\n\n\n\n\n\n\nbehavioral observation\n\n\nalternating renewal process\n\n\nRstats\n\n\n\n\n\n\n\n\n\nMay 31, 2014\n\n\n\n\n\n\n\n\n\n\n\n\nMeta-sandwich with extra mustard\n\n\n\n\n\n\nmeta-analysis\n\n\nsandwiches\n\n\nRstats\n\n\nrobust variance estimation\n\n\n\n\n\n\n\n\n\nApr 26, 2014\n\n\n\n\n\n\n\n\n\n\n\n\nAnother meta-sandwich\n\n\n\n\n\n\nmeta-analysis\n\n\nsandwiches\n\n\nRstats\n\n\nrobust variance estimation\n\n\n\n\n\n\n\n\n\nApr 23, 2014\n\n\n\n\n\n\n\n\n\n\n\n\nA meta-sandwich\n\n\n\n\n\n\nmeta-analysis\n\n\nsandwiches\n\n\nRstats\n\n\nrobust variance estimation\n\n\n\n\n\n\n\n\n\nApr 21, 2014\n\n\n\n\n\n\n\n\n\n\n\n\nSpecial Education Pro-Sem\n\n\n\n\n\n\nmeta-analysis\n\n\nsingle-case design\n\n\neffect size\n\n\n\n\n\n\n\n\n\nApr 10, 2014\n\n\n\n\n\n\n\n\n\n\n\n\nUpdate: parallel R on the TACC\n\n\n\n\n\n\nRstats\n\n\nprogramming\n\n\nsimulation\n\n\nTACC\n\n\n\n\n\n\n\n\n\nApr 8, 2014\n\n\n\n\n\n\n\n\n\n\n\n\nNew article: Measurement-comparable effect sizes for single-case studies of free-operant behavior\n\n\n\n\n\n\nbehavioral observation\n\n\nsingle-case design\n\n\neffect size\n\n\nresponse ratio\n\n\n\n\n\n\n\n\n\nFeb 4, 2014\n\n\n\n\n\n\n\n\n\n\n\n\nRunning R in parallel on the TACC\n\n\n\n\n\n\nRstats\n\n\nprogramming\n\n\nsimulation\n\n\nTACC\n\n\n\n\n\n\n\n\n\nDec 20, 2013\n\n\n\n\n\n\n\n\n\n\n\n\nDesigning simulation studies using R\n\n\n\n\n\n\nRstats\n\n\nsimulation\n\n\n\n\n\n\n\n\n\nDec 6, 2013\n\n\nJames\n\n\n\n\n\n\n\n\n\n\n\n\nTo what extent does partial interval recording over-estimate prevalence?\n\n\n\n\n\n\nbehavioral observation\n\n\nsimulation\n\n\nalternating renewal process\n\n\n\n\n\n\n\n\n\nOct 26, 2013\n\n\n\n\n\n\n\n\n\n\n\n\nARPobservation: Basic use\n\n\n\n\n\n\nbehavioral observation\n\n\nalternating renewal process\n\n\nRstats\n\n\n\n\n\n\n\n\n\nOct 25, 2013\n\n\n\n\n\n\n\n\n\n\n\n\nGetting started with ARPobservation\n\n\n\n\n\n\nbehavioral observation\n\n\nsimulation\n\n\nRstats\n\n\n\n\n\n\n\n\n\nOct 24, 2013\n\n\n\n\n\n\n\n\n\n\n\n\nReliability of UnGraphed single-case data: An example using the Shogren dataset\n\n\n\n\n\n\nsingle-case design\n\n\ninter-rater reliability\n\n\n\n\n\n\n\n\n\nOct 23, 2013\n\n\n\n\n\n\n\n\n\n\n\n\nAnother project idea: Meta-analytic methods for correlational data\n\n\n\n\n\n\nmeta-analysis\n\n\nrobust variance estimation\n\n\ncorrelation\n\n\n\n\n\n\n\n\n\nSep 13, 2013\n\n\n\n\n\n\n\n\n\n\n\n\nCurrent projects\n\n\n\n\n\n\nbehavioral observation\n\n\nsingle-case design\n\n\n\n\n\n\n\n\n\nAug 20, 2013\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "presentations/index.html",
    "href": "presentations/index.html",
    "title": "Presentations",
    "section": "",
    "text": "Date\n\n\nTitle\n\n\nEvent\n\n\nLocation\n\n\n\n\n\n\nMay 17, 2024\n\n\nModel-Building Considerations in Meta-Analysis of Dependent Effect Sizes\n\n\nVIVE\n\n\nCopenhagen, Denmark\n\n\n\n\nApr 12, 2024\n\n\nBayesian estimation of between-case standardized mean differences: A simulation study\n\n\nAERA 2024\n\n\nPhiladelphia, PA\n\n\n\n\nSep 29, 2023\n\n\nDiscussion of Stabilizing measures to reconcile accuracy and equity in performance measurement\n\n\nSREE 2023\n\n\nArlington, VA\n\n\n\n\nSep 27, 2023\n\n\nEquity-related moderator analysis in syntheses of dependent effect sizes: Conceptual and statistical considerations\n\n\nSREE 2023\n\n\nArlington, VA\n\n\n\n\nMay 18, 2023\n\n\nDetermining the Timing of Phase Changes: Some Statistical Perspective\n\n\nWISCC2023\n\n\nNashville, TN\n\n\n\n\nApr 26, 2023\n\n\nCalculating Effect Sizes for Single-Case Research: An Introduction to the SingleCaseES and scdhlm Web Applications and R Packages\n\n\nSmallisBeautiful2023\n\n\nonline\n\n\n\n\nApr 25, 2023\n\n\nEffect size measures for single-case research: Conceptual, practical, and statistical considerations\n\n\nSmallisBeautiful2023\n\n\nonline\n\n\n\n\nApr 16, 2023\n\n\nEmpirical benchmarks for between-case standardized mean differences from single-case multiple baseline designs examining academic interventions.\n\n\nAERA 2023\n\n\nChicago, IL\n\n\n\n\nApr 15, 2023\n\n\nDiscussion of ‘Moving from What Works to What Replicates: Promoting the Systematic Replication of Results.’\n\n\nAERA 2023\n\n\nChicago, IL\n\n\n\n\nMar 30, 2023\n\n\nClustered bootstrapping for selective reporting models in meta-analysis with dependent effects\n\n\nESMARConf2023\n\n\nonline\n\n\n\n\nJul 20, 2022\n\n\nA matter of emphasis: Comparison of working models for meta-analysis of dependent effect sizes\n\n\nSRSM 2022\n\n\nPortland, OR (and online)\n\n\n\n\nMay 19, 2022\n\n\nThe state of single case synthesis: Premises, tools, and possibilities\n\n\nSSCC\n\n\nVanderbilt University, Nashville, TN\n\n\n\n\nFeb 8, 2022\n\n\nSelective reporting in meta-analysis of dependent effect size estimates\n\n\nStanford QSU\n\n\nonline\n\n\n\n\nFeb 3, 2022\n\n\nEasy, cluster-robust standard errors with the clubSandwich package\n\n\nOsloRUG\n\n\nonline\n\n\n\n\nSep 27, 2021\n\n\nFour things every quantitative social scientist should know about meta-analysis\n\n\nEdPsych PIE\n\n\nEducational Sciences 259\n\n\n\n\nSep 2, 2021\n\n\nSynthesis of dependent effect sizes: Robust variance estimation with clubSandwich\n\n\nOsloRUG\n\n\nonline\n\n\n\n\nJun 23, 2021\n\n\nStatistical frontiers for selective reporting and publication bias\n\n\nSIPS 2021\n\n\nonline\n\n\n\n\nJan 21, 2021\n\n\nSynthesis of dependent effect sizes: Versatile models through metafor and clubSandwich\n\n\nESMARConf2021\n\n\nonline\n\n\n\n\nJul 22, 2019\n\n\nA generalized excess significance test for selective outcome reporting with dependent effect sizes\n\n\nSRSM 2019\n\n\nChicago, IL\n\n\n\n\nMay 26, 2019\n\n\nLog response ratio effect sizes: Rationale and methods for single case designs with behavioral outcomes\n\n\nABAI 2019\n\n\nChicago, IL\n\n\n\n\nApr 8, 2019\n\n\nEvaluating meta-analytic methods to detect outcome reporting bias in the presence of dependent effect sizes\n\n\nAERA 2019\n\n\nToronto, Ontario\n\n\n\n\nApr 7, 2019\n\n\nAn examination of measurement procedures and baseline behavioral outcomes in single-case research\n\n\nAERA 2019\n\n\nToronto, Ontario\n\n\n\n\nApr 7, 2019\n\n\nThe impact of response-guided designs on count outcomes in single-case design baselines\n\n\nAERA 2019\n\n\nToronto, Ontario\n\n\n\n\nMar 8, 2019\n\n\nSmall-sample cluster-robust variance estimators for two-stage least squares models\n\n\nSREE 2019\n\n\nWashington, DC\n\n\n\n\nOct 1, 2018\n\n\nCombining robust variance estimation with models for dependent effect sizes\n\n\nUT Psych\n\n\nAustin, TX\n\n\n\n\nJul 18, 2018\n\n\nCombining robust variance estimation with models for dependent effect sizes\n\n\nSRSM 2018\n\n\nBristol, UK\n\n\n\n\nApr 15, 2018\n\n\nMeta-analysis of dependent effects: A review and consolidation of methods\n\n\nAERA 2018\n\n\nNew York, NY\n\n\n\n\nApr 15, 2018\n\n\nMeta-analysis of single-case research: A brief and breezy tour\n\n\nAERA 2018\n\n\nNew York, NY\n\n\n\n\nJan 10, 2018\n\n\nA gradual effects model for single case designs\n\n\nIES 2018 PI Meeting\n\n\nWashington, DC\n\n\n\n\nJan 10, 2018\n\n\nRandomization inference for single-case experimental designs\n\n\nIES 2018 PI Meeting\n\n\nWashington, DC\n\n\n\n\nApr 30, 2017\n\n\nHeteroskedasticity-robust tests in linear regression: A review and evaluation of small-sample corrections\n\n\nAERA 2017\n\n\nSan Antonio, TX\n\n\n\n\nApr 28, 2017\n\n\nA nonlinear intervention analysis model for treatment reversal single-case designs\n\n\nAERA 2017\n\n\nSan Antonio, TX\n\n\n\n\nApr 28, 2017\n\n\nUsing response ratios for meta-analyzing single-case designs with behavioral outcomes\n\n\nAERA 2017\n\n\nSan Antonio, TX\n\n\n\n\nMar 2, 2017\n\n\nSmall sample corrections for use of cluster-robust standard errors in the analysis of school-based experiments\n\n\nSREE 2017\n\n\nWashington, DC\n\n\n\n\nDec 19, 2016\n\n\nEffect sizes for single-case research\n\n\nIES 2016\n\n\nWashington, DC\n\n\n\n\nJul 31, 2016\n\n\nSmall-sample methods for cluster-robust variance estimation and hypothesis testing in fixed effects models\n\n\nJSM 2016\n\n\nChicago, IL\n\n\n\n\nJul 18, 2016\n\n\nWhen large samples act small: The importance of small-sample adjustments for cluster-robust inference in impact evaluations\n\n\nAIR working group lecture series\n\n\nAustin, TX\n\n\n\n\nFeb 19, 2016\n\n\nWhen large samples act small: Cluster-robust variance estimation and hypothesis testing with few clusters\n\n\nPRC colloquium\n\n\nAustin, TX\n\n\n\n\nJul 8, 2015\n\n\nSmall-sample adjustments for multiple-contrast hypothesis tests of meta-regressions using robust variance estimation\n\n\nSRSM 2015\n\n\nNashville, TN\n\n\n\n\nApr 19, 2015\n\n\nOperational sensitivities of non-overlap effect sizes for single-case experimental designs\n\n\nAERA 2015\n\n\nChicago, IL\n\n\n\n\nApr 18, 2015\n\n\nSmall-sample adjustments for F-tests using robust variance estimation in meta-regression\n\n\nAERA 2015\n\n\nChicago, IL\n\n\n\n\nApr 16, 2015\n\n\nObservation procedures and Markov Chain models for estimating the prevalence and incidence of a state behavior\n\n\nAERA 2015\n\n\nChicago, IL\n\n\n\n\nMar 6, 2015\n\n\nSmall-sample adjustments for tests of moderators and model fit using robust variance estimation in meta-regression\n\n\nSREE 2015\n\n\nWashington, DC\n\n\n\n\nApr 4, 2014\n\n\nFour methods of analyzing partial interval recording data, with application to single-case research\n\n\nAERA 2014\n\n\nPhiladelphia, PA\n\n\n\n\nMar 21, 2014\n\n\nAddressing construct invalidity in partial interval recording data\n\n\nTUESAP 2014\n\n\nTexas A&M University, College Station, TX\n\n\n\n\nMar 6, 2014\n\n\nOn internal validity in multiple baseline designs\n\n\nSREE 2014\n\n\nWashington, DC\n\n\n\n\nMay 29, 2013\n\n\nSome Markov models for direct observation of behavior\n\n\nNU Statistics\n\n\nEvanston, IL\n\n\n\n\nMay 28, 2013\n\n\nEffect sizes and measurement comparability for meta-analysis of single-case research\n\n\nABAI 2013\n\n\nMinneapolis, MN\n\n\n\n\nApr 30, 2013\n\n\nObservation procedures and Markov chain models for estimating the prevalence and incidence of a behavior\n\n\nAERA 2013\n\n\nSan Francisco, CA\n\n\n\n\nMar 7, 2013\n\n\nOperationally comparable effect sizes for meta-analysis of single-case research\n\n\nSREE 2013\n\n\nWashington, DC\n\n\n\n\nNov 14, 2012\n\n\nSome implications of behavioral observation procedures for meta-analysis of single-case research\n\n\nUT Austin\n\n\nAustin, TX\n\n\n\n\nJan 23, 2008\n\n\nQuestion-order effects in social network name generators\n\n\nISSNA 2008\n\n\nSt. Petersburg Beach, FL\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "software/index.html",
    "href": "software/index.html",
    "title": "Software",
    "section": "",
    "text": "ARPobservation \n              \n            \n\n            \n              Simulate systematic direct observation data \n            \n            \n            \n          \n      \n      \n    \n  \n     \n    \n    \n      \n      \n      \n      \n        \n          \n          \n            \n          \n          \n          \n            \n            \n              \n              clubSandwich \n              \n            \n\n            \n              Cluster-robust variance estimation \n            \n            \n            \n          \n      \n      \n    \n  \n     \n    \n    \n      \n      \n      \n      \n        \n          \n          \n            \n          \n          \n          \n            \n            \n              \n              lmeInfo \n              \n            \n\n            \n              Information Matrices for 'lmeStruct' and 'glsStruct' Objects \n            \n            \n            \n          \n      \n      \n    \n  \n     \n    \n    \n      \n      \n      \n      \n        \n          \n          \n            \n          \n          \n          \n            \n            \n              \n              POMADE \n              \n            \n\n            \n              Power for Meta-Analysis of Dependent Effects \n            \n            \n            \n          \n      \n      \n    \n  \n     \n    \n    \n      \n      \n      \n      \n        \n          \n          \n            \n          \n          \n          \n            \n            \n              \n              scdhlm \n              \n            \n\n            \n              Between-case SMD for single-case designs \n            \n            \n            \n          \n      \n      \n    \n  \n     \n    \n    \n      \n      \n      \n      \n        \n          \n          \n            \n          \n          \n          \n            \n            \n              \n              simhelpers \n              \n            \n\n            \n              Helper package to assist in running simulation studies \n            \n            \n            \n          \n      \n      \n    \n  \n     \n    \n    \n      \n      \n      \n      \n        \n          \n          \n            \n          \n          \n          \n            \n            \n              \n              SingleCaseES \n              \n            \n\n            \n              Single-case design effect size calculator \n            \n            \n            \n          \n      \n      \n    \n  \n     \n    \n    \n      \n      \n      \n      \n        \n          \n          \n            \n          \n          \n          \n            \n            \n              \n              wildmeta \n              \n            \n\n            \n              Cluster-wild bootstrap for meta-regression \n            \n            \n            \n          \n      \n      \n    \n  \n    \n  \n\n\n\n\nNo matching items"
  },
  {
    "objectID": "teaching/index.html",
    "href": "teaching/index.html",
    "title": "Teaching",
    "section": "",
    "text": "Institution\n\n\nCourse Title\n\n\n\n\n\n\nUW Madison\n\n\nDesign & Analysis of Quasi-Experiments for Causal Inference\n\n\n\n\nUW Madison\n\n\nField Experiments in Education Research\n\n\n\n\nUW Madison\n\n\nMeta-analysis\n\n\n\n\nUT Austin\n\n\nCausal Inference\n\n\n\n\nUT Austin\n\n\nData Analysis, Simulation, and Programming in R\n\n\n\n\nUT Austin\n\n\nResearch Design and Methods for Psychology and Education\n\n\n\n\nUT Austin\n\n\nStatistical Analysis of Experimental Data\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "publication/index.html",
    "href": "publication/index.html",
    "title": "Publications",
    "section": "",
    "text": "Date\n\n\nAuthors\n\n\nTitle\n\n\nPublication\n\n\n\n\n\n\nMar 7, 2024\n\n\nadmin, Man Chen\n\n\nEquivalences between ad hoc strategies and meta-analytic models for dependent effect sizes\n\n\nJournal of Educational and Behavioral Statistics\n\n\n\n\nNov 9, 2023\n\n\nJohn Protzko, Jon Krosnick, Leif Nelson, Brian Nosek, Jordan Axt, Matt Berent, Nicholas Buttrick, Matthew DeBell, Charles R. Ebersole, Sebastian Lundmark, Bo MacInnis, Michael O’Donnell, Hannah Perfecto, admin, Scott Roeder, Jan Walleczek, Jonathan W. Schooler\n\n\nHigh replicability of newly-discovered social-behavioral findings is achievable.\n\n\nNature Human Behavior\n\n\n\n\nJul 21, 2023\n\n\nMikkel H. Vembye, admin, Terri D. Pigott\n\n\nConducting power analysis for meta-analysis of dependent effect sizes: Common guidelines and an introduction to the POMADE R package\n\n\nPre-print\n\n\n\n\nJul 21, 2023\n\n\nAnika Poppe, Franziska D. E. Ritter, Leonie Bais, admin, Marie-José van Tol, Branislava Ćurčić-Blake, Gerdina H.M. Pijnenborg, Lisette van der Meer\n\n\nThe efficacy of combining cognitive training and non-invasive brain stimulation: A transdiagnostic systematic review and meta-analysis\n\n\nPsychological Bulletin\n\n\n\n\nMay 26, 2023\n\n\nJoe Reichle, admin, Kimberly J. Vannest, Margaret Foster, Lauren M. Pierson, Sanikan Wattanawongwan, Man Chen, Marcus Fuller, April N. Haas, Bethany H. Bhat, Mary R. Sallese, S. D. Smith, Valeria Yllades, Daira Rodriguez, Amara Yoro, Jay B. Ganz\n\n\nSystematic review of variables related to instruction in augmentative and alternative communication implementation: Group and single-case design\n\n\nAmerican Journal of Speech-Language Pathology\n\n\n\n\nMar 9, 2023\n\n\nYoung Ri Lee, admin\n\n\nComparison of competing approaches to analyzing cross-classified data: Random effects models, ordinary least squares, or fixed effects with cluster robust standard errors\n\n\nPsychological Methods\n\n\n\n\nMar 8, 2023\n\n\nMan Chen, admin, David A. Klingbeil, Ethan R. Van Norman\n\n\nBetween-case standardized mean differences: Flexible methods for single-case designs\n\n\nJournal of School Psychology\n\n\n\n\nNov 22, 2022\n\n\nJennifer R. Ledford, Joseph Lambert, admin, Kathleen N. Zimmerman, Erin E. Barton\n\n\nSingle case design research in Special Education: Next generation standards and considerations\n\n\nExceptional Children\n\n\n\n\nOct 17, 2022\n\n\nMikkel H. Vembye, admin, Terri D. Pigott\n\n\nPower approximations for overall average effects in meta-analysis of dependent effect sizes\n\n\nJournal of Educational and Behavioral Statistics\n\n\n\n\nSep 30, 2022\n\n\nKatherine L. Winters, Javier Jasso, admin, Courtney Byrd\n\n\nInvestigating narrative performance in children with developmental language disorder: A systematic review and meta-analysis\n\n\nJournal of Speech, Language, and Hearing Research\n\n\n\n\nApr 29, 2022\n\n\nMan Chen, admin\n\n\nMulti-level meta-analysis of single-case experimental designs using robust variance estimation\n\n\nPsychological Methods\n\n\n\n\nApr 20, 2022\n\n\nJay B. Ganz, admin, Joe Reichle, Kimberly J. Vannest, Margaret Foster, Marcus Fuller, Lauren M. Pierson, Sanikan Wattanawongwan, Amando Bernal, Man Chen, April N. Haas, Rachel Skov, S. D. Smith, Valeria Yllades\n\n\nAugmentative and Alternative Communication intervention targets for school-aged participants with ASD and ID: A single-case systematic review and meta-analysis\n\n\nReview Journal of Autism and Developmental Disorders\n\n\n\n\nApr 20, 2022\n\n\nJay B. Ganz, admin, Joe Reichle, Kimberly J. Vannest, Margaret Foster, April N. Haas, Lauren M. Pierson, Sanikan Wattanawongwan, Amando Bernal, Man Chen, Rachel Skov, S. D. Smith\n\n\nConsidering instructional contexts in AAC interventions for people with ASD and/or IDD experiencing complex communication needs: A single-case design meta-analysis\n\n\nReview Journal of Autism and Developmental Disorders\n\n\n\n\nApr 20, 2022\n\n\nJay B. Ganz, admin, Joe Reichle, Kimberly J. Vannest, Margaret Foster, Lauren M. Pierson, Sanikan Wattanawongwan, Amando Bernal, Man Chen, April N. Haas, Mary Rose Sallese, Rachel Skov, S. D. Smith\n\n\nParticipant characteristics predicting communication outcomes in AAC implementation for individuals with ASD and IDD: Meta-analysis\n\n\nAugmentative and Alternative Communication\n\n\n\n\nApr 1, 2022\n\n\nadmin, Elizabeth Tipton\n\n\nMeta-Analysis with robust variance estimation: Expanding the range of working models\n\n\nPrevention Science\n\n\n\n\nFeb 8, 2022\n\n\nMegha Joshi, admin, S. Natasha Beretvas\n\n\nCluster wild bootstrapping to handle dependent effect sizes in meta-analysis with a small number of studies\n\n\nResearch Synthesis Methods\n\n\n\n\nFeb 1, 2022\n\n\nCharis L. Wahman, admin, Michaelene M. Ostrosky, Rosa Milagros Santos\n\n\nExamining the effects of social stories on challenging behavior and prosocial skills in young children: A systematic review and meta-analysis\n\n\nTopics in Early Childhood Special Education\n\n\n\n\nApr 20, 2021\n\n\nadmin, Megha Joshi\n\n\nEvaluating the Transition to College Mathematics Course in Texas high schools: Examining heterogeneity across schools and student characteristics\n\n\nGreater Texas Foundation White Paper\n\n\n\n\nFeb 1, 2021\n\n\nLaurie E. McLouth, C. Graham Ford, admin, Crystal Park, Allen C. Sherman, Kelly Trevino, John A. Salsman\n\n\nA systematic review and meta-analysis of effects of psychosocial interventions on spiritual well-being in adults with cancer\n\n\nPsycho-Oncology\n\n\n\n\nJan 5, 2021\n\n\nJennifer R. Ledford, admin\n\n\nSystematic review and meta-analysis of stay-play-talk interventions for improving social behaviors of young children\n\n\nJournal of Positive Behavioral Interventions\n\n\n\n\nDec 16, 2020\n\n\nadmin, Megha Joshi\n\n\nEvaluating the Transition to College Mathematics Course in Texas high schools: Findings from the second year of implementation\n\n\nGreater Texas Foundation White Paper\n\n\n\n\nJul 12, 2020\n\n\nMelissa A. Rodgers, admin\n\n\nEvaluating meta-analytic methods to detect selective reporting in the presence of dependent effect sizes\n\n\nPsychological Methods\n\n\n\n\nMar 26, 2020\n\n\nDaniel M. Swan, admin, S. Natasha Beretvas\n\n\nThe impact of response-guided designs on count outcomes in single-case experimental design baselines\n\n\nEvidence-Based Communication Assessment and Intervention\n\n\n\n\nNov 19, 2019\n\n\nJohn A. Salsman, admin, Stephen M. Schueller, Rosalba Hernandez, Mark Berendsen, Laurie E. Steffen McLouth, Judith T. Moskowitz\n\n\nPsychosocial interventions for cancer survivors: A meta-analysis of effects on positive affect\n\n\nJournal of Cancer Survivorship\n\n\n\n\nAug 2, 2019\n\n\nadmin, Daniel M. Swan, Kyle W. English\n\n\nAn examination of measurement procedures and characteristics of baseline outcome data in single-case research\n\n\nBehavior Modification\n\n\n\n\nJun 30, 2019\n\n\nadmin, Megha Joshi\n\n\nEvaluating the Transition to College Mathematics Course in Texas high schools: Findings from the first year of implementation\n\n\nGreater Texas Foundation White Paper\n\n\n\n\nJun 6, 2019\n\n\nTom V. Merluzzi, admin, Errol J. Philip, Stephanie J. Sohl, Mark Berendsen, John A. Salsman\n\n\nInterventions to enhance self-efficacy in cancer patients and survivors: A meta-analysis of randomized controlled trials\n\n\nPsycho-Oncology\n\n\n\n\nApr 29, 2019\n\n\nCrystal L. Park, admin, Kelly Trevino, Allen C. Sherman, Craig Esposito, Mark Berendsen, John A. Salsman\n\n\nEffects of psychosocial interventions on meaning and purpose in adults with cancer: A systematic review and meta-analysis\n\n\nCancer\n\n\n\n\nApr 1, 2019\n\n\nadmin\n\n\nProcedural sensitivities of effect sizes for single-case designs with behavioral outcome measures\n\n\nPsychological Methods\n\n\n\n\nMar 1, 2019\n\n\nadmin, Melissa A. Rodgers\n\n\nTesting for funnel plot asymmetry of standardized mean differences\n\n\nResearch Synthesis Methods\n\n\n\n\nJan 7, 2019\n\n\nElizabeth Tipton, admin, Hedyeh Ahmadi\n\n\nCurrent practices in meta-regression in psychology, education, and medicine\n\n\nResearch Synthesis Methods\n\n\n\n\nDec 27, 2018\n\n\nElizabeth Tipton, admin, Hedyeh Ahmadi\n\n\nA history of meta-regression: Technical, conceptual, and practical developments between 1974 and 2018\n\n\nResearch Synthesis Methods\n\n\n\n\nNov 2, 2018\n\n\nadmin, Elizabeth Tipton\n\n\nSmall sample methods for cluster-robust variance estimation and hypothesis testing in fixed effects models\n\n\nJournal of Business and Economic Statistics\n\n\n\n\nJun 2, 2018\n\n\nSamuel L. Odom, Erin E. Barton, Brian Reichow, Hariharan Swaminathan, admin\n\n\nBetween-case standardized effect size analysis of single case design: Examination of the two methods\n\n\nResearch in Developmental Disabilities, 79, 88-96\n\n\n\n\nMay 1, 2018\n\n\nDaniel M. Swan, admin\n\n\nA gradual effects model for single-case designs\n\n\nMultivariate Behavioral Research\n\n\n\n\nMar 7, 2018\n\n\nKathleen N. Zimmerman, admin, Jennifer R. Ledford, Erin E. Barton, Katherine E. Severini, Blair P. Lloyd\n\n\nSingle-case synthesis tools II: Comparing overlap measures and parametric effect sizes for synthesizing antecedent sensory-based interventions\n\n\nResearch in Developmental Disabilities\n\n\n\n\nMar 3, 2018\n\n\nKathleen N. Zimmerman, Jennifer R. Ledford, Katherine E. Severini, admin, Erin E. Barton, Blair P. Lloyd\n\n\nSingle-case synthesis tools I: Evaluating the quality and rigor of research on antecedent sensory-based interventions\n\n\nResearch in Developmental Disabilities\n\n\n\n\nFeb 1, 2018\n\n\nadmin\n\n\nUsing response ratios for meta-analyzing single-case designs with behavioral outcomes\n\n\nJournal of School Psychology\n\n\n\n\nNov 1, 2017\n\n\nDaniel M. Maggin, Kathleen L. Lane, admin\n\n\nIntroduction to the special issue on single-case systematic reviews and meta-analysis\n\n\nRemedial and Special Education\n\n\n\n\nJul 1, 2017\n\n\nDaniel M. Maggin, admin, Austin H. Johnson\n\n\nA meta-analysis of school-based group contingency interventions for students with challenging behavior: An update\n\n\nRemedial and Special Education\n\n\n\n\nJul 1, 2017\n\n\nErin E. Barton, admin, Daniel M. Maggin, Brian Reichow\n\n\nA meta-analysis of technology-aided instruction and intervention for students with ASD\n\n\nRemedial and Special Education\n\n\n\n\nMar 1, 2017\n\n\nEric A. Common, Kathleen L. Lane, admin, Austin H. Johnson, Liane E. Johl\n\n\nFunctional assessment-based interventions for students with or at-risk for high incidence disabilities: Field-testing single-case synthesis methods\n\n\nRemedial and Special Education\n\n\n\n\nJan 1, 2017\n\n\nadmin, John Ferron\n\n\nResearch synthesis and meta-analysis of single-case designs\n\n\nIn J. M. Kauffman, D. P. Hallahan, & P. C. Pullen (Eds.), Handbook of Special Education, 2nd Edition. New York, NY: Routledge\n\n\n\n\nDec 19, 2016\n\n\nJeffrey C. Valentine, Emily E. Tanner-Smith, admin, Timothy S. Lau\n\n\nBetween-case standardized mean difference effect sizes for single-case designs: A primer and tutorial using the scdhlm web application\n\n\nOslo, Norway: The Campbell Collaboration\n\n\n\n\nDec 15, 2015\n\n\nElizabeth Tipton, admin\n\n\nSmall-sample adjustments for tests of moderators and model fit using robust variance estimation in meta-regression\n\n\nJournal of Educational and Behavioral Statistics\n\n\n\n\nSep 1, 2015\n\n\nadmin\n\n\nMeasurement-comparable effect sizes for single-case studies of free-operant behavior\n\n\nPsychological Methods\n\n\n\n\nAug 10, 2015\n\n\nJohn A. Salsman, admin, Heather S. Jim, Alexis R. Munoz, Thomas V. Merluzzi, Logan George, Crystal L. Park, Suzanne C. Danhauer, Allen C. Sherman, Mallory A. Snyder, George Fitchett\n\n\nA meta-analytic approach to examine the relationship between religion/spirituality and mental health in cancer\n\n\nCancer\n\n\n\n\nAug 10, 2015\n\n\nHeather S. Jim, admin, Crystal L. Park, Suzanne C. Danhauer, Allen C. Sherman, George Fitchett, Thomas V. Merluzzi, Alexis R. Munoz, Logan George, Mallory A. Snyder, John A. Salsman\n\n\nReligion, spirituality, and physical health in cancer patients: A meta-analysis\n\n\nCancer\n\n\n\n\nAug 10, 2015\n\n\nAllen C. Sherman, Thomas V. Merluzzi, admin, Crystal L. Park, Logan George, George Fitchett, Heather S. Jim, Alexis R. Munoz, Suzanne C. Danhauer, Mallory A. Snyder, John A. Salsman\n\n\nA meta-analytic review of religious or spiritual involvement and social health among cancer patients\n\n\nCancer\n\n\n\n\nJun 19, 2015\n\n\nadmin, Daniel M. Swan\n\n\nFour methods for analyzing partial interval recording data, with application to single-case research\n\n\nMultivariate Behavioral Research\n\n\n\n\nOct 1, 2014\n\n\nadmin, Larry V. Hedges, William R. Shadish\n\n\nDesign-comparable effect sizes in multiple baseline designs: A general modeling framework\n\n\nJournal of Educational and Behavioral Statistics\n\n\n\n\nAug 1, 2014\n\n\nadmin, Christopher Runyon\n\n\nAlternating renewal process models for behavioral observation: Simulation methods and validity implications\n\n\nBehavioral Disorders\n\n\n\n\nJul 18, 2014\n\n\nWilliam R. Shadish, Larry V. Hedges, admin, David Rindskopf, Jonathan G. Boyajian, Kristynn J. Sullivan\n\n\nAnalyzing single-case designs: d, G, hierarchical models, Bayesian estimators, generalized additive models, and the hopes and fears of researchers about analyses\n\n\nIn T. R. Kratochwill & J. R. Levin (Eds.), Single-Case Intervention Research: Methodological and Data-Analysis Advances. Washington, D.C.: American Psychological Association\n\n\n\n\nApr 1, 2014\n\n\nWilliam R. Shadish, Larry V. Hedges, admin\n\n\nAnalysis and meta-analysis of single-case designs with a standardized mean difference statistic: A primer and applications\n\n\nJournal of School Psychology\n\n\n\n\nMar 1, 2014\n\n\nadmin\n\n\nConverting from d to r to z when the design uses extreme groups, dichotomization, or experimental control\n\n\nPsychological Methods\n\n\n\n\nAug 23, 2013\n\n\nLarry V. Hedges, admin, William R. Shadish\n\n\nA standardized mean difference effect size for multiple baseline designs\n\n\nResearch Synthesis Methods\n\n\n\n\nJul 18, 2013\n\n\nWilliam R. Shadish, Larry V. Hedges, admin, Jonathan G. Boyajian, Kristynn J. Sullivan, Alma Andrade, Jeannette Barrientos\n\n\nA d-statistic for single-case designs that is equivalent to the usual between-groups d-statistics\n\n\nNeuropsychological Rehabilitation\n\n\n\n\nJun 1, 2013\n\n\nadmin\n\n\nOperationally comparable effect sizes for meta-analysis of single-case research\n\n\nDissertation. Northwestern University, Department of Statistics\n\n\n\n\nAug 14, 2012\n\n\nLarry V. Hedges, admin, William R. Shadish\n\n\nA standardized mean difference effect size for single case designs\n\n\nResearch Synthesis Methods\n\n\n\n\nOct 1, 2009\n\n\nadmin, James P. Spillane\n\n\nQuestion-order effects in social network name generators\n\n\nSocial Networks\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "people/index.html",
    "href": "people/index.html",
    "title": "People",
    "section": "",
    "text": "Jingru Zhang \n              \n            \n\n            \n              Graduate student \n            \n            \n            \n              University of Wisconsin-Madison\n            \n\n            \n          \n      \n      \n    \n  \n     \n    \n    \n      \n      \n      \n      \n        \n          \n          \n            \n          \n          \n          \n            \n            \n              \n              Man Chen \n              \n            \n\n            \n              Graduate student \n            \n            \n            \n              University of Wisconsin-Madison\n            \n\n            \n          \n      \n      \n    \n  \n     \n    \n    \n      \n      \n      \n      \n        \n          \n          \n            \n          \n          \n          \n            \n            \n              \n              Paulina Grekov \n              \n            \n\n            \n              Graduate student \n            \n            \n            \n              University of Wisconsin-Madison\n            \n\n            \n          \n      \n      \n    \n  \n    \n  \n\n\n\n\nNo matching items"
  },
  {
    "objectID": "people/index.html#current-students",
    "href": "people/index.html#current-students",
    "title": "People",
    "section": "",
    "text": "Jingru Zhang \n              \n            \n\n            \n              Graduate student \n            \n            \n            \n              University of Wisconsin-Madison\n            \n\n            \n          \n      \n      \n    \n  \n     \n    \n    \n      \n      \n      \n      \n        \n          \n          \n            \n          \n          \n          \n            \n            \n              \n              Man Chen \n              \n            \n\n            \n              Graduate student \n            \n            \n            \n              University of Wisconsin-Madison\n            \n\n            \n          \n      \n      \n    \n  \n     \n    \n    \n      \n      \n      \n      \n        \n          \n          \n            \n          \n          \n          \n            \n            \n              \n              Paulina Grekov \n              \n            \n\n            \n              Graduate student \n            \n            \n            \n              University of Wisconsin-Madison\n            \n\n            \n          \n      \n      \n    \n  \n    \n  \n\n\n\n\nNo matching items"
  },
  {
    "objectID": "people/index.html#alumni",
    "href": "people/index.html#alumni",
    "title": "People",
    "section": "Alumni",
    "text": "Alumni\n\n\n\n  \n   \n\n     \n    \n    \n      \n      \n      \n      \n        \n          \n          \n            \n          \n          \n          \n            \n            \n              \n              Christopher Runyon \n              \n            \n\n            \n              Measurement Scientist \n            \n            \n            \n              National Board of Medical Examiners\n            \n\n            \n          \n      \n      \n    \n  \n     \n    \n    \n      \n      \n      \n      \n        \n          \n          \n            \n          \n          \n          \n            \n            \n              \n              Daniel M. Swan \n              \n            \n\n            \n              Research Associate \n            \n            \n            \n              Prevention Science Institute, University of Oregon\n            \n\n            \n          \n      \n      \n    \n  \n     \n    \n    \n      \n      \n      \n      \n        \n          \n          \n            \n          \n          \n          \n            \n            \n              \n              Gleb Furman \n              \n            \n\n            \n              Senior Quantitative Research Scientist \n            \n            \n            \n              Gibson Consulting Group\n            \n\n            \n          \n      \n      \n    \n  \n     \n    \n    \n      \n      \n      \n      \n        \n          \n          \n            \n          \n          \n          \n            \n            \n              \n              Megha Joshi \n              \n            \n\n            \n              Quantitative Researcher \n            \n            \n            \n              American Institutes for Research\n            \n\n            \n          \n      \n      \n    \n  \n     \n    \n    \n      \n      \n      \n      \n        \n          \n          \n            \n          \n          \n          \n            \n            \n              \n              Young Ri Lee \n              \n            \n\n            \n              Postdoctoral Scholar \n            \n            \n            \n              Urban Education Institute at the University of Chicago\n            \n\n            \n          \n      \n      \n    \n  \n    \n  \n\n\n\n\nNo matching items"
  },
  {
    "objectID": "software/SingleCaseES/index.html",
    "href": "software/SingleCaseES/index.html",
    "title": "SingleCaseES",
    "section": "",
    "text": "An R package for calculating basic effect size indices for single-case designs, including several non-overlap measures and parametric effect size measures, and for estimating the gradual effects model developed by Swan and Pustejovsky (2017).\n\nAvailable on the Comprehensive R Archive Network\nSource code and installation instructions on Github\nSingle case effect size calculator: An interactive web application for calculating basic effect size indices.\nGradual Effect Model calculator: An interactive web application for estimating effect sizes using the gradual effects model."
  },
  {
    "objectID": "software/scdhlm/index.html",
    "href": "software/scdhlm/index.html",
    "title": "scdhlm",
    "section": "",
    "text": "An R package implementing several methods of estimating a design-comparable standardized mean difference effect size based on data from a single-case design. Methods include those from Hedges, Pustejovsky, & Shadish (2012, 2013) and Pustejovsky, Hedges, & Shadish (2014).\n\nAvailable on the Comprehensive R Archive Network\nInstallation instructions\nSource code on Github\nscdhlm: An interactive web application for calculating design-comparable standardized mean difference effect sizes."
  },
  {
    "objectID": "software/lmeInfo/index.html",
    "href": "software/lmeInfo/index.html",
    "title": "lmeInfo",
    "section": "",
    "text": "lmeInfo provides analytic derivatives and information matrices for fitted linear mixed effects models and generalized least squares models estimated using nlme::lme() and nlme::gls(), respectively. The package includes functions for estimating the sampling variance-covariance of variance component parameters using the inverse Fisher information. The variance components include the parameters of the random effects structure (for lme models), the variance structure, and the correlation structure. The expected and average forms of the Fisher information matrix are used in the calculations, and models estimated by full maximum likelihood or restricted maximum likelihood are supported. The package also includes a function for estimating standardized mean difference effect sizes (Pustejovsky et al., 2014) based on fitted lme or gls models.\n\nR package available on the Comprehensive R Archive Network\nR source code on Github"
  },
  {
    "objectID": "software/clubSandwich/index.html",
    "href": "software/clubSandwich/index.html",
    "title": "clubSandwich",
    "section": "",
    "text": "R and Stata packages for calculating cluster-robust variance estimators (i.e., sandwich estimators) with small-sample corrections, including the bias-reduced linearization estimator of Bell and McCaffrey (2002) and extensions proposed in Tipton (2015), Tipton and Pustejovsky (2015), and Pustejovsky and Tipton (2016).\n\nR package available on the Comprehensive R Archive Network\nR source code on Github\nStata package available on the SSC Archive\nStata source code on Github"
  },
  {
    "objectID": "publication/using-response-ratios/index.html",
    "href": "publication/using-response-ratios/index.html",
    "title": "Using response ratios for meta-analyzing single-case designs with behavioral outcomes",
    "section": "",
    "text": "Supplementary materials  Journal  Pre-Print\nMethods for meta-analyzing single-case designs (SCDs) are needed to inform evidence-based practice in clinical and school settings and to draw broader and more defensible generalizations in areas where SCDs comprise a large part of the research base. The most widely used outcomes in single-case research are measures of behavior collected using systematic direct observation, which typically take the form of rates or proportions. For studies that use such measures, one simple and intuitive way to quantify effect sizes is in terms of proportionate change from baseline, using an effect size known as the log response ratio. This paper describes methods for estimating log response ratios and combining the estimates using meta-analysis. The methods are based on a simple model for comparing two phases, where the level of the outcome is stable within each phase and the repeated outcome measurements are independent. Although auto-correlation will lead to biased estimates of the sampling variance of the effect size, meta-analysis of response ratios can be conducted with robust variance estimation procedures that remain valid even when sampling variance estimates are biased. The methods are demonstrated using data from a recent meta-analysis on group contingency interventions for student problem behavior."
  },
  {
    "objectID": "publication/Transition-to-College-Mathematics-Year-2/index.html",
    "href": "publication/Transition-to-College-Mathematics-Year-2/index.html",
    "title": "Evaluating the Transition to College Mathematics Course in Texas high schools: Findings from the second year of implementation",
    "section": "",
    "text": "White paper  PDF\nTexas House Bill 5 introduced requirements that school districts partner with institutions of higher education to provide college preparatory courses in mathematics and English for high school seniors who are not yet college ready. As districts and college partners begin to respond to these provisions, there is a need for empirical research on the effects of different approaches to implementing the college preparatory courses. In response to House Bill 5 requirements, the Charles A. Dana Center has developed a model college preparatory mathematics course, Transition to College Mathematics Course (TCMC), which has been adopted by dozens of school districts across Texas over the past several school years. We examine the effects of TCMC on students’ progress into post-secondary education by comparing students who participated in TCMC during the 2017-18 school year (the second year of implementation) to observationally similar students from the same cohort but who did not enroll in the course. We find that students who took TCMC graduated at higher rates than comparison students. They had similar rates of overall enrollment in post-secondary education, but enrolled in community colleges at higher rates and in 4-year colleges or universities at lower rates than did comparison students. Enrollment tended to increase over the course of four semesters after high school graduation. Relative to comparison students, students who took TCMC were also less likely to take and less likely to pass college-level math coursework. These results must be interpreted cautiously because we were unable to fully assess and account for students’ college-readiness status at the start of their senior year."
  },
  {
    "objectID": "publication/Three-level-BC-SMD/index.html",
    "href": "publication/Three-level-BC-SMD/index.html",
    "title": "Between-case standardized mean differences: Flexible methods for single-case designs",
    "section": "",
    "text": "Journal Supplementary materials  Pre-Print  Code\nSingle-case designs (SCDs) are a class of research methods for evaluating the effects of academic and behavioral interventions in educational and clinical settings. Although visual analysis is typically the first and main method for primary analysis of data from SCDs, quantitative methods are useful for synthesizing results and drawing systematic generalizations across bodies of single-case research. Researchers who are interested in synthesizing findings across SCDs and between-group designs might consider using the between-case standardized mean difference (BC-SMD) effect size, which aims to put results from both types of studies into a common metric. Current BC-SMD methods are limited to treatment reversal design and across-participant multiple baseline design, yet more complex designs are used in practice. In this study, we extend available BC-SMD methods to several variations of the multiple baseline design, including the replicated multiple baseline across behaviors or settings, the clustered multiple baseline design, and the multivariate multiple baseline across participants. For each variation, we describe methods for estimating BC-SMD effect sizes and illustrate our proposed approach by re-analyzing data from a published SCD study."
  },
  {
    "objectID": "publication/TAII-meta-analysis/index.html",
    "href": "publication/TAII-meta-analysis/index.html",
    "title": "A meta-analysis of technology-aided instruction and intervention for students with ASD",
    "section": "",
    "text": "Journal\nThe adoption of methods and strategies validated through rigorous, experimentally oriented research is a core professional value of special education. We conducted a systematic review and meta-analysis examining the experimental literature on Technology-Aided Instruction and Intervention (TAII) using research identified as part of the National Autism Professional Development Project. We applied novel between-case effect size methods to the TAII single-case research base. In addition, we used meta-analytic methodologies to examine the methodological quality of the research, calculate average effect sizes to quantify the level of evidence for TAII, and compare effect sizes across single-case and group-based experimental research. Results identified one category of TAII—computer-assisted instruction—as an evidence-based practice across both single-case and group studies. The remaining two categories of TAII—augmentative and alternative communication and virtual reality—were not identified as evidence-based using What Works Clearinghouse summary ratings."
  },
  {
    "objectID": "publication/SMD-for-SCD/index.html",
    "href": "publication/SMD-for-SCD/index.html",
    "title": "A standardized mean difference effect size for single case designs",
    "section": "",
    "text": "Journal  R package  Web app\nSingle case designs are a set of research methods for evaluating treatment effects by assigning different treatments to the same individual and measuring outcomes over time and are used across fields such as behavior analysis, clinical psychology, special education, and medicine. Emerging standards for single case designs have focused attention on the need for effect sizes for summarizing and meta-analyzing findings from the designs; although many effect size measures have been proposed, there is little consensus regarding their use. This article proposes a new effect size measure for single case research that is directly comparable with the standardized mean difference (Cohen’s d) often used in between-subjects designs. Techniques are provided for estimating the new effect size, as well as its variance, from balanced or unbalanced treatment reversal designs. The proposed estimation methods are further evaluated using a simulation study and then demonstrated in two applications."
  },
  {
    "objectID": "publication/Single-case-next-generation-standards/index.html",
    "href": "publication/Single-case-next-generation-standards/index.html",
    "title": "Single case design research in Special Education: Next generation standards and considerations",
    "section": "",
    "text": "Journal Supplementary materials  Pre-Print\nSingle case design has a long history of use for assessing intervention effectiveness for children with disabilities. Although these designs have been widely employed for more than 50 years, recent years have been especially dynamic in terms of growth in the use of single case design and application of standards designed to improve the validity and applicability of findings. This growth expanded possibilities and inspired new questions about the contributions this methodology can make to generalizable knowledge about intervention in special education. In this paper, we discuss and extend previous standards for studies using single case designs (i.e., Horner et al., 2005). We identify new suggestions for internal validity, generality and acceptability, and reporting. We also provide considerations for single case synthesis and discuss the complexities of assessing accumulating evidence for a given practice."
  },
  {
    "objectID": "publication/school-based-group-contingencies-meta-analysis/index.html",
    "href": "publication/school-based-group-contingencies-meta-analysis/index.html",
    "title": "A meta-analysis of school-based group contingency interventions for students with challenging behavior: An update",
    "section": "",
    "text": "Supplementary materials  Journal  Pre-Print\nGroup contingencies are recognized as a potent intervention for addressing challenging student behavior in the classroom, with research reviews supporting the use of this intervention platform going back more than four decades. Over this time period, the field of education has increasingly emphasized the role of research evidence for informing practice, as reflected in the increased use of systematic reviews and meta-analyses. In the current article, we continue this trend by applying recently developed between-case effect size measures and transparent visual analysis procedures to synthesize an up-to-date set of group contingency studies that used single-case designs. Results corroborated recent systematic reviews by indicating that group contingencies are generally effective—particularly for addressing challenging behavior in general education classrooms. However, our review highlights the need for more research on students with disabilities and the need to collect and report information about participants’ functional level."
  },
  {
    "objectID": "publication/SCD-synthesis-tools-II/index.html",
    "href": "publication/SCD-synthesis-tools-II/index.html",
    "title": "Single-case synthesis tools II: Comparing overlap measures and parametric effect sizes for synthesizing antecedent sensory-based interventions",
    "section": "",
    "text": "Journal\nVarying methods for evaluating the outcomes of single case research designs (SCD) are currently used in reviews and meta-analyses of interventions. Quantitative effect size measures are often presented alongside visual analysis conclusions. Six measures across two classes—overlap measures (percentage non-overlapping data, improvement rate difference, and Tau) and parametric within-case effect sizes (standardized mean difference and log response ratio [increasing and decreasing])—were compared to determine if choice of synthesis method within and across classes impacts conclusions regarding effectiveness. The effectiveness of sensory-based interventions (SBI), a commonly used class of treatments for young children, was evaluated. Separately from evaluations of rigor and quality, authors evaluated behavior change between baseline and SBI conditions. SBI were unlikely to result in positive behavior change across all measures except IRD. However, subgroup analyses resulted in variable conclusions, indicating that the choice of measures for SCD meta-analyses can impact conclusions. Suggestions for using the log response ratio in SCD meta-analyses and considerations for understanding variability in SCD meta-analysis conclusions are discussed."
  },
  {
    "objectID": "publication/RVE-Meta-analysis-expanding-the-range/index.html",
    "href": "publication/RVE-Meta-analysis-expanding-the-range/index.html",
    "title": "Meta-Analysis with robust variance estimation: Expanding the range of working models",
    "section": "",
    "text": "Journal Supplementary materials  Pre-Print  Slides  Code  Data  Video\nIn prevention science and related fields, large meta-analyses are common, and these analyses often involve dependent effect size estimates. Robust variance estimation (RVE) methods provide a way to include all dependent effect sizes in a single meta-regression model, even when the nature of the dependence is unknown. RVE uses a working model of the dependence structure, but the two currently available working models are limited to each describing a single type of dependence. Drawing on flexible tools from multivariate meta-analysis, this paper describes an expanded range of working models, along with accompanying estimation methods, which offer benefits in terms of better capturing the types of data structures that occur in practice and improving the efficiency of meta-regression estimates. We describe how the methods can be implemented using existing software (the ‘metafor’ and ‘clubSandwich’ packages for R) and illustrate the approach in a meta-analysis of randomized trials examining the effects of brief alcohol interventions for adolescents and young adults."
  },
  {
    "objectID": "publication/RVE-for-meta-regression/index.html",
    "href": "publication/RVE-for-meta-regression/index.html",
    "title": "Small-sample adjustments for tests of moderators and model fit using robust variance estimation in meta-regression",
    "section": "",
    "text": "Journal  R package  Pre-Print\nMeta-analyses often include studies that report multiple effect sizes based on a common pool of subjects or that report effect sizes from several samples that were treated with very similar research protocols. The inclusion of such studies introduces dependence among the effect size estimates. When the number of studies is large, robust variance estimation (RVE) provides a method for pooling dependent effects, even when information on the exact dependence structure is not available. When the number of studies is small or moderate, however, test statistics and confidence intervals based on RVE can have inflated Type I error. This article describes and investigates several small-sample adjustments to F-statistics based on RVE. Simulation results demonstrate that one such test, which approximates the test statistic using Hotelling’s T-squared distribution, is level-α and uniformly more powerful than the others. An empirical application demonstrates how results based on this test compare to the large-sample F-test."
  },
  {
    "objectID": "publication/Religion-spirituality-social-health/index.html",
    "href": "publication/Religion-spirituality-social-health/index.html",
    "title": "A meta-analytic review of religious or spiritual involvement and social health among cancer patients",
    "section": "",
    "text": "Journal\nReligion and spirituality (R/S) play an important role in the daily lives of many cancer patients. There has been great interest in determining whether R/S factors are related to clinically relevant health outcomes. In this meta-analytic review, the authors examined associations between dimensions of R/S and social health (eg, social roles and relationships). A systematic search of the PubMed, PsycINFO, Cochrane Library, and Cumulative Index to Nursing and Allied Health Literature databases was conducted, and data were extracted by 4 pairs of investigators. Bivariate associations between specific R/S dimensions and social health outcomes were examined in a meta-analysis using a generalized estimating equation approach. In total, 78 independent samples encompassing 14,277 patients were included in the meta-analysis. Social health was significantly associated with overall R/S (Fisher z effect size = .20; P &lt; .001) and with each of the R/S dimensions (affective R/S effect size = 0.31 [P &lt; .001]; cognitive R/S effect size = .10 [P &lt; .01]; behavioral R/S effect size = .08 [P &lt; .05]; and ‘other’ R/S effect size = .13 [P &lt; .001]). Within these dimensions, specific variables tied to socialhealth included spiritual well being, spiritual struggle, images of God, R/S beliefs, and composite R/S measures (all P values &lt; .05). None of the demographic or clinical moderating variables examined were significant. Results suggest that several R/S dimensions are modestly associated with patients’ capacity to maintain satisfying social roles and relationships in the context of cancer. Further research is needed to examine the temporal nature of these associations and the mechanisms that underlie them."
  },
  {
    "objectID": "publication/Religion-spirituality-mental-health/index.html",
    "href": "publication/Religion-spirituality-mental-health/index.html",
    "title": "A meta-analytic approach to examine the relationship between religion/spirituality and mental health in cancer",
    "section": "",
    "text": "Journal\nReligion and spirituality (R/S) are patient-centered factors and often are resources for managing the emotional sequelae of the cancer experience. Studies investigating the correlation between R/S (eg, beliefs, experiences, coping) and mental health (eg, depression, anxiety, well being) in cancer have used very heterogeneous measures and have produced correspondingly inconsistent results. A meaningful synthesis of these findings has been lacking; thus, the objective of this review was to conduct a meta-analysis of the research on R/S and mental health. Four electronic databases were systematically reviewed, and 2073 abstracts met initial selection criteria. Reviewer pairs applied standardized coding schemes to extract indices of the correlation between R/S and mental health. In total, 617 effect sizes from 148 eligible studies were synthesized using meta-analytic generalized estimating equations, and subgroup analyses were performed to examine moderators of effects. The estimated mean correlation (Fisher z) was 0.19 (95% confidence interval [CI], 0.16-0.23), which varied as a function of R/S dimensions: affective R/S (z=0.38; 95% CI, 0.33-0.43), behavioral R/S (z=0.03; 95% CI, 20.02-0.08), cognitive R/S (z=0.10; 95% CI, 0.06-0.14), and ‘other’ R/S (z=0.08; 95% CI, 0.03-0.13). Aggregate, study-level demographic and clinical factors were not predictive of the relation between R/S and mental health. There was little indication of publication or reporting biases. The correlation between R/S and mental health generally was positive. The strength of that correlation was modest and varied as a function of the R/S dimensions and mental health domains assessed. The identification of optimal R/S measures and more sophisticated methodological approaches are needed to advance research."
  },
  {
    "objectID": "publication/Question-order-effects/index.html",
    "href": "publication/Question-order-effects/index.html",
    "title": "Question-order effects in social network name generators",
    "section": "",
    "text": "Journal  Pre-Print  Data\nSocial network surveys are an important tool for empirical research in a variety of fields, including the study of social capital and the evaluation of educational and social policy. A growing body of methodological research sheds light on the validity and reliability of social network survey data regarding a single relation, but much less attention has been paid to the measurement of multiplex networks and the validity of comparisons among criterion relations. In this paper, we identify ways that surveys designed to collect multiplex social network data might be vulnerable to question-order effects. We then test several hypotheses using a split-ballot experiment embedded in an online multiple name generator survey of teachers’ advice networks, collected for a study of complete networks. We conclude by discussing implications for the design of multiple name generator social network surveys."
  },
  {
    "objectID": "publication/psychosocial-interventions-for-spiritual-well-being/index.html",
    "href": "publication/psychosocial-interventions-for-spiritual-well-being/index.html",
    "title": "A systematic review and meta-analysis of effects of psychosocial interventions on spiritual well-being in adults with cancer",
    "section": "",
    "text": "Journal\nObjective Spiritual well‐being (SpWb) is an important dimension of health‐related quality of life for many cancer patients. Accordingly, an increasing number of psychosocial intervention studies have included SpWb as a study endpoint, and may improve SpWb even if not designed explicitly to do so. This meta‐analysis of randomized controlled trials (RCTs) evaluated effects of psychosocial interventions on SpWb in adults with cancer and tested potential moderators of intervention effects. Methods Six literature databases were systematically searched to identify RCTs of psychosocial interventions in which SpWb was an outcome. Doctoral‐level rater pairs extracted data using Covidence following Preferred Reporting Items for Systematic reviews and Meta‐Analyses guidelines. Standard meta‐analytic techniques were applied, including meta‐regression with robust variance estimation and risk‐of‐bias sensitivity analysis. Results Forty‐one RCTs were identified, encompassing 88 treatment effects among 3883 survivors. Interventions were associated with significant improvements in SpWb (\\(g = 0.22\\), 95% CI [0.14, 0.29], \\(p &lt; 0.0001\\)). Studies assessing the FACIT‐Sp demonstrated larger effect sizes than did those using other measures of SpWb (\\(g = 0.25\\), 95% CI [0.17, 0.34], vs. \\(g = 0.10\\), 95% CI [−0.02, 0.23], \\(p = 0.03\\)). No other intervention, clinical, or demographic characteristics significantly moderated effect size. Conclusions Psychosocial interventions are associated with small‐to‐medium‐sized effects on SpWb among cancer survivors. Future research should focus on conceptually coherent interventions explicitly targeting SpWb and evaluate interventions in samples that are diverse with respect to race and ethnicity, sex and cancer type."
  },
  {
    "objectID": "publication/procedural-sensitivities-of-SCD-effect-sizes/index.html",
    "href": "publication/procedural-sensitivities-of-SCD-effect-sizes/index.html",
    "title": "Procedural sensitivities of effect sizes for single-case designs with behavioral outcome measures",
    "section": "",
    "text": "Supplementary materials  Journal  Pre-Print\nA wide variety of effect size indices have been proposed for quantifying the magnitude of treatment effects in single-case designs. Commonly used measures include parametric indices such as the standardized mean difference, as well as non-overlap measures such as the percentage of non-overlapping data, improvement rate difference, and non-overlap of all pairs. Currently, little is known about the properties of these indices when applied to behavioral data collected by systematic direct observation, even though systematic direct observation is the most common method for outcome measurement in single-case research. This study uses Monte Carlo simulation to investigate the properties of several widely used single-case effect size measures when applied to systematic direct observation data. Results indicate that the magnitude of the non-overlap measures and of the standardized mean difference can be strongly influenced by procedural details of the study’s design, which is a significant limitation to using these indices as effect sizes for meta-analysis of single-case designs. A less widely used parametric index, the log-response ratio, has the advantage of being insensitive to sample size and observation session length, although its magnitude is influenced by the use of partial interval recording."
  },
  {
    "objectID": "publication/Operationally-comparable-effect-sizes/index.html",
    "href": "publication/Operationally-comparable-effect-sizes/index.html",
    "title": "Operationally comparable effect sizes for meta-analysis of single-case research",
    "section": "",
    "text": "Synopsis  PDF\nThis thesis studies quantitative methods for summarizing and synthesizing single-case studies, a class of research designs for evaluating the effects of interventions through repeated measurement of individuals. Despite long-standing interest in meta-analytic synthesis of single-case research, there remains a lack of consensus about appropriate methods, even about the most basic question of what effect size metrics are useful and appropriate. I argue that operational comparability, or invariance to heterogeneous operational procedures, is crucial property for an effect size metric. I then consider two problems with operational comparability that arise in single-case research. The first problem is to find effect sizes that can be applied across studies that use different research designs, such as single-case designs and two-group randomized experiments. The second problem is to find effect sizes that can be applied across studies that use varied operations for measuring the same construct. To address each of these problems, I propose structural models that capture essential features of multiple relevant operations (either design-related operations or measurement-related operations). I then use these structural models to precisely define target effect size parameters and to consider identification issues and estimation strategies.\nChapter 1 defines operational comparability and situates the concept within the broad methodological concerns of meta-analysis, then reviews relevant features of single-case research and previously proposed effect sizes. Chapter 2 describes an abstract set of modeling criteria for constructing design-comparable effect sizes. Chapters 3 applies the general criteria to the case of standardized mean differences and proposes an effect size estimator based on restricted maximum likelihood. Chapter 4 presents several applications of the proposed models and methods. Chapter 5 proposes measurement-comparability model and defines effect size measures for use in studies of free-operant behavior, one of the most common classes of outcomes in single-case research. Chapter 6 extends the proposed effect size models to incorporate more complex features, including time trends and serial dependence, and studies a method of estimating those models through a combination of marginal quasi-likelihood and Gaussian pseudo-likelihood estimating equations. Chapter 7 collects various further extensions, areas for further research, and concluding thoughts."
  },
  {
    "objectID": "publication/measurement-procedures-and-baseline-outcomes/index.html",
    "href": "publication/measurement-procedures-and-baseline-outcomes/index.html",
    "title": "An examination of measurement procedures and characteristics of baseline outcome data in single-case research",
    "section": "",
    "text": "Journal  Pre-Print  Code  Data\nThere has been growing interest in using statistical methods to analyze data and estimate effect size indices from studies that use single-case designs (SCDs), as a complement to traditional visual inspection methods. The validity of a statistical method rests on whether its assumptions are plausible representations of the process by which the data were collected, yet there is evidence that some assumptions—particularly regarding normality of error distributions—may be inappropriate for single-case data. To develop more appropriate modelling assumptions and statistical methods, researchers must attend to the features of real SCD data. In this study, we examine several features of SCDs with behavioral outcome measures in order to inform development of statistical methods. Drawing on a corpus of over 300 studies, including approximately 1800 cases, from seven systematic reviews that cover a range of interventions and outcome constructs, we report the distribution of study designs, distribution of outcome measurement procedures, and features of baseline outcome data distributions for the most common types of measurements used in single-case research. We discuss implications for the development of more realistic assumptions regarding outcome distributions in SCD studies, as well as the design of Monte Carlo simulation studies evaluating the performance of statistical analysis techniques for SCED data."
  },
  {
    "objectID": "publication/investigating-narrative-performance/index.html",
    "href": "publication/investigating-narrative-performance/index.html",
    "title": "Investigating narrative performance in children with developmental language disorder: A systematic review and meta-analysis",
    "section": "",
    "text": "Journal Supplementary materials  Pre-Print  Code  Data\nPurpose: Speech-language pathologists (SLPs) typically examine narrative performance when completing a comprehensive language assessment. However, there is significant variability in the methodologies used to evaluate narration. The primary aims of this systematic review and meta-analysis were to a) investigate how narrative assessment type (e.g., macrostructure, microstructure, internal state language) differentiates typically developing (TD) children from children with developmental language disorder (DLD), or, TD–DLD group differences, b) identify specific narrative assessment measures (e.g., number of different words) that result in greater TD–DLD differences, and, c) evaluate participant and sample characteristics (e.g., DLD inclusionary criteria) that may uniquely influence performance differences. Method: Three electronic databases (PsychInfo, ERIC, and PubMed) and ASHAWire were searched on July 30, 2019 to locate studies that reported oral narrative language measures for both DLD and TD groups between ages 4 and 12 years; studies focusing on written narration or other developmental disorders only were excluded. Thirty-seven primary studies were identified via a three-step study selection procedure. We extracted data related to the sample participants, the narrative task(s) and assessment measures, and research design. Standardized mean differences using a bias-corrected Hedges’ \\(g\\) were the calculated effect sizes (\\(N = 382\\)). Research questions were analyzed using mixed-effects meta-regression with robust variance estimation to account for effect size dependencies. Results: Searches identified eligible studies published between 1987 and 2019. An overall meta-analysis using 382 effect sizes obtained across 37 studies showed that children with DLD had decreased narrative performance relative to TD peers, with summary estimates ranging from -0.850, 95% CI [-1.016, -0.685] to -0.794, 95% CI [-0.963, -0.624], depending on the correlation assumed. Across all models, effect size estimates showed significant heterogeneity both between and within studies, even after accounting for effect size-, sample-, and study-level predictors. Grammatical accuracy (microstructure) and story grammar (macrostructure) yielded the most consistent evidence of significant TD–DLD group differences across statistical models. Conclusions: Present findings suggest some narrative assessment measures may yield significantly different performance between children with and without DLD. However, researchers need to be consistent in their inclusionary criteria, their description of sample characteristics, and in their reporting of the correlations of measures, in order to determine which assessment measures are more likely to yield group differences."
  },
  {
    "objectID": "publication/gradual-effects-model/index.html",
    "href": "publication/gradual-effects-model/index.html",
    "title": "A gradual effects model for single-case designs",
    "section": "",
    "text": "Supplementary materials  R package  Web app  Journal  Pre-Print\nSingle-case designs are a class of repeated measures experiments used to evaluate the effects of interventions for specialized populations, such as individuals with low-incidence disabilities. There has been growing interest in systematic reviews and syntheses of evidence from single-case designs, but there remains a need to further develop appropriate statistical models and effect sizes for data from the designs. We propose a novel model for single-case data that exhibit non-linear time trends created by an intervention that produces gradual effects, which build up and dissipate over time. The model expresses a structural relationship between a pattern of treatment assignment and an outcome variable, making it appropriate for both treatment reversal and multiple baseline designs. It is formulated as a generalized linear model so that it can be applied to outcomes measured as frequency counts or proportions, both of which are commonly used in single-case research, while providing readily interpretable effect size estimates such as log response ratios or log odds ratios. We demonstrate the gradual effects model by applying it to data from a single-case study and examine the performance of proposed estimation methods in a Monte Carlo simulation of frequency count data."
  },
  {
    "objectID": "publication/FABI-meta-analysis/index.html",
    "href": "publication/FABI-meta-analysis/index.html",
    "title": "Functional assessment-based interventions for students with or at-risk for high incidence disabilities: Field-testing single-case synthesis methods",
    "section": "",
    "text": "Journal\nThis systematic review investigated one systematic approach to designing, implementing, and evaluating functional assessment–based interventions (FABI) for use in supporting school-age students with or at-risk for high-incidence disabilities. We field tested several recently developed methods for single-case design syntheses. First, we appraised the quality of individual studies and the overall body of work using Council for Exceptional Children’s standards. Next, we calculated and meta-analyzed within-case and between-case effect sizes. Results indicated that studies were of high methodological quality, with nine studies identified as being methodologically sound and demonstrating positive outcomes across 14 participants. However, insufficient evidence was available to classify the evidence base for FABIs due to small number of participants within (fewer than recommended three) and across (fewer than recommended 20) studies. Nonetheless, average within-case effect sizes were equivalent to increases of 118% between baseline and intervention phases. Finally, potential moderating variables were examined. Limitations and future directions are discussed."
  },
  {
    "objectID": "publication/Efficacy-of-combining-cognitive-training-and-NIBS/index.html",
    "href": "publication/Efficacy-of-combining-cognitive-training-and-NIBS/index.html",
    "title": "The efficacy of combining cognitive training and non-invasive brain stimulation: A transdiagnostic systematic review and meta-analysis",
    "section": "",
    "text": "Journal  Pre-Print  Code  Data\nOver the past decade, an increasing number of studies investigated the innovative approach of supplementing cognitive training (CT) with non-invasive brain stimulation (NIBS) to increase the effects on outcomes. In this review, we aim to summarize the evidence for this treatment combination. We identified 72 published and unpublished studies (reporting 773 effect sizes) including 2518 participants from healthy and clinical populations indexed in PubMed, Medline, PsycINFO, ProQuest, Web of Science, and ClinicalTrials.gov (last search: 8/8/2022) that compared the effects of NIBS combined with CT on cognitive, symptoms and everyday functioning to CT alone at post-intervention and/or follow-up. We performed random-effects meta-analyses with robust variance estimation and assessed risk of bias with the Cochrane R.O.B. tool. Only four studies had low risk of bias in all domains, and many studies lacked standard controls such as keeping the outcome assessor and trainer unaware of the treatment condition. Following sensitivity analyses, only learning/memory robustly improved significantly more when CT was combined with NIBS compared to CT only (\\(g\\) = 0.18, 95% CI [0.07, 0.29]) at post-intervention, but not in the long-term. The effect was small and limited by substantial heterogeneity. The other seven cognitive outcome domains, symptoms, and everyday functioning did not benefit from adding NIBS to CT. Given the methodological limitation of prior studies, more high-quality trials that focus on the potential of combining NIBS and CT to enhance benefits in everyday functioning in the short- and long-term are needed to evaluate whether combining NIBS and CT is relevant for clinical practice."
  },
  {
    "objectID": "publication/Design-comparable-effect-sizes/index.html",
    "href": "publication/Design-comparable-effect-sizes/index.html",
    "title": "Design-comparable effect sizes in multiple baseline designs: A general modeling framework",
    "section": "",
    "text": "Supplementary materials  Journal  R package  Web app  Pre-Print\nIn single-case research, the multiple baseline design is a widely used approach for evaluating the effects of interventions on individuals. Multiple baseline designs involve repeated measurement of outcomes over time and the controlled introduction of a treatment at different times for different individuals. This article outlines a general framework for defining effect sizes in multiple baseline designs that are directly comparable to the standardized mean difference from a between-subjects randomized experiment. The target, design-comparable effect size parameter can be estimated using restricted maximum likelihood together with a small sample correction analogous to Hedges’s g. The approach is demonstrated using hierarchical linear models that include baseline time trends and treatment-by-time interactions. A simulation compares the performance of the proposed estimator to that of an alternative, and an application illustrates the model-fitting process."
  },
  {
    "objectID": "publication/Current-practices-in-meta-regression/index.html",
    "href": "publication/Current-practices-in-meta-regression/index.html",
    "title": "Current practices in meta-regression in psychology, education, and medicine",
    "section": "",
    "text": "Journal  Pre-Print  Data\nHaving surveyed the history and methods of meta‐regression in a previous paper,1 in this paper we review which and how meta‐regression methods are applied in recent research syntheses. To do so, we reviewed studies published in 2016 across four leading research synthesis journals: Psychological Bulletin, the Journal of Applied Psychology, Review of Education Research, and the Cochrane Library. We find that the best practices defined in the previous review are rarely carried out in practice. In light of the identified discrepancies, we consider how to move forward, first by identifying areas where further methods development is needed to address persistent problems in the field, and second by discussing how to more effectively disseminate points of methodological consensus."
  },
  {
    "objectID": "publication/Conducting-POMADE/index.html",
    "href": "publication/Conducting-POMADE/index.html",
    "title": "Conducting power analysis for meta-analysis of dependent effect sizes: Common guidelines and an introduction to the POMADE R package",
    "section": "",
    "text": "Supplementary materials  Pre-Print  Code\nSample size and statistical power are important factors to consider when planning a research synthesis. Power analysis methods have been developed for fixed effect or random effects models, but until recently these methods were limited to simple data structures with a single, independent effect per study. Recent work has provided power approximation formulas for meta-analyses involving studies with multiple, dependent effect size estimates, which are common in syntheses of social science research. Prior work focused on developing and validating the approximations, but did not address the practice challenges encountered in applying them for purposes of planning a synthesis involving dependent effect sizes. We aim to facilitate application of these recent developments by providing practical guidance on how to conduct power analysis for planning a meta-analysis of dependent effect sizes and by introducing a new R package, POMADE, designed for this purpose. We present a comprehensive overview of resources for finding information about the study design features and model parameters needed to conduct power analysis, along with detailed worked examples using the POMADE package. For presenting power analysis findings, we emphasize graphical tools that can depict power under a range of pausible assumptions and introduce a novel plot, the traffic light power plot, for conveying the degree of certainty in one’s assumptions."
  },
  {
    "objectID": "publication/cluster-wild-bootstrap-for-meta-analysis/index.html",
    "href": "publication/cluster-wild-bootstrap-for-meta-analysis/index.html",
    "title": "Cluster wild bootstrapping to handle dependent effect sizes in meta-analysis with a small number of studies",
    "section": "",
    "text": "Journal Supplementary materials  R package  Pre-Print  Video\nThe most common and well-known meta-regression models work under the assumption that there is only one effect size estimate per study and that the estimates are independent. However, meta-analytic reviews of social science research often include multiple effect size estimates per primary study, leading to dependence in the estimates. Some meta-analyses also include multiple studies conducted by the same lab or investigator, creating another potential source of dependence. An increasingly popular method to handle dependence is robust variance estimation (RVE), but this method can result in inflated Type I error rates when the number of studies is small. Small-sample correction methods for RVE have been shown to control Type I error rates adequately but may be overly conservative, especially for tests of multiple-contrast hypotheses. We evaluated an alternative method for handling dependence, cluster wild bootstrapping, which has been examined in the econometrics literature but not in the context of meta-analysis. Results from two simulation studies indicate that cluster wild bootstrapping maintains adequate Type I error rates and provides more power than extant small sample correction methods, particularly for multiple-contrast hypothesis tests. We recommend using cluster wild bootstrapping to conduct hypothesis tests for meta-analyses with a small number of studies. We have also created an R package that implements such tests."
  },
  {
    "objectID": "publication/BCSMD-examination-of-two-methods/index.html",
    "href": "publication/BCSMD-examination-of-two-methods/index.html",
    "title": "Between-case standardized effect size analysis of single case design: Examination of the two methods",
    "section": "",
    "text": "Journal\nAn increasing movement in single case research is to employ statistical analyses as one form of data analysis. Researchers have proposed different statistical approaches. The purpose of this paper is to examine the utility and discriminant validity of two novel types of between-case standardized effect size analyses with two existing systematic reviews. The between-case analyses found greater effect sizes for the studies in the object play review and smaller effect sizes for studies of sensory intervention, which were consistent with the overall conclusions reached in the original systematic reviews. These findings provide evidence of discriminant validity, although concerns remain around the methods’ utility across different single case research designs. Future directions for research and development also are provided."
  },
  {
    "objectID": "publication/BC-SMD-primer-and-applications/index.html",
    "href": "publication/BC-SMD-primer-and-applications/index.html",
    "title": "Analysis and meta-analysis of single-case designs with a standardized mean difference statistic: A primer and applications",
    "section": "",
    "text": "Journal\nThis article presents a d-statistic for single-case designs that is in the same metric as the d-statistic used in between-subjects designs such as randomized experiments and offers some reasons why such a statistic would be useful in SCD research. The d has a formal statistical development, is accompanied by appropriate power analyses, and can be estimated using user-friendly SPSS macros. We discuss both advantages and disadvantages of d compared to other approaches such as previous d-statistics, overlap statistics, and multilevel modeling. It requires at least three cases for computation and assumes normally distributed outcomes and stationarity, assumptions that are discussed in some detail. We also show how to test these assumptions. The core of the article then demonstrates in depth how to compute d for one study, including estimation of the autocorrelation and the ratio of between case variance to total variance (between case plus within case variance), how to compute power using a macro, and how to use the d to conduct a meta-analysis of studies using single-case designs in the free program R, including syntax in an appendix. This syntax includes how to read data, compute fixed and random effect average effect sizes, prepare a forest plot and a cumulative meta-analysis, estimate various influence statistics to identify studies contributing to heterogeneity and effect size, and do various kinds of publication bias analyses. This d may prove useful for both the analysis and meta-analysis of data from SCDs."
  },
  {
    "objectID": "publication/analyzing-SCD-hopes-and-fears/index.html",
    "href": "publication/analyzing-SCD-hopes-and-fears/index.html",
    "title": "Analyzing single-case designs: d, G, hierarchical models, Bayesian estimators, generalized additive models, and the hopes and fears of researchers about analyses",
    "section": "",
    "text": "{.btn .btn-outline-primary}\nNew approaches to the analyses of single-case designs are proliferating, which some single-case design researchers welcome and others view with skepticism. In this chapter we describe some of the analyses that we have been exploring, all of which can be conceptualized as versions of hierarchical models as a unifying framework. The approaches include a d-statistic for the (AB)k design that estimates the same parameter as the usual between-groups d-statistic, Bayesian approaches to the same and similar models, hierarchical generalized linear models that model outcomes as binomial or Poisson rather than the usual assumptions of normality, and semiparametric generalized additive models that allow diagnosis of trend and linearity. Throughout, we illustrate the analyses using a common example and show how the different analyses provide different insights into the data. We conclude with a discussion of potential criticisms and skepticism expressed by some researchers about such analyses, along with reasons why the field is increasingly likely to develop and use such analyses despite the criticisms."
  },
  {
    "objectID": "publication/AAC-instructional-contexts/index.html",
    "href": "publication/AAC-instructional-contexts/index.html",
    "title": "Considering instructional contexts in AAC interventions for people with ASD and/or IDD experiencing complex communication needs: A single-case design meta-analysis",
    "section": "",
    "text": "Journal  Pre-Print  Code  Data\nFor children with autism or intellectual and developmental disabilities who also have complex communication needs, communication is a necessary skill set to increase independence and quality of life. Understanding the how, where, and communication style being taught is important for identifying deficits in the field as well as which interventions are most effective. This meta-analysis sought to identify effectiveness among different settings, behavioral strategies, and moderator variables. A systematic search and screening process identified 114 eligible studies with 330 participants; overall outcomes indicate that augmentative and alternative communication interventions were effective with Tau effects ranging from 0.53 to 1.03 and log response ratio effects ranging from 0.21 to 2.90. However, no instructional context variables systematically predicted differences in intervention effectiveness."
  },
  {
    "objectID": "publication/AAC-communication-outcomes/index.html",
    "href": "publication/AAC-communication-outcomes/index.html",
    "title": "Augmentative and Alternative Communication intervention targets for school-aged participants with ASD and ID: A single-case systematic review and meta-analysis",
    "section": "",
    "text": "Journal  Pre-Print  Code  Data\nObjective: This meta-analysis reviews the literature on communication modes, communicative functions, and types of augmentative and alternative communication (AAC) interventions for school-age participants with autism spectrum disorders and/or intellectual disabilities who experience complex communication needs. Considering potential differences related to outcomes that were targeted for intervention could help identify the most effective means of individualizing AAC interventions. Methods: We performed a systematic literature search using Academic Search Ultimate, ERIC, PsycINFO, Web of Science, and Proquest Dissertations & Theses Global to retrieve research conducted between 1978 and the beginning of 2020. Studies included in the synthesis are (a) in English; (b) has one or more participants with an intellectual delay, developmental disability(ies); (c) reported the results of an augmentative and alternative communication (AAC) intervention to supplement or replace conventional speech for people with complex communication needs; (d) was a SCED; (e) measured social-communicative outcomes. We synthesized results across studies using multi-level meta-analyses of two case-level effect size metrics, Tau and log response ratio. We conducted moderator analyses using meta-regression with robust variance estimation. Results: Across 114 included studies with 330 participants and 767 effect size, overall Tau effects were moderate, Tau = 0.72, 95% CI [0.67, 0.77], and heterogeneous. For the subset of data series where log response ratio could be estimated, the overall average effect was LRR = 1.86, 95% CI [1.58, 2.13], and effects were highly heterogeneous. There were few statistically significant differences found between moderator categories, which included communication mode, communicative function, and type of AAC implemented. Conclusions: This meta-analysis highlights the potential differences related to outcomes that were targeted for AAC interventions for individuals with ASD and IDD. AAC intervention has been shown to improve communication outcomes in this population. However, there was a lack of sufficient data to analyze for some potential moderators such as insufficient descriptive information on participant characteristics. This is likely due to the heterogeneity of the participants and implementation factors; however, these factors were frequently underreported by original study authors which disallowed systematic analysis. That said, there is a need for more detailed participant characteristic descriptions in original research reports to support future aggregation across the literature. Sponsorship: We received funding for the review from the Institute of Education Sciences. Protocol: The review protocol was registered in the PROSPERO system (CRD42018112428)."
  },
  {
    "objectID": "presentations/SREE-2023-Equity-related-moderator-analysis.html",
    "href": "presentations/SREE-2023-Equity-related-moderator-analysis.html",
    "title": "Equity-related moderator analysis in syntheses of dependent effect sizes: Conceptual and statistical considerations",
    "section": "",
    "text": "Society for Research on Educational Effectiveness Conference, Arlington, VA\n Slides\n\nBackground/Context\nIn meta-analyses examining educational interventions, researchers seek to understand the distribution of intervention impacts, in order to draw generalizations about what works, for whom, and under what conditions. One common way to examine equity implications in such reviews is through moderator analysis, which involves modeling how intervention effect sizes vary depending on the characteristics of primary study participants. For example, one might estimate associations between effect size and the percentage of the primary study participants who were from a rural school, from a low-income family, identified as a specific racial or ethnic group, or designated as an English Language Learner. Such moderator analyses can provide insights about the populations and contexts where an intervention is more or less effective—that is, they can address questions of who benefits and how the effects of an educational intervention are distributed.\nMeta-analyses of educational interventions often include primary studies that report multiple relevant effect size estimates, such for more than one measure of an outcome construct, at multiple time-points, for multiple versions of an intervention, or for different sub-groups of participants. This leads to a data structure where the effects from a given study are correlated, necessitating the use of statistical methods that are appropriate for dependent observations. Methodological research in this area has provided estimation and inference methods that which can handle dependent effect sizes, including multi-level meta-analyses (Van den Noortgate et al., 2013, 2015), robust variance estimation (Hedges et al., 2010), and combinations thereof (Fernández-Castilla et al., 2020; Pustejovsky & Tipton, 2022). However, there has been much less attention to the specific forms of moderator analysis that are of interest in practice.\n\n\nPurpose/Objective/Research Question\nWe aim to identify conceptual and statistical considerations for moderator analysis of equity-related variables in meta-analyses involving dependent effect sizes. Specifically, we distinguish between direct evidence and contextual evidence about equity of impacts and show that the choice of meta-analytic model can be consequential for analyses involving direct evidence. We then examine how meta-analysts currently conduct equity-related moderator analyses, by reviewing completed research synthesis projects funded by the Institute of Education Sciences (IES) over the period of 2002 to 2018. We find that most projects do not distinguish between direct and contextual evidence and use analytic approaches that are inefficient for synthesizing direct evidence. Conceptual Considerations\nModerator analyses of equity-related variables can be carried out by regressing effect size estimates on predictors encoding participant characteristics. Consider a synthesis in which some primary studies contribute multiple effect sizes. In this data structure, a predictor might represent a study-level characteristic or one that varies across the effects within a given study. The level of variation is especially salient for analysis of equity-related variables because study-level characteristics and effect-level characteristics represent qualitatively different types of evidence. For study-level predictors, associations with effect size pertain to the study’s context and are not necessarily indicative of individual-level variation in impacts. Thus, interpretation is challenging because studies vary in many ways, with many possible sources of confounding. For effect-level predictors, within-study variation represents direct evidence about individual-level moderation (e.g., a comparison of impacts between low-income and higher-income participants in the same study), unconfounded by study-level characteristics.\nWe describe different strategies for separately investing direct and contextual evidence about moderation, including a) decomposing the predictor into study-level average and within-study centered components or b) inclusion of the raw predictor and the study-level average in a meta-regression. Although strategy (a) has been recommended previously in the context of meta-analysis of dependent effects (Tanner-Smith & Tipton, 2014), our presentation makes explicit the connection to equity-related moderator analysis and specifies the data requirements for applying it.\n\n\nStatistical Considerations\nMeta-regression with dependent effect sizes involves choosing a working model for the dependence structure, which determines the set of weights used for estimating the meta-regression. Several different working models have been proposed, including correlated effects and hierarchical effects models (Hedges et al., 2010), a correlated-and-hierarchical effects model (Pustejovsky & Tipton, 2022) and the multi-level meta-analysis model (Van den Noortgate et al., 2013, 2015). Ad hoc strategies, such as aggregating effects to the study level or ignoring dependence, can also be understood as working models.\nPrevious research and guidance about the choice of working model has argued that the choice of working model is fairly inconsequential so long as the working model is roughly similar the true dependence structure (Hedges et al., 2010; Tanner-Smith et al., 2016; Tanner-Smith & Tipton, 2014). In the appendix, we examine the exact weights assigned by a variety of different working models to studies with direct evidence and contextual evidence. Contrary to past guidance, we find that different working models can lead to quite different weighting—particularly for direct evidence (i.e., study mean-centered predictors).\n\n\nCurrent Practice\nTo understand current practices for analysis of equity-related moderator variables, we reviewed completed meta-analysis projects funded by IES over the period of 2002 to 2018. We identified grants that (a) had journal articles reporting a meta-analysis, (b) were not methodological, and (c) were not training programs. A search of the IES website for project descriptions that included the word “meta-analysis” returned 80 results, of which 25 met inclusion criteria. Table 1 summarizes the approaches to moderator analyses used in these projects. Most projects reported some form of meta-regression analysis, but very few described a centering strategy and only one project used study-mean centering. Notably, the correlated effects and hierarchical effects working models were commonly used, yet these models involve inefficient weighting of direct evidence.\n\n\nConclusions\nIn light of this review of current practice, the conceptual and statistical considerations that we describe suggest that there is substantial room for improvement in how meta-analysts conduct moderation analysis, particularly for equity-related variables where individual-level variation is of primary interest. Even under this simple—simplistic, even—conception of equity, bringing systematic review and meta-analysis methods to bear to address inequities in the education system will require not only improving analytic practices, but also changing how primary investigations frame questions, collect data, and report findings."
  },
  {
    "objectID": "presentations/AERA-2019-response-guided-algorithms.html",
    "href": "presentations/AERA-2019-response-guided-algorithms.html",
    "title": "The impact of response-guided designs on count outcomes in single-case design baselines",
    "section": "",
    "text": "American Educational Research Association annual convention, Toronto, Ontario\n PDF"
  },
  {
    "objectID": "presentations/ABAI-2013-effect-sizes.html",
    "href": "presentations/ABAI-2013-effect-sizes.html",
    "title": "Effect sizes and measurement comparability for meta-analysis of single-case research",
    "section": "",
    "text": "Association for Behavior Analysis International annual convention, Minneapolis, MN\n Slides"
  },
  {
    "objectID": "people/Young-Ri-Lee/index.html",
    "href": "people/Young-Ri-Lee/index.html",
    "title": "Young Ri Lee",
    "section": "",
    "text": "Young Ri completed her PhD in the Quantitative Methods program at The University of Texas at Austin. She is currently a postdoctoral scholar in Developmental and Learning Sciences at the Urban Education Institute of the University of Chicago. Her research interests include Hierarchical Linear Modeling (HLM), Robust Variance Estimation methods (RVE), and meta-analysis.\n\n\n\n\nPhD in Quantitative Methods | 2023  University of Texas at Austin\nMA in Educational Measurement and Statistics | 2017  Korea University\nBA in Education | 2014  Korea University\n\n\n\n\n\nHierarchical Linear Modeling\nRobust Variance Estimation methods\nMeta-analysis"
  },
  {
    "objectID": "people/Young-Ri-Lee/index.html#education",
    "href": "people/Young-Ri-Lee/index.html#education",
    "title": "Young Ri Lee",
    "section": "",
    "text": "PhD in Quantitative Methods | 2023  University of Texas at Austin\nMA in Educational Measurement and Statistics | 2017  Korea University\nBA in Education | 2014  Korea University"
  },
  {
    "objectID": "people/Young-Ri-Lee/index.html#interests",
    "href": "people/Young-Ri-Lee/index.html#interests",
    "title": "Young Ri Lee",
    "section": "",
    "text": "Hierarchical Linear Modeling\nRobust Variance Estimation methods\nMeta-analysis"
  },
  {
    "objectID": "people/Megha-Joshi/index.html",
    "href": "people/Megha-Joshi/index.html",
    "title": "Megha Joshi",
    "section": "",
    "text": "Megha completed her PhD in the Quantitative Methods program at The University of Texas at Austin. Her dissertation work examined cluster wild bootstrapping to handle dependent effect sizes in meta-analyses with a small number of studies. She now works as quantitative researcher at the American Institutes for Research. Her research interests include causal inference, meta-analysis, and missing data analysis.\n\n\n\n\nPhD in Quantitative Methods | 2021  The University of Texas at Austin\nBA in Psychology and Art History | 2014  Bryn Mawr College\n\n\n\n\n\nCausal inference\nMeta-analysis\nMachine learning\nMissing data analysis\nRobust variance estimation"
  },
  {
    "objectID": "people/Megha-Joshi/index.html#education",
    "href": "people/Megha-Joshi/index.html#education",
    "title": "Megha Joshi",
    "section": "",
    "text": "PhD in Quantitative Methods | 2021  The University of Texas at Austin\nBA in Psychology and Art History | 2014  Bryn Mawr College"
  },
  {
    "objectID": "people/Megha-Joshi/index.html#interests",
    "href": "people/Megha-Joshi/index.html#interests",
    "title": "Megha Joshi",
    "section": "",
    "text": "Causal inference\nMeta-analysis\nMachine learning\nMissing data analysis\nRobust variance estimation"
  },
  {
    "objectID": "people/Jingru-Zhang/index.html",
    "href": "people/Jingru-Zhang/index.html",
    "title": "Jingru Zhang",
    "section": "",
    "text": "Jingru Zhang is a doctoral student in the Quantitative Methods area in the Educational Psychology Department at UW-Madison. Her research interests involves methodological development and their applications from design and modeling perspectives, with a focus on causal inference, meta-analysis, mediation analysis, and generalizable and transportable inference.\n\n\n\n\nPhD in Quantitative Methods | 2027 (expected)  University of Wisconsin-Madison\nEdM in Measurement, Evaluation, and Statistics | 2022  Teachers College, Columbia University\n\n\n\n\n\nCausal inference\nMeta-analysis\nMediation analysis\nGeneralizable and transportable inference"
  },
  {
    "objectID": "people/Jingru-Zhang/index.html#education",
    "href": "people/Jingru-Zhang/index.html#education",
    "title": "Jingru Zhang",
    "section": "",
    "text": "PhD in Quantitative Methods | 2027 (expected)  University of Wisconsin-Madison\nEdM in Measurement, Evaluation, and Statistics | 2022  Teachers College, Columbia University"
  },
  {
    "objectID": "people/Jingru-Zhang/index.html#interests",
    "href": "people/Jingru-Zhang/index.html#interests",
    "title": "Jingru Zhang",
    "section": "",
    "text": "Causal inference\nMeta-analysis\nMediation analysis\nGeneralizable and transportable inference"
  },
  {
    "objectID": "people/Gleb-Furman/index.html",
    "href": "people/Gleb-Furman/index.html",
    "title": "Gleb Furman",
    "section": "",
    "text": "Gleb did his doctoral work on causal inference at UT Austin. He is currently a senior quantitative research scientist at Gibson Consulting Group.\n\n\n\n\nPhD in Quantitative Methods | 2023  University of Texas at Austin\nMEd in Quantitative Methods | 2015  University of Texas at Austin\nBA in Psychology | 2010  Baruch College\n\n\n\n\n\nResearch Design\nEvaluation\nGeneralizability"
  },
  {
    "objectID": "people/Gleb-Furman/index.html#education",
    "href": "people/Gleb-Furman/index.html#education",
    "title": "Gleb Furman",
    "section": "",
    "text": "PhD in Quantitative Methods | 2023  University of Texas at Austin\nMEd in Quantitative Methods | 2015  University of Texas at Austin\nBA in Psychology | 2010  Baruch College"
  },
  {
    "objectID": "people/Gleb-Furman/index.html#interests",
    "href": "people/Gleb-Furman/index.html#interests",
    "title": "Gleb Furman",
    "section": "",
    "text": "Research Design\nEvaluation\nGeneralizability"
  },
  {
    "objectID": "people/Christopher-Runyon/index.html",
    "href": "people/Christopher-Runyon/index.html",
    "title": "Christopher Runyon",
    "section": "",
    "text": "Chris did his doctoral work on causal inference at UT Austin. He is currently a measurement scientist in the Center for Advanced Assessment at the National Board of Medical Examiners, where his research focuses on developing automated scoring systems for constructed item responses.\n\n\n\n\nPhD in Quantitative Methods | 2020  University of Texas at Austin\nMA in Experimental Psychology | 2012  James Madison University\nBS in Psychology | 2008  Virginia Commonwealth University\nBA in Philosophy | 2008  Virginia Commonwealth University\nBA in Religious Studies (Buddhism) | 2003  University of Virginia\n\n\n\n\n\nCausal Inference\nR\nNatural Language Processing\nMachine Learning"
  },
  {
    "objectID": "people/Christopher-Runyon/index.html#education",
    "href": "people/Christopher-Runyon/index.html#education",
    "title": "Christopher Runyon",
    "section": "",
    "text": "PhD in Quantitative Methods | 2020  University of Texas at Austin\nMA in Experimental Psychology | 2012  James Madison University\nBS in Psychology | 2008  Virginia Commonwealth University\nBA in Philosophy | 2008  Virginia Commonwealth University\nBA in Religious Studies (Buddhism) | 2003  University of Virginia"
  },
  {
    "objectID": "people/Christopher-Runyon/index.html#interests",
    "href": "people/Christopher-Runyon/index.html#interests",
    "title": "Christopher Runyon",
    "section": "",
    "text": "Causal Inference\nR\nNatural Language Processing\nMachine Learning"
  },
  {
    "objectID": "people/Daniel-M.-Swan/index.html",
    "href": "people/Daniel-M.-Swan/index.html",
    "title": "Daniel M. Swan",
    "section": "",
    "text": "Danny is currently a research associate working at the Prevention Science Institute at University of Oregon, where he provides support for evidence reviewer training for the What Works Clearinghouse and assists with revision and refinement of the WWC’s Standards and Procedures Handbooks. His dissertation work involved examining the impact of response-guided designs on treatment effect estimates from single-case designs. His personal research interests center on analysis and meta-analysis of single-case designs (SCDs) in the context of education. He is particularly interested in the quality indicators of primary studies, the different kinds of inferences made in SCDs, and how the common, frequently unspoken research practices in this field might impact the intervention estimates from these kinds of studies.\n\n\n\n\nPhD in Quantitative Methods | 2019  University of Texas at Austin\nMEd in Quantitative Methods | 2014  University of Texas at Austin\nMS in Psychological Sciences | 2010  University of Texas at Dallas\nBA in Psychology | 2008  University of Texas at Dallas\n\n\n\n\n\nResearch methodology\nSingle-case designs\nMeta-analysis\nGeneralized linear models"
  },
  {
    "objectID": "people/Daniel-M.-Swan/index.html#education",
    "href": "people/Daniel-M.-Swan/index.html#education",
    "title": "Daniel M. Swan",
    "section": "",
    "text": "PhD in Quantitative Methods | 2019  University of Texas at Austin\nMEd in Quantitative Methods | 2014  University of Texas at Austin\nMS in Psychological Sciences | 2010  University of Texas at Dallas\nBA in Psychology | 2008  University of Texas at Dallas"
  },
  {
    "objectID": "people/Daniel-M.-Swan/index.html#interests",
    "href": "people/Daniel-M.-Swan/index.html#interests",
    "title": "Daniel M. Swan",
    "section": "",
    "text": "Research methodology\nSingle-case designs\nMeta-analysis\nGeneralized linear models"
  },
  {
    "objectID": "people/Man-Chen/index.html",
    "href": "people/Man-Chen/index.html",
    "title": "Man Chen",
    "section": "",
    "text": "Man Chen is a doctoral student in the Quantitative Methods area in the Educational Psychology Department at UW-Madison. Her research interests include meta-analysis, single-case experimental designs, and hierarchical linear modeling. She is currently working as a project assistant at the Wisconsin Center for Educational Research (WCER) with Dr. Pustejovsky.\n\n\n\n\nPhD in Quantitative Methods | 2023 (expected)  University of Wisconsin-Madison\nMEd in Quantitative Methods | 2018  The University of Texas at Austin\n\n\n\n\n\nMeta-analysis\nSingle-case experimental designs\nHierarchical linear modeling"
  },
  {
    "objectID": "people/Man-Chen/index.html#education",
    "href": "people/Man-Chen/index.html#education",
    "title": "Man Chen",
    "section": "",
    "text": "PhD in Quantitative Methods | 2023 (expected)  University of Wisconsin-Madison\nMEd in Quantitative Methods | 2018  The University of Texas at Austin"
  },
  {
    "objectID": "people/Man-Chen/index.html#interests",
    "href": "people/Man-Chen/index.html#interests",
    "title": "Man Chen",
    "section": "",
    "text": "Meta-analysis\nSingle-case experimental designs\nHierarchical linear modeling"
  },
  {
    "objectID": "people/Paulina-Grekov/index.html",
    "href": "people/Paulina-Grekov/index.html",
    "title": "Paulina Grekov",
    "section": "",
    "text": "Paulina Grekov is a doctoral student in the Quantitative Methods area in the Educational Psychology Department at UW-Madison. Her research interests include meta-analysis, single-case experimental designs, hierarchical linear modeling, and research synthesis. She is currently working as a research assistant with Dr. Pustejovsky.\n\n\n\n\nPhD in Quantitative Methods | 2026 (expected)  University of Wisconsin-Madison\n\n\n\n\n\nMeta-analysis\nSingle-case experimental designs\nHierarchical linear modeling\nResearch synthesis"
  },
  {
    "objectID": "people/Paulina-Grekov/index.html#education",
    "href": "people/Paulina-Grekov/index.html#education",
    "title": "Paulina Grekov",
    "section": "",
    "text": "PhD in Quantitative Methods | 2026 (expected)  University of Wisconsin-Madison"
  },
  {
    "objectID": "people/Paulina-Grekov/index.html#interests",
    "href": "people/Paulina-Grekov/index.html#interests",
    "title": "Paulina Grekov",
    "section": "",
    "text": "Meta-analysis\nSingle-case experimental designs\nHierarchical linear modeling\nResearch synthesis"
  },
  {
    "objectID": "presentations/AERA-2019-outcome-measurement-procedures.html",
    "href": "presentations/AERA-2019-outcome-measurement-procedures.html",
    "title": "An examination of measurement procedures and baseline behavioral outcomes in single-case research",
    "section": "",
    "text": "American Educational Research Association annual convention, Toronto, Ontario\n PDF"
  },
  {
    "objectID": "presentations/VIVE-2024-Dependent-Effects.html",
    "href": "presentations/VIVE-2024-Dependent-Effects.html",
    "title": "Model-Building Considerations in Meta-Analysis of Dependent Effect Sizes",
    "section": "",
    "text": "Danish Center for Social Science Research, Copenhagen, Denmark\n Slides\nIn fields ranging from Education to Economics to Ecology, meta-analysts often encounter complicated data structures, in which some or all primary studies include multiple effect size estimates. These estimates may be correlated because they are based on data from a common sample or a partially overlapping sample, or may be statistically dependent due to use of common study operations. A broad analytic strategy for dealing with such data is to specify a “working model” to roughly characterize the dependence structure, then use robust inference strategies that work well even if the working model is mis-specified relative to the true data-generating process. Although the technical and computational aspects of this strategy are now well developed, questions remain about how to apply it effectively in practice. In this talk, I will examine two practical questions related to how to build models for meta-analyses involving dependent effect sizes. First, I will illustrate some connections between working models and simpler, ad hoc techniques for dealing with effect size multiplicity, arguing that these connections provide useful heuristics to guide specification of random effects structures in multi-level and multi-variate meta-analysis. Second, I will describe some analytic strategies for conducting equity-related moderator analyses, where predictors involve personal characteristics of the primary study participants that can vary both within and between studies. I distinguish between direct evidence and contextual evidence about equity of impacts and show that the choice of working model can be consequential for analyses involving direct evidence. Throughout, I will highlight some open issues and practical challenges involved in modeling dependent effect sizes."
  },
  {
    "objectID": "publication/AAC-group-and-SCD/index.html",
    "href": "publication/AAC-group-and-SCD/index.html",
    "title": "Systematic review of variables related to instruction in augmentative and alternative communication implementation: Group and single-case design",
    "section": "",
    "text": "Journal  Pre-Print  Code  Data\nPurpose: This article provides a systematic review and analysis of group and single-case studies addressing augmentative and alternative communication (AAC) intervention with school-aged persons having autism spectrum disorder (ASD) and/or intellectual/developmental disabilities resulting in complex communication needs (CCNs). Specifically, we examined participant characteristics in group-design studies reporting AAC intervention outcomes and how these compared to those reported in single-case experimental designs (SCEDs). In addition, we compared the status of intervention features reported in group and SCED studies with respect to instructional strategies utilized. Participants: Participants included school-aged individuals with CCNs who also experienced ASD or ASD with an intellectual delay who utilized aided or unaided AAC. Method: A systematic review using descriptive statistics and effect sizes was implemented. Results: Findings revealed that participant features such as race, ethnicity, and home language continue to be underreported in both SCED and group-design studies. Participants in SCED investigations more frequently used multiple communication modes when compared to participants in group studies. The status of pivotal skills such as imitation was sparsely reported in both types of studies. With respect to instructional features, group-design studies were more apt to utilize clinical rather than educational or home settings when compared with SCED studies. In addition, SCED studies were more apt to utilize instructional methods that closely adhered to instructional features more typically characterized as being associated with behavioral approaches. Conclusion: The authors discuss future research needs, practice implications, and a more detailed specification of treatment intensity parameters for future research."
  },
  {
    "objectID": "publication/AAC-participant-characteristics/index.html",
    "href": "publication/AAC-participant-characteristics/index.html",
    "title": "Participant characteristics predicting communication outcomes in AAC implementation for individuals with ASD and IDD: Meta-analysis",
    "section": "",
    "text": "Journal  Pre-Print  Code  Data\nThis meta-analysis examined social communication outcomes in augmentative and alternative communication (AAC) interventions, or those that involved aided (e.g., speech generating devices, picture point systems) or unaided AAC (e.g., gestures, manual sign language) as a component of intervention, and the extent to which communication outcomes were predicted by participant characteristics. Variables of interest included chronological age, communication mode used prior to intervention, number of words produced and imitation skills of participants prior to intervention. Investigators identified 117 primary studies that implemented AAC interventions with school-aged individuals (up to 22 years) with autism spectrum disorder and/or intellectual disability associated with complex communication needs and assessed social-communication outcomes. All included studies involved single-case experimental designs and met basic study design quality standards. We synthesized findings across studies using two complementary effect size indices, Tau(AB) and the log response ratio, and multi-level meta-analysis with robust variance estimation. With Tau(AB), the overall average effect across 338 participants was 0.72, 95% CI [0.67, 0.76], with a high degree of heterogeneity across studies. With the log response ratio, the overall average effect corresponded to a 538% increase from baseline levels of responding, 95% CI [388%, 733%], with a high degree of heterogeneity across studies and contrasts. Moderator analyses detected few differences in effectiveness when comparing across diagnoses, ages, the number and type of communication modes the participants used prior to intervention, the number of words used by the participants prior to intervention, and imitation use prior to intervention."
  },
  {
    "objectID": "publication/ARP-for-behavioral-observation/index.html",
    "href": "publication/ARP-for-behavioral-observation/index.html",
    "title": "Alternating renewal process models for behavioral observation: Simulation methods and validity implications",
    "section": "",
    "text": "Journal  R package  Pre-Print\nDirect observation recording procedures produce reductive summary measurements of an underlying stream of behavior. Previous methodological studies of these recording procedures have employed simulation methods for generating random behavior streams, many of which amount to special cases of a statistical model known as the alternating renewal process. This paper describes the alternating renewal process model in its general form, demonstrates how it provides an organizing framework for most past simulation research on direct observation procedures, and introduces a freely available software package that implements the model. The software can be used to simulate behavior streams as well as data from many common recording procedures, including continuous recording, momentary time sampling, event counting, and interval recording procedures. Several examples illustrate how the software can be used to study the validity and reliability of direct observation data and to develop measurement strategies during the planning phases of empirical studies."
  },
  {
    "objectID": "publication/BC-SMD-primer-and-tutorial/index.html",
    "href": "publication/BC-SMD-primer-and-tutorial/index.html",
    "title": "Between-case standardized mean difference effect sizes for single-case designs: A primer and tutorial using the scdhlm web application",
    "section": "",
    "text": "R package  Web app  PDF\nWe describe a standardised mean difference statistic (d) for single-case designs that is equivalent to the usual d in between-groups experiments. We show how it can be used to summarise treatment effects over cases within a study, to do power analyses in planning new studies and grant proposals, and to meta-analyse effects across studies of the same question. We discuss limitations of this d-statistic, and possible remedies to them. Even so, this d-statistic is better founded statistically than other effect size measures for single-case design, and unlike many general linear model approaches such as multilevel modelling or generalised additive models, it produces a standardised effect size that can be integrated over studies with different outcome measures. SPSS macros for both effect size computation and power analysis are available."
  },
  {
    "objectID": "publication/between-groups-d-statistic/index.html",
    "href": "publication/between-groups-d-statistic/index.html",
    "title": "A d-statistic for single-case designs that is equivalent to the usual between-groups d-statistics",
    "section": "",
    "text": "Journal\nWe describe a standardised mean difference statistic (d) for single-case designs that is equivalent to the usual d in between-groups experiments. We show how it can be used to summarise treatment effects over cases within a study, to do power analyses in planning new studies and grant proposals, and to meta-analyse effects across studies of the same question. We discuss limitations of this d-statistic, and possible remedies to them. Even so, this d-statistic is better founded statistically than other effect size measures for single-case design, and unlike many general linear model approaches such as multilevel modelling or generalised additive models, it produces a standardised effect size that can be integrated over studies with different outcome measures. SPSS macros for both effect size computation and power analysis are available."
  },
  {
    "objectID": "publication/competing-approaches-for-cross-classified-data/index.html",
    "href": "publication/competing-approaches-for-cross-classified-data/index.html",
    "title": "Comparison of competing approaches to analyzing cross-classified data: Random effects models, ordinary least squares, or fixed effects with cluster robust standard errors",
    "section": "",
    "text": "Journal Supplementary materials  Pre-Print\nCross-classified random effects modeling (CCREM) is a common approach for analyzing cross-classified data in education. However, when the focus of a study is on the regression coefficients at level one rather than on the random effects, ordinary least squares regression with cluster robust variance estimators (OLS-CRVE) or fixed effects regression with CRVE (FE-CRVE) could be appropriate approaches. These alternative methods may be advantageous because they rely on weaker assumptions than what is required by CCREM. We conducted a Monte Carlo Simulation study to compare the performance of CCREM, OLS-CRVE, and FE-CRVE in models with crossed random effects, including conditions where homoscedasticity assumptions and exogeneity assumptions held and conditions where they were violated. We found that CCREM performed the best when its assumptions are all met. However, when homoscedasticity assumptions are violated, OLS-CRVE and FE-CRVE provided similar or better performance than CCREM. FE-CRVE showed the best performance when the exogeneity assumption is violated. Thus, we recommend two-way FE-CRVE as a good alternative to CCREM, particularly if the homoscedasticity or exogeneity assumptions of the CCREM might be in doubt."
  },
  {
    "objectID": "publication/converting-from-d-to-r-to-z/index.html",
    "href": "publication/converting-from-d-to-r-to-z/index.html",
    "title": "Converting from d to r to z when the design uses extreme groups, dichotomization, or experimental control",
    "section": "",
    "text": "Supplementary materials  Journal  Pre-Print  Code\nMeta-analyses of the relationship between 2 continuous variables sometimes involves conversions between different effect sizes, but methodological literature offers conflicting guidance about how to make such conversions. This article provides methods for converting from a standardized mean difference to a correlation coefficient (and from there to Fisher’s z) under 3 types of study designs: extreme groups, dichotomization of a continuous variable, and controlled experiments. Also provided are formulas and recommendations regarding how the sampling variance of effect size statistics should be estimated in each of these cases. The conversion formula for extreme groups designs, originally due to Feldt (1961), can be viewed as a generalization of Hunter and Schmidt’s (1990) method for dichotomization designs. A simulation study examines the finite-sample properties of the proposed methods. The conclusion highlights areas where current guidance in the literature should be amended or clarified."
  },
  {
    "objectID": "publication/Decline-effects/index.html",
    "href": "publication/Decline-effects/index.html",
    "title": "High replicability of newly-discovered social-behavioral findings is achievable.",
    "section": "",
    "text": "Journal Supplementary materials  Pre-Print\nFailures to replicate evidence of new discoveries have forced scientists to ask whether this unreliability is due to suboptimal implementation of optimal methods or whether presumptively optimal methods are not, in fact, optimal. This paper reports an investigation by four coordinated laboratories of the prospective replicability of 16 novel experimental findings using current optimal practices: high statistical power, preregistration, and complete methodological transparency. In contrast to past systematic replication efforts that reported replication rates averaging 50%, replication attempts here produced the expected effects with significance testing (p&lt;.05) in 86% of attempts, slightly exceeding maximum expected replicability based on observed effect size and sample size. When one lab attempted to replicate an effect discovered by another lab, the effect size in the replications was 97% that of the original study. This high replication rate justifies confidence in rigor enhancing methods and suggests that past failures to replicate may be attributable to departures from optimal procedures."
  },
  {
    "objectID": "publication/effects-of-social-stories-on-challenging-behavior-and-prosocial-skills/index.html",
    "href": "publication/effects-of-social-stories-on-challenging-behavior-and-prosocial-skills/index.html",
    "title": "Examining the effects of social stories on challenging behavior and prosocial skills in young children: A systematic review and meta-analysis",
    "section": "",
    "text": "Journal  Pre-Print  Code  Data\nSocial stories are a commonly used intervention practice in early childhood special education. Recent systematic reviews have documented the evidence-base for social stories, but findings are mixed. We examined the efficacy of social stories for young children (i.e., 3-5 years) with challenging behavior across 12 single-case studies, that included 30 participants. The What Works Clearinghouse standards for single case research design were used to evaluate the rigor of studies that included social stories as a primary intervention. For studies meeting standards, we synthesized findings on the efficacy of social stories using meta-analysis techniques and a recently developed parametric effect size measure, the log response ratio. Trends in participants’ response to treatment also were explored. Results indicate variability in rigor and efficacy for the use of social stories as an isolated intervention and in combination with other intervention approaches. Additional studies that investigate the efficacy of social stories as a primary intervention are warranted."
  },
  {
    "objectID": "publication/Equivalences-between-ad-hoc-strategies-and-models/index.html",
    "href": "publication/Equivalences-between-ad-hoc-strategies-and-models/index.html",
    "title": "Equivalences between ad hoc strategies and meta-analytic models for dependent effect sizes",
    "section": "",
    "text": "Journal Supplementary materials  Pre-Print  Code  Data\nMeta-analyses of educational research findings frequently involve statistically dependent effect size estimates. Meta-analysts have often addressed dependence issues using ad hoc approaches that involve modifying the data to conform to the assumptions of models for independent effect size estimates, such as aggregating estimates to obtain one summary estimate per study, conducting separate analyses of distinct subgroups of estimates, or combinations thereof. We demonstrate that these ad hoc approaches correspond exactly to certain multivariate models for dependent effect sizes. Specifically, we describe classes of multivariate random effects models that have likelihoods equivalent to those of models for effect sizes that have been averaged by study, classified into subgroups, or both. The equivalence also applies to robust variance estimation methods."
  },
  {
    "objectID": "publication/Four-methods-for-PIR/index.html",
    "href": "publication/Four-methods-for-PIR/index.html",
    "title": "Four methods for analyzing partial interval recording data, with application to single-case research",
    "section": "",
    "text": "Supplementary materials  Journal  Pre-Print\nPartial interval recording (PIR) is a procedure for collecting measurements during direct observation of behavior. It is used in several areas of educational and psychological research, particularly in connection with single-case research. Measurements collected using partial interval recording suffer from construct invalidity because they are not readily interpretable in terms of the underlying characteristics of the behavior. Using an alternating renewal process model for the behavior under observation, we demonstrate that ignoring the construct invalidity of PIR data can produce misleading inferences, such as inferring that an intervention reduces the prevalence of an undesirable behavior when in fact it has the opposite effect. We then propose four different methods for analyzing PIR summary measurements, each of which can be used to draw inferences about interpretable behavioral parameters. We demonstrate the methods by applying them to data from two single-case studies of problem behavior."
  },
  {
    "objectID": "publication/History-of-meta-regression/index.html",
    "href": "publication/History-of-meta-regression/index.html",
    "title": "A history of meta-regression: Technical, conceptual, and practical developments between 1974 and 2018",
    "section": "",
    "text": "Journal  Pre-Print\nAt the beginning of the development of meta‐analysis, understanding the role of moderators was given the highest priority, with meta‐regression provided as a method for achieving this goal. Yet in current practice, meta‐regression is not as commonly used as anticipated. This paper seeks to understand this mismatch by reviewing the history of meta‐regression methods over the past 40 years. We divide this time span into four periods and examine three types of methodological developments within each period: technical, conceptual, and practical. Our focus is broad and includes development of methods in the fields of education, psychology, and medicine. We conclude the paper with a discussion of five consensus points, as well as open questions and areas of research for the future."
  },
  {
    "objectID": "publication/interventions-to-enhance-self-efficacy-in-cancer-patients/index.html",
    "href": "publication/interventions-to-enhance-self-efficacy-in-cancer-patients/index.html",
    "title": "Interventions to enhance self-efficacy in cancer patients and survivors: A meta-analysis of randomized controlled trials",
    "section": "",
    "text": "Journal  Data\nObjective: Self-efficacy expectations are associated with improvements in problematic outcomes widely considered clinically significant (i.e., emotional distress, fatigue, pain), related to positive health behaviors, and, as a type of personal agency, inherently valuable. Self-efficacy expectancies, estimates of confidence to execute behaviors, are important in that changes in selfefficacy expectations are positively related to future behaviors that promote health and wellbeing. The current meta-analysis investigated the impact of psychological interventions on self-efficacy expectations for a variety of health behaviors among cancer patients. Methods: Ovid Medline, PsycINFO, CINAHL, EMBASE, Cochrane Library, and Web of Science were searched with specific search terms for identifying randomized controlled trials (RCTs) that focused on psychologically-based interventions. Included studies had: 1) an adult cancer sample, 2) a self-efficacy expectation measure of specific behaviors and 3) an RCT design. Standard screening and reliability procedures were used for selecting and coding studies. Coding included theoretically informed moderator variables. Results: Across 79 RCTs, 223 effect sizes, and 8678 participants, the weighted average effect of self-efficacy expectations was estimated as g=0.274 (p&lt;.001). Consistent with Self-Efficacy Theory, the average effect for in-person intervention delivery (g=0.329) was significantly greater than for all other formats (g=0.154, p=.023; e.g., audiovisual, print, telephone, web/internet). Conclusions: The results establish the impact of psychological interventions on self-efficacy expectations as comparable in effect size to commonly reported outcomes (distress, fatigue, pain). Additionally, the result that in-person interventions achieved the largest effect is supported by Social Learning Theory and could inform research related to the development and evaluation of interventions."
  },
  {
    "objectID": "publication/Measurement-comparable-effect-sizes/index.html",
    "href": "publication/Measurement-comparable-effect-sizes/index.html",
    "title": "Measurement-comparable effect sizes for single-case studies of free-operant behavior",
    "section": "",
    "text": "Supplementary materials  Journal  Pre-Print\nSingle-case research comprises a set of designs and methods for evaluating the effects of interventions, practices, or programs on individual cases, through comparison of outcomes measured at different points in time. Although there has long been interest in meta-analytic techniques for synthesizing single-case research, there has been little scrutiny of whether proposed effect sizes remain on a directly comparable metric when outcomes are measured using different operational procedures. Much of single-case research focuses on behavioral outcomes in free-operant contexts, which may be measured using a variety of different direct observation procedures. This article describes a suite of effect sizes for quantifying changes in free-operant behavior, motivated by an alternating renewal process model that allows measurement comparability to be established in precise terms. These effect size metrics have the advantage of comporting with how direct observation data are actually collected and summarized. Effect size estimators are proposed that are applicable when the behavior being measured remains stable within a given treatment condition. The methods are illustrated by 2 examples, including a re-analysis of a systematic review of the effects of choice-making opportunities on problem behavior."
  },
  {
    "objectID": "publication/Meta-analysis-of-SCD/index.html",
    "href": "publication/Meta-analysis-of-SCD/index.html",
    "title": "Research synthesis and meta-analysis of single-case designs",
    "section": "",
    "text": "PDF"
  },
  {
    "objectID": "publication/Power-approximations-for-dependent-effects/index.html",
    "href": "publication/Power-approximations-for-dependent-effects/index.html",
    "title": "Power approximations for overall average effects in meta-analysis of dependent effect sizes",
    "section": "",
    "text": "Journal Supplementary materials  R package  Pre-Print  Code\nMeta-analytic models for dependent effect sizes have grown increasingly sophisticated over the last few decades, which has created challenges for a priori power calculations. We introduce power approximations for tests of average effect sizes based upon several common approaches for handling dependent effect sizes. In a Monte Carlo simulation, we show that the new power formulas can accurately approximate the true power of meta-analytic models for dependent effect sizes. Lastly, we investigate the Type I error rate and power for several common models, finding that tests using robust variance estimation provide better Type I error calibration than tests with model-based variance estimation. We consider implications for practice with respect to selecting a working model and an inferential approach."
  },
  {
    "objectID": "publication/psychosocial-interventions-for-positive-affect/index.html",
    "href": "publication/psychosocial-interventions-for-positive-affect/index.html",
    "title": "Psychosocial interventions for cancer survivors: A meta-analysis of effects on positive affect",
    "section": "",
    "text": "Journal\nPurpose Positive affect has demonstrated unique benefits in the context of health-related stress and is emerging as an important target for psychosocial interventions. The primary objective of this meta-analysis was to determine whether psychosocial interventions increase positive affect in cancer survivors. Methods We coded 28 randomized controlled trials of psychosocial interventions assessing 2082 cancer survivors from six electronic databases. We calculated 76 effect sizes for positive affect and conducted synthesis using random effects models with robust variance estimation. Tests for moderation included demographic, clinical, and intervention characteristics. Results Interventions had a modest effect on positive affect (g = 0.35, 95% CI [0.16, 0.54]) with substantial heterogeneity of effects across studies (\\(\\hat\\tau^2\\) = 0.40; \\(I^2\\) = 78%). Three significant moderators were identified: in-person interventions outperformed remote interventions (P = .046), effects were larger when evaluated against standard of care or wait list control conditions versus attentional, educational, or component controls (P = .009), and trials with survivors of early-stage cancer diagnoses yielded larger effects than those with advanced-stage diagnoses (P = .046). We did not detect differential benefits of psychosocial interventions across samples varying in sex, age, on-treatment versus off-treatment status, or cancer type. Although no conclusive evidence suggested outcome reporting biases (P = .370), effects were smaller in studies with lower risk of bias. Conclusions In-person interventions with survivors of early-stage cancers hold promise for enhancing positive affect, but more methodological rigor is needed. Implications for Cancer Survivors Positive affect strategies can be an explicit target in evidence-based medicine and have a role in patient-centered survivorship care, providing tools to uniquely mobilize human strengths."
  },
  {
    "objectID": "publication/psychosocial-interventions-meaning-and-purpose/index.html",
    "href": "publication/psychosocial-interventions-meaning-and-purpose/index.html",
    "title": "Effects of psychosocial interventions on meaning and purpose in adults with cancer: A systematic review and meta-analysis",
    "section": "",
    "text": "Journal\nMeaning and purpose in life are associated with the mental and physical health of patients with cancer and survivors and also constitute highly valued outcomes in themselves. Because meaning and purpose are often threatened by a cancer diagnosis and treatment, interventions have been developed to promote meaning and purpose. The present meta-analysis of randomized controlled trials (RCTs) evaluated effects of psychosocial interventions on meaning/purpose in adults with cancer and tested potential moderators of intervention effects. Six literature databases were systematically searched to identify RCTs of psychosocial interventions in which meaning or purpose was an outcome. Using Preferred Reporting Items for Systematic Reviews and Meta‐Analyses guidelines, rater pairs extracted and evaluated data for quality. Findings were synthesized across studies with standard meta‐analytic methods, including meta‐regression with robust variance estimation and risk-of-bias sensitivity analysis. Twenty-nine RCTs were identified, and they encompassed 82 treatment effects among 2305 patients/survivors. Psychosocial interventions were associated with significant improvements in meaning/purpose (g = 0.37; 95% CI, 0.22-0.52; P &lt; .0001). Interventions designed to enhance meaning/purpose (g = 0.42; 95% CI, 0.24-0.60) demonstrated significantly higher effect sizes than those targeting other primary outcomes (g = 0.18; 95% CI, 0.09-0.27; P = .009). Few other intervention, clinical, or demographic characteristics tested were significant moderators. In conclusion, the results suggest that psychosocial interventions are associated with small to medium effects in enhancing meaning/purpose among patients with cancer, and the benefits are comparable to those of interventions designed to reduce depression, pain, and fatigue in patients with cancer. Methodological concerns include small samples and ambiguity regarding allocation concealment. Future research should focus on explicitly meaning-centered interventions and identify optimal treatment or survivorship phases for implementation."
  },
  {
    "objectID": "publication/RASE-special-issue-introduction/index.html",
    "href": "publication/RASE-special-issue-introduction/index.html",
    "title": "Introduction to the special issue on single-case systematic reviews and meta-analysis",
    "section": "",
    "text": "Journal\nThis special issue provides an update on recent conceptual and methodological developments for conducting systematic reviews and meta-analyses of single-case research. In this introductory article, we (a) describe the important role of systematic reviews and meta-analyses within special education; (b) discuss several methodological issues authors must consider when planning and conducting a rigorous single-case review; and (c) summarize current approaches for addressing each of these issues. Following this overview, we describe each article in the special issue, paying particular attention to the methodological areas highlighted. We conclude with recommendations for continued research and development in several areas."
  },
  {
    "objectID": "publication/Religion-spirituality-physical-health/index.html",
    "href": "publication/Religion-spirituality-physical-health/index.html",
    "title": "Religion, spirituality, and physical health in cancer patients: A meta-analysis",
    "section": "",
    "text": "Journal\nAlthough religion/spirituality (R/S) is important in its own right for many cancer patients, a large body of research has examined whether R/S is also associated with better physical health outcomes. This literature has been characterized by heterogeneity in sample composition, measures of R/S, and measures of physical health. In an effort to synthesize previous findings, a meta-analysis of the relation between R/S and patient-reported physical health in cancer patients was performed. A search of PubMed, PsycINFO, the Cumulative Index to Nursing and Allied Health Literature, and the Cochrane Library yielded 2073 abstracts, which were independently evaluated by pairs of raters. The meta-analysis was conducted for 497 effect sizes from 101 unique samples encompassing more than 32,000 adult cancer patients. R/S measures were categorized into affective, behavioral, cognitive, and ‘other’ dimensions. Physical health measures were categorized into physical well-being, functional well-being, and physical symptoms. Average estimated correlations (Fisher z scores) were calculated with generalized estimating equations with robust variance estimation. Overall R/S was associated with overall physical health (z=0.153, P&lt;.001); this relation was not moderated by sociodemographic or clinical variables. Affective R/S was associated with physical well-being (z=0.167, P&lt;.001), functional well-being (z=0.343, P&lt;.001), and physical symptoms (z=0.282, P&lt;.001). Cognitive R/S was associated with physical well-being (z=0.079, P&lt;.05) and functional well-being (z=0.090, P&lt;.01). ‘Other’ R/S was associated with functional well-being (z=0.100, P&lt;.05). In conclusion, the results of the current meta-analysis suggest that greater R/S is associated with better patient-reported physical health. These results underscore the importance of attending to patients’ religious and spiritual needs as part of comprehensive cancer care."
  },
  {
    "objectID": "publication/response-guided-designs-in-SCED-baselines/index.html",
    "href": "publication/response-guided-designs-in-SCED-baselines/index.html",
    "title": "The impact of response-guided designs on count outcomes in single-case experimental design baselines",
    "section": "",
    "text": "Journal  Code\nIn single-case experimental design (SCED) research, researchers often choose when to start treatment based on whether the baseline data collected so far are stable, using what is called a response-guided design. There is evidence that response-guided designs are common, and researchers have described a variety of criteria for assessing stability. With many of these criteria, making judgments about stability could yield data with limited variability, which may have consequences for statistical inference and effect size estimates. However, little research has examined the impact of response-guided design on the resulting data. Drawing on both applied and methodological research, we describe several algorithms as models for response-guided design. We use simulation methods to assess how using a response-guided design impacts the baseline data pattern. The simulations generate baseline data in the form of frequency counts, a common type of outcome in SCEDs. Most of the response-guided algorithms we identified lead to baselines with approximately unbiased mean levels, but nearly all of them lead to underestimates in the baseline variance. We discuss implications for the use of response-guided designs in practice and for the plausibility of specific algorithms as representations of actual research practice."
  },
  {
    "objectID": "publication/RVE-in-fixed-effects-models/index.html",
    "href": "publication/RVE-in-fixed-effects-models/index.html",
    "title": "Small sample methods for cluster-robust variance estimation and hypothesis testing in fixed effects models",
    "section": "",
    "text": "Journal  Correction  R package  Pre-Print\nIn panel data models and other regressions with unobserved effects, fixed effects estimation is often paired with cluster-robust variance estimation (CRVE) to account for heteroscedasticity and un-modeled dependence among the errors. Although asymptotically consistent, CRVE can be biased downward when the number of clusters is small, leading to hypothesis tests with rejection rates that are too high. More accurate tests can be constructed using bias-reduced linearization (BRL), which corrects the CRVE based on a working model, in conjunction with a Satterthwaite approximation for t-tests. We propose a generalization of BRL that can be applied in models with arbitrary sets of fixed effects, where the original BRL method is undefined, and describe how to apply the method when the regression is estimated after absorbing the fixed effects. We also propose a small-sample test for multiple-parameter hypotheses, which generalizes the Satterthwaite approximation for t-tests. In simulations covering a wide range of scenarios, we find that the conventional cluster-robust Wald test can severely over-reject while the proposed small-sample test maintains Type I error close to nominal levels. The proposed methods are implemented in an R package called clubSandwich. This article has online supplementary materials."
  },
  {
    "objectID": "publication/SCD-synthesis-tools-I/index.html",
    "href": "publication/SCD-synthesis-tools-I/index.html",
    "title": "Single-case synthesis tools I: Evaluating the quality and rigor of research on antecedent sensory-based interventions",
    "section": "",
    "text": "Journal\nTools for evaluating the quality and rigor of single case research designs (SCD) are often used when conducting SCD syntheses. Preferred components include evaluations of design features related to the internal validity of SCD to obtain quality and/or rigor ratings. Three tools for evaluating the quality and rigor of SCD (Council for Exceptional Children, What Works Clearinghouse, and Single-Case Analysis and Design Framework) were compared to determine if conclusions regarding the effectiveness of antecedent sensory-based interventions for young children changed based on choice of quality evaluation tool. Evaluation of SCD quality differed across tools, suggesting selection of quality evaluation tools impacts evaluation findings. Suggestions for selecting an appropriate quality and rigor assessment tool are provided and across-tool conclusions are drawn regarding the quality and rigor of studies. Finally, authors provide guidance for using quality evaluations in conjunction with outcome analyses when conducting syntheses of interventions evaluated in the context of SCD."
  },
  {
    "objectID": "publication/SCED-MLMA-RVE/index.html",
    "href": "publication/SCED-MLMA-RVE/index.html",
    "title": "Multi-level meta-analysis of single-case experimental designs using robust variance estimation",
    "section": "",
    "text": "Journal Supplementary materials  Pre-Print  Code\nSingle-case experimental designs (SCEDs) are used to study the effects of interventions on the behavior of individual cases, by making comparisons between repeated measurements of an outcome under different conditions. In research areas where SCEDs are prevalent, there is a need for methods to synthesize results across multiple studies. One approach to synthesis uses a multi-level meta-analysis (MLMA) model to describe the distribution of effect sizes across studies and across cases within studies. However, MLMA relies on having accurate sampling variances of effect size estimates for each case, which may not be possible due to auto-correlation in the raw data series. One possible solution is to combine MLMA with robust variance estimation (RVE), which provides valid assessments of uncertainty even if the sampling variances of effect size estimates are inaccurate. Another possible solution is to forgo MLMA and use simpler, ordinary least squares (OLS) methods with RVE. This study evaluates the performance of effect size estimators and methods of synthesizing SCEDs in the presence of auto-correlation, for several different effect size metrics, via a Monte Carlo simulation designed to emulate the features of real data series. Results demonstrate that the MLMA model with RVE performs properly in terms of bias, accuracy, and confidence interval coverage for estimating overall average log response ratios. The OLS estimator corrected with RVE performs the best in estimating overall average Tau effect sizes. None of the available methods perform adequately for meta-analysis of within-case standardized mean differences."
  },
  {
    "objectID": "publication/selective-reporting-with-dependent-effects/index.html",
    "href": "publication/selective-reporting-with-dependent-effects/index.html",
    "title": "Evaluating meta-analytic methods to detect selective reporting in the presence of dependent effect sizes",
    "section": "",
    "text": "Journal Supplementary materials  Pre-Print\nMeta-analysis is a set of statistical tools used to synthesize results from multiple studies evaluating a common research question. Two methodological challenges when conducting meta-analysis include selective reporting and correlated dependent effect sizes. Selective reporting is often a result of selective publication practices based on the statistical significance of study findings, which threatens the validity of meta-analytic results. One of the main sources of dependent effect sizes is the inclusion of multiple outcome measures from a primary study. This violates conventional, univariate meta-analytic techniques. Meta-analysts lack validated methods to detect the presence of selective reporting while incorporating methods to handle dependent effect sizes. This study evaluates currently available univariate selective reporting methods, when ignoring dependence, selecting one effect size per study, or aggregating dependent correlated effect sizes. This study also proposes and examines an Egger’s Regression variant incorporated with Robust Variance Estimation (RVE) to handle within-study dependence. A Monte Carlo simulation study assess the performance of the methods for Type I error rates in the absence of selective reporting, and power to detect selective reporting when introduced. Ignoring dependence inflates Type I error rates for all univariate detection methods. Type I error rates are maintained with regression tests when dependent effect sizes are sampled, aggregated or modeled using RVE. However, all selective reporting methods evaluated in this study have little to no power to detect selection bias, except under strong selection censoring."
  },
  {
    "objectID": "publication/SMD-for-MBD/index.html",
    "href": "publication/SMD-for-MBD/index.html",
    "title": "A standardized mean difference effect size for multiple baseline designs",
    "section": "",
    "text": "Journal  R package  Web app\nSingle-case designs are a class of research methods for evaluating treatment effects by measuring outcomes repeatedly over time while systematically introducing different condition (e.g., treatment and control) to the same individual. The designs are used across fields such as behavior analysis, clinical psychology, special education, and medicine. Emerging standards for single-case designs have focused attention on methods for summarizing and meta-analyzing findings and on the need for effect sizes indices that are comparable to those used in between-subjects designs. In the previous work, we discussed how to define and estimate an effect size that is directly comparable to the standardized mean difference often used in between-subjects research based on the data from a particular type of single-case design, the treatment reversal or (AB)k design. This paper extends the effect size measure to another type of single-case study, the multiple baseline design. We propose estimation methods for the effect size and its variance, study the estimators using simulation, and demonstrate the approach in two applications."
  },
  {
    "objectID": "publication/stay-play-talk-meta-analysis/index.html",
    "href": "publication/stay-play-talk-meta-analysis/index.html",
    "title": "Systematic review and meta-analysis of stay-play-talk interventions for improving social behaviors of young children",
    "section": "",
    "text": "Journal Supplementary materials  Pre-Print\nStay-play-talk (SPT) is a peer-mediated intervention which involves training peer implementers to stay in proximity to, play with, and talk to a focal child who has disabilities or lower social competence. This systematic review and meta-analysis investigated the contexts in which SPT interventions have been conducted, the methodological adequacy of the research assessing its effects, and the outcomes for both peer implementers and focal children. Studies have primarily occurred in inclusive preschool settings during free play activities, with researchers serving as facilitators. Average effects were positive for both peer implementers and focal children, although considerable heterogeneity across studies was observed. Additional research is needed to determine what peer implementer and focal child characteristics moderate intervention success, what modifications are needed for children who have complex communication needs, and optimal procedural variations (e.g., group size, training time)."
  },
  {
    "objectID": "publication/Testing-for-funnel-plot-asymmetry-of-SMDs/index.html",
    "href": "publication/Testing-for-funnel-plot-asymmetry-of-SMDs/index.html",
    "title": "Testing for funnel plot asymmetry of standardized mean differences",
    "section": "",
    "text": "Journal  Pre-Print  Code\nPublication bias and other forms of outcome reporting bias are critical threats to the validity of findings from research syntheses. A variety of methods have been proposed for detecting selective outcome reporting in a collection of effect size estimates, including several methods based on assessment of asymmetry of funnel plots, such as Egger’s regression test, the rank correlation test, and the Trim-and-Fill test. Previous research has demonstated that Egger’s regression test is mis-calibrated when applied to log-odds ratio effect size estimates, due to artifactual correlation between the effect size estimate and its standard error. This study examines similar problems that occur in meta-analyses of the standardized mean difference, a ubiquitous effect size measure in educational and psychological research. In a simulation study of standardized mean difference effect sizes, we assess the Type I error rates of conventional tests of funnel plot asymmetry, as well as the likelihood ratio test from a three-parameter selection model. Results demonstrate that the conventional tests have inflated Type I error due to correlation between the effect size estimate and its standard error, while tests based on either a simple modification to the conventional standard error formula or a variance-stabilizing transformation both maintain close-to-nominal Type I error."
  },
  {
    "objectID": "publication/Transition-to-College-Mathematics-Year-1/index.html",
    "href": "publication/Transition-to-College-Mathematics-Year-1/index.html",
    "title": "Evaluating the Transition to College Mathematics Course in Texas high schools: Findings from the first year of implementation",
    "section": "",
    "text": "White paper  PDF\nTexas House Bill 5 introduced requirements that school districts partner with institutions of higher education to provide college preparatory courses in mathematics and English for high school seniors who are not yet college ready. As districts and college partners begin to respond to these provisions, there is a need for empirical research on the effects of different approaches to implementing the college preparatory courses. In response to House Bill 5 requirements, the Charles A. Dana Center has developed a model college preparatory mathematics course, Transition to College Mathematics Course (TCMC), which has been adopted by dozens of school districts across Texas over the past several school years. We examine the effects of TCMC on students’ progress into post-secondary education by comparing students who participated in TCMC during the 2016-17 school year (the first year of implementation) to observationally similar students, either from a previous cohort that did not have access to TCMC or from the same cohort but who did not enroll in the course. We find that, although students who took TCMC graduated at slightly higher rates than comparison students, they had lower rates of enrollment in post-secondary education, driven by lower rates of enrollment in 4-year colleges or universities. Enrollment gradually became more similar over the four semesters following graduation from high school. We find that students who took TCMC were also less likely than students in the comparison group to pass college-level and developmental math courses. Longer-term cumulative outcomes showed stronger reductions in rates of math course passage. However, these results must be interpreted cautiously because we were unable to fully assess and account for students’ college-readiness status at the start of their senior year."
  },
  {
    "objectID": "publication/Transition-to-College-Mathematics-Year-3/index.html",
    "href": "publication/Transition-to-College-Mathematics-Year-3/index.html",
    "title": "Evaluating the Transition to College Mathematics Course in Texas high schools: Examining heterogeneity across schools and student characteristics",
    "section": "",
    "text": "PDF\nTexas House Bill 5 introduced requirements that school districts partner with institutions of higher education to provide college preparatory courses in mathematics and English language arts for high school seniors who are not yet college ready. In response to House Bill 5 requirements, the Charles A. Dana Center developed a model college preparatory mathematics course, Transition to College Mathematics Course (TCMC), which has been adopted by dozens of school districts across Texas over the past several school years. In prior work, we examined the effects of TCMC on students’ progress into post-secondary education by comparing two student cohorts who participated in TCMC to observationally similar students from the same cohort but who did not enroll in the course. In this report, we investigate the extent of heterogeneity in the effects of participating in TCMC. We find little evidence that the program was differentially effective for students from different socio-economic backgrounds, nor do we find evidence that program effects varied by the number of years that it had been offered. However, for key outcomes such as rates of passing a college-level math course, the effects of participating in TCMC may have varied across the schools that offered the course. Just as in prior work, these findings must be interpreted cautiously because we were unable to fully assess and account for students’ college-readiness status at the start of their senior year."
  },
  {
    "objectID": "software/ARPobservation/index.html",
    "href": "software/ARPobservation/index.html",
    "title": "ARPobservation",
    "section": "",
    "text": "An R package for simulating different methods of recording data based on direct observation of behavior, where behavior is modeled by an alternating renewal process.\n\nAvailable on the Comprehensive R Archive Network\nInstallation instructions\nSource code on Github\nARPsimulator: An interactive web application for simulating systematic direct observation data based on the alternating renewal process model."
  },
  {
    "objectID": "software/POMADE/index.html",
    "href": "software/POMADE/index.html",
    "title": "POMADE",
    "section": "",
    "text": "An R package for computing power levels, minimum detectable effect sizes, and minimum required sample sizes for the test of the overall average effect size in meta-analysis of dependent effect sizes. The package also includes functions for creating plots of power analysis results."
  },
  {
    "objectID": "software/simhelpers/index.html",
    "href": "software/simhelpers/index.html",
    "title": "simhelpers",
    "section": "",
    "text": "Monte Carlo simulations are computer experiments designed to study the performance of statistical methods under known data-generating conditions. The goal of simhelpers is to assist in running such simulation studies. The main tools in the package consist of functions to calculate measures of estimator performance, such as bias, root mean squared error, rejection rates. The functions also calculate the associated Monte Carlo standard errors (MCSE) of the performance measures. The functions use the tidyeval principles, so that they play well with dplyr and fit easily into a %&gt;%-centric workflow.\n\nAvailable on the Comprehensive R Archive Network\nSource code and installation instructions on Github"
  },
  {
    "objectID": "software/wildmeta/index.html",
    "href": "software/wildmeta/index.html",
    "title": "wildmeta",
    "section": "",
    "text": "An R package for conducting hypothesis tests of meta-regression models using cluster wild bootstrapping, based on methods examined in Joshi, Pustejovsky, and Beretvas (2021)."
  },
  {
    "objectID": "posts/Distribution-of-signficant-effects/index.html",
    "href": "posts/Distribution-of-signficant-effects/index.html",
    "title": "Distribution of the number of significant effect sizes",
    "section": "",
    "text": "A while back, I posted the outline of a problem about the number of significant effect size estimates in a study that reports multiple outcomes. This problem interests me because it connects to the issue of selective reporting of study results, which creates problems for meta-analysis. Here, I’ll re-state the problem in slightly more general terms and then make some notes about what’s going on.\nConsider a study that assesses some effect size across \\(m\\) different outcomes. (We’ll be thinking about one study at a time here, so no need to index the study as we would in a meta-analysis problem.) Let \\(T_i\\) denote the effect size estimate for outcome \\(i\\), let \\(V_i\\) denote the sampling variance of the effect size estimate for outcome \\(i\\), and let \\(\\theta_i\\) denote the true effect size parameter for corresponding to outcome \\(i\\). Assume that the study outcomes \\(\\left[T_i\\right]_{i=1}^m\\) follow a correlated-and-hierarchical effects model, in which \\[T_i = \\mu + u + v_i + e_i,\\] where the study-level error \\(u \\sim N\\left(0,\\tau^2\\right)\\), the effect-specific error \\(v_i \\stackrel{iid}{\\sim} N\\left(0, \\omega^2\\right)\\), and the vector of sampling errors \\(\\left[e_i\\right]_{i=1}^m\\) is multivariate normal with mean \\(\\mathbf{0}\\), known variances \\(\\text{Var}(e_i) = \\sigma^2\\), and compound symmetric correlation structure \\(\\text{cor}(e_h, e_i) = \\rho\\).\nDefine \\(A_i\\) as an indicator that is equal to one if \\(T_i\\) is statistically significant at level \\(\\alpha\\) based on a one-sided test, and otherwise equal to zero. (Equivalently, let \\(A_i\\) be equal to one if the effect is statistically significant at level \\(2 \\alpha\\) and in the theoretically expected direction.) Formally, \\[A_i = I\\left(\\frac{T_i}{\\sigma} &gt; q_\\alpha \\right)\\] where \\(q_\\alpha = \\Phi^{-1}(1 - \\alpha)\\) is the critical value from a standard normal distribution (e.g., \\(q_{.05} = 1.645\\), \\(q_{.025} = 1.96\\)). Let \\(N_A = \\sum_{i=1}^m A_i\\) denote the total number of statistically significant effect sizes in the study. The question is: what is the distribution of \\(N_A\\)."
  },
  {
    "objectID": "posts/Distribution-of-signficant-effects/index.html#compound-symmetry-to-the-rescue",
    "href": "posts/Distribution-of-signficant-effects/index.html#compound-symmetry-to-the-rescue",
    "title": "Distribution of the number of significant effect sizes",
    "section": "Compound symmetry to the rescue",
    "text": "Compound symmetry to the rescue\nAs I noted in the previous post, this set-up means that the effect size estimates have a compound symmetric distribution. We can make this a bit more explicit by writing the sampling errors in terms of the sum of a component that’s common acrosss outcomes and a component that’s specific to each outcome. Thus, let \\(e_i = f + g_i\\), where \\(f \\sim N\\left(0, \\rho \\sigma^2 \\right)\\) and \\(g_i \\stackrel{iid}{\\sim} N \\left(0, (1 - \\rho) \\sigma^2\\right)\\). Let me also define \\(\\zeta = \\mu + u + f\\) as the conditional mean of the effects. It then follows that the effect size estimates are conditionally independent, given the common components: \\[\n\\left(T_i | \\zeta \\right) \\stackrel{iid}{\\sim} N\\left(\\zeta, \\omega^2 + (1 - \\rho) \\sigma^2\\right)\n\\] Furthermore, the conditional probability of a significant effect is \\[\n\\text{Pr}(A_i = 1 | \\zeta) = \\Phi\\left(\\frac{\\zeta - q_{\\alpha} \\sigma}{\\sqrt{\\omega^2 + (1 - \\rho)\\sigma^2}}\\right)\n\\] and \\(A_1,...,A_m\\) are mutually independent, conditional on \\(\\zeta\\). Therefore, the conditional distribution of \\(N_A\\) is binomial, \\[\n\\left(N_A | \\zeta\\right) \\sim Bin(m, \\pi)\n\\] where \\[\n\\pi = \\Phi\\left(\\frac{\\zeta - q_{\\alpha} \\sigma}{\\sqrt{\\omega^2 + (1 - \\rho)\\sigma^2}}\\right).\n\\] What about the unconditional distribution?\nTo get rid of the \\(\\zeta\\), we need to integrate over its distribution, which leads to \\[\n\\text{Pr}(N_A = a) = \\text{E}\\left[\\text{Pr}\\left(N_A | \\zeta\\right)\\right] = \\int f_{N_A}\\left(a | \\zeta, \\omega, \\sigma, \\rho, m\\right) \\times f_\\zeta(\\zeta | \\mu, \\tau, \\sigma, \\rho) \\ d \\zeta,\n\\] where \\(f_{N_A}\\left(a | \\zeta, \\omega, \\sigma, \\rho \\right)\\) is a binomial density with size \\(m\\) and probability \\(\\pi = \\pi(\\zeta, \\omega, \\sigma, \\rho)\\) and \\(f_\\zeta(\\zeta | \\mu, \\tau, \\sigma, \\rho)\\) is a normal density with mean \\(\\mu\\) and variance \\(\\tau^2 + \\rho \\sigma^2\\).\nThis distribution is what you might call a binomial-normal convolution or a random-intercept probit model (where the random intercept is \\(\\zeta\\)). As far as I know, the distribution cannot be evaluated analytically but instead must be calculated using some sort of numerical integration routine. Here is an interactive graph of the probability mass function (the probability points are calculated using Gaussian quadrature):\n\nmath = require(\"mathjs\")\nnorm = import('https://unpkg.com/norm-dist@3.1.0/index.js?module')\n\nquad_points = JSON.parse(all_quad_points).at(qp - 1)\n\nsigma = 2 / math.sqrt(ESS)\n\nzeta_sd = math.sqrt(tau**2 + rho * sigma**2)\n\ncrit = norm.icdf(1 - alpha)\n\nbinomial_coefs = Array(m+1).fill(null).map((x,index) =&gt; {\n  return math.combinations(m, index);\n})\n\nprobs = quad_points.map(zeta =&gt; {\n  let Z = (zeta[0] * zeta_sd + mu - crit * sigma) / math.sqrt(omega**2 + (1 - rho) * sigma**2);\n  return [norm.cdf(Z), zeta[1]];\n})\n\np_binom_norm = binomial_coefs.map((coef, a) =&gt; {\n  let p = probs.map((x) =&gt; {\n    return (x[0]**a) * ((1 - x[0])**(m - a)) * x[1];\n  });\n  return coef * math.sum(p);\n})\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPlot.plot({\n  x: {\n    label: \"Number of significant effect sizes\"\n  },\n  y: {\n    domain: [0, 1],\n    label: \"Probability\"\n  },\n  marks: [\n    Plot.ruleY(0),\n    Plot.barY(p_binom_norm, {\n      fill: \"steelblue\"\n    }),\n  ]\n})\n\n\n\n\n\n\n\n\n\nviewof m = Inputs.range(\n  [1, 30], \n  {value: 6, step: 1, label: \"Number of effect sizes (m):\"}\n)\n\nviewof ESS = Inputs.range(\n  [4, 300], \n  {value: 80, step: 1, label: \"Effective sample size:\"}\n)\n\nviewof mu = Inputs.range(\n  [-2, 2], \n  {value: 0.3, step: 0.01, label: \"Average effect size (mu):\"}\n)\n\nviewof tau = Inputs.range(\n  [0, 1], \n  {value: 0.1, step: 0.01, label: \"Between-study SD (tau):\"}\n)\n\nviewof omega = Inputs.range(\n  [0, 1], \n  {value: 0.1, step: 0.01, label: \"Within-study SD (omega):\"}\n)\n\nviewof rho = Inputs.range(\n  [0, 1], \n  {value: 0.6, step: 0.01, label: \"Sampling error correlation (rho):\"}\n)\n\nviewof alpha = Inputs.range(\n  [0.005, 0.995], \n  {value: 0.025, step: .005, label: \"One-sided significance threshold (alpha):\"}\n)\n\nviewof qp = Inputs.range(\n  [1, 30], \n  {value: 21, step: 1, label: \"Number of quadrature points:\"}\n)"
  },
  {
    "objectID": "posts/Distribution-of-signficant-effects/index.html#just-the-moments-please",
    "href": "posts/Distribution-of-signficant-effects/index.html#just-the-moments-please",
    "title": "Distribution of the number of significant effect sizes",
    "section": "Just the moments, please",
    "text": "Just the moments, please\nIf all we care about is the expectation of \\(N_A\\), we don’t need to bother with all the conditioning business and can just look at the marginal distribution of the effect size estimates taken individually. Marginally, \\(T_i\\) is normally distributed with mean \\(\\mu\\) and variance \\(\\tau^2 + \\omega^2 + \\sigma^2\\), so \\(\\text{Pr}(A_i = 1) = \\psi\\), where \\[\n\\psi = \\Phi\\left(\\frac{\\mu - q_{\\alpha} \\sigma}{\\sqrt{\\tau^2 + \\omega^2 + \\sigma^2}}\\right).\n\\] By the linearity of expectations, \\[\n\\text{E}(N_A) = \\sum_{i=1}^m \\text{E}(A_i) = m \\psi.\n\\]\nWe can also get an approximation for the variance of \\(N_A\\) by working with its conditional distribution above. By the rule of variance decomposition, \\[\n\\begin{aligned}\n\\text{Var}(N_A) &= \\text{E}\\left[\\text{Var}\\left(N_A | \\zeta\\right)\\right] + \\text{Var}\\left[\\text{E}\\left(N_A | \\zeta\\right)\\right] \\\\\n&= m \\times \\text{E}\\left[\\pi (1 - \\pi)\\right] + m^2 \\times \\text{Var}\\left[\\pi\\right]\\\\\n&= m \\times \\text{E}\\left[\\pi\\right] \\left(1 - \\text{E}\\left[\\pi\\right]\\right) + m (m - 1) \\times \\text{Var}\\left[\\pi\\right],\n\\end{aligned}\n\\] where \\(\\pi\\) is, as defined above, a function of \\(\\zeta\\) and thus a random variable. Now, \\(\\text{E}(\\pi) = \\psi\\) and we can get something close to \\(\\text{Var}(\\pi)\\) using a first-order approximation: \\[\n\\text{Var}\\left(\\pi\\right) \\approx \\left(\\left.\\frac{\\delta \\pi}{\\delta \\zeta}\\right|_{\\zeta = \\mu}\\right)^2 \\times \\text{Var}\\left(\\zeta\\right) = \\left[\\phi\\left(\\frac{\\mu - q_{\\alpha} \\sigma}{\\sqrt{\\omega^2 + (1 - \\rho)\\sigma^2}}\\right)\\right]^2 \\times \\frac{\\tau^2 + \\rho \\sigma^2}{\\omega^2 + (1 - \\rho)\\sigma^2}.\n\\] Thus, \\[\n\\begin{aligned}\n\\text{Var}(N_A) \\approx m \\times \\psi \\left(1 - \\psi\\right) + m (m - 1) \\times \\left[\\phi\\left(\\frac{\\mu - q_{\\alpha} \\sigma}{\\sqrt{\\omega^2 + (1 - \\rho)\\sigma^2}}\\right)\\right]^2 \\times \\frac{\\tau^2 + \\rho \\sigma^2}{\\omega^2 + (1 - \\rho)\\sigma^2}.\n\\end{aligned}\n\\] If the amount of common variation is small, so \\(\\tau^2\\) is near zero and \\(\\rho\\) is near zero, then the contribution of the second term will be small, and \\(N_A\\) will act more or less like a binomial random variable with size \\(m\\) and probability \\(\\psi\\). On the other hand, if the amount of independent variation in the effect sizes is small, so \\(\\omega^2\\) is near zero and \\(\\rho\\) is near 1, then the term on the right will approach \\(m(m - 1)\\psi(1 - \\psi)\\) and \\(\\text{Var}\\left(N_A\\right)\\) will approach \\(m^2 \\psi(1 - \\psi)\\), or the variance of \\(m\\) times a single Bernoulli variate. So you could say that \\(N_A\\) has anywhere between \\(1\\) and \\(m\\) variate’s worth of information in it, depending on the degree of correlation between the effect size estimates."
  },
  {
    "objectID": "posts/Approximating-cluster-robust-Wald-tests/index.html",
    "href": "posts/Approximating-cluster-robust-Wald-tests/index.html",
    "title": "Approximating the distribution of cluster-robust Wald statistics",
    "section": "",
    "text": "In Tipton and Pustejovsky (2015), we examined several different small-sample approximations for cluster-robust Wald test statistics, which are like \\(F\\) statistics but based on cluster-robust variance estimators. These statistics are, frankly, kind of weird and awkward to work with, and the approximations that we examined were far from perfect. In this post, I will look in detail at the robust Wald statistic for a simple but common scenario: a one-way ANOVA problem with clusters of dependent observations. \\(\\def\\Pr{{\\text{Pr}}}\n\\def\\E{{\\text{E}}}\n\\def\\Var{{\\text{Var}}}\n\\def\\Cov{{\\text{Cov}}}\n\\def\\cor{{\\text{cor}}}\n\\def\\bm{\\mathbf}\n\\def\\bs{\\boldsymbol}\\)\nConsider a setup where clusters can be classified into one of \\(C\\) categories, with each cluster of observations falling into a single category. Let \\(\\bs\\mu = \\left[\\mu_c \\right]_{c=1}^C\\) denote the means of these categories. Suppose we have an estimator of those means \\(\\bs{\\hat\\mu} = \\left[\\hat\\mu_c\\right]_{c=1}^C\\) and a corresponding cluster-robust variance estimator \\(\\bm{V}^R = \\bigoplus_{c=1}^C V^R_c\\). Note that \\(\\bm{V}^R\\) is diagonal because the estimators for each category are independent. Assume that the robust variance estimator is unbiased so \\(\\E\\left(V^R_c\\right) = \\Var\\left( \\hat\\mu_c \\right) = \\psi_c\\) for \\(c = 1,...,C\\). Let \\(\\bs\\Psi = \\bigoplus_{c=1}^C \\psi_c\\).\nSuppose that we want to test the null hypothesis that that means of the categories are all equal, \\(H_0: \\mu_1 = \\mu_2 = \\cdots = \\mu_C\\). We can express this null using a \\(q \\times C\\) contrast matrix \\(\\bm{C} = \\left[-\\bm{1}_q \\ \\bm{I}_q \\right]\\), where \\(q = C - 1\\). The null hypothesis is then \\(\\bm{C} \\bs\\mu = \\bm{0}_q\\). The corresponding cluster-robust Wald statistic is \\[\nQ = \\bs{\\hat\\mu}' \\bm{C}' \\left(\\bm{C} \\bm{V}^R \\bm{C}'\\right)^{-1} \\bm{C} \\bs{\\hat\\mu}.\n\\] Under the null hypothesis, the distribution of \\(Q\\) will converge to a \\(\\chi^2_q\\) as the number of clusters in each category grows large. However, with a limited number of clusters in some of the categories, this approximate reference distribution is not very accurate and tests based on it can have wildly inflated type I error rates.\nIn the paper, we considered several different ways of approximating the distribution of \\(Q\\) that work at smaller sample sizes. One class of approaches to approximating the sampling distribution of \\(Q\\) is to use a Hotelling’s \\(T^2\\) distribution with degrees of freedom \\(\\eta\\). Given the degrees of freedom, Hotelling’s \\(T^2\\) is a multiple of an \\(F\\) distribution: \\[\n\\frac{\\eta - q + 1}{\\eta q} Q \\sim F(q, \\eta - q + 1).\n\\] The question is then how to determine \\(\\eta\\).\nSeveral of the approaches that we considered are based on representing the \\(Q\\) statistic as \\[\nQ = \\bm{z}' \\bm{D}^{-1} \\bm{z},\n\\] where \\(\\bs\\Omega = \\bm{C} \\bs\\Psi \\bm{C}'\\), \\(\\bm{z} = \\bs\\Omega^{-1/2}\\bm{C}\\hat\\mu_c\\), \\(\\bm{G} = \\bs\\Omega^{-1/2} \\bm{C}\\), and \\[\n\\bm{D} = \\bm{G} \\bm{V}^R \\bm{G}'.\n\\] The various approaches we considered involve different ways of approximating the sampling distribution of \\(\\bm{D}\\).\nOne of the approximations involves finding degrees of freedom \\(\\eta\\) by following a strategy suggested by Zhang (2012, 2013). These degrees of freedom are given by \\[\n\\eta_Z = \\frac{q(q + 1)}{\\sum_{s=1}^q \\sum_{t = 1}^q \\Var(d_{st})},\n\\] where \\(d_{st}\\) is the entry in row \\(s\\), column \\(t\\) of \\(\\bm{D}\\). To find \\(\\eta_Z\\), we can compute the denominator using general formulas given in the paper. However, with a bit of analysis we can find a much simpler expression for the special case of one-way ANOVA.\nBefore going further, it’s useful to observe that \\(\\bm{D}\\) is invariant to linear transformations of \\(\\bm{C}\\). In particular, an equivalent way to write the null hypothesis is as \\(H_0: \\bs\\Psi_{\\circ}^{-1/2} \\bm{C} = \\bm{0}_q\\), where \\(\\bs\\Psi_{\\circ} = \\bigoplus_{c=2}^C \\psi_c\\) is the diagonal of the true sampling variances of categories 2 through \\(C\\), omitting the first category. Thus, let me redefine \\[\n\\bs\\Omega = \\bs\\Psi_{\\circ}^{-1/2} \\bm{C} \\bs\\Psi \\bm{C}'\\bs\\Psi_{\\circ}^{-1/2},\n\\] \\(\\bm{z} = \\bs\\Omega^{-1/2}\\bs\\Psi_{\\circ}^{-1/2}\\bm{C}\\hat\\mu_c\\), and \\[\n\\bm{G} = \\bs\\Omega^{-1/2} \\bs\\Psi_{\\circ}^{-1/2} \\bm{C}.\n\\] This transformation of the constraint matrix will make it possible to find a closed-form expression for \\(\\bs\\Omega^{-1/2}\\).\nNow, observe that \\[\n\\begin{aligned}\n\\bs\\Omega &= \\bs\\Psi_{\\circ}^{-1/2} \\bm{C} \\bs\\Psi \\bm{C}'\\bs\\Psi_{\\circ}^{-1/2} \\\\\n&= \\bs\\Psi_{\\circ}^{-1/2} \\left(\\bs\\Psi_{\\circ} + \\psi_1 \\bm{1}_q \\bm{1}_q'\\right)\\bs\\Psi_{\\circ}^{-1/2} \\\\\n&= \\bm{I}_q + \\psi_1 \\bm{f} \\bm{f}',\n\\end{aligned}\n\\] where \\(\\bm{f} = \\bs\\Psi_{\\circ}^{-1/2} \\bm{1}_q = \\left[ \\psi_c^{-1/2}\\right]_{c = 2}^C\\). From the Woodbury identity, \\[\n\\bs\\Omega^{-1} = \\bm{I} - \\frac{1}{W} \\bm{f} \\bm{f}',\n\\] where \\(W = \\sum_{c=1}^C \\frac{1}{\\psi_c}\\).\nFasi, Higham, and Liu (2023) provide formulas for \\(p^{th}\\) roots of low-rank updates to scaled identity matrices. Their results provide a neat closed-form expression for \\(\\bs\\Omega^{-1/2}\\). From their Equation (1.9), \\[\n\\bs\\Omega^{-1/2} = \\mathbf{I}_q - \\kappa \\ \\bm{f} \\bm{f}',\n\\] where \\(\\kappa = \\frac{\\sqrt{\\psi_1}}{W \\sqrt{\\psi_1} + \\sqrt{W}}\\). Further, we can write the \\(q \\times C\\) matrix \\(\\bm{G}\\) as \\[\n\\begin{aligned}\n\\bm{G} &= \\bs\\Omega^{-1/2} \\bs\\Psi_{\\circ}^{-1/2} \\bm{C} \\\\\n&= \\left( \\mathbf{I}_q - \\kappa \\ \\bm{f} \\bm{f}' \\right) \\bs\\Psi_{\\circ}^{-1/2} \\left[-\\bm{1}_q, \\ \\bm{I}_q \\right] \\\\\n&= \\left[\\frac{\\kappa(W \\psi_1 - 1) - \\psi_1}{\\psi_1} \\bm{f},  \\left( \\mathbf{I}_q - \\kappa \\ \\bm{f} \\bm{f}' \\right) \\bs\\Psi_{\\circ}^{-1/2}\\right],\n\\end{aligned}\n\\] with entries given by \\[\ng_{sc} = \\begin{cases}\n\\frac{\\kappa(W \\psi_1 - 1) - \\psi_1}{\\psi_1 \\sqrt{\\psi_{s+1}}} & \\text{if} \\quad c = 1 \\\\\n\\frac{I(s+1 = c)}{\\sqrt{\\psi_{c}}} - \\frac{\\kappa}{\\psi_c \\sqrt{\\psi_{s+1}}} & \\text{if} \\quad c &gt; 1.\n\\end{cases}\n\\] Because \\(\\bm{D} = \\bm{G} \\bm{V}^R \\bm{G}'\\) and \\(\\bm{V}^R\\) is diagonal, we can write the entries of \\(\\bm{D}\\) as \\[\nd_{st} = \\sum_{c=1}^C g_{sc} g_{tc} V^R_c.\n\\] And because the variance estimators for each category are independent, \\[\n\\Var(d_{st}) = \\sum_{c=1}^C g_{sc}^2 g_{tc}^2 \\Var(V^R_c).\n\\] In prior work, we derived expressions for the Satterthwaite degrees of freedom for variances of average effect sizes, and the same formulas can be applied here with the category-specific \\(V^R_c\\). Let me write \\(\\nu_c = 2\\left[\\E(V^R_c)\\right]^2 / \\Var(V^R_c)\\) for the degrees of freedom corresponding to category \\(c\\). Then \\[\n\\Var(d_{st}) = 2 \\sum_{c=1}^C g_{sc}^2 g_{tc}^2 \\frac{\\psi_c^2}{\\nu_c}.\n\\] We can use this to obtain an expression for Zhang’s approximate degrees of freedom: \\[\n\\begin{aligned}\nq(q + 1)\\eta_Z^{-1} &= \\sum_{s=1}^q \\sum_{t = 1}^q \\Var(d_{st}) \\\\\n&= 2\\sum_{s=1}^q \\sum_{t = 1}^q \\sum_{c=1}^C g_{sc}^2 g_{tc}^2 \\frac{\\psi_c^2}{\\nu_c} \\\\\n&= 2\\sum_{c=1}^C \\frac{\\psi_c^2}{\\nu_c} \\left(\\sum_{s=1}^q g_{sc}^2\\right)^2.\n\\end{aligned}\n\\] Now, all we need to do is simplify… \\[\n\\begin{aligned}\n\\sum_{s=1}^q g_{s1}^2 &= \\sum_{s=1}^q \\frac{\\left(\\kappa(W \\psi_1 - 1) - \\psi_1\\right)^2}{\\psi_1^2 \\psi_{s+1}} \\\\\n&= \\frac{\\left(\\kappa(W \\psi_1 - 1) - \\psi_1\\right)^2}{\\psi_1^2} \\sum_{c=2}^C \\frac{1}{\\psi_{s+1}} \\\\\n&= \\frac{\\left(\\kappa(W \\psi_1 - 1) - \\psi_1\\right)^2}{\\psi_1^2} \\frac{(W \\psi_1 - 1)}{\\psi_1} \\\\\n&= \\text{...a bunch of tedious algebra...} \\\\\n&= \\frac{1}{\\psi_1^2} \\left(\\psi_1 - \\frac{1}{W}\\right)\n\\end{aligned}\n\\] and, for \\(c = 2,...,C\\), \\[\n\\begin{aligned}\n\\sum_{s=1}^q g_{sc}^2 &= \\sum_{s=1}^q \\left(\\frac{I(s+1 = c)}{\\sqrt{\\psi_{c}}} - \\frac{\\kappa}{\\psi_c \\sqrt{\\psi_{s+1}}}\\right)^2 \\\\\n&= \\frac{1}{\\psi_c} - \\frac{2 \\kappa}{\\psi_c^2} + \\frac{\\kappa^2}{\\psi_c^2}\\sum_{s=1}^q \\frac{1}{\\psi_{s+1}} \\\\\n&= \\frac{1}{\\psi_c} - \\frac{2 \\kappa}{\\psi_c^2} + \\frac{\\kappa^2}{\\psi_c^2}\\frac{(W \\psi_1 - 1)}{\\psi_1} \\\\\n&= \\text{...a bunch of tedious algebra...} \\\\\n&= \\frac{1}{\\psi_c^2} \\left(\\psi_c - \\frac{1}{W}\\right)\n\\end{aligned}\n\\] Thus, \\[\n\\begin{aligned}\nq(q + 1)\\eta_Z^{-1} &= 2\\sum_{c=1}^C \\frac{\\psi_c^2}{\\nu_c} \\left(\\sum_{s=1}^q g_{sc}^2\\right)^2 \\\\\n&= 2\\sum_{c=1}^C \\frac{1}{\\nu_c \\psi_c^2}\\left(\\psi_c - \\frac{1}{W}\\right)^2 \\\\\n&= 2\\sum_{c=1}^C \\frac{1}{\\nu_c}\\left(1 - \\frac{1}{\\psi_c W}\\right)^2\n\\end{aligned}\n\\] or, rearranging, \\[\n\\eta_Z = \\frac{C(C - 1)}{2 \\sum_{c=1}^C \\frac{1}{\\nu_c}\\left(1 - \\frac{1}{\\psi_c W}\\right)^2}.\n\\] It’s a surprisingly clean formula! Once these degrees of freedom are calculated, the degrees of freedom for the reference \\(F\\) distribution would be \\(q\\) and \\(\\eta_Z - q + 1\\).\nIn the paper, we also considered two other degrees of freedom approximations, which involve not only the variances of \\(d_{st}\\) but also the covariances between entries. In principle, one could follow similar algebra to get expressions for these other degrees of freedom as well. However, our simulations indicated that the other degrees of freedom approximations tend to be overly conservative and produce type-I error rates way below the nominal level (essentially, hardly ever rejecting the null) and less accurate than HTZ. So, there’s not much reason to work through them unless you find algebra enjoyable for its own sake.\nA further question about this cluster-robust Wald statistic is how to approximate its sampling distribution under specific alternative hypotheses. In other words, given a vector of means \\(\\mu_1,...,\\mu_C\\) where the null does not hold, plus some information to determine \\(\\psi_c\\) and \\(\\nu_c\\) for \\(c = 1,...,C\\), how could we approximate the distribution of \\(Q\\)? We need something like a non-central Hotelling’s \\(T^2\\) distribution…"
  }
]