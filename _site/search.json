[
  {
    "objectID": "posts/clubSandwich-for-CRVE-FE/index.html",
    "href": "posts/clubSandwich-for-CRVE-FE/index.html",
    "title": "Clustered standard errors and hypothesis tests in fixed effects models",
    "section": "",
    "text": "UPDATED April 09, 2024 to use current syntax for constraints argument in clubSandwich::Wald_test().\nI’ve recently been working with my colleague Beth Tipton on methods for cluster-robust variance estimation in the context of some common econometric models, focusing in particular on fixed effects models for panel data—or what statisticians would call “longitudinal data” or “repeated measures.” We have a new working paper, which you can find here.\nThe importance of using CRVE (i.e., “clustered standard errors”) in panel models is now widely recognized. Less widely recognized, perhaps, is the fact that standard methods for constructing hypothesis tests and confidence intervals based on CRVE can perform quite poorly in when you have only a limited number of independent clusters. What’s worse, it can be hard to determine what counts as a large-enough sample to trust standard CRVE methods, because the finite-sample behavior of the variance estimators and test statistics depends on the configuration of the covariates, not just the total sample size. For example, suppose you have state-level panel data from 50 states across 15 years and are trying to estimate the effect of some policy using difference-in-differences. If only 5 or 6 states have variation in the policy variable over time, then you’re almost certainly in small-sample territory. And the sample size issues can be subtler than this, too, as I’ll show below.\nOne solution to this problem is to use bias-reduced linearization (BRL), which was proposed by Bell and McCaffrey (2002) and has recently begun to receive attention from econometricians (e.g., Cameron & Miller, 2015; Imbens & Kolesar, 2015). The idea of BRL is to correct the bias of standard CRVE based on a working model, and then to use a degrees-of-freedom correction for Wald tests based on the bias-reduced CRVE. That may seem silly (after all, the whole point of CRVE is to avoid making distributional assumptions about the errors in your model), but it turns out that the correction can help quite a bit, even when the working model is wrong. The degrees-of-freedom correction is based on a standard Satterthwaite-type approximation, and also relies on the working model. There’s now quite a bit of evidence (which we review in the working paper) that BRL performs well even in samples with a small number of clusters.\nIn the working paper, we make two contributions to all this:\nThe paper explains all this in greater detail, and also reports a fairly extensive simulation study that we designed to emuluate the types of covariates and study designs encountered in micro-economic applications. We’ve also got an R package that implements our methods (plus some other variants of CRVE, which I’ll explain some other time) in a fairly streamlined way. Here’s an example of how to use the package to do inference for a fixed effects panel data model."
  },
  {
    "objectID": "posts/clubSandwich-for-CRVE-FE/index.html#effects-of-changing-the-minimum-legal-drinking-age",
    "href": "posts/clubSandwich-for-CRVE-FE/index.html#effects-of-changing-the-minimum-legal-drinking-age",
    "title": "Clustered standard errors and hypothesis tests in fixed effects models",
    "section": "Effects of changing the minimum legal drinking age",
    "text": "Effects of changing the minimum legal drinking age\nCarpenter and Dobkin (2011) analyzed the effects of changes in the minimum legal drinking age on rates of motor vehicle fatalies among 18-20 year olds, using state-level panel data from the National Highway Traffic Administration’s Fatal Accident Reporting System. In their new textbook, Angrist and Pischke (2014) developed a stylized example based on Carpenter and Dobkin’s work. I’ll use Angrist and Pischke’s data and follow their analysis, just because their data are easily available.\nThe outcome is the incidence of deaths in motor vehicle crashes among 18-20 year-olds (per 100,000 residents), for each state plus the District of Columbia, over the period 1970 to 1983. Tthere were several changes in the minimum legal drinking age during this time period, with variability in the timing of changes across states. Angrist and Pischke (following Carpenter and Dobkin) use a difference-in-differences strategy to estimate the effects of lowering the minimum legal drinking age from 21 to 18. A basic specification is\n\\[y_{it} = \\alpha_i + \\beta_t + \\gamma r_{it} + \\epsilon_{it},\\]\nfor \\(i\\) = 1,…,51 and \\(t\\) = 1970,…,1983. In this model, \\(\\alpha_i\\) is a state-specific fixed effect, \\(\\beta_t\\) is a year-specific fixed effect, \\(r_{it}\\) is the proportion of 18-20 year-olds in state \\(i\\) in year \\(t\\) who are legally allowed to drink, and \\(\\gamma\\) captures the effect of shifting the minimum legal drinking age from 21 to 18. Following Angrist and Pischke’s analysis, I’ll estimate this model both by (unweighted) OLs and by weighted least squares with weights corresponding to population size in a given state and year.\n\nUnweighted OLS\nThe following code does some simple data-munging and the estimates the model by OLS:\n\n# get data from Angrist & Pischke's website\nlibrary(foreign)\ndeaths &lt;- read.dta(\"http://masteringmetrics.com/wp-content/uploads/2015/01/deaths.dta\", convert.factors=FALSE)\n\n# subset for 18-20 year-olds, deaths in motor vehicle accidents\nMVA_deaths &lt;- subset(deaths, agegr==2 & dtype==2 & year &lt;= 1983, select = c(-dtype, -agegr))\n\n# fit by OLS\nlm_unweighted &lt;- lm(mrate ~ 0 + legal + factor(state) + factor(year), data = MVA_deaths)\n\nThe coef_test function from clubSandwich can then be used to test the hypothesis that changing the minimum legal drinking age has no effect on motor vehicle deaths in this cohort (i.e., \\(H_0: \\gamma = 0\\)). The usual way to test this is to cluster the standard errors by state, calculate the robust Wald statistic, and compare that to a standard normal reference distribution. The code and results are as follows:\n\n# devtools::install_github(\"jepusto/clubSandwich\") # install the clubSandwich package\nlibrary(clubSandwich)\n\nRegistered S3 method overwritten by 'clubSandwich':\n  method    from    \n  bread.mlm sandwich\n\ncoef_test(lm_unweighted, vcov = \"CR1\", cluster = MVA_deaths$state, test = \"z\")[\"legal\",]\n\n Coef. Estimate   SE t-stat d.f. (z) p-val (z) Sig.\n legal     7.59 2.38   3.19      Inf   0.00143   **\n\n\nOur work argues shows that a better approach would be to use the bias-reduced linearization CRVE, together with Satterthwaite degrees of freedom. In the package, the BRL adjustment is called “CR2” because it is directly analogous to the HC2 correction used in heteroskedasticity-robust variance estimation. When applied to an OLS model estimated by lm, the default working model is an identity matrix, which amounts to the “working” assumption that the errors are all uncorrelated and homoskedastic. Here’s how to apply this approach in the example:\n\ncoef_test(lm_unweighted, vcov = \"CR2\", cluster = MVA_deaths$state, test = \"Satterthwaite\")[\"legal\",]\n\n Coef. Estimate   SE t-stat d.f. (Satt) p-val (Satt) Sig.\n legal     7.59 2.43   3.12        25.7      0.00442   **\n\n\nThe Satterthwaite degrees of freedom will be different for each coefficient in the model, and so the coef_test function reports them right alongside the standard error. In this case, the degrees of freedom are about half of what you might expect, given that there are 51 clusters. The p-value for the CR2+Satterthwaite test is about twice as large as the p-value based on the standard Wald test. But of course, the coefficient is still statistically significant at conventional levels, and so the inference doesn’t change.\n\n\nUnweighted “within” estimation\nThe plm package in R provides another way to estimate the same model. It is convenient because it absorbs the state and year fixed effects before estimating the effect of legal. The clubSandwich package works with fitted plm models too:\n\nlibrary(plm)\nplm_unweighted &lt;- plm(mrate ~ legal, data = MVA_deaths, \n                      effect = \"twoways\", index = c(\"state\",\"year\"))\ncoef_test(plm_unweighted, vcov = \"CR1S\", cluster = \"individual\", test = \"z\")\n\n Coef. Estimate   SE t-stat d.f. (z) p-val (z) Sig.\n legal     7.59 2.38   3.19      Inf   0.00143   **\n\ncoef_test(plm_unweighted, vcov = \"CR2\", cluster = \"individual\", test = \"Satterthwaite\")\n\n Coef. Estimate   SE t-stat d.f. (Satt) p-val (Satt) Sig.\n legal     7.59 2.43   3.12        25.7      0.00442   **\n\n\nFor the standard approach, I’ve used the variant of the correction factor implemented in Stata (called CR1S in the clubSandwich package), but this makes very little difference in the standard error or the p-value. For the test based on CR2, the degrees of freedom are slightly different than the results based on the fitted lm model, but the p-values agree to four decimals. The differences in degrees of freedom are due to numerical imprecision in the calculations.\n\n\nPopulation-weighted estimation\nThe difference between the standard method and the new method are not terribly exciting in the above example. However, things change quite a bit if the model is estimated using population weights. As far as I know, plm does not handle weighted least squares, and so I go back to fitting in lm with dummies for all the fixed effects.\n\nlm_weighted &lt;- lm(mrate ~ 0 + legal + factor(state) + factor(year), \n                  weights = pop, data = MVA_deaths)\ncoef_test(lm_weighted, vcov = \"CR1\", cluster = MVA_deaths$state, test = \"z\")[\"legal\",]\n\n Coef. Estimate   SE t-stat d.f. (z) p-val (z) Sig.\n legal      7.5 2.16   3.47      Inf    &lt;0.001  ***\n\ncoef_test(lm_weighted, vcov = \"CR2\", cluster = MVA_deaths$state, test = \"Satterthwaite\")[\"legal\",]\n\n Coef. Estimate  SE t-stat d.f. (Satt) p-val (Satt) Sig.\n legal      7.5 2.3   3.27        8.65       0.0103    *\n\n\nUsing population weights slightly reduces the point estimate of the effect, while also slightly increasing its precision. If you were following the standard approach, you would probably be happy with the weighted estimates and wouldn’t think about it any further. However, our recommended approach—using the CR2 variance estimator and Satterthwaite correction—produces a p-value that is an order of magnitude larger (though still significant at the conventional 5% level). The degrees of freedom are just {r} round(coef_test(lm_weighted, vcov = \"CR2\", cluster = MVA_deaths$state, test = \"Satterthwaite\")[\"legal\",\"df\"], 1)—drastically smaller than would be expected based on the number of clusters.\nEven with weights, the coef_test function uses an “independent, homoskedastic” working model as a default for lm objects. In the present example, the outcome is a standardized rate and so a better assumption might be that the error variances are inversely proportional to population size. The following code uses this alternate working model:\n\ncoef_test(lm_weighted, vcov = \"CR2\", \n          cluster = MVA_deaths$state, target = 1 / MVA_deaths$pop, \n          test = \"Satterthwaite\")[\"legal\",]\n\n Coef. Estimate  SE t-stat d.f. (Satt) p-val (Satt) Sig.\n legal      7.5 2.2   3.41          13      0.00467   **\n\n\nThe new working model leads to slightly smaller standard errors and a couple of additional degrees of freedom, though we remain in small-sample territory.\n\n\nRobust Hausman test\nCRVE is also used in specification tests, as in the Hausman-type test for endogeneity of unobserved effects. Suppose that the model includes an additional control for the beer taxation rate in state \\(i\\) at time \\(t\\), denoted \\(s_{it}\\). The (unweighted) fixed effects model is then\n\\[y_{it} = \\alpha_i + \\beta_t + \\gamma_1 r_{it} + \\gamma_2 s_{it} + \\epsilon_{it},\\]\nand the estimated effects are as follows:\n\nlm_FE &lt;- lm(mrate ~ 0 + legal + beertaxa + factor(state) + factor(year), data = MVA_deaths)\ncoef_test(lm_FE, vcov = \"CR2\", cluster = MVA_deaths$state, test = \"Satterthwaite\")[c(\"legal\",\"beertaxa\"),]\n\n    Coef. Estimate   SE t-stat d.f. (Satt) p-val (Satt) Sig.\n    legal     7.59 2.51  3.019       24.58      0.00583   **\n beertaxa     3.82 5.27  0.725        5.77      0.49663     \n\n\nIf the unobserved effects \\(\\alpha_1,...,\\alpha_{51}\\) are uncorrelated with the regressors, then a more efficient way to estimate \\(\\gamma_1,\\gamma_2\\) is by weighted least squares, with weights based on a random effects model. However, if the unobserved effects covary with \\(\\mathbf{r}_i, \\mathbf{s}_i\\), then the random-effects estimates will be biased.\nWe can test for whether endogeneity is a problem by including group-centered covariates as additional regressors. Let \\(\\tilde{r}_{it} = r_{it} - \\frac{1}{T}\\sum_t r_{it}\\), with \\(\\tilde{s}_{it}\\) defined analogously. Now estimate the regression\n\\[y_{it} = \\beta_t + \\gamma_1 r_{it} + \\gamma_2 s_{it} + \\delta_1 \\tilde{r}_{it} + \\delta_2 \\tilde{s}_{it} + \\epsilon_{it},\\]\nwhich does not include state fixed effects. The parameters \\(\\delta_1,\\delta_2\\) represent the differences between the random effects and fixed effects estimands of \\(\\gamma_1, \\gamma_2\\). If these are both zero, then the random effects estimator is unbiased. Thus, the joint test for \\(H_0: \\delta_1 = \\delta_2 = 0\\) amounts to a test for non-endogeneity of the unobserved effects.\nFor efficiency, we should estimate this using weighted least squares, but OLS will work too:\n\nMVA_deaths &lt;- within(MVA_deaths, {\n  legal_cent &lt;- legal - tapply(legal, state, mean)[factor(state)]\n  beer_cent &lt;- beertaxa - tapply(beertaxa, state, mean)[factor(state)]\n})\n\nlm_Hausman &lt;- lm(mrate ~ 0 + legal + beertaxa + legal_cent + beer_cent + factor(year), data = MVA_deaths)\ncoef_test(lm_Hausman, vcov = \"CR2\", cluster = MVA_deaths$state, test = \"Satterthwaite\")[1:4,]\n\n      Coef. Estimate   SE  t-stat d.f. (Satt) p-val (Satt) Sig.\n      legal   -9.180 7.62 -1.2042       24.94       0.2398     \n   beertaxa    3.395 9.40  0.3613        6.44       0.7295     \n legal_cent   16.768 8.53  1.9665       33.98       0.0575    .\n  beer_cent    0.424 9.25  0.0458        5.86       0.9650     \n\n\nTo conduct a joint test on the centered covariates, we can use the Wald_test function. The usual way to test this hypothesis would be to use the CR1 variance estimator to calculate the robust Wald statistic, then use a \\(\\chi^2_2\\) reference distribution (or equivalently, compare a re-scaled Wald statistic to an \\(F(2,\\infty)\\) distribution). The Wald_test function reports the latter version:\n\nWald_test(\n  lm_Hausman, \n  constraints = constrain_zero(c(\"legal_cent\",\"beer_cent\")), \n  vcov = \"CR1\", \n  cluster = MVA_deaths$state, \n  test = \"chi-sq\"\n)\n\n   test Fstat df_num df_denom  p_val sig\n chi-sq  2.93      2      Inf 0.0534   .\n\n\nThe test is just shy of significance at the 5% level. If we instead use the CR2 variance estimator and our newly proposed approximate F-test (which is the default in Wald_test), then we get:\n\nWald_test(\n  lm_Hausman, \n  constraints = constrain_zero(c(\"legal_cent\",\"beer_cent\")), \n  vcov = \"CR2\", \n  cluster = MVA_deaths$state\n)\n\n test Fstat df_num df_denom p_val sig\n  HTZ  2.57      2     12.4 0.117    \n\n\nThe low degrees of freedom of the test indicate that we’re definitely in small-sample territory and should not trust the asymptotic \\(\\chi^2\\) approximation."
  },
  {
    "objectID": "posts/clubSandwich-for-CRVE-FE/index.html#references",
    "href": "posts/clubSandwich-for-CRVE-FE/index.html#references",
    "title": "Clustered standard errors and hypothesis tests in fixed effects models",
    "section": "References",
    "text": "References\n\nAngrist, J. D., & Pischke, J.-S. (2009). Mostly harmless econometrics: An empiricist’s companion. Princeton, NJ: Princeton University Press.\nAngrist, J. D. and Pischke, J.-S. (2014). Mastering ’metrics: The Path from Cause to Effect. Princeton, NJ: Princeton University Press.\nBell, R. M., & McCaffrey, D. F. (2002). Bias reduction in standard errors for linear regression with multi-stage samples. Survey Methodology, 28(2), 169-181.\nCameron, A. C., & Miller, D. L. (2015). A practitioner’s guide to cluster-robust inference. URL: http://cameron.econ.ucdavis.edu/research/Cameron_Miller_JHR_2015_February.pdf\nCarpenter, C., & Dobkin, C. (2011). The minimum legal drinking age and public health. Journal of Economic Perspectives, 25(2), 133-156. doi:10.1257/jep.25.2.133\nImbens, G. W., & Kolesar, M. (2015). Robust standard errors in small samples: Some practical advice. URL: https://www.princeton.edu/~mkolesar/papers/small-robust.pdf"
  },
  {
    "objectID": "posts/clubSandwich-for-RVE-meta-analysis/index.html",
    "href": "posts/clubSandwich-for-RVE-meta-analysis/index.html",
    "title": "The clubSandwich package for meta-analysis with RVE",
    "section": "",
    "text": "UPDATED April 09, 2024 to use current syntax for constraints argument in clubSandwich::Wald_test().\nI’ve recently been working on small-sample correction methods for hypothesis tests in linear regression models with cluster-robust variance estimation. My colleague (and grad-schoolmate) Beth Tipton has developed small-sample adjustments for t-tests (of single regression coefficients) in the context of meta-regression models with robust variance estimation, and together we have developed methods for multiple-contrast hypothesis tests. We have an R package (called clubSandwich) that implements all this stuff, not only for meta-regression models but also for other models and contexts where cluster-robust variance estimation is often used.\nThe alpha-version of the package is currently available on Github. See the Github README for instructions on how to install it in R. Below I demonstrate how to use the package to get robust variance estimates, t-tests, and F-tests, all with small-sample corrections. The example uses a dataset of effect sizes from a Campbell Collaboration systematic review of dropout prevention programs, conducted by Sandra Jo Wilson and her colleagues.\nThe original analysis included a meta-regression with covariates that capture methodological, participant, and program characteristics. I’ll use a regression specification that is similar to Model III from Wilson et al. (2011), but treat the evaluator_independence and implementation_quality variables as categorical rather than interval-level; the original analysis clustered at the level of the sample (some studies reported results from multiple samples), whereas I will cluster at the study level. I fit the model two ways, first using the robumeta package and then using metafor.\n\nrobumeta model\n\noptions(width=150)\nlibrary(robumeta)\nlibrary(clubSandwich)\n\nRegistered S3 method overwritten by 'clubSandwich':\n  method    from    \n  bread.mlm sandwich\n\ndata(dropoutPrevention)\n\nm3_robu &lt;- robu(LOR1 ~ study_design + attrition + group_equivalence + adjusted\n                + outcome + evaluator_independence\n                + male_pct + white_pct + average_age\n                + implementation_quality + program_site + duration + service_hrs, \n                data = dropoutPrevention, studynum = studyID, var.eff.size = varLOR, \n                modelweights = \"HIER\")\nprint(m3_robu)\n\nRVE: Hierarchical Effects Model with Small-Sample Corrections \n\nModel: LOR1 ~ study_design + attrition + group_equivalence + adjusted + outcome + evaluator_independence + male_pct + white_pct + average_age + implementation_quality + program_site + duration + service_hrs \n\nNumber of clusters = 152 \nNumber of outcomes = 385 (min = 1 , mean = 2.53 , median = 1 , max = 30 )\nOmega.sq = 0.24907 \nTau.sq = 0.1024663 \n\n                                                Estimate   StdErr t-value  dfs    P(|t|&gt;) 95% CI.L 95% CI.U Sig\n1                                 X.Intercept.  0.016899 0.615399  0.0275 16.9 0.97841541 -1.28228  1.31608    \n2          study_designNon.random..non.matched -0.002626 0.185142 -0.0142 40.5 0.98875129 -0.37667  0.37141    \n3                       study_designRandomized -0.086872 0.140044 -0.6203 38.6 0.53869676 -0.37024  0.19650    \n4                                    attrition  0.118889 0.247228  0.4809 15.5 0.63732597 -0.40666  0.64444    \n5                            group_equivalence  0.502463 0.195838  2.5657 28.7 0.01579282  0.10174  0.90318  **\n6                        adjustedadjusted.data -0.322480 0.125413 -2.5713 33.8 0.01470796 -0.57741 -0.06755  **\n7                              outcomeenrolled  0.097059 0.139842  0.6941 16.5 0.49727848 -0.19862  0.39274    \n8                            outcomegraduation  0.147643 0.134938  1.0942 30.2 0.28253825 -0.12786  0.42315    \n9                        outcomegraduation.ged  0.258034 0.169134  1.5256 16.3 0.14632629 -0.10006  0.61613    \n10 evaluator_independenceIndirect..influential -0.765085 0.399109 -1.9170  6.2 0.10212896 -1.73406  0.20389    \n11              evaluator_independencePlanning -0.920874 0.346536 -2.6574  5.6 0.04027061 -1.78381 -0.05794  **\n12              evaluator_independenceDelivery -0.916673 0.304303 -3.0124  4.7 0.03212299 -1.71432 -0.11903  **\n13                                    male_pct  0.167965 0.181538  0.9252 16.4 0.36824526 -0.21609  0.55202    \n14                                   white_pct  0.022915 0.149394  0.1534 21.8 0.87950385 -0.28704  0.33287    \n15                                 average_age  0.037102 0.027053  1.3715 21.2 0.18458247 -0.01913  0.09333    \n16     implementation_qualityPossible.problems  0.411779 0.128898  3.1946 26.7 0.00358205  0.14714  0.67642 ***\n17  implementation_qualityNo.apparent.problems  0.658570 0.123874  5.3164 34.6 0.00000635  0.40699  0.91015 ***\n18                           program_sitemixed  0.444384 0.172635  2.5741 28.6 0.01550504  0.09109  0.79768  **\n19                program_siteschool.classroom  0.426658 0.159773  2.6704 37.4 0.01115192  0.10303  0.75028  **\n20    program_siteschool..outside.of.classroom  0.262517 0.160519  1.6354 30.1 0.11236814 -0.06525  0.59028    \n21                                    duration  0.000427 0.000873  0.4895 36.7 0.62736846 -0.00134  0.00220    \n22                                 service_hrs -0.003434 0.005012 -0.6852 36.7 0.49752503 -0.01359  0.00672    \n---\nSignif. codes: &lt; .01 *** &lt; .05 ** &lt; .10 *\n---\nNote: If df &lt; 4, do not trust the results\n\n\nNote that robumeta produces small-sample corrected standard errors and t-tests, and so there is no need to repeat those calculations with clubSandwich. The evaluator_independence variable has four levels, and it might be of interest to test whether the average program effects differ by the degree of evaluator independence. The null hypothesis in this case is that the 10th, 11th, and 12th regression coefficients are all equal to zero. A small-sample adjusted F-test for this hypothesis can be obtained as follows. (The vcov = \"CR2\" option means that the standard errors will be corrected using the bias-reduced linearization method proposed by McCaffrey, Bell, and Botts, 2001.)\n\nWald_test(m3_robu, constraints = constrain_zero(10:12), vcov = \"CR2\")\n\n test Fstat df_num df_denom  p_val sig\n  HTZ  2.78      3     16.8 0.0732   .\n\n\nBy default, the Wald_test function provides an F-type test with degrees of freedom estimated using the approximate Hotelling’s \\(T^2_Z\\) method. The test has less than 17 degrees of freedom, even though there are 152 independent studies in the data, and has a p-value of .07, so not-quite-significant at conventional levels. The low degrees of freedom are a consequence of the fact that one of the levels of evaluator independence has only a few effect sizes in it:\n\ntable(dropoutPrevention$evaluator_independence)\n\n\n          Independent Indirect, influential              Planning              Delivery \n                    6                    33                    43                   303 \n\n\n\n\nmetafor model\nOur package also works with models fit using the metafor package. Here I re-fit the same regression specification, but use REML to estimate the variance components (robumeta uses a method-of-moments estimator) and use a somewhat different weighting scheme than that used in robumeta.\n\nlibrary(metafor)\nm3_metafor &lt;- rma.mv(LOR1 ~ study_design + attrition + group_equivalence + adjusted\n                      + outcome + evaluator_independence\n                      + male_pct + white_pct + average_age\n                      + implementation_quality + program_site + duration + service_hrs, \n                      V = varLOR, random = list(~ 1 | studyID, ~ 1 | studySample),\n                     data = dropoutPrevention)\nsummary(m3_metafor)\n\n\nMultivariate Meta-Analysis Model (k = 385; method: REML)\n\n   logLik   Deviance        AIC        BIC       AICc   \n-489.0357   978.0714  1026.0714  1119.5371  1029.6217   \n\nVariance Components:\n\n            estim    sqrt  nlvls  fixed       factor \nsigma^2.1  0.2274  0.4769    152     no      studyID \nsigma^2.2  0.1145  0.3384    317     no  studySample \n\nTest for Residual Heterogeneity:\nQE(df = 363) = 1588.4397, p-val &lt; .0001\n\nTest of Moderators (coefficients 2:22):\nQM(df = 21) = 293.8694, p-val &lt; .0001\n\nModel Results:\n\n                                             estimate      se     zval    pval    ci.lb    ci.ub      \nintrcpt                                        0.5296  0.7250   0.7304  0.4651  -0.8915   1.9506      \nstudy_designNon-random, non-matched           -0.0494  0.1722  -0.2871  0.7741  -0.3870   0.2881      \nstudy_designRandomized                         0.0653  0.1628   0.4010  0.6884  -0.2538   0.3843      \nattrition                                     -0.1366  0.2429  -0.5623  0.5739  -0.6126   0.3395      \ngroup_equivalence                              0.4071  0.1573   2.5877  0.0097   0.0988   0.7155   ** \nadjustedadjusted data                         -0.3581  0.1532  -2.3371  0.0194  -0.6585  -0.0578    * \noutcomeenrolled                               -0.2831  0.0771  -3.6709  0.0002  -0.4343  -0.1320  *** \noutcomegraduation                             -0.0913  0.0657  -1.3896  0.1646  -0.2201   0.0375      \noutcomegraduation/ged                          0.6983  0.0805   8.6750  &lt;.0001   0.5406   0.8561  *** \nevaluator_independenceIndirect, influential   -0.7530  0.4949  -1.5214  0.1282  -1.7230   0.2171      \nevaluator_independencePlanning                -0.7700  0.4869  -1.5814  0.1138  -1.7242   0.1843      \nevaluator_independenceDelivery                -1.0016  0.4600  -2.1774  0.0294  -1.9033  -0.1000    * \nmale_pct                                       0.1021  0.1715   0.5951  0.5518  -0.2341   0.4382      \nwhite_pct                                      0.1223  0.1804   0.6777  0.4979  -0.2313   0.4758      \naverage_age                                    0.0061  0.0291   0.2091  0.8344  -0.0509   0.0631      \nimplementation_qualityPossible problems        0.4738  0.1609   2.9445  0.0032   0.1584   0.7892   ** \nimplementation_qualityNo apparent problems     0.6318  0.1471   4.2965  &lt;.0001   0.3436   0.9201  *** \nprogram_sitemixed                              0.3289  0.2413   1.3631  0.1729  -0.1440   0.8019      \nprogram_siteschool classroom                   0.2920  0.1736   1.6821  0.0926  -0.0482   0.6321    . \nprogram_siteschool, outside of classroom       0.1616  0.1898   0.8515  0.3945  -0.2104   0.5337      \nduration                                       0.0013  0.0009   1.3423  0.1795  -0.0006   0.0031      \nservice_hrs                                   -0.0003  0.0047  -0.0654  0.9478  -0.0096   0.0090      \n\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nmetafor produces model-based standard errors, t-tests, and confidence intervals. The coef_test function from clubSandwich will calculate robust standard errors and robust t-tests for each of the coefficients:\n\ncoef_test(m3_metafor, vcov = \"CR2\")\n\n                                       Coef.  Estimate       SE  t-stat d.f. (Satt) p-val (Satt) Sig.\n                                     intrcpt  0.529569 0.724851  0.7306       20.08      0.47347     \n         study_designNon-random, non-matched -0.049434 0.204152 -0.2421       58.42      0.80952     \n                      study_designRandomized  0.065272 0.149146  0.4376       53.17      0.66342     \n                                   attrition -0.136575 0.306429 -0.4457       10.52      0.66485     \n                           group_equivalence  0.407108 0.210917  1.9302       23.10      0.06595    .\n                       adjustedadjusted data -0.358124 0.136132 -2.6307       43.20      0.01176    *\n                             outcomeenrolled -0.283124 0.237199 -1.1936        7.08      0.27108     \n                           outcomegraduation -0.091295 0.091465 -0.9981        9.95      0.34188     \n                       outcomegraduation/ged  0.698328 0.364882  1.9138        8.02      0.09188    .\n evaluator_independenceIndirect, influential -0.752994 0.447670 -1.6820        6.56      0.13929     \n              evaluator_independencePlanning -0.769968 0.403898 -1.9063        6.10      0.10446     \n              evaluator_independenceDelivery -1.001648 0.355989 -2.8137        4.89      0.03834    *\n                                    male_pct  0.102055 0.148410  0.6877        9.68      0.50782     \n                                   white_pct  0.122255 0.141470  0.8642       16.88      0.39961     \n                                 average_age  0.006084 0.033387  0.1822       15.79      0.85772     \n     implementation_qualityPossible problems  0.473789 0.148660  3.1871       22.44      0.00419   **\n  implementation_qualityNo apparent problems  0.631842 0.138073  4.5761       28.68      &lt; 0.001  ***\n                           program_sitemixed  0.328941 0.196848  1.6710       27.47      0.10607     \n                program_siteschool classroom  0.291952 0.146014  1.9995       42.70      0.05195    .\n    program_siteschool, outside of classroom  0.161640 0.171700  0.9414       29.27      0.35420     \n                                    duration  0.001270 0.000978  1.2988       31.96      0.20332     \n                                 service_hrs -0.000309 0.004828 -0.0641       49.63      0.94915     \n\n\nNote that coef_test assumed that it should cluster based on studyID, which is the outer-most random effect in the metafor model. This can also be specified explicitly by including the option cluster = dropoutPrevention$studyID in the call.\nThe F-test for degree of evaluator independence uses the same syntax as before:\n\nWald_test(m3_metafor, constraints = constrain_zero(10:12), vcov = \"CR2\")\n\n test Fstat df_num df_denom  p_val sig\n  HTZ  2.71      3     18.3 0.0753   .\n\n\nDespite some differences in weighting schemes, the p-value is very close to the result obtained using robumeta."
  }
]