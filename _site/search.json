[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "James E. Pustejovsky",
    "section": "",
    "text": "BlueSky\n  \n  \n     \n  \n  \n    \n     \n  \n\n  \n  \nI am a statistician and associate professor in the School of Education at the University of Wisconsin-Madison, where I teach in the Educational Psychology Department and the graduate program in Quantitative Methods. My research involves developing statistical methods for problems in education, psychology, and other areas of social science research, with a focus on methods related to research synthesis and meta-analysis.\n\n\n\n\nPhD in Statistics | 2013  Northwestern University\nBA in Economics | 2003  Boston College\n\n\n\n\n\nMeta-analysis\nCausal inference\nRobust statistical methods\nEducation statistics\nSingle case experimental designs"
  },
  {
    "objectID": "index.html#education",
    "href": "index.html#education",
    "title": "James E. Pustejovsky",
    "section": "",
    "text": "PhD in Statistics | 2013  Northwestern University\nBA in Economics | 2003  Boston College"
  },
  {
    "objectID": "index.html#interests",
    "href": "index.html#interests",
    "title": "James E. Pustejovsky",
    "section": "",
    "text": "Meta-analysis\nCausal inference\nRobust statistical methods\nEducation statistics\nSingle case experimental designs"
  },
  {
    "objectID": "working-papers.html",
    "href": "working-papers.html",
    "title": "Working Papers",
    "section": "",
    "text": "Conducting power analysis for meta-analysis of dependent effect sizes: Common guidelines and an introduction to the POMADE R package\n      \n      \n      Mikkel H. Vembye, James E. Pustejovsky, Terri D. Pigott\n      \n      \n      Jul 21, 2023\n      \n      \n        \n          \n            \n              \n                \n                Supplementary materials\n              \n            \n          \n            \n              \n                \n                Pre-Print\n              \n            \n          \n            \n              \n                \n                Code\n              \n            \n          \n        \n      \n    \n  \n  \n\n  \n\n\nNo matching items\n\n\n\nMore publications…"
  },
  {
    "objectID": "contact.html",
    "href": "contact.html",
    "title": "Contact",
    "section": "",
    "text": "pustejovsky@wisc.edu\n 608-262-0842\n 1082C Education Sciences, 1025 West Johnson St., Madison, WI 53706-1706"
  },
  {
    "objectID": "posts/index.html",
    "href": "posts/index.html",
    "title": "Posts",
    "section": "",
    "text": "Distribution of the number of significant effect sizes\n\n\nIn a study reporting multiple outcomes\n\n\n\neffect size\n\n\ndistribution theory\n\n\n\n\n\n\n\n\n\nMar 28, 2024\n\n\nJames E. Pustejovsky\n\n\n\n\n\n\n\nApproximating the distribution of cluster-robust Wald statistics\n\n\n\n\n\n\nrobust variance estimation\n\n\ndistribution theory\n\n\n\n\n\n\n\n\n\nMar 24, 2024\n\n\nJames E. Pustejovsky\n\n\n\n\n\n\n\nImplementing Consul’s generalized Poisson distribution in Stan\n\n\n\n\n\n\nBayes\n\n\nsimulation\n\n\ndistribution-theory\n\n\ngeneralized linear model\n\n\nprogramming\n\n\nRstats\n\n\n\n\n\n\n\n\n\nDec 6, 2023\n\n\nJames E. Pustejovsky\n\n\n\n\n\n\n\nImplementing Efron’s double Poisson distribution in Stan\n\n\n\n\n\n\nBayes\n\n\nsimulation\n\n\ndistribution-theory\n\n\ngeneralized linear model\n\n\nprogramming\n\n\nRstats\n\n\n\n\n\n\n\n\n\nSep 15, 2023\n\n\nJames E. Pustejovsky\n\n\n\n\n\n\n\nCluster-Bootstrapping a meta-analytic selection model\n\n\n\n\n\n\nbootstrap\n\n\ndependent effect sizes\n\n\nmeta-analysis\n\n\npublication bias\n\n\nprogramming\n\n\nRstats\n\n\n\nIn this post, we will sketch out what we think is a promising and pragmatic method for examining selective reporting while also accounting for effect size dependency. The method is to use a cluster-level bootstrap, which involves re-sampling clusters of observations to approximate the sampling distribution of an estimator. To illustrate this technique, we will demonstrate how to bootstrap a Vevea-Hedges selection model.\n\n\n\n\n\nMar 30, 2023\n\n\nJames E. Pustejovsky, James E. Pustejovsky, Megha Joshi\n\n\n\n\n\n\n\nCohen’s \\(d_z\\) makes me dizzy when considering measurement error\n\n\n\n\n\n\neffect size\n\n\nstandardized mean difference\n\n\ndesign-comparable SMD\n\n\nmeasurement-error\n\n\n\nMeta-analyses in education, psychology, and related fields rely heavily of Cohen’s \\(d\\), or the standardized mean difference effect size, for quantitatively describing the magnitude and direction of intervention effects. In these fields, Cohen’s \\(d\\) is so pervasive that its use is nearly automatic, and analysts rarely question its utility or consider alternatives (response ratios, anyone? POMP?). Despite this state of affairs, working with Cohen’s \\(d\\) is theoretically challenging because the standardized mean difference metric does not have a singular definition. Rather, its definition depends on the choice of the standardizing variance used in the denominator.\n\n\n\n\n\nFeb 17, 2023\n\n\nJames E. Pustejovsky\n\n\n\n\n\n\n\nCorrigendum to Pustejovsky and Tipton (2018), redux\n\n\nA revised version of Theorem 2\n\n\n\nrobust variance estimation\n\n\neconometrics\n\n\nmatrix algebra\n\n\n\nIn my 2018 paper with Beth Tipton, published in the Journal of Business and Economic Statistics, we considered how to do cluster-robust variance estimation in fixed effects models estimated by weighted (or unweighted) least squares. We were recently alerted that Theorem 2 in the paper is incorrect as stated. It turns out, the conditions in the original version of the theorem are too general. A more limited version of the Theorem does actually hold, but only for models estimated using ordinary (unweighted) least squares, under a working model that assumes independent, homoskedastic errors. In this post, I’ll give the revised theorem, following the notation and setup of the previous post (so better read that first, or what follows won’t make much sense!).\n\n\n\n\n\nNov 7, 2022\n\n\nJames E. Pustejovsky\n\n\n\n\n\n\n\nCorrigendum to Pustejovsky and Tipton (2018)\n\n\nTheorem 2 is incorrect as stated\n\n\n\nrobust variance estimation\n\n\neconometrics\n\n\nmatrix algebra\n\n\n\nIn my 2018 paper with Beth Tipton, published in the Journal of Business and Economic Statistics, we considered how to do cluster-robust variance estimation in fixed effects models estimated by weighted (or unweighted) least squares. A careful reader recently alerted us to a problem with Theorem 2 in the paper, which concerns a computational short cut for a certain cluster-robust variance estimator in models with cluster-specific fixed effects. The theorem is incorrect as stated, and we are currently working on issuing a correction for the published version of the paper. In the interim, this post details the problem with Theorem 2. I’ll first review the CR2 variance estimator, then describe the assertion of the theorem, and then provide a numerical counter-example demonstrating that the assertion is not correct as stated.\n\n\n\n\n\nSep 28, 2022\n\n\nJames E. Pustejovsky\n\n\n\n\n\n\n\nVariance component estimates in meta-analysis with mis-specified sampling correlation\n\n\n\n\n\n\nmeta-analysis\n\n\ndependent effect sizes\n\n\ndistribution theory\n\n\nhierarchical models\n\n\n\n\n\n\n\n\n\nNov 28, 2021\n\n\nJames E. Pustejovsky\n\n\n\n\n\n\n\nImplications of mean-variance relationships for standardized mean differences\n\n\n\n\n\n\nstandardized mean difference\n\n\nresponse ratio\n\n\ndistribution theory\n\n\nmeta-analysis\n\n\n\nA question came up on the R-SIG-meta-analysis listserv about whether it was reasonable to use the standardized mean difference metric for synthesizing studies where the outcomes are measured as proportions. I think this is an interesting question because, while the SMD could work perfectly fine as an effect size metric for proportions, there are also other alternatives that could be considered, such as odds ratios or response ratios or raw differences in proportions. Further, there are some situations where the SMD has disadvantages for synthesizing contrasts between proportions. Thus, it’s a situation where one has to make a choice about the effect size metric, and where the most common metric (the SMD) might not be the right answer. In this post, I want to provide a bit more detail regarding why I think mean-variance relationships in raw data can signal that the standardized mean differences might be less useful as an effect size metric compared to alternatives.\n\n\n\n\n\nNov 2, 2021\n\n\nJames E. Pustejovsky\n\n\n\n\n\n\n\nInverting partitioned matrices\n\n\n\n\n\n\nmatrix algebra\n\n\n\nThere’s lots of linear algebra out there that’s quite useful for statistics, but that I never learned in school or never had cause to study in depth. In the same spirit as my previous post on the Woodbury identity, I thought I would share my notes on another helpful bit of math about matrices. At some point in high school or college, you might have learned how to invert a small matrix by hand. It turns out that there’s a straight-forward generalization of this formula to matrices of arbitrary size, but that are partitioned into four pieces.\n\n\n\n\n\nOct 20, 2021\n\n\nJames E. Pustejovsky\n\n\n\n\n\n\n\nStandardized mean differences in single-group, repeated measures designs\n\n\n\n\n\n\neffect size\n\n\nstandardized mean difference\n\n\ndistribution theory\n\n\n\n\n\n\n\n\n\nOct 6, 2021\n\n\nJames E. Pustejovsky\n\n\n\n\n\n\n\nFinding the distribution of significant effect sizes\n\n\nIn a study reporting multiple outcomes\n\n\n\neffect size\n\n\ndistribution theory\n\n\n\n\n\n\n\n\n\nApr 27, 2021\n\n\nJames E. Pustejovsky\n\n\n\n\n\n\n\nThe Woodbury identity\n\n\nA life-hack for analyzing hierarchical models\n\n\n\nhierarchical models\n\n\nmatrix algebra\n\n\n\n\n\n\n\n\n\nDec 4, 2020\n\n\nJames E. Pustejovsky\n\n\n\n\n\n\n\nAn ANCOVA puzzler\n\n\n\n\n\n\nmeta-analysis\n\n\neffect size\n\n\nstandardized mean difference\n\n\n\n\n\n\n\n\n\nNov 24, 2020\n\n\nJames E. Pustejovsky\n\n\n\n\n\n\n\nFrom Longhorn to Badger\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAug 28, 2020\n\n\nJames E. Pustejovsky\n\n\n\n\n\n\n\nWhat do meta-analysts mean by ‘multivariate’ meta-analysis?\n\n\n\n\n\n\nmeta-analysis\n\n\nmultivariate\n\n\ndependent effect sizes\n\n\n\n\n\n\n\n\n\nJun 27, 2020\n\n\nJames E. Pustejovsky\n\n\n\n\n\n\n\nWeighting in multivariate meta-analysis\n\n\n\n\n\n\nmeta-analysis\n\n\nweighting\n\n\n\nOne common question about multivariate/multi-level meta-analysis is how such models assign weight to individual effect size estimates. When a version of the question came up recently on the R-sig-meta-analysis listserv, Dr. Wolfgang Viechtbauer offered a whole blog post in reply, demonstrating how weights work in simpler fixed effect and random effects meta-analysis and then how things get more complicated in multivariate models. In this post, I’ll try to add some further intuition on how weights work in certain multivariate meta-analysis models. Most of the discussion will apply to models that include multiple level of random effects, but no predictors. I’ll also comment briefly on meta-regression models with only study-level predictor variables, and finally give some pointers to work on more complicated models.\n\n\n\n\n\nJun 9, 2020\n\n\nJames E. Pustejovsky\n\n\n\n\n\n\n\nAn update on code folding with blogdown + Academic theme\n\n\n\n\n\n\nprogramming\n\n\nRstats\n\n\n\n\n\n\n\n\n\nMay 3, 2020\n\n\nJames E. Pustejovsky\n\n\n\n\n\n\n\nSimulating correlated standardized mean differences for meta-analysis\n\n\n\n\n\n\neffect size\n\n\nstandardized mean difference\n\n\nmeta-analysis\n\n\nsimulation\n\n\nprogramming\n\n\ndistribution theory\n\n\n\n\n\n\n\n\n\nSep 30, 2019\n\n\nJames E. Pustejovsky\n\n\n\n\n\n\n\nSometimes, aggregating effect sizes is fine\n\n\n\n\n\n\neffect size\n\n\nmeta-analysis\n\n\ndependent effect sizes\n\n\n\n\n\n\n\n\n\nJul 2, 2019\n\n\nJames E. Pustejovsky\n\n\n\n\n\n\n\nCode folding with blogdown + Academic theme\n\n\n\n\n\n\nprogramming\n\n\nRstats\n\n\n\n\n\n\n\n\n\nApr 14, 2019\n\n\nJames E. Pustejovsky\n\n\n\n\n\n\n\nCRAN downloads of my packages\n\n\n\n\n\n\nprogramming\n\n\nRstats\n\n\nmeta-analysis\n\n\n\n\n\n\n\n\n\nApr 9, 2019\n\n\nJames E. Pustejovsky\n\n\n\n\n\n\n\nSystematic Reviews and Meta-analysis SIG at AERA 2019\n\n\n\n\n\n\nmeta-analysis\n\n\nAERA\n\n\n\n\n\n\n\n\n\nMar 26, 2019\n\n\nJames E. Pustejovsky\n\n\n\n\n\n\n\nA handmade clubSandwich for multi-site trials\n\n\n\n\n\n\nsandwiches\n\n\nrobust variance estimation\n\n\neconometrics\n\n\nweighting\n\n\n\n\n\n\n\n\n\nMar 9, 2019\n\n\nJames E. Pustejovsky\n\n\n\n\n\n\n\nEffective sample size aggregation\n\n\n\n\n\n\neconometrics\n\n\ncausal inference\n\n\nweighting\n\n\n\n\n\n\n\n\n\nJan 22, 2019\n\n\nJames E. Pustejovsky\n\n\n\n\n\n\n\nEasily simulate thousands of single-case designs\n\n\n\n\n\n\nbehavioral observation\n\n\nsimulation\n\n\nsingle-case design\n\n\n\n\n\n\n\n\n\nJun 21, 2018\n\n\nJames E. Pustejovsky\n\n\n\n\n\n\n\nNew paper: A gradual effects model for single-case designs\n\n\n\n\n\n\nsingle-case design\n\n\neffect size\n\n\ngeneralized linear model\n\n\n\n\n\n\n\n\n\nMay 14, 2018\n\n\nJames E. Pustejovsky\n\n\n\n\n\n\n\nclubSandwich at the Austin R User Group Meetup\n\n\n\n\n\n\nRstats\n\n\nrobust variance estimation\n\n\nsandwiches\n\n\n\n\n\n\n\n\n\nApr 26, 2018\n\n\nJames E. Pustejovsky\n\n\n\n\n\n\n\nSampling variance of Pearson r in a two-level design\n\n\n\n\n\n\neffect size\n\n\ncorrelation\n\n\nmeta-analysis\n\n\ndelta method\n\n\ndistribution theory\n\n\n\n\n\n\n\n\n\nApr 19, 2018\n\n\nJames E. Pustejovsky\n\n\n\n\n\n\n\nThe multivariate delta method\n\n\n\n\n\n\ndelta method\n\n\ndistribution theory\n\n\n\n\n\n\n\n\n\nApr 11, 2018\n\n\nJames E. Pustejovsky\n\n\n\n\n\n\n\nNew paper: Using response ratios for meta-analyzing SCDs with behavioral outcomes\n\n\n\n\n\n\nsingle-case design\n\n\neffect size\n\n\nresponse ratio\n\n\nmeta-analysis\n\n\n\n\n\n\n\n\n\nMar 16, 2018\n\n\nJames E. Pustejovsky\n\n\n\n\n\n\n\nNew paper: procedural sensitivities of effect size measures for SCDs\n\n\n\n\n\n\nsingle-case design\n\n\neffect size\n\n\nresponse ratio\n\n\nnon-overlap measures\n\n\nsimulation\n\n\n\n\n\n\n\n\n\nJan 11, 2018\n\n\nJames E. Pustejovsky\n\n\n\n\n\n\n\nBack from the IES PI meeting\n\n\n\n\n\n\nsingle-case design\n\n\nhypothesis testing\n\n\n\n\n\n\n\n\n\nJan 10, 2018\n\n\nJames E. Pustejovsky\n\n\n\n\n\n\n\n2SLS standard errors and the delta-method\n\n\n\n\n\n\ninstrumental variables\n\n\ncausal inference\n\n\ndelta method\n\n\ndistribution theory\n\n\n\n\n\n\n\n\n\nOct 7, 2017\n\n\nJames E. Pustejovsky\n\n\n\n\n\n\n\nPooling clubSandwich results across multiple imputations\n\n\n\n\n\n\nmissing data\n\n\nsandwiches\n\n\nsmall-sample\n\n\nRstats\n\n\n\n\n\n\n\n\n\nSep 27, 2017\n\n\nJames E. Pustejovsky\n\n\n\n\n\n\n\nImputing covariance matrices for meta-analysis of correlated effects\n\n\n\n\n\n\nmeta-analysis\n\n\nsandwiches\n\n\nrobust variance estimation\n\n\nRstats\n\n\n\n\n\n\n\n\n\nAug 10, 2017\n\n\nJames E. Pustejovsky\n\n\n\n\n\n\n\nThe siren song of significance\n\n\n\n\n\n\npre-registration\n\n\nhypothesis testing\n\n\n\n\n\n\n\n\n\nJun 19, 2017\n\n\nJames E. Pustejovsky\n\n\n\n\n\n\n\nYou wanna PEESE of d’s?\n\n\n\n\n\n\nmeta-analysis\n\n\npublication bias\n\n\nstandardized mean difference\n\n\nsimulation\n\n\n\n\n\n\n\n\n\nApr 27, 2017\n\n\nJames E. Pustejovsky\n\n\n\n\n\n\n\nNew working paper: Using log response ratios for meta-analyzing SCDs with behavioral outcomes\n\n\n\n\n\n\nsingle-case design\n\n\neffect size\n\n\nresponse ratio\n\n\nmeta-analysis\n\n\n\n\n\n\n\n\n\nApr 26, 2017\n\n\nJames E. Pustejovsky\n\n\n\n\n\n\n\nPresentation at IES 2016 PI meeting\n\n\n\n\n\n\nsingle-case design\n\n\neffect size\n\n\n\n\n\n\n\n\n\nDec 19, 2016\n\n\nJames E. Pustejovsky\n\n\n\n\n\n\n\nNew tutorial paper on BC-SMD effect sizes\n\n\n\n\n\n\nsingle-case design\n\n\neffect size\n\n\ndesign-comparable SMD\n\n\n\n\n\n\n\n\n\nDec 19, 2016\n\n\nJames E. Pustejovsky\n\n\n\n\n\n\n\nBug in nlme::lme with fixed sigma and REML estimation\n\n\n\n\n\n\nRstats\n\n\nprogramming\n\n\nhierarchical models\n\n\nnlme\n\n\n\n\n\n\n\n\n\nNov 7, 2016\n\n\nJames E. Pustejovsky\n\n\n\n\n\n\n\nWhat is Tau-U?\n\n\n\n\n\n\neffect size\n\n\nsingle-case design\n\n\nnon-overlap measures\n\n\n\n\n\n\n\n\n\nNov 3, 2016\n\n\nJames E. Pustejovsky\n\n\n\n\n\n\n\nNew working paper: Procedural sensitivities of SCD effect sizes\n\n\n\n\n\n\neffect size\n\n\nsingle-case design\n\n\nresponse ratio\n\n\nnon-overlap measures\n\n\nsimulation\n\n\n\n\n\n\n\n\n\nOct 17, 2016\n\n\nJames E. Pustejovsky\n\n\n\n\n\n\n\nSimulation studies in R (Fall, 2016 version)\n\n\n\n\n\n\nRstats\n\n\nsimulation\n\n\nprogramming\n\n\n\n\n\n\n\n\n\nSep 28, 2016\n\n\nJames E. Pustejovsky\n\n\n\n\n\n\n\nBug in nlme::getVarCov\n\n\n\n\n\n\nRstats\n\n\nprogramming\n\n\nhierarchical models\n\n\nnlme\n\n\n\n\n\n\n\n\n\nAug 10, 2016\n\n\nJames E. Pustejovsky\n\n\n\n\n\n\n\nAlternative formulas for the standardized mean difference\n\n\n\n\n\n\nmeta-analysis\n\n\neffect size\n\n\ndistribution theory\n\n\nstandardized mean difference\n\n\n\n\n\n\n\n\n\nJun 3, 2016\n\n\nJames E. Pustejovsky\n\n\n\n\n\n\n\nAssigning after dplyr\n\n\n\n\n\n\nRstats\n\n\nprogramming\n\n\n\n\n\n\n\n\n\nMay 13, 2016\n\n\nJames E. Pustejovsky\n\n\n\n\n\n\n\nUnlucky randomization\n\n\n\n\n\n\nexperimental design\n\n\n\n\n\n\n\n\n\nMay 11, 2016\n\n\nJames E. Pustejovsky\n\n\n\n\n\n\n\nThe sampling distribution of sample variances\n\n\n\n\n\n\ndistribution theory\n\n\n\n\n\n\n\n\n\nApr 25, 2016\n\n\nJames E. Pustejovsky\n\n\n\n\n\n\n\nTau-U\n\n\n\n\n\n\neffect size\n\n\nsingle-case design\n\n\nnon-overlap measures\n\n\n\n\n\n\n\n\n\nMar 23, 2016\n\n\nJames E. Pustejovsky\n\n\n\n\n\n\n\nStandard errors and confidence intervals for NAP\n\n\n\n\n\n\neffect size\n\n\nsingle-case design\n\n\nnon-overlap measures\n\n\ndistribution theory\n\n\n\n\n\n\n\n\n\nFeb 28, 2016\n\n\nJames E. Pustejovsky\n\n\n\n\n\n\n\nEstimating average effects in regression discontinuities with covariate interactions\n\n\n\n\n\n\neconometrics\n\n\nRstats\n\n\ncausal inference\n\n\nregression discontinuity\n\n\n\n\n\n\n\n\n\nJan 27, 2016\n\n\nJames E. Pustejovsky\n\n\n\n\n\n\n\nRegression discontinuities with covariate interactions in the rdd package\n\n\n\n\n\n\neconometrics\n\n\nRstats\n\n\ncausal inference\n\n\nregression discontinuity\n\n\n\n\n\n\n\n\n\nJan 25, 2016\n\n\nJames E. Pustejovsky\n\n\n\n\n\n\n\nClustered standard errors and hypothesis tests in fixed effects models\n\n\n\n\n\n\neconometrics\n\n\nfixed effects\n\n\nsandwiches\n\n\nRstats\n\n\n\n\n\n\n\n\n\nJan 10, 2016\n\n\nJames E. Pustejovsky\n\n\n\n\n\n\n\nSpecial Education Pro-Sem\n\n\n\n\n\n\nmeta-analysis\n\n\nsingle-case design\n\n\neffect size\n\n\n\n\n\n\n\n\n\nNov 24, 2015\n\n\nJames E. Pustejovsky\n\n\n\n\n\n\n\nCorrelations between standardized mean differences\n\n\n\n\n\n\nmeta-analysis\n\n\neffect size\n\n\nstandardized mean difference\n\n\ndistribution theory\n\n\n\n\n\n\n\n\n\nSep 17, 2015\n\n\nJames E. Pustejovsky\n\n\n\n\n\n\n\nFatal crashes in Austin/Travis County\n\n\n\n\n\n\ntransportation\n\n\n\n\n\n\n\n\n\nAug 20, 2015\n\n\nJames E. Pustejovsky\n\n\n\n\n\n\n\nThe clubSandwich package for meta-analysis with RVE\n\n\n\n\n\n\nmeta-analysis\n\n\nrobust variance estimation\n\n\nsandwiches\n\n\nRstats\n\n\n\n\n\n\n\n\n\nJul 10, 2015\n\n\nJames E. Pustejovsky\n\n\n\n\n\n\n\nNew article: Four methods for analyzing PIR data\n\n\n\n\n\n\nbehavioral observation\n\n\nsingle-case design\n\n\n\n\n\n\n\n\n\nFeb 11, 2015\n\n\nJames E. Pustejovsky\n\n\n\n\n\n\n\nGetting started with scdhlm\n\n\n\n\n\n\nsingle-case design\n\n\ndesign-comparable SMD\n\n\nRstats\n\n\n\n\n\n\n\n\n\nOct 19, 2014\n\n\nJames E. Pustejovsky\n\n\n\n\n\n\n\nWanted: PIR data\n\n\n\n\n\n\nbehavioral observation\n\n\n\n\n\n\n\n\n\nSep 3, 2014\n\n\nJames E. Pustejovsky\n\n\n\n\n\n\n\nNew article: Design-comparable effect sizes in multiple baseline designs: A general modeling framework\n\n\n\n\n\n\nsingle-case design\n\n\neffect size\n\n\nhierarchical models\n\n\ndesign-comparable SMD\n\n\n\n\n\n\n\n\n\nJul 20, 2014\n\n\nJames E. Pustejovsky\n\n\n\n\n\n\n\nARPobservation now on CRAN\n\n\n\n\n\n\nbehavioral observation\n\n\nalternating renewal process\n\n\nRstats\n\n\n\n\n\n\n\n\n\nMay 31, 2014\n\n\nJames E. Pustejovsky\n\n\n\n\n\n\n\nMeta-sandwich with extra mustard\n\n\n\n\n\n\nmeta-analysis\n\n\nsandwiches\n\n\nRstats\n\n\nrobust variance estimation\n\n\n\n\n\n\n\n\n\nApr 26, 2014\n\n\nJames E. Pustejovsky\n\n\n\n\n\n\n\nAnother meta-sandwich\n\n\n\n\n\n\nmeta-analysis\n\n\nsandwiches\n\n\nRstats\n\n\nrobust variance estimation\n\n\n\n\n\n\n\n\n\nApr 23, 2014\n\n\nJames E. Pustejovsky\n\n\n\n\n\n\n\nA meta-sandwich\n\n\n\n\n\n\nmeta-analysis\n\n\nsandwiches\n\n\nRstats\n\n\nrobust variance estimation\n\n\n\n\n\n\n\n\n\nApr 21, 2014\n\n\nJames E. Pustejovsky\n\n\n\n\n\n\n\nSpecial Education Pro-Sem\n\n\n\n\n\n\nmeta-analysis\n\n\nsingle-case design\n\n\neffect size\n\n\n\n\n\n\n\n\n\nApr 10, 2014\n\n\nJames E. Pustejovsky\n\n\n\n\n\n\n\nUpdate: parallel R on the TACC\n\n\n\n\n\n\nRstats\n\n\nprogramming\n\n\nsimulation\n\n\nTACC\n\n\n\n\n\n\n\n\n\nApr 8, 2014\n\n\nJames E. Pustejovsky\n\n\n\n\n\n\n\nNew article: Measurement-comparable effect sizes for single-case studies of free-operant behavior\n\n\n\n\n\n\nbehavioral observation\n\n\nsingle-case design\n\n\neffect size\n\n\nresponse ratio\n\n\n\n\n\n\n\n\n\nFeb 4, 2014\n\n\nJames E. Pustejovsky\n\n\n\n\n\n\n\nRunning R in parallel on the TACC\n\n\n\n\n\n\nRstats\n\n\nprogramming\n\n\nsimulation\n\n\nTACC\n\n\n\n\n\n\n\n\n\nDec 20, 2013\n\n\nJames E. Pustejovsky\n\n\n\n\n\n\n\nDesigning simulation studies using R\n\n\n\n\n\n\nRstats\n\n\nsimulation\n\n\n\n\n\n\n\n\n\nDec 6, 2013\n\n\nJames E. Pustejovsky\n\n\n\n\n\n\n\nTo what extent does partial interval recording over-estimate prevalence?\n\n\n\n\n\n\nbehavioral observation\n\n\nsimulation\n\n\nalternating renewal process\n\n\n\n\n\n\n\n\n\nOct 26, 2013\n\n\nJames E. Pustejovsky\n\n\n\n\n\n\n\nARPobservation: Basic use\n\n\n\n\n\n\nbehavioral observation\n\n\nalternating renewal process\n\n\nRstats\n\n\n\n\n\n\n\n\n\nOct 25, 2013\n\n\nJames E. Pustejovsky\n\n\n\n\n\n\n\nGetting started with ARPobservation\n\n\n\n\n\n\nbehavioral observation\n\n\nsimulation\n\n\nRstats\n\n\n\n\n\n\n\n\n\nOct 24, 2013\n\n\nJames E. Pustejovsky\n\n\n\n\n\n\n\nReliability of UnGraphed single-case data: An example using the Shogren dataset\n\n\n\n\n\n\nsingle-case design\n\n\ninter-rater reliability\n\n\n\n\n\n\n\n\n\nOct 23, 2013\n\n\nJames E. Pustejovsky\n\n\n\n\n\n\n\nAnother project idea: Meta-analytic methods for correlational data\n\n\n\n\n\n\nmeta-analysis\n\n\nrobust variance estimation\n\n\ncorrelation\n\n\n\n\n\n\n\n\n\nSep 13, 2013\n\n\nJames E. Pustejovsky\n\n\n\n\n\n\n\nCurrent projects\n\n\n\n\n\n\nbehavioral observation\n\n\nsingle-case design\n\n\n\n\n\n\n\n\n\nAug 20, 2013\n\n\nJames E. Pustejovsky\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "presentations/index.html",
    "href": "presentations/index.html",
    "title": "Presentations",
    "section": "",
    "text": "May 17, 2024\n    \n    \n      Model-Building Considerations in Meta-Analysis of Dependent Effect Sizes\n      VIVE | Copenhagen, Denmark\n      \n        \n          \n            \n              \n                \n                Slides\n              \n            \n          \n        \n      \n    \n  \n  \n  \n              \n      Apr 12, 2024\n    \n    \n      Bayesian estimation of between-case standardized mean differences: A simulation study\n      AERA 2024 | Philadelphia, PA\n      \n        \n          \n            \n              \n                \n                PDF\n              \n            \n          \n        \n      \n    \n  \n  \n  \n              \n      Sep 29, 2023\n    \n    \n      Discussion of Stabilizing measures to reconcile accuracy and equity in performance measurement\n      SREE 2023 | Arlington, VA\n      \n        \n          \n            \n              \n                \n                Slides\n              \n            \n          \n        \n      \n    \n  \n  \n  \n              \n      Sep 27, 2023\n    \n    \n      Equity-related moderator analysis in syntheses of dependent effect sizes: Conceptual and statistical considerations\n      SREE 2023 | Arlington, VA\n      \n        \n          \n            \n              \n                \n                Slides\n              \n            \n          \n        \n      \n    \n  \n  \n  \n              \n      May 18, 2023\n    \n    \n      Determining the Timing of Phase Changes: Some Statistical Perspective\n      WISCC2023 | Nashville, TN\n      \n        \n          \n            \n              \n                \n                Slides\n              \n            \n          \n        \n      \n    \n  \n  \n  \n              \n      Apr 26, 2023\n    \n    \n      Calculating Effect Sizes for Single-Case Research: An Introduction to the SingleCaseES and scdhlm Web Applications and R Packages\n      SmallisBeautiful2023 | online\n      \n        \n          \n            \n              \n                \n                Slides\n              \n            \n          \n        \n      \n    \n  \n  \n  \n              \n      Apr 25, 2023\n    \n    \n      Effect size measures for single-case research: Conceptual, practical, and statistical considerations\n      SmallisBeautiful2023 | online\n      \n        \n          \n            \n              \n                \n                Slides\n              \n            \n          \n        \n      \n    \n  \n  \n  \n              \n      Apr 16, 2023\n    \n    \n      Empirical benchmarks for between-case standardized mean differences from single-case multiple baseline designs examining academic interventions.\n      AERA 2023 | Chicago, IL\n      \n        \n          \n            \n              \n                \n                Slides\n              \n            \n          \n        \n      \n    \n  \n  \n  \n              \n      Apr 15, 2023\n    \n    \n      Discussion of ‘Moving from What Works to What Replicates: Promoting the Systematic Replication of Results.’\n      AERA 2023 | Chicago, IL\n      \n        \n          \n            \n              \n                \n                Slides\n              \n            \n          \n        \n      \n    \n  \n  \n  \n              \n      Mar 30, 2023\n    \n    \n      Clustered bootstrapping for selective reporting models in meta-analysis with dependent effects\n      ESMARConf2023 | online\n      \n        \n          \n            \n              \n                \n                Slides\n              \n            \n          \n            \n              \n                \n                Video\n              \n            \n          \n            \n              \n                \n                Code\n              \n            \n          \n        \n      \n    \n  \n  \n  \n              \n      Jul 20, 2022\n    \n    \n      A matter of emphasis: Comparison of working models for meta-analysis of dependent effect sizes\n      SRSM 2022 | Portland, OR (and online)\n      \n        \n          \n            \n              \n                \n                Slides\n              \n            \n          \n        \n      \n    \n  \n  \n  \n              \n      May 19, 2022\n    \n    \n      The state of single case synthesis: Premises, tools, and possibilities\n      SSCC | Vanderbilt University, Nashville, TN\n      \n        \n          \n            \n              \n                \n                Slides\n              \n            \n          \n        \n      \n    \n  \n  \n  \n              \n      Apr 21, 2022\n    \n    \n      Synthesis of non-overlap of all pairs using logistic transformation or binomial generalized linear mixed model\n      AERA 2022 | San Diego, CA\n      \n        \n          \n            \n              \n                \n                Slides\n              \n            \n          \n            \n              \n                \n                PDF\n              \n            \n          \n        \n      \n    \n  \n  \n  \n              \n      Feb 8, 2022\n    \n    \n      Selective reporting in meta-analysis of dependent effect size estimates\n      Stanford QSU | online\n      \n        \n          \n            \n              \n                \n                Slides\n              \n            \n          \n        \n      \n    \n  \n  \n  \n              \n      Feb 3, 2022\n    \n    \n      Easy, cluster-robust standard errors with the clubSandwich package\n      OsloRUG | online\n      \n        \n          \n            \n              \n                \n                Slides\n              \n            \n          \n            \n              \n                \n                Video\n              \n            \n          \n        \n      \n    \n  \n  \n  \n              \n      Sep 27, 2021\n    \n    \n      Four things every quantitative social scientist should know about meta-analysis\n      EdPsych PIE | Educational Sciences 259\n      \n        \n          \n            \n              \n                \n                Slides\n              \n            \n          \n        \n      \n    \n  \n  \n  \n              \n      Sep 2, 2021\n    \n    \n      Synthesis of dependent effect sizes: Robust variance estimation with clubSandwich\n      OsloRUG | online\n      \n        \n          \n            \n              \n                \n                PDF\n              \n            \n          \n            \n              \n                \n                Slides\n              \n            \n          \n            \n              \n                \n                Video\n              \n            \n          \n        \n      \n    \n  \n  \n  \n              \n      Jun 23, 2021\n    \n    \n      Statistical frontiers for selective reporting and publication bias\n      SIPS 2021 | online\n      \n        \n          \n            \n              \n                \n                Slides\n              \n            \n          \n            \n              \n                \n                Code\n              \n            \n          \n        \n      \n    \n  \n  \n  \n              \n      Jan 21, 2021\n    \n    \n      Synthesis of dependent effect sizes: Versatile models through metafor and clubSandwich\n      ESMARConf2021 | online\n      \n        \n          \n            \n              \n                \n                PDF\n              \n            \n          \n            \n              \n                \n                Slides\n              \n            \n          \n            \n              \n                \n                Video\n              \n            \n          \n        \n      \n    \n  \n  \n  \n              \n      Jul 22, 2019\n    \n    \n      A generalized excess significance test for selective outcome reporting with dependent effect sizes\n      SRSM 2019 | Chicago, IL\n      \n        \n          \n            \n              \n                \n                Slides\n              \n            \n          \n        \n      \n    \n  \n  \n  \n              \n      May 26, 2019\n    \n    \n      Log response ratio effect sizes: Rationale and methods for single case designs with behavioral outcomes\n      ABAI 2019 | Chicago, IL\n      \n        \n          \n            \n              \n                \n                Slides\n              \n            \n          \n        \n      \n    \n  \n  \n  \n              \n      Apr 8, 2019\n    \n    \n      Evaluating meta-analytic methods to detect outcome reporting bias in the presence of dependent effect sizes\n      AERA 2019 | Toronto, Ontario\n      \n        \n          \n            \n              \n                \n                PDF\n              \n            \n          \n        \n      \n    \n  \n  \n  \n              \n      Apr 7, 2019\n    \n    \n      An examination of measurement procedures and baseline behavioral outcomes in single-case research\n      AERA 2019 | Toronto, Ontario\n      \n        \n          \n            \n              \n                \n                PDF\n              \n            \n          \n        \n      \n    \n  \n  \n  \n              \n      Apr 7, 2019\n    \n    \n      The impact of response-guided designs on count outcomes in single-case design baselines\n      AERA 2019 | Toronto, Ontario\n      \n        \n          \n            \n              \n                \n                PDF\n              \n            \n          \n        \n      \n    \n  \n  \n  \n              \n      Mar 8, 2019\n    \n    \n      Small-sample cluster-robust variance estimators for two-stage least squares models\n      SREE 2019 | Washington, DC\n      \n        \n          \n            \n              \n                \n                Slides\n              \n            \n          \n        \n      \n    \n  \n  \n  \n              \n      Oct 1, 2018\n    \n    \n      Combining robust variance estimation with models for dependent effect sizes\n      UT Psych | Austin, TX\n      \n        \n          \n            \n              \n                \n                Slides\n              \n            \n          \n            \n              \n                \n                Code\n              \n            \n          \n        \n      \n    \n  \n  \n  \n              \n      Jul 18, 2018\n    \n    \n      Combining robust variance estimation with models for dependent effect sizes\n      SRSM 2018 | Bristol, UK\n      \n        \n          \n            \n              \n                \n                Slides\n              \n            \n          \n        \n      \n    \n  \n  \n  \n              \n      Apr 15, 2018\n    \n    \n      Meta-analysis of dependent effects: A review and consolidation of methods\n      AERA 2018 | New York, NY\n      \n        \n          \n            \n              \n                \n                Slides\n              \n            \n          \n        \n      \n    \n  \n  \n  \n              \n      Apr 15, 2018\n    \n    \n      Meta-analysis of single-case research: A brief and breezy tour\n      AERA 2018 | New York, NY\n      \n        \n          \n            \n              \n                \n                Slides\n              \n            \n          \n        \n      \n    \n  \n  \n  \n              \n      Jan 10, 2018\n    \n    \n      A gradual effects model for single case designs\n      IES 2018 PI Meeting | Washington, DC\n      \n        \n          \n            \n              \n                \n                PDF\n              \n            \n          \n        \n      \n    \n  \n  \n  \n              \n      Jan 10, 2018\n    \n    \n      Randomization inference for single-case experimental designs\n      IES 2018 PI Meeting | Washington, DC\n      \n        \n          \n            \n              \n                \n                Slides\n              \n            \n          \n        \n      \n    \n  \n  \n  \n              \n      Apr 30, 2017\n    \n    \n      Heteroskedasticity-robust tests in linear regression: A review and evaluation of small-sample corrections\n      AERA 2017 | San Antonio, TX\n      \n        \n          \n            \n              \n                \n                Slides\n              \n            \n          \n        \n      \n    \n  \n  \n  \n              \n      Apr 28, 2017\n    \n    \n      A nonlinear intervention analysis model for treatment reversal single-case designs\n      AERA 2017 | San Antonio, TX\n      \n        \n          \n            \n              \n                \n                PDF\n              \n            \n          \n        \n      \n    \n  \n  \n  \n              \n      Apr 28, 2017\n    \n    \n      Using response ratios for meta-analyzing single-case designs with behavioral outcomes\n      AERA 2017 | San Antonio, TX\n      \n        \n          \n            \n              \n                \n                PDF\n              \n            \n          \n        \n      \n    \n  \n  \n  \n              \n      Mar 2, 2017\n    \n    \n      Small sample corrections for use of cluster-robust standard errors in the analysis of school-based experiments\n      SREE 2017 | Washington, DC\n      \n        \n          \n            \n              \n                \n                Slides\n              \n            \n          \n        \n      \n    \n  \n  \n  \n              \n      Dec 19, 2016\n    \n    \n      Effect sizes for single-case research\n      IES 2016 | Washington, DC\n      \n        \n          \n            \n              \n                \n                Slides\n              \n            \n          \n        \n      \n    \n  \n  \n  \n              \n      Jul 31, 2016\n    \n    \n      Small-sample methods for cluster-robust variance estimation and hypothesis testing in fixed effects models\n      JSM 2016 | Chicago, IL\n      \n        \n          \n            \n              \n                \n                Slides\n              \n            \n          \n        \n      \n    \n  \n  \n  \n              \n      Jul 18, 2016\n    \n    \n      When large samples act small: The importance of small-sample adjustments for cluster-robust inference in impact evaluations\n      AIR working group lecture series | Austin, TX\n      \n        \n          \n            \n              \n                \n                Slides\n              \n            \n          \n        \n      \n    \n  \n  \n  \n              \n      Feb 19, 2016\n    \n    \n      When large samples act small: Cluster-robust variance estimation and hypothesis testing with few clusters\n      PRC colloquium | Austin, TX\n      \n        \n          \n            \n              \n                \n                Slides\n              \n            \n          \n        \n      \n    \n  \n  \n  \n              \n      Jul 8, 2015\n    \n    \n      Small-sample adjustments for multiple-contrast hypothesis tests of meta-regressions using robust variance estimation\n      SRSM 2015 | Nashville, TN\n      \n        \n          \n            \n              \n                \n                PDF\n              \n            \n          \n            \n              \n                \n                Slides\n              \n            \n          \n        \n      \n    \n  \n  \n  \n              \n      Apr 19, 2015\n    \n    \n      Operational sensitivities of non-overlap effect sizes for single-case experimental designs\n      AERA 2015 | Chicago, IL\n      \n        \n          \n            \n              \n                \n                PDF\n              \n            \n          \n        \n      \n    \n  \n  \n  \n              \n      Apr 18, 2015\n    \n    \n      Small-sample adjustments for F-tests using robust variance estimation in meta-regression\n      AERA 2015 | Chicago, IL\n      \n        \n          \n            \n              \n                \n                PDF\n              \n            \n          \n            \n              \n                \n                Slides\n              \n            \n          \n        \n      \n    \n  \n  \n  \n              \n      Apr 16, 2015\n    \n    \n      Observation procedures and Markov Chain models for estimating the prevalence and incidence of a state behavior\n      AERA 2015 | Chicago, IL\n      \n        \n          \n            \n              \n                \n                PDF\n              \n            \n          \n            \n              \n                \n                Slides\n              \n            \n          \n        \n      \n    \n  \n  \n  \n              \n      Mar 6, 2015\n    \n    \n      Small-sample adjustments for tests of moderators and model fit using robust variance estimation in meta-regression\n      SREE 2015 | Washington, DC\n      \n        \n          \n            \n              \n                \n                Slides\n              \n            \n          \n        \n      \n    \n  \n  \n  \n              \n      Apr 4, 2014\n    \n    \n      Four methods of analyzing partial interval recording data, with application to single-case research\n      AERA 2014 | Philadelphia, PA\n      \n        \n          \n            \n              \n                \n                PDF\n              \n            \n          \n        \n      \n    \n  \n  \n  \n              \n      Mar 21, 2014\n    \n    \n      Addressing construct invalidity in partial interval recording data\n      TUESAP 2014 | Texas A&M University, College Station, TX\n      \n        \n          \n            \n              \n                \n                Slides\n              \n            \n          \n        \n      \n    \n  \n  \n  \n              \n      Mar 6, 2014\n    \n    \n      On internal validity in multiple baseline designs\n      SREE 2014 | Washington, DC\n      \n        \n          \n            \n              \n                \n                Slides\n              \n            \n          \n        \n      \n    \n  \n  \n  \n              \n      May 29, 2013\n    \n    \n      Some Markov models for direct observation of behavior\n      NU Statistics | Evanston, IL\n      \n        \n          \n            \n              \n                \n                Slides\n              \n            \n          \n        \n      \n    \n  \n  \n  \n              \n      May 28, 2013\n    \n    \n      Effect sizes and measurement comparability for meta-analysis of single-case research\n      ABAI 2013 | Minneapolis, MN\n      \n        \n          \n            \n              \n                \n                Slides\n              \n            \n          \n        \n      \n    \n  \n  \n  \n              \n      Apr 30, 2013\n    \n    \n      Observation procedures and Markov chain models for estimating the prevalence and incidence of a behavior\n      AERA 2013 | San Francisco, CA\n      \n        \n          \n            \n              \n                \n                PDF\n              \n            \n          \n        \n      \n    \n  \n  \n  \n              \n      Mar 7, 2013\n    \n    \n      Operationally comparable effect sizes for meta-analysis of single-case research\n      SREE 2013 | Washington, DC\n      \n        \n          \n            \n              \n                \n                Slides\n              \n            \n          \n        \n      \n    \n  \n  \n  \n              \n      Nov 14, 2012\n    \n    \n      Some implications of behavioral observation procedures for meta-analysis of single-case research\n      UT Austin | Austin, TX\n      \n        \n          \n            \n              \n                \n                Slides\n              \n            \n          \n        \n      \n    \n  \n  \n  \n              \n      Jan 23, 2008\n    \n    \n      Question-order effects in social network name generators\n      ISSNA 2008 | St. Petersburg Beach, FL\n      \n        \n          \n            \n              \n                \n                PDF\n              \n            \n          \n        \n      \n    \n  \n  \n  \n\n\nNo matching items"
  },
  {
    "objectID": "software/index.html",
    "href": "software/index.html",
    "title": "Software",
    "section": "",
    "text": "ARPobservation\n      \n      \n      \n          Simulate systematic direct observation data \n      \n      \n      \n      James E. Pustejovsky\n      \n      \n        \n          \n            \n              \n                \n                CRAN\n              \n            \n          \n            \n              \n                \n                \n              \n            \n          \n        \n      \n    \n  \n  \n  \n  \n    \n      \n        \n          \n        \n      \n    \n      \n          clubSandwich\n      \n      \n      \n          Cluster-robust variance estimation \n      \n      \n      \n      James E. Pustejovsky\n      \n      \n        \n          \n            \n              \n                \n                CRAN\n              \n            \n          \n            \n              \n                \n                Website\n              \n            \n          \n            \n              \n                \n                \n              \n            \n          \n        \n      \n    \n  \n  \n  \n  \n    \n      \n        \n          \n        \n      \n    \n      \n          lmeInfo\n      \n      \n      \n          Information Matrices for 'lmeStruct' and 'glsStruct' Objects \n      \n      \n      \n      James E. Pustejovsky, Man Chen\n      \n      \n        \n          \n            \n              \n                \n                CRAN\n              \n            \n          \n            \n              \n                \n                Website\n              \n            \n          \n            \n              \n                \n                \n              \n            \n          \n        \n      \n    \n  \n  \n  \n  \n    \n      \n        \n          \n        \n      \n    \n      \n          POMADE\n      \n      \n      \n          Power for Meta-Analysis of Dependent Effects \n      \n      \n      \n      Mikkel H. Vembye, James E. Pustejovsky\n      \n      \n        \n          \n            \n              \n                \n                CRAN\n              \n            \n          \n            \n              \n                \n                Website\n              \n            \n          \n            \n              \n                \n                \n              \n            \n          \n        \n      \n    \n  \n  \n  \n  \n    \n      \n        \n          \n        \n      \n    \n      \n          scdhlm\n      \n      \n      \n          Between-case SMD for single-case designs \n      \n      \n      \n      James E. Pustejovsky, Man Chen, Bethany H. Bhat\n      \n      \n        \n          \n            \n              \n                \n                CRAN\n              \n            \n          \n            \n              \n                \n                Website\n              \n            \n          \n            \n              \n                \n                \n              \n            \n          \n        \n      \n    \n  \n  \n  \n  \n    \n      \n        \n          \n        \n      \n    \n      \n          simhelpers\n      \n      \n      \n          Helper package to assist in running simulation studies \n      \n      \n      \n      Megha Joshi, James E. Pustejovsky\n      \n      \n        \n          \n            \n              \n                \n                CRAN\n              \n            \n          \n            \n              \n                \n                Website\n              \n            \n          \n            \n              \n                \n                \n              \n            \n          \n        \n      \n    \n  \n  \n  \n  \n    \n      \n        \n          \n        \n      \n    \n      \n          SingleCaseES\n      \n      \n      \n          Single-case design effect size calculator \n      \n      \n      \n      James E. Pustejovsky, Man Chen, Daniel M. Swan, Paulina Grekov\n      \n      \n        \n          \n            \n              \n                \n                CRAN\n              \n            \n          \n            \n              \n                \n                Website\n              \n            \n          \n            \n              \n                \n                \n              \n            \n          \n        \n      \n    \n  \n  \n  \n  \n    \n      \n        \n          \n        \n      \n    \n      \n          wildmeta\n      \n      \n      \n          Cluster-wild bootstrap for meta-regression \n      \n      \n      \n      Megha Joshi, James E. Pustejovsky, Pierce Cappelli\n      \n      \n        \n          \n            \n              \n                \n                CRAN\n              \n            \n          \n            \n              \n                \n                Website\n              \n            \n          \n            \n              \n                \n                \n              \n            \n          \n        \n      \n    \n  \n  \n  \n\n\nNo matching items"
  },
  {
    "objectID": "teaching/index.html",
    "href": "teaching/index.html",
    "title": "Teaching",
    "section": "",
    "text": "At UW Madison\n\n\n\n\n\nCourse Title\n\n\n\n\n\n\nDesign & Analysis of Quasi-Experiments for Causal Inference\n\n\n\n\nField Experiments in Education Research\n\n\n\n\nMeta-analysis\n\n\n\n\n\nNo matching items\n\n\n\n\nAt UT Austin\n\n\n\n\n\nCourse Title\n\n\n\n\n\n\nCausal Inference\n\n\n\n\nData Analysis, Simulation, and Programming in R\n\n\n\n\nResearch Design and Methods for Psychology and Education\n\n\n\n\nStatistical Analysis of Experimental Data\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "publication/index.html",
    "href": "publication/index.html",
    "title": "Publications and Pre-Prints",
    "section": "",
    "text": "Jun 24, 2024\n    \n    \n      \n        Determining associations between intervention amount and outcomes for young autistic children: A systematic review and meta-analysis\n      \n      \n      Micheal Sandbank, James E. Pustejovsky, Kristen Bottema-Beutel, Nicolette Caldwell, Jacob I. Feldman, Shannon Crowley LaPoint, Tiffany Woynaroski\n      \n      \n        \n          JAMA Pediatrics,\n          \n          \n            forthcoming.\n          \n        \n      \n      \n        \n          \n            \n              \n                \n                Journal\n              \n            \n          \n            \n              \n                \n                Code\n              \n            \n          \n            \n              \n                \n                Data\n              \n            \n          \n        \n      \n    \n  \n  \n  \n              \n      Mar 7, 2024\n    \n    \n      \n        Equivalences between ad hoc strategies and meta-analytic models for dependent effect sizes\n      \n      \n      James E. Pustejovsky, Man Chen\n      \n      \n        \n          Journal of Educational and Behavioral Statistics,\n          \n          \n            advance online publication.\n          \n        \n      \n      \n        \n          \n            \n              \n                \n                Journal\n              \n            \n          \n            \n              \n                \n                Supplementary materials\n              \n            \n          \n            \n              \n                \n                Pre-Print\n              \n            \n          \n            \n              \n                \n                Code\n              \n            \n          \n            \n              \n                \n                Data\n              \n            \n          \n        \n      \n    \n  \n  \n  \n              \n      Nov 9, 2023\n    \n    \n      \n        High replicability of newly-discovered social-behavioral findings is achievable.\n      \n      \n      John Protzko, Jon Krosnick, Leif Nelson, Brian Nosek, Jordan Axt, Matt Berent, Nicholas Buttrick, Matthew DeBell, Charles R. Ebersole, Sebastian Lundmark, Bo MacInnis, Michael O'Donnell, Hannah Perfecto, James E. Pustejovsky, Scott Roeder, Jan Walleczek, Jonathan W. Schooler\n      \n      \n        \n          Nature Human Behavior,\n          8,\n          \n            311-319.\n          \n        \n      \n      \n        \n          \n            \n              \n                \n                Journal\n              \n            \n          \n            \n              \n                \n                Supplementary materials\n              \n            \n          \n            \n              \n                \n                Pre-Print\n              \n            \n          \n        \n      \n    \n  \n  \n  \n              \n      Jul 21, 2023\n    \n    \n      \n        Conducting power analysis for meta-analysis of dependent effect sizes: Common guidelines and an introduction to the POMADE R package\n      \n      \n      Mikkel H. Vembye, James E. Pustejovsky, Terri D. Pigott\n      \n      \n        \n      \n      \n        \n          \n            \n              \n                \n                Supplementary materials\n              \n            \n          \n            \n              \n                \n                Pre-Print\n              \n            \n          \n            \n              \n                \n                Code\n              \n            \n          \n        \n      \n    \n  \n  \n  \n              \n      Jul 21, 2023\n    \n    \n      \n        The efficacy of combining cognitive training and non-invasive brain stimulation: A transdiagnostic systematic review and meta-analysis\n      \n      \n      Anika Poppe, Franziska D. E. Ritter, Leonie Bais, James E. Pustejovsky, Marie-José van Tol, Branislava Ćurčić-Blake, Gerdina H.M. Pijnenborg, Lisette van der Meer\n      \n      \n        \n          Psychological Bulletin,\n          150(2),\n          \n            192-213.\n          \n        \n      \n      \n        \n          \n            \n              \n                \n                Journal\n              \n            \n          \n            \n              \n                \n                Pre-Print\n              \n            \n          \n            \n              \n                \n                Code\n              \n            \n          \n            \n              \n                \n                Data\n              \n            \n          \n        \n      \n    \n  \n  \n  \n              \n      May 26, 2023\n    \n    \n      \n        Systematic review of variables related to instruction in augmentative and alternative communication implementation: Group and single-case design\n      \n      \n      Joe Reichle, James E. Pustejovsky, Kimberly J. Vannest, Margaret Foster, Lauren M. Pierson, Sanikan Wattanawongwan, Man Chen, Marcus Fuller, April N. Haas, Bethany H. Bhat, Mary R. Sallese, S. D. Smith, Valeria Yllades, Daira Rodriguez, Amara Yoro, Jay B. Ganz\n      \n      \n        \n          American Journal of Speech-Language Pathology,\n          32(4),\n          \n            1734-1757.\n          \n        \n      \n      \n        \n          \n            \n              \n                \n                Journal\n              \n            \n          \n            \n              \n                \n                Pre-Print\n              \n            \n          \n            \n              \n                \n                Code\n              \n            \n          \n            \n              \n                \n                Data\n              \n            \n          \n        \n      \n    \n  \n  \n  \n              \n      Mar 9, 2023\n    \n    \n      \n        Comparison of competing approaches to analyzing cross-classified data: Random effects models, ordinary least squares, or fixed effects with cluster robust standard errors\n      \n      \n      Young Ri Lee, James E. Pustejovsky\n      \n      \n        \n          Psychological Methods,\n          \n          \n            advance online publication.\n          \n        \n      \n      \n        \n          \n            \n              \n                \n                Journal\n              \n            \n          \n            \n              \n                \n                Supplementary materials\n              \n            \n          \n            \n              \n                \n                Pre-Print\n              \n            \n          \n        \n      \n    \n  \n  \n  \n              \n      Mar 8, 2023\n    \n    \n      \n        Between-case standardized mean differences: Flexible methods for single-case designs\n      \n      \n      Man Chen, James E. Pustejovsky, David A. Klingbeil, Ethan R. Van Norman\n      \n      \n        \n          Journal of School Psychology,\n          98,\n          \n            16-38.\n          \n        \n      \n      \n        \n          \n            \n              \n                \n                Journal\n              \n            \n          \n            \n              \n                \n                Supplementary materials\n              \n            \n          \n            \n              \n                \n                Pre-Print\n              \n            \n          \n            \n              \n                \n                Code\n              \n            \n          \n        \n      \n    \n  \n  \n  \n              \n      Jan 20, 2023\n    \n    \n      \n        A case for increased rigor in AAC research: A methodological quality review\n      \n      \n      Jay B. Ganz, James E. Pustejovsky, Joe Reichle, Kimberly J. Vannest, Lauren M. Pierson, Sanikan Wattanawongwan, Margaret Foster, Marcus Fuller, April N. Haas, Mary Rose Sallese, S. D. Smith, Valeria Yllades\n      \n      \n        \n          Education and Training in Autism and Developmental Disabilities,\n          58(1),\n          \n            3-21.\n          \n        \n      \n      \n        \n          \n            \n              \n                \n                Pre-Print\n              \n            \n          \n            \n              \n                \n                Code\n              \n            \n          \n            \n              \n                \n                Data\n              \n            \n          \n        \n      \n    \n  \n  \n  \n              \n      Dec 14, 2022\n    \n    \n      \n        Social validity, cost, acceptability, and feasibility of Augmentative and Alternative Communication devices used for individuals with autism spectrum disorder and intellectual disability: A systematic review\n      \n      \n      Lauren M. Pierson, Jay B. Ganz, James E. Pustejovsky, Joe Reichle, Kimberly J. Vannest, Sanikan Wattanawongwan, Margaret Foster, Marcus C. Fuller, April N. Haas, Mary Rose Sallese, S. D. Smith, Valeria Yllades, Emily Kenny, Peyton Morgan, Scout Paterson\n      \n      \n        \n          Perspectives of the ASHA Special Interest Groups,\n          7(6),\n          \n            1917-1940.\n          \n        \n      \n      \n        \n          \n            \n              \n                \n                Journal\n              \n            \n          \n            \n              \n                \n                Data\n              \n            \n          \n        \n      \n    \n  \n  \n  \n              \n      Nov 22, 2022\n    \n    \n      \n        Single case design research in Special Education: Next generation standards and considerations\n      \n      \n      Jennifer R. Ledford, Joseph Lambert, James E. Pustejovsky, Kathleen N. Zimmerman, Nicole Hollins, Erin E. Barton\n      \n      \n        \n          Exceptional Children,\n          89(4),\n          \n            379-396.\n          \n        \n      \n      \n        \n          \n            \n              \n                \n                Journal\n              \n            \n          \n            \n              \n                \n                Supplementary materials\n              \n            \n          \n            \n              \n                \n                Pre-Print\n              \n            \n          \n        \n      \n    \n  \n  \n  \n              \n      Oct 17, 2022\n    \n    \n      \n        Power approximations for overall average effects in meta-analysis of dependent effect sizes\n      \n      \n      Mikkel H. Vembye, James E. Pustejovsky, Terri D. Pigott\n      \n      \n        \n          Journal of Educational and Behavioral Statistics,\n          48(1),\n          \n            70-102.\n          \n        \n      \n      \n        \n          \n            \n              \n                \n                Journal\n              \n            \n          \n            \n              \n                \n                Supplementary materials\n              \n            \n          \n            \n              \n                \n                R package\n              \n            \n          \n            \n              \n                \n                Pre-Print\n              \n            \n          \n            \n              \n                \n                Code\n              \n            \n          \n        \n      \n    \n  \n  \n  \n              \n      Sep 30, 2022\n    \n    \n      \n        Investigating narrative performance in children with developmental language disorder: A systematic review and meta-analysis\n      \n      \n      Katherine L. Winters, Javier Jasso, James E. Pustejovsky, Courtney Byrd\n      \n      \n        \n          Journal of Speech, Language, and Hearing Research,\n          65(10),\n          \n            3908-3929.\n          \n        \n      \n      \n        \n          \n            \n              \n                \n                Journal\n              \n            \n          \n            \n              \n                \n                Supplementary materials\n              \n            \n          \n            \n              \n                \n                Pre-Print\n              \n            \n          \n            \n              \n                \n                Code\n              \n            \n          \n            \n              \n                \n                Data\n              \n            \n          \n        \n      \n    \n  \n  \n  \n              \n      Apr 29, 2022\n    \n    \n      \n        Multi-level meta-analysis of single-case experimental designs using robust variance estimation\n      \n      \n      Man Chen, James E. Pustejovsky\n      \n      \n        \n          Psychological Methods,\n          \n          \n            forthcoming.\n          \n        \n      \n      \n        \n          \n            \n              \n                \n                Journal\n              \n            \n          \n            \n              \n                \n                Supplementary materials\n              \n            \n          \n            \n              \n                \n                Pre-Print\n              \n            \n          \n            \n              \n                \n                Code\n              \n            \n          \n        \n      \n    \n  \n  \n  \n              \n      Apr 20, 2022\n    \n    \n      \n        Augmentative and Alternative Communication intervention targets for school-aged participants with ASD and ID: A single-case systematic review and meta-analysis\n      \n      \n      Jay B. Ganz, James E. Pustejovsky, Joe Reichle, Kimberly J. Vannest, Margaret Foster, Marcus Fuller, Lauren M. Pierson, Sanikan Wattanawongwan, Amando Bernal, Man Chen, April N. Haas, Rachel Skov, S. D. Smith, Valeria Yllades\n      \n      \n        \n          Review Journal of Autism and Developmental Disorders,\n          \n          \n        \n      \n      \n        \n          \n            \n              \n                \n                Journal\n              \n            \n          \n            \n              \n                \n                Pre-Print\n              \n            \n          \n            \n              \n                \n                Code\n              \n            \n          \n            \n              \n                \n                Data\n              \n            \n          \n        \n      \n    \n  \n  \n  \n              \n      Apr 20, 2022\n    \n    \n      \n        Considering instructional contexts in AAC interventions for people with ASD and/or IDD experiencing complex communication needs: A single-case design meta-analysis\n      \n      \n      Jay B. Ganz, James E. Pustejovsky, Joe Reichle, Kimberly J. Vannest, Margaret Foster, April N. Haas, Lauren M. Pierson, Sanikan Wattanawongwan, Amando Bernal, Man Chen, Rachel Skov, S. D. Smith\n      \n      \n        \n          Review Journal of Autism and Developmental Disorders,\n          10,\n          \n            615-629.\n          \n        \n      \n      \n        \n          \n            \n              \n                \n                Journal\n              \n            \n          \n            \n              \n                \n                Pre-Print\n              \n            \n          \n            \n              \n                \n                Code\n              \n            \n          \n            \n              \n                \n                Data\n              \n            \n          \n        \n      \n    \n  \n  \n  \n              \n      Apr 20, 2022\n    \n    \n      \n        Participant characteristics predicting communication outcomes in AAC implementation for individuals with ASD and IDD: Meta-analysis\n      \n      \n      Jay B. Ganz, James E. Pustejovsky, Joe Reichle, Kimberly J. Vannest, Margaret Foster, Lauren M. Pierson, Sanikan Wattanawongwan, Amando Bernal, Man Chen, April N. Haas, Mary Rose Sallese, Rachel Skov, S. D. Smith\n      \n      \n        \n          Augmentative and Alternative Communication,\n          39(1),\n          \n            7-22.\n          \n        \n      \n      \n        \n          \n            \n              \n                \n                Journal\n              \n            \n          \n            \n              \n                \n                Pre-Print\n              \n            \n          \n            \n              \n                \n                Code\n              \n            \n          \n            \n              \n                \n                Data\n              \n            \n          \n        \n      \n    \n  \n  \n  \n              \n      Apr 1, 2022\n    \n    \n      \n        Meta-Analysis with robust variance estimation: Expanding the range of working models\n      \n      \n      James E. Pustejovsky, Elizabeth Tipton\n      \n      \n        \n          Prevention Science,\n          23,\n          \n            425-438.\n          \n        \n      \n      \n        \n          \n            \n              \n                \n                Journal\n              \n            \n          \n            \n              \n                \n                Supplementary materials\n              \n            \n          \n            \n              \n                \n                Pre-Print\n              \n            \n          \n            \n              \n                \n                Slides\n              \n            \n          \n            \n              \n                \n                Code\n              \n            \n          \n            \n              \n                \n                Data\n              \n            \n          \n            \n              \n                \n                Video\n              \n            \n          \n        \n      \n    \n  \n  \n  \n              \n      Feb 8, 2022\n    \n    \n      \n        Cluster wild bootstrapping to handle dependent effect sizes in meta-analysis with a small number of studies\n      \n      \n      Megha Joshi, James E. Pustejovsky, S. Natasha Beretvas\n      \n      \n        \n          Research Synthesis Methods,\n          13(4),\n          \n            457-477.\n          \n        \n      \n      \n        \n          \n            \n              \n                \n                Journal\n              \n            \n          \n            \n              \n                \n                Supplementary materials\n              \n            \n          \n            \n              \n                \n                R package\n              \n            \n          \n            \n              \n                \n                Pre-Print\n              \n            \n          \n            \n              \n                \n                Video\n              \n            \n          \n        \n      \n    \n  \n  \n  \n              \n      Feb 1, 2022\n    \n    \n      \n        Examining the effects of social stories on challenging behavior and prosocial skills in young children: A systematic review and meta-analysis\n      \n      \n      Charis L. Wahman, James E. Pustejovsky, Michaelene M. Ostrosky, Rosa Milagros Santos\n      \n      \n        \n          Topics in Early Childhood Special Education,\n          41(4),\n          \n            267-279.\n          \n        \n      \n      \n        \n          \n            \n              \n                \n                Journal\n              \n            \n          \n            \n              \n                \n                Pre-Print\n              \n            \n          \n            \n              \n                \n                Code\n              \n            \n          \n            \n              \n                \n                Data\n              \n            \n          \n        \n      \n    \n  \n  \n  \n              \n      Jul 7, 2021\n    \n    \n      \n        The relative effects of integrating word reading and word meaning instruction to word reading instruction alone on the accuracy, fluency, and word meaning knowledge of 4th-5th grade students with dyslexia\n      \n      \n      Christy R. Austin, Sharon Vaughn, Nathan H. Clemens, James E. Pustejovsky, Alexis N. Boucher\n      \n      \n        \n          Scientific Studies of Reading,\n          26(3),\n          \n            204-222.\n          \n        \n      \n      \n        \n          \n            \n              \n                \n                Journal\n              \n            \n          \n        \n      \n    \n  \n  \n  \n              \n      Apr 20, 2021\n    \n    \n      \n        Evaluating the Transition to College Mathematics Course in Texas high schools: Examining heterogeneity across schools and student characteristics\n      \n      \n      James E. Pustejovsky, Megha Joshi\n      \n      \n        \n          Greater Texas Foundation White Paper,\n          \n          \n        \n      \n      \n        \n          \n            \n              \n                \n                PDF\n              \n            \n          \n        \n      \n    \n  \n  \n  \n              \n      Feb 1, 2021\n    \n    \n      \n        A systematic review and meta-analysis of effects of psychosocial interventions on spiritual well-being in adults with cancer\n      \n      \n      Laurie E. McLouth, C. Graham Ford, James E. Pustejovsky, Crystal Park, Allen C. Sherman, Kelly Trevino, John A. Salsman\n      \n      \n        \n          Psycho-Oncology,\n          30(2),\n          \n            147-158.\n          \n        \n      \n      \n        \n          \n            \n              \n                \n                Journal\n              \n            \n          \n        \n      \n    \n  \n  \n  \n              \n      Jan 5, 2021\n    \n    \n      \n        Systematic review and meta-analysis of stay-play-talk interventions for improving social behaviors of young children\n      \n      \n      Jennifer R. Ledford, James E. Pustejovsky\n      \n      \n        \n          Journal of Positive Behavioral Interventions,\n          \n          \n            forthcoming.\n          \n        \n      \n      \n        \n          \n            \n              \n                \n                Journal\n              \n            \n          \n            \n              \n                \n                Supplementary materials\n              \n            \n          \n            \n              \n                \n                Pre-Print\n              \n            \n          \n        \n      \n    \n  \n  \n  \n              \n      Dec 16, 2020\n    \n    \n      \n        Evaluating the Transition to College Mathematics Course in Texas high schools: Findings from the second year of implementation\n      \n      \n      James E. Pustejovsky, Megha Joshi\n      \n      \n        \n          Greater Texas Foundation White Paper,\n          \n          \n        \n      \n      \n        \n          \n            \n              \n                \n                White paper\n              \n            \n          \n            \n              \n                \n                PDF\n              \n            \n          \n        \n      \n    \n  \n  \n  \n              \n      Jul 12, 2020\n    \n    \n      \n        Evaluating meta-analytic methods to detect selective reporting in the presence of dependent effect sizes\n      \n      \n      Melissa A. Rodgers, James E. Pustejovsky\n      \n      \n        \n          Psychological Methods,\n          26(2),\n          \n            141-160.\n          \n        \n      \n      \n        \n          \n            \n              \n                \n                Journal\n              \n            \n          \n            \n              \n                \n                Supplementary materials\n              \n            \n          \n            \n              \n                \n                Pre-Print\n              \n            \n          \n        \n      \n    \n  \n  \n  \n              \n      Mar 26, 2020\n    \n    \n      \n        The impact of response-guided designs on count outcomes in single-case experimental design baselines\n      \n      \n      Daniel M. Swan, James E. Pustejovsky, S. Natasha Beretvas\n      \n      \n        \n          Evidence-Based Communication Assessment and Intervention,\n          \n          \n            forthcoming.\n          \n        \n      \n      \n        \n          \n            \n              \n                \n                Journal\n              \n            \n          \n            \n              \n                \n                Code\n              \n            \n          \n        \n      \n    \n  \n  \n  \n              \n      Nov 19, 2019\n    \n    \n      \n        Psychosocial interventions for cancer survivors: A meta-analysis of effects on positive affect\n      \n      \n      John A. Salsman, James E. Pustejovsky, Stephen M. Schueller, Rosalba Hernandez, Mark Berendsen, Laurie E. Steffen McLouth, Judith T. Moskowitz\n      \n      \n        \n          Journal of Cancer Survivorship,\n          13,\n          \n            943-955.\n          \n        \n      \n      \n        \n          \n            \n              \n                \n                Journal\n              \n            \n          \n        \n      \n    \n  \n  \n  \n              \n      Aug 2, 2019\n    \n    \n      \n        An examination of measurement procedures and characteristics of baseline outcome data in single-case research\n      \n      \n      James E. Pustejovsky, Daniel M. Swan, Kyle W. English\n      \n      \n        \n          Behavior Modification,\n          47(6),\n          \n            1423-1454.\n          \n        \n      \n      \n        \n          \n            \n              \n                \n                Journal\n              \n            \n          \n            \n              \n                \n                Pre-Print\n              \n            \n          \n            \n              \n                \n                Code\n              \n            \n          \n            \n              \n                \n                Data\n              \n            \n          \n        \n      \n    \n  \n  \n  \n              \n      Jun 30, 2019\n    \n    \n      \n        Evaluating the Transition to College Mathematics Course in Texas high schools: Findings from the first year of implementation\n      \n      \n      James E. Pustejovsky, Megha Joshi\n      \n      \n        \n          Greater Texas Foundation White Paper,\n          \n          \n        \n      \n      \n        \n          \n            \n              \n                \n                White paper\n              \n            \n          \n            \n              \n                \n                PDF\n              \n            \n          \n        \n      \n    \n  \n  \n  \n              \n      Jun 6, 2019\n    \n    \n      \n        Interventions to enhance self-efficacy in cancer patients and survivors: A meta-analysis of randomized controlled trials\n      \n      \n      Tom V. Merluzzi, James E. Pustejovsky, Errol J. Philip, Stephanie J. Sohl, Mark Berendsen, John A. Salsman\n      \n      \n        \n          Psycho-Oncology,\n          28(9),\n          \n            1781-1790.\n          \n        \n      \n      \n        \n          \n            \n              \n                \n                Journal\n              \n            \n          \n            \n              \n                \n                Data\n              \n            \n          \n        \n      \n    \n  \n  \n  \n              \n      Apr 29, 2019\n    \n    \n      \n        Effects of psychosocial interventions on meaning and purpose in adults with cancer: A systematic review and meta-analysis\n      \n      \n      Crystal L. Park, James E. Pustejovsky, Kelly Trevino, Allen C. Sherman, Craig Esposito, Mark Berendsen, John A. Salsman\n      \n      \n        \n          Cancer,\n          125(14),\n          \n            2383-2393.\n          \n        \n      \n      \n        \n          \n            \n              \n                \n                Journal\n              \n            \n          \n        \n      \n    \n  \n  \n  \n              \n      Apr 1, 2019\n    \n    \n      \n        Procedural sensitivities of effect sizes for single-case designs with behavioral outcome measures\n      \n      \n      James E. Pustejovsky\n      \n      \n        \n          Psychological Methods,\n          24(2),\n          \n            217-235.\n          \n        \n      \n      \n        \n          \n            \n              \n                \n                Supplementary materials\n              \n            \n          \n            \n              \n                \n                Journal\n              \n            \n          \n            \n              \n                \n                Pre-Print\n              \n            \n          \n        \n      \n    \n  \n  \n  \n              \n      Mar 1, 2019\n    \n    \n      \n        Testing for funnel plot asymmetry of standardized mean differences\n      \n      \n      James E. Pustejovsky, Melissa A. Rodgers\n      \n      \n        \n          Research Synthesis Methods,\n          10(1),\n          \n            57-71.\n          \n        \n      \n      \n        \n          \n            \n              \n                \n                Journal\n              \n            \n          \n            \n              \n                \n                Pre-Print\n              \n            \n          \n            \n              \n                \n                Code\n              \n            \n          \n        \n      \n    \n  \n  \n  \n              \n      Jan 7, 2019\n    \n    \n      \n        Current practices in meta-regression in psychology, education, and medicine\n      \n      \n      Elizabeth Tipton, James E. Pustejovsky, Hedyeh Ahmadi\n      \n      \n        \n          Research Synthesis Methods,\n          10(2),\n          \n            180-194.\n          \n        \n      \n      \n        \n          \n            \n              \n                \n                Journal\n              \n            \n          \n            \n              \n                \n                Pre-Print\n              \n            \n          \n            \n              \n                \n                Data\n              \n            \n          \n        \n      \n    \n  \n  \n  \n              \n      Dec 27, 2018\n    \n    \n      \n        A history of meta-regression: Technical, conceptual, and practical developments between 1974 and 2018\n      \n      \n      Elizabeth Tipton, James E. Pustejovsky, Hedyeh Ahmadi\n      \n      \n        \n          Research Synthesis Methods,\n          10(2),\n          \n            161-179.\n          \n        \n      \n      \n        \n          \n            \n              \n                \n                Journal\n              \n            \n          \n            \n              \n                \n                Pre-Print\n              \n            \n          \n        \n      \n    \n  \n  \n  \n              \n      Nov 2, 2018\n    \n    \n      \n        Small sample methods for cluster-robust variance estimation and hypothesis testing in fixed effects models\n      \n      \n      James E. Pustejovsky, Elizabeth Tipton\n      \n      \n        \n          Journal of Business and Economic Statistics,\n          36(4),\n          \n            672-683.\n          \n        \n      \n      \n        \n          \n            \n              \n                \n                Journal\n              \n            \n          \n            \n              \n                \n                Correction\n              \n            \n          \n            \n              \n                \n                R package\n              \n            \n          \n            \n              \n                \n                Pre-Print\n              \n            \n          \n        \n      \n    \n  \n  \n  \n              \n      Jun 2, 2018\n    \n    \n      \n        Between-case standardized effect size analysis of single case design: Examination of the two methods\n      \n      \n      Samuel L. Odom, Erin E. Barton, Brian Reichow, Hariharan Swaminathan, James E. Pustejovsky\n      \n      \n        \n          _Research in Developmental Disabilities, 79_, 88-96,\n          \n          \n        \n      \n      \n        \n          \n            \n              \n                \n                Journal\n              \n            \n          \n        \n      \n    \n  \n  \n  \n              \n      May 1, 2018\n    \n    \n      \n        A gradual effects model for single-case designs\n      \n      \n      Daniel M. Swan, James E. Pustejovsky\n      \n      \n        \n          Multivariate Behavioral Research,\n          53(4),\n          \n            574-593.\n          \n        \n      \n      \n        \n          \n            \n              \n                \n                Supplementary materials\n              \n            \n          \n            \n              \n                \n                R package\n              \n            \n          \n            \n              \n                \n                Web app\n              \n            \n          \n            \n              \n                \n                Journal\n              \n            \n          \n            \n              \n                \n                Pre-Print\n              \n            \n          \n        \n      \n    \n  \n  \n  \n              \n      Mar 7, 2018\n    \n    \n      \n        Single-case synthesis tools II: Comparing overlap measures and parametric effect sizes for synthesizing antecedent sensory-based interventions\n      \n      \n      Kathleen N. Zimmerman, James E. Pustejovsky, Jennifer R. Ledford, Erin E. Barton, Katherine E. Severini, Blair P. Lloyd\n      \n      \n        \n          Research in Developmental Disabilities,\n          79,\n          \n            65-76.\n          \n        \n      \n      \n        \n          \n            \n              \n                \n                Journal\n              \n            \n          \n        \n      \n    \n  \n  \n  \n              \n      Mar 3, 2018\n    \n    \n      \n        Single-case synthesis tools I: Evaluating the quality and rigor of research on antecedent sensory-based interventions\n      \n      \n      Kathleen N. Zimmerman, Jennifer R. Ledford, Katherine E. Severini, James E. Pustejovsky, Erin E. Barton, Blair P. Lloyd\n      \n      \n        \n          Research in Developmental Disabilities,\n          79,\n          \n            19-32.\n          \n        \n      \n      \n        \n          \n            \n              \n                \n                Journal\n              \n            \n          \n        \n      \n    \n  \n  \n  \n              \n      Feb 1, 2018\n    \n    \n      \n        Using response ratios for meta-analyzing single-case designs with behavioral outcomes\n      \n      \n      James E. Pustejovsky\n      \n      \n        \n          Journal of School Psychology,\n          16,\n          \n            99-112.\n          \n        \n      \n      \n        \n          \n            \n              \n                \n                Supplementary materials\n              \n            \n          \n            \n              \n                \n                Journal\n              \n            \n          \n            \n              \n                \n                Pre-Print\n              \n            \n          \n        \n      \n    \n  \n  \n  \n              \n      Nov 1, 2017\n    \n    \n      \n        Introduction to the special issue on single-case systematic reviews and meta-analysis\n      \n      \n      Daniel M. Maggin, Kathleen L. Lane, James E. Pustejovsky\n      \n      \n        \n          Remedial and Special Education,\n          38(6),\n          \n            323-330.\n          \n        \n      \n      \n        \n          \n            \n              \n                \n                Journal\n              \n            \n          \n        \n      \n    \n  \n  \n  \n              \n      Jul 1, 2017\n    \n    \n      \n        A meta-analysis of school-based group contingency interventions for students with challenging behavior: An update\n      \n      \n      Daniel M. Maggin, James E. Pustejovsky, Austin H. Johnson\n      \n      \n        \n          Remedial and Special Education,\n          38(6),\n          \n            353-370.\n          \n        \n      \n      \n        \n          \n            \n              \n                \n                Supplementary materials\n              \n            \n          \n            \n              \n                \n                Journal\n              \n            \n          \n            \n              \n                \n                Pre-Print\n              \n            \n          \n        \n      \n    \n  \n  \n  \n              \n      Jul 1, 2017\n    \n    \n      \n        A meta-analysis of technology-aided instruction and intervention for students with ASD\n      \n      \n      Erin E. Barton, James E. Pustejovsky, Daniel M. Maggin, Brian Reichow\n      \n      \n        \n          Remedial and Special Education,\n          38(6),\n          \n            371-386.\n          \n        \n      \n      \n        \n          \n            \n              \n                \n                Journal\n              \n            \n          \n        \n      \n    \n  \n  \n  \n              \n      Mar 1, 2017\n    \n    \n      \n        Functional assessment-based interventions for students with or at-risk for high incidence disabilities: Field-testing single-case synthesis methods\n      \n      \n      Eric A. Common, Kathleen L. Lane, James E. Pustejovsky, Austin H. Johnson, Liane E. Johl\n      \n      \n        \n          Remedial and Special Education,\n          38(6),\n          \n            331-352.\n          \n        \n      \n      \n        \n          \n            \n              \n                \n                Journal\n              \n            \n          \n        \n      \n    \n  \n  \n  \n              \n      Jan 1, 2017\n    \n    \n      \n        Research synthesis and meta-analysis of single-case designs\n      \n      \n      James E. Pustejovsky, John Ferron\n      \n      \n        \n          In J. M. Kauffman, D. P. Hallahan, & P. C. Pullen (Eds.), _Handbook of Special Education, 2nd Edition_. New York, NY: Routledge,\n          \n          \n        \n      \n      \n        \n          \n            \n              \n                \n                PDF\n              \n            \n          \n        \n      \n    \n  \n  \n  \n              \n      Dec 19, 2016\n    \n    \n      \n        Between-case standardized mean difference effect sizes for single-case designs: A primer and tutorial using the scdhlm web application\n      \n      \n      Jeffrey C. Valentine, Emily E. Tanner-Smith, James E. Pustejovsky, Timothy S. Lau\n      \n      \n        \n          Oslo, Norway: The Campbell Collaboration,\n          \n          \n        \n      \n      \n        \n          \n            \n              \n                \n                R package\n              \n            \n          \n            \n              \n                \n                Web app\n              \n            \n          \n            \n              \n                \n                PDF\n              \n            \n          \n        \n      \n    \n  \n  \n  \n              \n      Dec 15, 2015\n    \n    \n      \n        Small-sample adjustments for tests of moderators and model fit using robust variance estimation in meta-regression\n      \n      \n      Elizabeth Tipton, James E. Pustejovsky\n      \n      \n        \n          Journal of Educational and Behavioral Statistics,\n          40(6),\n          \n            604-634.\n          \n        \n      \n      \n        \n          \n            \n              \n                \n                Journal\n              \n            \n          \n            \n              \n                \n                R package\n              \n            \n          \n            \n              \n                \n                Pre-Print\n              \n            \n          \n        \n      \n    \n  \n  \n  \n              \n      Sep 1, 2015\n    \n    \n      \n        Measurement-comparable effect sizes for single-case studies of free-operant behavior\n      \n      \n      James E. Pustejovsky\n      \n      \n        \n          Psychological Methods,\n          20(3),\n          \n            342-359.\n          \n        \n      \n      \n        \n          \n            \n              \n                \n                Supplementary materials\n              \n            \n          \n            \n              \n                \n                Journal\n              \n            \n          \n            \n              \n                \n                Pre-Print\n              \n            \n          \n        \n      \n    \n  \n  \n  \n              \n      Aug 10, 2015\n    \n    \n      \n        A meta-analytic approach to examine the relationship between religion/spirituality and mental health in cancer\n      \n      \n      John A. Salsman, James E. Pustejovsky, Heather S. Jim, Alexis R. Munoz, Thomas V. Merluzzi, Logan George, Crystal L. Park, Suzanne C. Danhauer, Allen C. Sherman, Mallory A. Snyder, George Fitchett\n      \n      \n        \n          Cancer,\n          121(21),\n          \n            3769-3778.\n          \n        \n      \n      \n        \n          \n            \n              \n                \n                Journal\n              \n            \n          \n        \n      \n    \n  \n  \n  \n              \n      Aug 10, 2015\n    \n    \n      \n        Religion, spirituality, and physical health in cancer patients: A meta-analysis\n      \n      \n      Heather S. Jim, James E. Pustejovsky, Crystal L. Park, Suzanne C. Danhauer, Allen C. Sherman, George Fitchett, Thomas V. Merluzzi, Alexis R. Munoz, Logan George, Mallory A. Snyder, John A. Salsman\n      \n      \n        \n          Cancer,\n          121(21),\n          \n            3760-3768.\n          \n        \n      \n      \n        \n          \n            \n              \n                \n                Journal\n              \n            \n          \n        \n      \n    \n  \n  \n  \n              \n      Aug 10, 2015\n    \n    \n      \n        A meta-analytic review of religious or spiritual involvement and social health among cancer patients\n      \n      \n      Allen C. Sherman, Thomas V. Merluzzi, James E. Pustejovsky, Crystal L. Park, Logan George, George Fitchett, Heather S. Jim, Alexis R. Munoz, Suzanne C. Danhauer, Mallory A. Snyder, John A. Salsman\n      \n      \n        \n          Cancer,\n          121(21),\n          \n            3779-3788.\n          \n        \n      \n      \n        \n          \n            \n              \n                \n                Journal\n              \n            \n          \n        \n      \n    \n  \n  \n  \n              \n      Jun 19, 2015\n    \n    \n      \n        Four methods for analyzing partial interval recording data, with application to single-case research\n      \n      \n      James E. Pustejovsky, Daniel M. Swan\n      \n      \n        \n          Multivariate Behavioral Research,\n          50(3),\n          \n            365-380.\n          \n        \n      \n      \n        \n          \n            \n              \n                \n                Supplementary materials\n              \n            \n          \n            \n              \n                \n                Journal\n              \n            \n          \n            \n              \n                \n                Pre-Print\n              \n            \n          \n        \n      \n    \n  \n  \n  \n              \n      Oct 1, 2014\n    \n    \n      \n        Design-comparable effect sizes in multiple baseline designs: A general modeling framework\n      \n      \n      James E. Pustejovsky, Larry V. Hedges, William R. Shadish\n      \n      \n        \n          Journal of Educational and Behavioral Statistics,\n          39(5),\n          \n            368-393.\n          \n        \n      \n      \n        \n          \n            \n              \n                \n                Supplementary materials\n              \n            \n          \n            \n              \n                \n                Journal\n              \n            \n          \n            \n              \n                \n                R package\n              \n            \n          \n            \n              \n                \n                Web app\n              \n            \n          \n            \n              \n                \n                Pre-Print\n              \n            \n          \n        \n      \n    \n  \n  \n  \n              \n      Aug 1, 2014\n    \n    \n      \n        Alternating renewal process models for behavioral observation: Simulation methods and validity implications\n      \n      \n      James E. Pustejovsky, Christopher Runyon\n      \n      \n        \n          Behavioral Disorders,\n          39(4),\n          \n            211-2227.\n          \n        \n      \n      \n        \n          \n            \n              \n                \n                Journal\n              \n            \n          \n            \n              \n                \n                R package\n              \n            \n          \n            \n              \n                \n                Pre-Print\n              \n            \n          \n        \n      \n    \n  \n  \n  \n              \n      Jul 18, 2014\n    \n    \n      \n        Analyzing single-case designs: d, G, hierarchical models, Bayesian estimators, generalized additive models, and the hopes and fears of researchers about analyses\n      \n      \n      William R. Shadish, Larry V. Hedges, James E. Pustejovsky, David Rindskopf, Jonathan G. Boyajian, Kristynn J. Sullivan\n      \n      \n        \n          In T. R. Kratochwill & J. R. Levin (Eds.), _Single-Case Intervention Research: Methodological and Data-Analysis Advances_. Washington, D.C.: American Psychological Association,\n          \n          \n        \n      \n      \n        \n          \n        \n      \n    \n  \n  \n  \n              \n      Apr 1, 2014\n    \n    \n      \n        Analysis and meta-analysis of single-case designs with a standardized mean difference statistic: A primer and applications\n      \n      \n      William R. Shadish, Larry V. Hedges, James E. Pustejovsky\n      \n      \n        \n          Journal of School Psychology,\n          52,\n          \n            123-147.\n          \n        \n      \n      \n        \n          \n            \n              \n                \n                Journal\n              \n            \n          \n        \n      \n    \n  \n  \n  \n              \n      Mar 1, 2014\n    \n    \n      \n        Converting from d to r to z when the design uses extreme groups, dichotomization, or experimental control\n      \n      \n      James E. Pustejovsky\n      \n      \n        \n          Psychological Methods,\n          19(1),\n          \n            92-112.\n          \n        \n      \n      \n        \n          \n            \n              \n                \n                Supplementary materials\n              \n            \n          \n            \n              \n                \n                Journal\n              \n            \n          \n            \n              \n                \n                Pre-Print\n              \n            \n          \n            \n              \n                \n                Code\n              \n            \n          \n        \n      \n    \n  \n  \n  \n              \n      Aug 23, 2013\n    \n    \n      \n        A standardized mean difference effect size for multiple baseline designs\n      \n      \n      Larry V. Hedges, James E. Pustejovsky, William R. Shadish\n      \n      \n        \n          Research Synthesis Methods,\n          4(4),\n          \n            324-341.\n          \n        \n      \n      \n        \n          \n            \n              \n                \n                Journal\n              \n            \n          \n            \n              \n                \n                R package\n              \n            \n          \n            \n              \n                \n                Web app\n              \n            \n          \n        \n      \n    \n  \n  \n  \n              \n      Jul 18, 2013\n    \n    \n      \n        A d-statistic for single-case designs that is equivalent to the usual between-groups d-statistics\n      \n      \n      William R. Shadish, Larry V. Hedges, James E. Pustejovsky, Jonathan G. Boyajian, Kristynn J. Sullivan, Alma Andrade, Jeannette Barrientos\n      \n      \n        \n          Neuropsychological Rehabilitation,\n          24(3-4),\n          \n            528-553.\n          \n        \n      \n      \n        \n          \n            \n              \n                \n                Journal\n              \n            \n          \n        \n      \n    \n  \n  \n  \n              \n      Jun 1, 2013\n    \n    \n      \n        Operationally comparable effect sizes for meta-analysis of single-case research\n      \n      \n      James E. Pustejovsky\n      \n      \n        \n          Dissertation. Northwestern University, Department of Statistics,\n          \n          \n        \n      \n      \n        \n          \n            \n              \n                \n                Synopsis\n              \n            \n          \n            \n              \n                \n                PDF\n              \n            \n          \n        \n      \n    \n  \n  \n  \n              \n      Aug 14, 2012\n    \n    \n      \n        A standardized mean difference effect size for single case designs\n      \n      \n      Larry V. Hedges, James E. Pustejovsky, William R. Shadish\n      \n      \n        \n          Research Synthesis Methods,\n          3,\n          \n            224-239.\n          \n        \n      \n      \n        \n          \n            \n              \n                \n                Journal\n              \n            \n          \n            \n              \n                \n                R package\n              \n            \n          \n            \n              \n                \n                Web app\n              \n            \n          \n        \n      \n    \n  \n  \n  \n              \n      Oct 1, 2010\n    \n    \n      \n        Organizing for instruction: A comparative study of public, charter, & Catholic schools\n      \n      \n      Lisa Dorner, James P. Spillane, James E. Pustejovsky\n      \n      \n        \n          Journal of Educational Change,\n          12,\n          \n            71-98.\n          \n        \n      \n      \n        \n          \n            \n              \n                \n                Journal\n              \n            \n          \n        \n      \n    \n  \n  \n  \n              \n      Nov 1, 2009\n    \n    \n      \n        Understanding teacher leadership in middle school mathematics: A collaborative research effort\n      \n      \n      James E. Pustejovsky, James P. Spillane, Ruth Heaton, W. Jim Lewis\n      \n      \n        \n          Journal of Mathematics and Science: Collaborative Explorations,\n          11,\n          \n            19-40.\n          \n        \n      \n      \n        \n          \n            \n              \n                \n                Journal\n              \n            \n          \n        \n      \n    \n  \n  \n  \n              \n      Oct 1, 2009\n    \n    \n      \n        Question-order effects in social network name generators\n      \n      \n      James E. Pustejovsky, James P. Spillane\n      \n      \n        \n          Social Networks,\n          31(4),\n          \n            221-229.\n          \n        \n      \n      \n        \n          \n            \n              \n                \n                Journal\n              \n            \n          \n            \n              \n                \n                Pre-Print\n              \n            \n          \n            \n              \n                \n                Data\n              \n            \n          \n        \n      \n    \n  \n  \n  \n              \n      Mar 21, 2009\n    \n    \n      \n        Taking a distributed perspective in studying school leadership and management: The challenge of study operations\n      \n      \n      James P. Spillane, Eric M. Camburn, James E. Pustejovsky, Amber Stitziel-Pareja, Geoff Lewis\n      \n      \n        \n          Distributed Leadership: Different Perspectives,\n          \n          \n            47-80.\n          \n        \n      \n      \n    \n  \n  \n  \n              \n      Mar 21, 2008\n    \n    \n      \n        Taking a distributed perspective: epistemological and methodological tradeoffs in operationalizing the leader-plus aspect\n      \n      \n      James P. Spillane, Eric M. Camburn, James E. Pustejovsky, Amber Stitziel-Pareja, Geoff Lewis\n      \n      \n        \n          Journal of Educational Administration,\n          46(2),\n          \n            189-213.\n          \n        \n      \n      \n        \n          \n            \n              \n                \n                Journal\n              \n            \n          \n        \n      \n    \n  \n  \n  \n\n\nNo matching items"
  },
  {
    "objectID": "people/index.html",
    "href": "people/index.html",
    "title": "People",
    "section": "",
    "text": "Bethany Hamilton Bhat \n              \n            \n\n            \n              Graduate student \n            \n            \n            \n              University of Texas at Austin\n            \n\n            \n          \n      \n      \n    \n  \n     \n    \n    \n      \n      \n      \n      \n        \n          \n          \n            \n          \n          \n          \n            \n            \n              \n              Jingru Zhang \n              \n            \n\n            \n              Graduate student \n            \n            \n            \n              University of Wisconsin-Madison\n            \n\n            \n          \n      \n      \n    \n  \n     \n    \n    \n      \n      \n      \n      \n        \n          \n          \n            \n          \n          \n          \n            \n            \n              \n              Man Chen \n              \n            \n\n            \n              Graduate student \n            \n            \n            \n              University of Wisconsin-Madison\n            \n\n            \n          \n      \n      \n    \n  \n     \n    \n    \n      \n      \n      \n      \n        \n          \n          \n            \n          \n          \n          \n            \n            \n              \n              Melissa A. Rodgers \n              \n            \n\n            \n              Graduate student \n            \n            \n            \n              The University of Texas at Austin\n            \n\n            \n          \n      \n      \n    \n  \n     \n    \n    \n      \n      \n      \n      \n        \n          \n          \n            \n          \n          \n          \n            \n            \n              \n              Paulina Grekov \n              \n            \n\n            \n              Graduate student \n            \n            \n            \n              University of Wisconsin-Madison\n            \n\n            \n          \n      \n      \n    \n  \n    \n  \n\n\n\n\nNo matching items"
  },
  {
    "objectID": "people/index.html#current-students",
    "href": "people/index.html#current-students",
    "title": "People",
    "section": "",
    "text": "Bethany Hamilton Bhat \n              \n            \n\n            \n              Graduate student \n            \n            \n            \n              University of Texas at Austin\n            \n\n            \n          \n      \n      \n    \n  \n     \n    \n    \n      \n      \n      \n      \n        \n          \n          \n            \n          \n          \n          \n            \n            \n              \n              Jingru Zhang \n              \n            \n\n            \n              Graduate student \n            \n            \n            \n              University of Wisconsin-Madison\n            \n\n            \n          \n      \n      \n    \n  \n     \n    \n    \n      \n      \n      \n      \n        \n          \n          \n            \n          \n          \n          \n            \n            \n              \n              Man Chen \n              \n            \n\n            \n              Graduate student \n            \n            \n            \n              University of Wisconsin-Madison\n            \n\n            \n          \n      \n      \n    \n  \n     \n    \n    \n      \n      \n      \n      \n        \n          \n          \n            \n          \n          \n          \n            \n            \n              \n              Melissa A. Rodgers \n              \n            \n\n            \n              Graduate student \n            \n            \n            \n              The University of Texas at Austin\n            \n\n            \n          \n      \n      \n    \n  \n     \n    \n    \n      \n      \n      \n      \n        \n          \n          \n            \n          \n          \n          \n            \n            \n              \n              Paulina Grekov \n              \n            \n\n            \n              Graduate student \n            \n            \n            \n              University of Wisconsin-Madison\n            \n\n            \n          \n      \n      \n    \n  \n    \n  \n\n\n\n\nNo matching items"
  },
  {
    "objectID": "people/index.html#alumni",
    "href": "people/index.html#alumni",
    "title": "People",
    "section": "Alumni",
    "text": "Alumni\n\n\n\n  \n   \n\n     \n    \n    \n      \n      \n      \n      \n        \n          \n          \n            \n          \n          \n          \n            \n            \n              \n              Christopher Runyon \n              \n            \n\n            \n              Measurement Scientist \n            \n            \n            \n              National Board of Medical Examiners\n            \n\n            \n          \n      \n      \n    \n  \n     \n    \n    \n      \n      \n      \n      \n        \n          \n          \n            \n          \n          \n          \n            \n            \n              \n              Daniel M. Swan \n              \n            \n\n            \n              Research Associate \n            \n            \n            \n              Prevention Science Institute, University of Oregon\n            \n\n            \n          \n      \n      \n    \n  \n     \n    \n    \n      \n      \n      \n      \n        \n          \n          \n            \n          \n          \n          \n            \n            \n              \n              Gleb Furman \n              \n            \n\n            \n              Senior Quantitative Research Scientist \n            \n            \n            \n              Gibson Consulting Group\n            \n\n            \n          \n      \n      \n    \n  \n     \n    \n    \n      \n      \n      \n      \n        \n          \n          \n            \n          \n          \n          \n            \n            \n              \n              Megha Joshi \n              \n            \n\n            \n              Quantitative Researcher \n            \n            \n            \n              American Institutes for Research\n            \n\n            \n          \n      \n      \n    \n  \n     \n    \n    \n      \n      \n      \n      \n        \n          \n          \n            \n          \n          \n          \n            \n            \n              \n              Young Ri Lee \n              \n            \n\n            \n              Postdoctoral Scholar \n            \n            \n            \n              Urban Education Institute at the University of Chicago\n            \n\n            \n          \n      \n      \n    \n  \n    \n  \n\n\n\n\nNo matching items"
  },
  {
    "objectID": "software/SingleCaseES/index.html",
    "href": "software/SingleCaseES/index.html",
    "title": "SingleCaseES",
    "section": "",
    "text": "An R package for calculating basic effect size indices for single-case designs, including several non-overlap measures and parametric effect size measures, and for estimating the gradual effects model developed by Swan and Pustejovsky (2017).\n\nAvailable on the Comprehensive R Archive Network\nSource code and installation instructions on Github\nSingle case effect size calculator: An interactive web application for calculating basic effect size indices.\nGradual Effect Model calculator: An interactive web application for estimating effect sizes using the gradual effects model."
  },
  {
    "objectID": "software/scdhlm/index.html",
    "href": "software/scdhlm/index.html",
    "title": "scdhlm",
    "section": "",
    "text": "An R package implementing several methods of estimating a design-comparable standardized mean difference effect size based on data from a single-case design. Methods include those from Hedges, Pustejovsky, & Shadish (2012, 2013) and Pustejovsky, Hedges, & Shadish (2014).\n\nAvailable on the Comprehensive R Archive Network\nInstallation instructions\nSource code on Github\nscdhlm: An interactive web application for calculating design-comparable standardized mean difference effect sizes."
  },
  {
    "objectID": "software/lmeInfo/index.html",
    "href": "software/lmeInfo/index.html",
    "title": "lmeInfo",
    "section": "",
    "text": "lmeInfo provides analytic derivatives and information matrices for fitted linear mixed effects models and generalized least squares models estimated using nlme::lme() and nlme::gls(), respectively. The package includes functions for estimating the sampling variance-covariance of variance component parameters using the inverse Fisher information. The variance components include the parameters of the random effects structure (for lme models), the variance structure, and the correlation structure. The expected and average forms of the Fisher information matrix are used in the calculations, and models estimated by full maximum likelihood or restricted maximum likelihood are supported. The package also includes a function for estimating standardized mean difference effect sizes (Pustejovsky et al., 2014) based on fitted lme or gls models.\n\nR package available on the Comprehensive R Archive Network\nR source code on Github"
  },
  {
    "objectID": "software/clubSandwich/index.html",
    "href": "software/clubSandwich/index.html",
    "title": "clubSandwich",
    "section": "",
    "text": "R and Stata packages for calculating cluster-robust variance estimators (i.e., sandwich estimators) with small-sample corrections, including the bias-reduced linearization estimator of Bell and McCaffrey (2002) and extensions proposed in Tipton (2015), Tipton and Pustejovsky (2015), and Pustejovsky and Tipton (2016).\n\nR package available on the Comprehensive R Archive Network\nR source code on Github\nStata package available on the SSC Archive\nStata source code on Github"
  },
  {
    "objectID": "publication/using-response-ratios/index.html",
    "href": "publication/using-response-ratios/index.html",
    "title": "Using response ratios for meta-analyzing single-case designs with behavioral outcomes",
    "section": "",
    "text": "Methods for meta-analyzing single-case designs (SCDs) are needed to inform evidence-based practice in clinical and school settings and to draw broader and more defensible generalizations in areas where SCDs comprise a large part of the research base. The most widely used outcomes in single-case research are measures of behavior collected using systematic direct observation, which typically take the form of rates or proportions. For studies that use such measures, one simple and intuitive way to quantify effect sizes is in terms of proportionate change from baseline, using an effect size known as the log response ratio. This paper describes methods for estimating log response ratios and combining the estimates using meta-analysis. The methods are based on a simple model for comparing two phases, where the level of the outcome is stable within each phase and the repeated outcome measurements are independent. Although auto-correlation will lead to biased estimates of the sampling variance of the effect size, meta-analysis of response ratios can be conducted with robust variance estimation procedures that remain valid even when sampling variance estimates are biased. The methods are demonstrated using data from a recent meta-analysis on group contingency interventions for student problem behavior."
  },
  {
    "objectID": "publication/Transition-to-College-Mathematics-Year-2/index.html",
    "href": "publication/Transition-to-College-Mathematics-Year-2/index.html",
    "title": "Evaluating the Transition to College Mathematics Course in Texas high schools: Findings from the second year of implementation",
    "section": "",
    "text": "Texas House Bill 5 introduced requirements that school districts partner with institutions of higher education to provide college preparatory courses in mathematics and English for high school seniors who are not yet college ready. As districts and college partners begin to respond to these provisions, there is a need for empirical research on the effects of different approaches to implementing the college preparatory courses. In response to House Bill 5 requirements, the Charles A. Dana Center has developed a model college preparatory mathematics course, Transition to College Mathematics Course (TCMC), which has been adopted by dozens of school districts across Texas over the past several school years. We examine the effects of TCMC on students’ progress into post-secondary education by comparing students who participated in TCMC during the 2017-18 school year (the second year of implementation) to observationally similar students from the same cohort but who did not enroll in the course. We find that students who took TCMC graduated at higher rates than comparison students. They had similar rates of overall enrollment in post-secondary education, but enrolled in community colleges at higher rates and in 4-year colleges or universities at lower rates than did comparison students. Enrollment tended to increase over the course of four semesters after high school graduation. Relative to comparison students, students who took TCMC were also less likely to take and less likely to pass college-level math coursework. These results must be interpreted cautiously because we were unable to fully assess and account for students’ college-readiness status at the start of their senior year."
  },
  {
    "objectID": "publication/Three-level-BC-SMD/index.html",
    "href": "publication/Three-level-BC-SMD/index.html",
    "title": "Between-case standardized mean differences: Flexible methods for single-case designs",
    "section": "",
    "text": "Single-case designs (SCDs) are a class of research methods for evaluating the effects of academic and behavioral interventions in educational and clinical settings. Although visual analysis is typically the first and main method for primary analysis of data from SCDs, quantitative methods are useful for synthesizing results and drawing systematic generalizations across bodies of single-case research. Researchers who are interested in synthesizing findings across SCDs and between-group designs might consider using the between-case standardized mean difference (BC-SMD) effect size, which aims to put results from both types of studies into a common metric. Current BC-SMD methods are limited to treatment reversal design and across-participant multiple baseline design, yet more complex designs are used in practice. In this study, we extend available BC-SMD methods to several variations of the multiple baseline design, including the replicated multiple baseline across behaviors or settings, the clustered multiple baseline design, and the multivariate multiple baseline across participants. For each variation, we describe methods for estimating BC-SMD effect sizes and illustrate our proposed approach by re-analyzing data from a published SCD study."
  },
  {
    "objectID": "publication/TAII-meta-analysis/index.html",
    "href": "publication/TAII-meta-analysis/index.html",
    "title": "A meta-analysis of technology-aided instruction and intervention for students with ASD",
    "section": "",
    "text": "The adoption of methods and strategies validated through rigorous, experimentally oriented research is a core professional value of special education. We conducted a systematic review and meta-analysis examining the experimental literature on Technology-Aided Instruction and Intervention (TAII) using research identified as part of the National Autism Professional Development Project. We applied novel between-case effect size methods to the TAII single-case research base. In addition, we used meta-analytic methodologies to examine the methodological quality of the research, calculate average effect sizes to quantify the level of evidence for TAII, and compare effect sizes across single-case and group-based experimental research. Results identified one category of TAII—computer-assisted instruction—as an evidence-based practice across both single-case and group studies. The remaining two categories of TAII—augmentative and alternative communication and virtual reality—were not identified as evidence-based using What Works Clearinghouse summary ratings."
  },
  {
    "objectID": "publication/SMD-for-SCD/index.html",
    "href": "publication/SMD-for-SCD/index.html",
    "title": "A standardized mean difference effect size for single case designs",
    "section": "",
    "text": "Single case designs are a set of research methods for evaluating treatment effects by assigning different treatments to the same individual and measuring outcomes over time and are used across fields such as behavior analysis, clinical psychology, special education, and medicine. Emerging standards for single case designs have focused attention on the need for effect sizes for summarizing and meta-analyzing findings from the designs; although many effect size measures have been proposed, there is little consensus regarding their use. This article proposes a new effect size measure for single case research that is directly comparable with the standardized mean difference (Cohen’s d) often used in between-subjects designs. Techniques are provided for estimating the new effect size, as well as its variance, from balanced or unbalanced treatment reversal designs. The proposed estimation methods are further evaluated using a simulation study and then demonstrated in two applications."
  },
  {
    "objectID": "publication/Single-case-next-generation-standards/index.html",
    "href": "publication/Single-case-next-generation-standards/index.html",
    "title": "Single case design research in Special Education: Next generation standards and considerations",
    "section": "",
    "text": "Single case design has a long history of use for assessing intervention effectiveness for children with disabilities. Although these designs have been widely employed for more than 50 years, recent years have been especially dynamic in terms of growth in the use of single case design and application of standards designed to improve the validity and applicability of findings. This growth expanded possibilities and inspired new questions about the contributions this methodology can make to generalizable knowledge about intervention in special education. In this paper, we discuss and extend previous standards for studies using single case designs (i.e., Horner et al., 2005). We identify new suggestions for internal validity, generality and acceptability, and reporting. We also provide considerations for single case synthesis and discuss the complexities of assessing accumulating evidence for a given practice."
  },
  {
    "objectID": "publication/school-based-group-contingencies-meta-analysis/index.html",
    "href": "publication/school-based-group-contingencies-meta-analysis/index.html",
    "title": "A meta-analysis of school-based group contingency interventions for students with challenging behavior: An update",
    "section": "",
    "text": "Group contingencies are recognized as a potent intervention for addressing challenging student behavior in the classroom, with research reviews supporting the use of this intervention platform going back more than four decades. Over this time period, the field of education has increasingly emphasized the role of research evidence for informing practice, as reflected in the increased use of systematic reviews and meta-analyses. In the current article, we continue this trend by applying recently developed between-case effect size measures and transparent visual analysis procedures to synthesize an up-to-date set of group contingency studies that used single-case designs. Results corroborated recent systematic reviews by indicating that group contingencies are generally effective—particularly for addressing challenging behavior in general education classrooms. However, our review highlights the need for more research on students with disabilities and the need to collect and report information about participants’ functional level."
  },
  {
    "objectID": "publication/SCD-synthesis-tools-II/index.html",
    "href": "publication/SCD-synthesis-tools-II/index.html",
    "title": "Single-case synthesis tools II: Comparing overlap measures and parametric effect sizes for synthesizing antecedent sensory-based interventions",
    "section": "",
    "text": "Varying methods for evaluating the outcomes of single case research designs (SCD) are currently used in reviews and meta-analyses of interventions. Quantitative effect size measures are often presented alongside visual analysis conclusions. Six measures across two classes—overlap measures (percentage non-overlapping data, improvement rate difference, and Tau) and parametric within-case effect sizes (standardized mean difference and log response ratio [increasing and decreasing])—were compared to determine if choice of synthesis method within and across classes impacts conclusions regarding effectiveness. The effectiveness of sensory-based interventions (SBI), a commonly used class of treatments for young children, was evaluated. Separately from evaluations of rigor and quality, authors evaluated behavior change between baseline and SBI conditions. SBI were unlikely to result in positive behavior change across all measures except IRD. However, subgroup analyses resulted in variable conclusions, indicating that the choice of measures for SCD meta-analyses can impact conclusions. Suggestions for using the log response ratio in SCD meta-analyses and considerations for understanding variability in SCD meta-analysis conclusions are discussed."
  },
  {
    "objectID": "publication/RVE-Meta-analysis-expanding-the-range/index.html",
    "href": "publication/RVE-Meta-analysis-expanding-the-range/index.html",
    "title": "Meta-Analysis with robust variance estimation: Expanding the range of working models",
    "section": "",
    "text": "In prevention science and related fields, large meta-analyses are common, and these analyses often involve dependent effect size estimates. Robust variance estimation (RVE) methods provide a way to include all dependent effect sizes in a single meta-regression model, even when the nature of the dependence is unknown. RVE uses a working model of the dependence structure, but the two currently available working models are limited to each describing a single type of dependence. Drawing on flexible tools from multivariate meta-analysis, this paper describes an expanded range of working models, along with accompanying estimation methods, which offer benefits in terms of better capturing the types of data structures that occur in practice and improving the efficiency of meta-regression estimates. We describe how the methods can be implemented using existing software (the ‘metafor’ and ‘clubSandwich’ packages for R) and illustrate the approach in a meta-analysis of randomized trials examining the effects of brief alcohol interventions for adolescents and young adults."
  },
  {
    "objectID": "publication/RVE-for-meta-regression/index.html",
    "href": "publication/RVE-for-meta-regression/index.html",
    "title": "Small-sample adjustments for tests of moderators and model fit using robust variance estimation in meta-regression",
    "section": "",
    "text": "Meta-analyses often include studies that report multiple effect sizes based on a common pool of subjects or that report effect sizes from several samples that were treated with very similar research protocols. The inclusion of such studies introduces dependence among the effect size estimates. When the number of studies is large, robust variance estimation (RVE) provides a method for pooling dependent effects, even when information on the exact dependence structure is not available. When the number of studies is small or moderate, however, test statistics and confidence intervals based on RVE can have inflated Type I error. This article describes and investigates several small-sample adjustments to F-statistics based on RVE. Simulation results demonstrate that one such test, which approximates the test statistic using Hotelling’s T-squared distribution, is level-α and uniformly more powerful than the others. An empirical application demonstrates how results based on this test compare to the large-sample F-test."
  },
  {
    "objectID": "publication/Religion-spirituality-social-health/index.html",
    "href": "publication/Religion-spirituality-social-health/index.html",
    "title": "A meta-analytic review of religious or spiritual involvement and social health among cancer patients",
    "section": "",
    "text": "Religion and spirituality (R/S) play an important role in the daily lives of many cancer patients. There has been great interest in determining whether R/S factors are related to clinically relevant health outcomes. In this meta-analytic review, the authors examined associations between dimensions of R/S and social health (eg, social roles and relationships). A systematic search of the PubMed, PsycINFO, Cochrane Library, and Cumulative Index to Nursing and Allied Health Literature databases was conducted, and data were extracted by 4 pairs of investigators. Bivariate associations between specific R/S dimensions and social health outcomes were examined in a meta-analysis using a generalized estimating equation approach. In total, 78 independent samples encompassing 14,277 patients were included in the meta-analysis. Social health was significantly associated with overall R/S (Fisher z effect size = .20; P &lt; .001) and with each of the R/S dimensions (affective R/S effect size = 0.31 [P &lt; .001]; cognitive R/S effect size = .10 [P &lt; .01]; behavioral R/S effect size = .08 [P &lt; .05]; and ‘other’ R/S effect size = .13 [P &lt; .001]). Within these dimensions, specific variables tied to socialhealth included spiritual well being, spiritual struggle, images of God, R/S beliefs, and composite R/S measures (all P values &lt; .05). None of the demographic or clinical moderating variables examined were significant. Results suggest that several R/S dimensions are modestly associated with patients’ capacity to maintain satisfying social roles and relationships in the context of cancer. Further research is needed to examine the temporal nature of these associations and the mechanisms that underlie them."
  },
  {
    "objectID": "publication/Religion-spirituality-mental-health/index.html",
    "href": "publication/Religion-spirituality-mental-health/index.html",
    "title": "A meta-analytic approach to examine the relationship between religion/spirituality and mental health in cancer",
    "section": "",
    "text": "Religion and spirituality (R/S) are patient-centered factors and often are resources for managing the emotional sequelae of the cancer experience. Studies investigating the correlation between R/S (eg, beliefs, experiences, coping) and mental health (eg, depression, anxiety, well being) in cancer have used very heterogeneous measures and have produced correspondingly inconsistent results. A meaningful synthesis of these findings has been lacking; thus, the objective of this review was to conduct a meta-analysis of the research on R/S and mental health. Four electronic databases were systematically reviewed, and 2073 abstracts met initial selection criteria. Reviewer pairs applied standardized coding schemes to extract indices of the correlation between R/S and mental health. In total, 617 effect sizes from 148 eligible studies were synthesized using meta-analytic generalized estimating equations, and subgroup analyses were performed to examine moderators of effects. The estimated mean correlation (Fisher z) was 0.19 (95% confidence interval [CI], 0.16-0.23), which varied as a function of R/S dimensions: affective R/S (z=0.38; 95% CI, 0.33-0.43), behavioral R/S (z=0.03; 95% CI, 20.02-0.08), cognitive R/S (z=0.10; 95% CI, 0.06-0.14), and ‘other’ R/S (z=0.08; 95% CI, 0.03-0.13). Aggregate, study-level demographic and clinical factors were not predictive of the relation between R/S and mental health. There was little indication of publication or reporting biases. The correlation between R/S and mental health generally was positive. The strength of that correlation was modest and varied as a function of the R/S dimensions and mental health domains assessed. The identification of optimal R/S measures and more sophisticated methodological approaches are needed to advance research."
  },
  {
    "objectID": "publication/Question-order-effects/index.html",
    "href": "publication/Question-order-effects/index.html",
    "title": "Question-order effects in social network name generators",
    "section": "",
    "text": "Social network surveys are an important tool for empirical research in a variety of fields, including the study of social capital and the evaluation of educational and social policy. A growing body of methodological research sheds light on the validity and reliability of social network survey data regarding a single relation, but much less attention has been paid to the measurement of multiplex networks and the validity of comparisons among criterion relations. In this paper, we identify ways that surveys designed to collect multiplex social network data might be vulnerable to question-order effects. We then test several hypotheses using a split-ballot experiment embedded in an online multiple name generator survey of teachers’ advice networks, collected for a study of complete networks. We conclude by discussing implications for the design of multiple name generator social network surveys."
  },
  {
    "objectID": "publication/psychosocial-interventions-for-spiritual-well-being/index.html",
    "href": "publication/psychosocial-interventions-for-spiritual-well-being/index.html",
    "title": "A systematic review and meta-analysis of effects of psychosocial interventions on spiritual well-being in adults with cancer",
    "section": "",
    "text": "Objective Spiritual well‐being (SpWb) is an important dimension of health‐related quality of life for many cancer patients. Accordingly, an increasing number of psychosocial intervention studies have included SpWb as a study endpoint, and may improve SpWb even if not designed explicitly to do so. This meta‐analysis of randomized controlled trials (RCTs) evaluated effects of psychosocial interventions on SpWb in adults with cancer and tested potential moderators of intervention effects. Methods Six literature databases were systematically searched to identify RCTs of psychosocial interventions in which SpWb was an outcome. Doctoral‐level rater pairs extracted data using Covidence following Preferred Reporting Items for Systematic reviews and Meta‐Analyses guidelines. Standard meta‐analytic techniques were applied, including meta‐regression with robust variance estimation and risk‐of‐bias sensitivity analysis. Results Forty‐one RCTs were identified, encompassing 88 treatment effects among 3883 survivors. Interventions were associated with significant improvements in SpWb (\\(g = 0.22\\), 95% CI [0.14, 0.29], \\(p &lt; 0.0001\\)). Studies assessing the FACIT‐Sp demonstrated larger effect sizes than did those using other measures of SpWb (\\(g = 0.25\\), 95% CI [0.17, 0.34], vs. \\(g = 0.10\\), 95% CI [−0.02, 0.23], \\(p = 0.03\\)). No other intervention, clinical, or demographic characteristics significantly moderated effect size. Conclusions Psychosocial interventions are associated with small‐to‐medium‐sized effects on SpWb among cancer survivors. Future research should focus on conceptually coherent interventions explicitly targeting SpWb and evaluate interventions in samples that are diverse with respect to race and ethnicity, sex and cancer type."
  },
  {
    "objectID": "publication/procedural-sensitivities-of-SCD-effect-sizes/index.html",
    "href": "publication/procedural-sensitivities-of-SCD-effect-sizes/index.html",
    "title": "Procedural sensitivities of effect sizes for single-case designs with behavioral outcome measures",
    "section": "",
    "text": "A wide variety of effect size indices have been proposed for quantifying the magnitude of treatment effects in single-case designs. Commonly used measures include parametric indices such as the standardized mean difference, as well as non-overlap measures such as the percentage of non-overlapping data, improvement rate difference, and non-overlap of all pairs. Currently, little is known about the properties of these indices when applied to behavioral data collected by systematic direct observation, even though systematic direct observation is the most common method for outcome measurement in single-case research. This study uses Monte Carlo simulation to investigate the properties of several widely used single-case effect size measures when applied to systematic direct observation data. Results indicate that the magnitude of the non-overlap measures and of the standardized mean difference can be strongly influenced by procedural details of the study’s design, which is a significant limitation to using these indices as effect sizes for meta-analysis of single-case designs. A less widely used parametric index, the log-response ratio, has the advantage of being insensitive to sample size and observation session length, although its magnitude is influenced by the use of partial interval recording."
  },
  {
    "objectID": "publication/Operationally-comparable-effect-sizes/index.html",
    "href": "publication/Operationally-comparable-effect-sizes/index.html",
    "title": "Operationally comparable effect sizes for meta-analysis of single-case research",
    "section": "",
    "text": "This thesis studies quantitative methods for summarizing and synthesizing single-case studies, a class of research designs for evaluating the effects of interventions through repeated measurement of individuals. Despite long-standing interest in meta-analytic synthesis of single-case research, there remains a lack of consensus about appropriate methods, even about the most basic question of what effect size metrics are useful and appropriate. I argue that operational comparability, or invariance to heterogeneous operational procedures, is crucial property for an effect size metric. I then consider two problems with operational comparability that arise in single-case research. The first problem is to find effect sizes that can be applied across studies that use different research designs, such as single-case designs and two-group randomized experiments. The second problem is to find effect sizes that can be applied across studies that use varied operations for measuring the same construct. To address each of these problems, I propose structural models that capture essential features of multiple relevant operations (either design-related operations or measurement-related operations). I then use these structural models to precisely define target effect size parameters and to consider identification issues and estimation strategies.\nChapter 1 defines operational comparability and situates the concept within the broad methodological concerns of meta-analysis, then reviews relevant features of single-case research and previously proposed effect sizes. Chapter 2 describes an abstract set of modeling criteria for constructing design-comparable effect sizes. Chapters 3 applies the general criteria to the case of standardized mean differences and proposes an effect size estimator based on restricted maximum likelihood. Chapter 4 presents several applications of the proposed models and methods. Chapter 5 proposes measurement-comparability model and defines effect size measures for use in studies of free-operant behavior, one of the most common classes of outcomes in single-case research. Chapter 6 extends the proposed effect size models to incorporate more complex features, including time trends and serial dependence, and studies a method of estimating those models through a combination of marginal quasi-likelihood and Gaussian pseudo-likelihood estimating equations. Chapter 7 collects various further extensions, areas for further research, and concluding thoughts."
  },
  {
    "objectID": "publication/measurement-procedures-and-baseline-outcomes/index.html",
    "href": "publication/measurement-procedures-and-baseline-outcomes/index.html",
    "title": "An examination of measurement procedures and characteristics of baseline outcome data in single-case research",
    "section": "",
    "text": "There has been growing interest in using statistical methods to analyze data and estimate effect size indices from studies that use single-case designs (SCDs), as a complement to traditional visual inspection methods. The validity of a statistical method rests on whether its assumptions are plausible representations of the process by which the data were collected, yet there is evidence that some assumptions—particularly regarding normality of error distributions—may be inappropriate for single-case data. To develop more appropriate modelling assumptions and statistical methods, researchers must attend to the features of real SCD data. In this study, we examine several features of SCDs with behavioral outcome measures in order to inform development of statistical methods. Drawing on a corpus of over 300 studies, including approximately 1800 cases, from seven systematic reviews that cover a range of interventions and outcome constructs, we report the distribution of study designs, distribution of outcome measurement procedures, and features of baseline outcome data distributions for the most common types of measurements used in single-case research. We discuss implications for the development of more realistic assumptions regarding outcome distributions in SCD studies, as well as the design of Monte Carlo simulation studies evaluating the performance of statistical analysis techniques for SCED data."
  },
  {
    "objectID": "publication/investigating-narrative-performance/index.html",
    "href": "publication/investigating-narrative-performance/index.html",
    "title": "Investigating narrative performance in children with developmental language disorder: A systematic review and meta-analysis",
    "section": "",
    "text": "Purpose: Speech-language pathologists (SLPs) typically examine narrative performance when completing a comprehensive language assessment. However, there is significant variability in the methodologies used to evaluate narration. The primary aims of this systematic review and meta-analysis were to a) investigate how narrative assessment type (e.g., macrostructure, microstructure, internal state language) differentiates typically developing (TD) children from children with developmental language disorder (DLD), or, TD–DLD group differences, b) identify specific narrative assessment measures (e.g., number of different words) that result in greater TD–DLD differences, and, c) evaluate participant and sample characteristics (e.g., DLD inclusionary criteria) that may uniquely influence performance differences. Method: Three electronic databases (PsychInfo, ERIC, and PubMed) and ASHAWire were searched on July 30, 2019 to locate studies that reported oral narrative language measures for both DLD and TD groups between ages 4 and 12 years; studies focusing on written narration or other developmental disorders only were excluded. Thirty-seven primary studies were identified via a three-step study selection procedure. We extracted data related to the sample participants, the narrative task(s) and assessment measures, and research design. Standardized mean differences using a bias-corrected Hedges’ \\(g\\) were the calculated effect sizes (\\(N = 382\\)). Research questions were analyzed using mixed-effects meta-regression with robust variance estimation to account for effect size dependencies. Results: Searches identified eligible studies published between 1987 and 2019. An overall meta-analysis using 382 effect sizes obtained across 37 studies showed that children with DLD had decreased narrative performance relative to TD peers, with summary estimates ranging from -0.850, 95% CI [-1.016, -0.685] to -0.794, 95% CI [-0.963, -0.624], depending on the correlation assumed. Across all models, effect size estimates showed significant heterogeneity both between and within studies, even after accounting for effect size-, sample-, and study-level predictors. Grammatical accuracy (microstructure) and story grammar (macrostructure) yielded the most consistent evidence of significant TD–DLD group differences across statistical models. Conclusions: Present findings suggest some narrative assessment measures may yield significantly different performance between children with and without DLD. However, researchers need to be consistent in their inclusionary criteria, their description of sample characteristics, and in their reporting of the correlations of measures, in order to determine which assessment measures are more likely to yield group differences."
  },
  {
    "objectID": "publication/gradual-effects-model/index.html",
    "href": "publication/gradual-effects-model/index.html",
    "title": "A gradual effects model for single-case designs",
    "section": "",
    "text": "Single-case designs are a class of repeated measures experiments used to evaluate the effects of interventions for specialized populations, such as individuals with low-incidence disabilities. There has been growing interest in systematic reviews and syntheses of evidence from single-case designs, but there remains a need to further develop appropriate statistical models and effect sizes for data from the designs. We propose a novel model for single-case data that exhibit non-linear time trends created by an intervention that produces gradual effects, which build up and dissipate over time. The model expresses a structural relationship between a pattern of treatment assignment and an outcome variable, making it appropriate for both treatment reversal and multiple baseline designs. It is formulated as a generalized linear model so that it can be applied to outcomes measured as frequency counts or proportions, both of which are commonly used in single-case research, while providing readily interpretable effect size estimates such as log response ratios or log odds ratios. We demonstrate the gradual effects model by applying it to data from a single-case study and examine the performance of proposed estimation methods in a Monte Carlo simulation of frequency count data."
  },
  {
    "objectID": "publication/FABI-meta-analysis/index.html",
    "href": "publication/FABI-meta-analysis/index.html",
    "title": "Functional assessment-based interventions for students with or at-risk for high incidence disabilities: Field-testing single-case synthesis methods",
    "section": "",
    "text": "This systematic review investigated one systematic approach to designing, implementing, and evaluating functional assessment–based interventions (FABI) for use in supporting school-age students with or at-risk for high-incidence disabilities. We field tested several recently developed methods for single-case design syntheses. First, we appraised the quality of individual studies and the overall body of work using Council for Exceptional Children’s standards. Next, we calculated and meta-analyzed within-case and between-case effect sizes. Results indicated that studies were of high methodological quality, with nine studies identified as being methodologically sound and demonstrating positive outcomes across 14 participants. However, insufficient evidence was available to classify the evidence base for FABIs due to small number of participants within (fewer than recommended three) and across (fewer than recommended 20) studies. Nonetheless, average within-case effect sizes were equivalent to increases of 118% between baseline and intervention phases. Finally, potential moderating variables were examined. Limitations and future directions are discussed."
  },
  {
    "objectID": "publication/Efficacy-of-combining-cognitive-training-and-NIBS/index.html",
    "href": "publication/Efficacy-of-combining-cognitive-training-and-NIBS/index.html",
    "title": "The efficacy of combining cognitive training and non-invasive brain stimulation: A transdiagnostic systematic review and meta-analysis",
    "section": "",
    "text": "Over the past decade, an increasing number of studies investigated the innovative approach of supplementing cognitive training (CT) with non-invasive brain stimulation (NIBS) to increase the effects on outcomes. In this review, we aim to summarize the evidence for this treatment combination. We identified 72 published and unpublished studies (reporting 773 effect sizes) including 2518 participants from healthy and clinical populations indexed in PubMed, Medline, PsycINFO, ProQuest, Web of Science, and ClinicalTrials.gov (last search: 8/8/2022) that compared the effects of NIBS combined with CT on cognitive, symptoms and everyday functioning to CT alone at post-intervention and/or follow-up. We performed random-effects meta-analyses with robust variance estimation and assessed risk of bias with the Cochrane R.O.B. tool. Only four studies had low risk of bias in all domains, and many studies lacked standard controls such as keeping the outcome assessor and trainer unaware of the treatment condition. Following sensitivity analyses, only learning/memory robustly improved significantly more when CT was combined with NIBS compared to CT only (\\(g\\) = 0.18, 95% CI [0.07, 0.29]) at post-intervention, but not in the long-term. The effect was small and limited by substantial heterogeneity. The other seven cognitive outcome domains, symptoms, and everyday functioning did not benefit from adding NIBS to CT. Given the methodological limitation of prior studies, more high-quality trials that focus on the potential of combining NIBS and CT to enhance benefits in everyday functioning in the short- and long-term are needed to evaluate whether combining NIBS and CT is relevant for clinical practice."
  },
  {
    "objectID": "publication/Design-comparable-effect-sizes/index.html",
    "href": "publication/Design-comparable-effect-sizes/index.html",
    "title": "Design-comparable effect sizes in multiple baseline designs: A general modeling framework",
    "section": "",
    "text": "In single-case research, the multiple baseline design is a widely used approach for evaluating the effects of interventions on individuals. Multiple baseline designs involve repeated measurement of outcomes over time and the controlled introduction of a treatment at different times for different individuals. This article outlines a general framework for defining effect sizes in multiple baseline designs that are directly comparable to the standardized mean difference from a between-subjects randomized experiment. The target, design-comparable effect size parameter can be estimated using restricted maximum likelihood together with a small sample correction analogous to Hedges’s g. The approach is demonstrated using hierarchical linear models that include baseline time trends and treatment-by-time interactions. A simulation compares the performance of the proposed estimator to that of an alternative, and an application illustrates the model-fitting process."
  },
  {
    "objectID": "publication/Current-practices-in-meta-regression/index.html",
    "href": "publication/Current-practices-in-meta-regression/index.html",
    "title": "Current practices in meta-regression in psychology, education, and medicine",
    "section": "",
    "text": "Having surveyed the history and methods of meta‐regression in a previous paper,1 in this paper we review which and how meta‐regression methods are applied in recent research syntheses. To do so, we reviewed studies published in 2016 across four leading research synthesis journals: Psychological Bulletin, the Journal of Applied Psychology, Review of Education Research, and the Cochrane Library. We find that the best practices defined in the previous review are rarely carried out in practice. In light of the identified discrepancies, we consider how to move forward, first by identifying areas where further methods development is needed to address persistent problems in the field, and second by discussing how to more effectively disseminate points of methodological consensus."
  },
  {
    "objectID": "publication/Conducting-POMADE/index.html",
    "href": "publication/Conducting-POMADE/index.html",
    "title": "Conducting power analysis for meta-analysis of dependent effect sizes: Common guidelines and an introduction to the POMADE R package",
    "section": "",
    "text": "Sample size and statistical power are important factors to consider when planning a research synthesis. Power analysis methods have been developed for fixed effect or random effects models, but until recently these methods were limited to simple data structures with a single, independent effect per study. Recent work has provided power approximation formulas for meta-analyses involving studies with multiple, dependent effect size estimates, which are common in syntheses of social science research. Prior work focused on developing and validating the approximations, but did not address the practice challenges encountered in applying them for purposes of planning a synthesis involving dependent effect sizes. We aim to facilitate application of these recent developments by providing practical guidance on how to conduct power analysis for planning a meta-analysis of dependent effect sizes and by introducing a new R package, POMADE, designed for this purpose. We present a comprehensive overview of resources for finding information about the study design features and model parameters needed to conduct power analysis, along with detailed worked examples using the POMADE package. For presenting power analysis findings, we emphasize graphical tools that can depict power under a range of pausible assumptions and introduce a novel plot, the traffic light power plot, for conveying the degree of certainty in one’s assumptions."
  },
  {
    "objectID": "publication/cluster-wild-bootstrap-for-meta-analysis/index.html",
    "href": "publication/cluster-wild-bootstrap-for-meta-analysis/index.html",
    "title": "Cluster wild bootstrapping to handle dependent effect sizes in meta-analysis with a small number of studies",
    "section": "",
    "text": "The most common and well-known meta-regression models work under the assumption that there is only one effect size estimate per study and that the estimates are independent. However, meta-analytic reviews of social science research often include multiple effect size estimates per primary study, leading to dependence in the estimates. Some meta-analyses also include multiple studies conducted by the same lab or investigator, creating another potential source of dependence. An increasingly popular method to handle dependence is robust variance estimation (RVE), but this method can result in inflated Type I error rates when the number of studies is small. Small-sample correction methods for RVE have been shown to control Type I error rates adequately but may be overly conservative, especially for tests of multiple-contrast hypotheses. We evaluated an alternative method for handling dependence, cluster wild bootstrapping, which has been examined in the econometrics literature but not in the context of meta-analysis. Results from two simulation studies indicate that cluster wild bootstrapping maintains adequate Type I error rates and provides more power than extant small sample correction methods, particularly for multiple-contrast hypothesis tests. We recommend using cluster wild bootstrapping to conduct hypothesis tests for meta-analyses with a small number of studies. We have also created an R package that implements such tests."
  },
  {
    "objectID": "publication/BCSMD-examination-of-two-methods/index.html",
    "href": "publication/BCSMD-examination-of-two-methods/index.html",
    "title": "Between-case standardized effect size analysis of single case design: Examination of the two methods",
    "section": "",
    "text": "An increasing movement in single case research is to employ statistical analyses as one form of data analysis. Researchers have proposed different statistical approaches. The purpose of this paper is to examine the utility and discriminant validity of two novel types of between-case standardized effect size analyses with two existing systematic reviews. The between-case analyses found greater effect sizes for the studies in the object play review and smaller effect sizes for studies of sensory intervention, which were consistent with the overall conclusions reached in the original systematic reviews. These findings provide evidence of discriminant validity, although concerns remain around the methods’ utility across different single case research designs. Future directions for research and development also are provided."
  },
  {
    "objectID": "publication/BC-SMD-primer-and-applications/index.html",
    "href": "publication/BC-SMD-primer-and-applications/index.html",
    "title": "Analysis and meta-analysis of single-case designs with a standardized mean difference statistic: A primer and applications",
    "section": "",
    "text": "This article presents a d-statistic for single-case designs that is in the same metric as the d-statistic used in between-subjects designs such as randomized experiments and offers some reasons why such a statistic would be useful in SCD research. The d has a formal statistical development, is accompanied by appropriate power analyses, and can be estimated using user-friendly SPSS macros. We discuss both advantages and disadvantages of d compared to other approaches such as previous d-statistics, overlap statistics, and multilevel modeling. It requires at least three cases for computation and assumes normally distributed outcomes and stationarity, assumptions that are discussed in some detail. We also show how to test these assumptions. The core of the article then demonstrates in depth how to compute d for one study, including estimation of the autocorrelation and the ratio of between case variance to total variance (between case plus within case variance), how to compute power using a macro, and how to use the d to conduct a meta-analysis of studies using single-case designs in the free program R, including syntax in an appendix. This syntax includes how to read data, compute fixed and random effect average effect sizes, prepare a forest plot and a cumulative meta-analysis, estimate various influence statistics to identify studies contributing to heterogeneity and effect size, and do various kinds of publication bias analyses. This d may prove useful for both the analysis and meta-analysis of data from SCDs."
  },
  {
    "objectID": "publication/analyzing-SCD-hopes-and-fears/index.html",
    "href": "publication/analyzing-SCD-hopes-and-fears/index.html",
    "title": "Analyzing single-case designs: d, G, hierarchical models, Bayesian estimators, generalized additive models, and the hopes and fears of researchers about analyses",
    "section": "",
    "text": "New approaches to the analyses of single-case designs are proliferating, which some single-case design researchers welcome and others view with skepticism. In this chapter we describe some of the analyses that we have been exploring, all of which can be conceptualized as versions of hierarchical models as a unifying framework. The approaches include a d-statistic for the (AB)k design that estimates the same parameter as the usual between-groups d-statistic, Bayesian approaches to the same and similar models, hierarchical generalized linear models that model outcomes as binomial or Poisson rather than the usual assumptions of normality, and semiparametric generalized additive models that allow diagnosis of trend and linearity. Throughout, we illustrate the analyses using a common example and show how the different analyses provide different insights into the data. We conclude with a discussion of potential criticisms and skepticism expressed by some researchers about such analyses, along with reasons why the field is increasingly likely to develop and use such analyses despite the criticisms."
  },
  {
    "objectID": "publication/AAC-instructional-contexts/index.html",
    "href": "publication/AAC-instructional-contexts/index.html",
    "title": "Considering instructional contexts in AAC interventions for people with ASD and/or IDD experiencing complex communication needs: A single-case design meta-analysis",
    "section": "",
    "text": "For children with autism or intellectual and developmental disabilities who also have complex communication needs, communication is a necessary skill set to increase independence and quality of life. Understanding the how, where, and communication style being taught is important for identifying deficits in the field as well as which interventions are most effective. This meta-analysis sought to identify effectiveness among different settings, behavioral strategies, and moderator variables. A systematic search and screening process identified 114 eligible studies with 330 participants; overall outcomes indicate that augmentative and alternative communication interventions were effective with Tau effects ranging from 0.53 to 1.03 and log response ratio effects ranging from 0.21 to 2.90. However, no instructional context variables systematically predicted differences in intervention effectiveness."
  },
  {
    "objectID": "publication/AAC-communication-outcomes/index.html",
    "href": "publication/AAC-communication-outcomes/index.html",
    "title": "Augmentative and Alternative Communication intervention targets for school-aged participants with ASD and ID: A single-case systematic review and meta-analysis",
    "section": "",
    "text": "Objective: This meta-analysis reviews the literature on communication modes, communicative functions, and types of augmentative and alternative communication (AAC) interventions for school-age participants with autism spectrum disorders and/or intellectual disabilities who experience complex communication needs. Considering potential differences related to outcomes that were targeted for intervention could help identify the most effective means of individualizing AAC interventions. Methods: We performed a systematic literature search using Academic Search Ultimate, ERIC, PsycINFO, Web of Science, and Proquest Dissertations & Theses Global to retrieve research conducted between 1978 and the beginning of 2020. Studies included in the synthesis are (a) in English; (b) has one or more participants with an intellectual delay, developmental disability(ies); (c) reported the results of an augmentative and alternative communication (AAC) intervention to supplement or replace conventional speech for people with complex communication needs; (d) was a SCED; (e) measured social-communicative outcomes. We synthesized results across studies using multi-level meta-analyses of two case-level effect size metrics, Tau and log response ratio. We conducted moderator analyses using meta-regression with robust variance estimation. Results: Across 114 included studies with 330 participants and 767 effect size, overall Tau effects were moderate, Tau = 0.72, 95% CI [0.67, 0.77], and heterogeneous. For the subset of data series where log response ratio could be estimated, the overall average effect was LRR = 1.86, 95% CI [1.58, 2.13], and effects were highly heterogeneous. There were few statistically significant differences found between moderator categories, which included communication mode, communicative function, and type of AAC implemented. Conclusions: This meta-analysis highlights the potential differences related to outcomes that were targeted for AAC interventions for individuals with ASD and IDD. AAC intervention has been shown to improve communication outcomes in this population. However, there was a lack of sufficient data to analyze for some potential moderators such as insufficient descriptive information on participant characteristics. This is likely due to the heterogeneity of the participants and implementation factors; however, these factors were frequently underreported by original study authors which disallowed systematic analysis. That said, there is a need for more detailed participant characteristic descriptions in original research reports to support future aggregation across the literature. Sponsorship: We received funding for the review from the Institute of Education Sciences. Protocol: The review protocol was registered in the PROSPERO system (CRD42018112428)."
  },
  {
    "objectID": "presentations/SREE-2023-Equity-related-moderator-analysis.html",
    "href": "presentations/SREE-2023-Equity-related-moderator-analysis.html",
    "title": "Equity-related moderator analysis in syntheses of dependent effect sizes: Conceptual and statistical considerations",
    "section": "",
    "text": "Background/Context\nIn meta-analyses examining educational interventions, researchers seek to understand the distribution of intervention impacts, in order to draw generalizations about what works, for whom, and under what conditions. One common way to examine equity implications in such reviews is through moderator analysis, which involves modeling how intervention effect sizes vary depending on the characteristics of primary study participants. For example, one might estimate associations between effect size and the percentage of the primary study participants who were from a rural school, from a low-income family, identified as a specific racial or ethnic group, or designated as an English Language Learner. Such moderator analyses can provide insights about the populations and contexts where an intervention is more or less effective—that is, they can address questions of who benefits and how the effects of an educational intervention are distributed.\nMeta-analyses of educational interventions often include primary studies that report multiple relevant effect size estimates, such for more than one measure of an outcome construct, at multiple time-points, for multiple versions of an intervention, or for different sub-groups of participants. This leads to a data structure where the effects from a given study are correlated, necessitating the use of statistical methods that are appropriate for dependent observations. Methodological research in this area has provided estimation and inference methods that which can handle dependent effect sizes, including multi-level meta-analyses (Van den Noortgate et al., 2013, 2015), robust variance estimation (Hedges et al., 2010), and combinations thereof (Fernández-Castilla et al., 2020; Pustejovsky & Tipton, 2022). However, there has been much less attention to the specific forms of moderator analysis that are of interest in practice.\n\n\nPurpose/Objective/Research Question\nWe aim to identify conceptual and statistical considerations for moderator analysis of equity-related variables in meta-analyses involving dependent effect sizes. Specifically, we distinguish between direct evidence and contextual evidence about equity of impacts and show that the choice of meta-analytic model can be consequential for analyses involving direct evidence. We then examine how meta-analysts currently conduct equity-related moderator analyses, by reviewing completed research synthesis projects funded by the Institute of Education Sciences (IES) over the period of 2002 to 2018. We find that most projects do not distinguish between direct and contextual evidence and use analytic approaches that are inefficient for synthesizing direct evidence. Conceptual Considerations\nModerator analyses of equity-related variables can be carried out by regressing effect size estimates on predictors encoding participant characteristics. Consider a synthesis in which some primary studies contribute multiple effect sizes. In this data structure, a predictor might represent a study-level characteristic or one that varies across the effects within a given study. The level of variation is especially salient for analysis of equity-related variables because study-level characteristics and effect-level characteristics represent qualitatively different types of evidence. For study-level predictors, associations with effect size pertain to the study’s context and are not necessarily indicative of individual-level variation in impacts. Thus, interpretation is challenging because studies vary in many ways, with many possible sources of confounding. For effect-level predictors, within-study variation represents direct evidence about individual-level moderation (e.g., a comparison of impacts between low-income and higher-income participants in the same study), unconfounded by study-level characteristics.\nWe describe different strategies for separately investing direct and contextual evidence about moderation, including a) decomposing the predictor into study-level average and within-study centered components or b) inclusion of the raw predictor and the study-level average in a meta-regression. Although strategy (a) has been recommended previously in the context of meta-analysis of dependent effects (Tanner-Smith & Tipton, 2014), our presentation makes explicit the connection to equity-related moderator analysis and specifies the data requirements for applying it.\n\n\nStatistical Considerations\nMeta-regression with dependent effect sizes involves choosing a working model for the dependence structure, which determines the set of weights used for estimating the meta-regression. Several different working models have been proposed, including correlated effects and hierarchical effects models (Hedges et al., 2010), a correlated-and-hierarchical effects model (Pustejovsky & Tipton, 2022) and the multi-level meta-analysis model (Van den Noortgate et al., 2013, 2015). Ad hoc strategies, such as aggregating effects to the study level or ignoring dependence, can also be understood as working models.\nPrevious research and guidance about the choice of working model has argued that the choice of working model is fairly inconsequential so long as the working model is roughly similar the true dependence structure (Hedges et al., 2010; Tanner-Smith et al., 2016; Tanner-Smith & Tipton, 2014). In the appendix, we examine the exact weights assigned by a variety of different working models to studies with direct evidence and contextual evidence. Contrary to past guidance, we find that different working models can lead to quite different weighting—particularly for direct evidence (i.e., study mean-centered predictors).\n\n\nCurrent Practice\nTo understand current practices for analysis of equity-related moderator variables, we reviewed completed meta-analysis projects funded by IES over the period of 2002 to 2018. We identified grants that (a) had journal articles reporting a meta-analysis, (b) were not methodological, and (c) were not training programs. A search of the IES website for project descriptions that included the word “meta-analysis” returned 80 results, of which 25 met inclusion criteria. Table 1 summarizes the approaches to moderator analyses used in these projects. Most projects reported some form of meta-regression analysis, but very few described a centering strategy and only one project used study-mean centering. Notably, the correlated effects and hierarchical effects working models were commonly used, yet these models involve inefficient weighting of direct evidence.\n\n\nConclusions\nIn light of this review of current practice, the conceptual and statistical considerations that we describe suggest that there is substantial room for improvement in how meta-analysts conduct moderation analysis, particularly for equity-related variables where individual-level variation is of primary interest. Even under this simple—simplistic, even—conception of equity, bringing systematic review and meta-analysis methods to bear to address inequities in the education system will require not only improving analytic practices, but also changing how primary investigations frame questions, collect data, and report findings."
  },
  {
    "objectID": "presentations/AERA-2019-response-guided-algorithms.html",
    "href": "presentations/AERA-2019-response-guided-algorithms.html",
    "title": "The impact of response-guided designs on count outcomes in single-case design baselines",
    "section": "",
    "text": "American Educational Research Association annual convention, Toronto, Ontario\n PDF"
  },
  {
    "objectID": "presentations/ABAI-2013-effect-sizes.html",
    "href": "presentations/ABAI-2013-effect-sizes.html",
    "title": "Effect sizes and measurement comparability for meta-analysis of single-case research",
    "section": "",
    "text": "ReuseCC BY 4.0CitationBibTeX citation:@online{pustejovsky2013,\n  author = {Pustejovsky, James E.},\n  title = {Effect Sizes and Measurement Comparability for Meta-Analysis\n    of Single-Case Research},\n  date = {2013-05-28},\n  url = {https://jepusto.com/presentations/ABAI-2013-effect-sizes.html},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nPustejovsky, James E. 2013. “Effect Sizes and Measurement\nComparability for Meta-Analysis of Single-Case Research.” May 28,\n2013. https://jepusto.com/presentations/ABAI-2013-effect-sizes.html."
  },
  {
    "objectID": "people/Young-Ri-Lee/index.html",
    "href": "people/Young-Ri-Lee/index.html",
    "title": "Young Ri Lee",
    "section": "",
    "text": "Young Ri completed her PhD in the Quantitative Methods program at The University of Texas at Austin. She is currently a postdoctoral scholar in Developmental and Learning Sciences at the Urban Education Institute of the University of Chicago. Her research interests include Hierarchical Linear Modeling (HLM), Robust Variance Estimation methods (RVE), and meta-analysis.\n\n\n\n\nPhD in Quantitative Methods | 2023  University of Texas at Austin\nMA in Educational Measurement and Statistics | 2017  Korea University\nBA in Education | 2014  Korea University\n\n\n\n\n\nHierarchical Linear Modeling\nRobust Variance Estimation methods\nMeta-analysis"
  },
  {
    "objectID": "people/Young-Ri-Lee/index.html#education",
    "href": "people/Young-Ri-Lee/index.html#education",
    "title": "Young Ri Lee",
    "section": "",
    "text": "PhD in Quantitative Methods | 2023  University of Texas at Austin\nMA in Educational Measurement and Statistics | 2017  Korea University\nBA in Education | 2014  Korea University"
  },
  {
    "objectID": "people/Young-Ri-Lee/index.html#interests",
    "href": "people/Young-Ri-Lee/index.html#interests",
    "title": "Young Ri Lee",
    "section": "",
    "text": "Hierarchical Linear Modeling\nRobust Variance Estimation methods\nMeta-analysis"
  },
  {
    "objectID": "people/Megha-Joshi/index.html",
    "href": "people/Megha-Joshi/index.html",
    "title": "Megha Joshi",
    "section": "",
    "text": "Megha completed her PhD in the Quantitative Methods program at The University of Texas at Austin. Her dissertation work examined cluster wild bootstrapping to handle dependent effect sizes in meta-analyses with a small number of studies. She now works as quantitative researcher at the American Institutes for Research. Her research interests include causal inference, meta-analysis, and missing data analysis.\n\n\n\n\nPhD in Quantitative Methods | 2021  The University of Texas at Austin\nBA in Psychology and Art History | 2014  Bryn Mawr College\n\n\n\n\n\nCausal inference\nMeta-analysis\nMachine learning\nMissing data analysis\nRobust variance estimation"
  },
  {
    "objectID": "people/Megha-Joshi/index.html#education",
    "href": "people/Megha-Joshi/index.html#education",
    "title": "Megha Joshi",
    "section": "",
    "text": "PhD in Quantitative Methods | 2021  The University of Texas at Austin\nBA in Psychology and Art History | 2014  Bryn Mawr College"
  },
  {
    "objectID": "people/Megha-Joshi/index.html#interests",
    "href": "people/Megha-Joshi/index.html#interests",
    "title": "Megha Joshi",
    "section": "",
    "text": "Causal inference\nMeta-analysis\nMachine learning\nMissing data analysis\nRobust variance estimation"
  },
  {
    "objectID": "people/Jingru-Zhang/index.html",
    "href": "people/Jingru-Zhang/index.html",
    "title": "Jingru Zhang",
    "section": "",
    "text": "Jingru Zhang is a doctoral student in the Quantitative Methods area in the Educational Psychology Department at UW-Madison. Her research interests involves methodological development and their applications from design and modeling perspectives, with a focus on causal inference, meta-analysis, mediation analysis, and generalizable and transportable inference.\n\n\n\n\nPhD in Quantitative Methods | 2027 (expected)  University of Wisconsin-Madison\nEdM in Measurement, Evaluation, and Statistics | 2022  Teachers College, Columbia University\n\n\n\n\n\nCausal inference\nMeta-analysis\nMediation analysis\nGeneralizable and transportable inference"
  },
  {
    "objectID": "people/Jingru-Zhang/index.html#education",
    "href": "people/Jingru-Zhang/index.html#education",
    "title": "Jingru Zhang",
    "section": "",
    "text": "PhD in Quantitative Methods | 2027 (expected)  University of Wisconsin-Madison\nEdM in Measurement, Evaluation, and Statistics | 2022  Teachers College, Columbia University"
  },
  {
    "objectID": "people/Jingru-Zhang/index.html#interests",
    "href": "people/Jingru-Zhang/index.html#interests",
    "title": "Jingru Zhang",
    "section": "",
    "text": "Causal inference\nMeta-analysis\nMediation analysis\nGeneralizable and transportable inference"
  },
  {
    "objectID": "people/Gleb-Furman/index.html",
    "href": "people/Gleb-Furman/index.html",
    "title": "Gleb Furman",
    "section": "",
    "text": "Gleb did his doctoral work on causal inference at UT Austin. He is currently a senior quantitative research scientist at Gibson Consulting Group.\n\n\n\n\nPhD in Quantitative Methods | 2023  University of Texas at Austin\nMEd in Quantitative Methods | 2015  University of Texas at Austin\nBA in Psychology | 2010  Baruch College\n\n\n\n\n\nResearch Design\nEvaluation\nGeneralizability"
  },
  {
    "objectID": "people/Gleb-Furman/index.html#education",
    "href": "people/Gleb-Furman/index.html#education",
    "title": "Gleb Furman",
    "section": "",
    "text": "PhD in Quantitative Methods | 2023  University of Texas at Austin\nMEd in Quantitative Methods | 2015  University of Texas at Austin\nBA in Psychology | 2010  Baruch College"
  },
  {
    "objectID": "people/Gleb-Furman/index.html#interests",
    "href": "people/Gleb-Furman/index.html#interests",
    "title": "Gleb Furman",
    "section": "",
    "text": "Research Design\nEvaluation\nGeneralizability"
  },
  {
    "objectID": "people/Christopher-Runyon/index.html",
    "href": "people/Christopher-Runyon/index.html",
    "title": "Christopher Runyon",
    "section": "",
    "text": "Chris did his doctoral work on causal inference at UT Austin. He is currently a measurement scientist in the Center for Advanced Assessment at the National Board of Medical Examiners, where his research focuses on developing automated scoring systems for constructed item responses.\n\n\n\n\nPhD in Quantitative Methods | 2020  University of Texas at Austin\nMA in Experimental Psychology | 2012  James Madison University\nBS in Psychology | 2008  Virginia Commonwealth University\nBA in Philosophy | 2008  Virginia Commonwealth University\nBA in Religious Studies (Buddhism) | 2003  University of Virginia\n\n\n\n\n\nCausal Inference\nR\nNatural Language Processing\nMachine Learning"
  },
  {
    "objectID": "people/Christopher-Runyon/index.html#education",
    "href": "people/Christopher-Runyon/index.html#education",
    "title": "Christopher Runyon",
    "section": "",
    "text": "PhD in Quantitative Methods | 2020  University of Texas at Austin\nMA in Experimental Psychology | 2012  James Madison University\nBS in Psychology | 2008  Virginia Commonwealth University\nBA in Philosophy | 2008  Virginia Commonwealth University\nBA in Religious Studies (Buddhism) | 2003  University of Virginia"
  },
  {
    "objectID": "people/Christopher-Runyon/index.html#interests",
    "href": "people/Christopher-Runyon/index.html#interests",
    "title": "Christopher Runyon",
    "section": "",
    "text": "Causal Inference\nR\nNatural Language Processing\nMachine Learning"
  },
  {
    "objectID": "people/Daniel-M.-Swan/index.html",
    "href": "people/Daniel-M.-Swan/index.html",
    "title": "Daniel M. Swan",
    "section": "",
    "text": "Danny is currently a research associate working at the Prevention Science Institute at University of Oregon, where he provides support for evidence reviewer training for the What Works Clearinghouse and assists with revision and refinement of the WWC’s Standards and Procedures Handbooks. His dissertation work involved examining the impact of response-guided designs on treatment effect estimates from single-case designs. His personal research interests center on analysis and meta-analysis of single-case designs (SCDs) in the context of education. He is particularly interested in the quality indicators of primary studies, the different kinds of inferences made in SCDs, and how the common, frequently unspoken research practices in this field might impact the intervention estimates from these kinds of studies.\n\n\n\n\nPhD in Quantitative Methods | 2019  University of Texas at Austin\nMEd in Quantitative Methods | 2014  University of Texas at Austin\nMS in Psychological Sciences | 2010  University of Texas at Dallas\nBA in Psychology | 2008  University of Texas at Dallas\n\n\n\n\n\nResearch methodology\nSingle-case designs\nMeta-analysis\nGeneralized linear models"
  },
  {
    "objectID": "people/Daniel-M.-Swan/index.html#education",
    "href": "people/Daniel-M.-Swan/index.html#education",
    "title": "Daniel M. Swan",
    "section": "",
    "text": "PhD in Quantitative Methods | 2019  University of Texas at Austin\nMEd in Quantitative Methods | 2014  University of Texas at Austin\nMS in Psychological Sciences | 2010  University of Texas at Dallas\nBA in Psychology | 2008  University of Texas at Dallas"
  },
  {
    "objectID": "people/Daniel-M.-Swan/index.html#interests",
    "href": "people/Daniel-M.-Swan/index.html#interests",
    "title": "Daniel M. Swan",
    "section": "",
    "text": "Research methodology\nSingle-case designs\nMeta-analysis\nGeneralized linear models"
  },
  {
    "objectID": "people/Man-Chen/index.html",
    "href": "people/Man-Chen/index.html",
    "title": "Man Chen",
    "section": "",
    "text": "Man Chen is a doctoral student in the Quantitative Methods area in the Educational Psychology Department at UW-Madison. Her research interests include meta-analysis, single-case experimental designs, and hierarchical linear modeling. She is currently working as a project assistant at the Wisconsin Center for Educational Research (WCER) with Dr. Pustejovsky.\n\n\n\n\nPhD in Quantitative Methods | 2023 (expected)  University of Wisconsin-Madison\nMEd in Quantitative Methods | 2018  The University of Texas at Austin\n\n\n\n\n\nMeta-analysis\nSingle-case experimental designs\nHierarchical linear modeling"
  },
  {
    "objectID": "people/Man-Chen/index.html#education",
    "href": "people/Man-Chen/index.html#education",
    "title": "Man Chen",
    "section": "",
    "text": "PhD in Quantitative Methods | 2023 (expected)  University of Wisconsin-Madison\nMEd in Quantitative Methods | 2018  The University of Texas at Austin"
  },
  {
    "objectID": "people/Man-Chen/index.html#interests",
    "href": "people/Man-Chen/index.html#interests",
    "title": "Man Chen",
    "section": "",
    "text": "Meta-analysis\nSingle-case experimental designs\nHierarchical linear modeling"
  },
  {
    "objectID": "people/Paulina-Grekov/index.html",
    "href": "people/Paulina-Grekov/index.html",
    "title": "Paulina Grekov",
    "section": "",
    "text": "Paulina Grekov is a doctoral student in the Quantitative Methods area in the Educational Psychology Department at UW-Madison. Her research interests include meta-analysis, single-case experimental designs, hierarchical linear modeling, and research synthesis. She is currently working as a research assistant with Dr. Pustejovsky.\n\n\n\n\nPhD in Quantitative Methods | 2026 (expected)  University of Wisconsin-Madison\n\n\n\n\n\nMeta-analysis\nSingle-case experimental designs\nHierarchical linear modeling\nResearch synthesis"
  },
  {
    "objectID": "people/Paulina-Grekov/index.html#education",
    "href": "people/Paulina-Grekov/index.html#education",
    "title": "Paulina Grekov",
    "section": "",
    "text": "PhD in Quantitative Methods | 2026 (expected)  University of Wisconsin-Madison"
  },
  {
    "objectID": "people/Paulina-Grekov/index.html#interests",
    "href": "people/Paulina-Grekov/index.html#interests",
    "title": "Paulina Grekov",
    "section": "",
    "text": "Meta-analysis\nSingle-case experimental designs\nHierarchical linear modeling\nResearch synthesis"
  },
  {
    "objectID": "presentations/AERA-2019-outcome-measurement-procedures.html",
    "href": "presentations/AERA-2019-outcome-measurement-procedures.html",
    "title": "An examination of measurement procedures and baseline behavioral outcomes in single-case research",
    "section": "",
    "text": "American Educational Research Association annual convention, Toronto, Ontario\n PDF"
  },
  {
    "objectID": "presentations/VIVE-2024-Dependent-Effects.html",
    "href": "presentations/VIVE-2024-Dependent-Effects.html",
    "title": "Model-Building Considerations in Meta-Analysis of Dependent Effect Sizes",
    "section": "",
    "text": "In fields ranging from Education to Economics to Ecology, meta-analysts often encounter complicated data structures, in which some or all primary studies include multiple effect size estimates. These estimates may be correlated because they are based on data from a common sample or a partially overlapping sample, or may be statistically dependent due to use of common study operations. A broad analytic strategy for dealing with such data is to specify a “working model” to roughly characterize the dependence structure, then use robust inference strategies that work well even if the working model is mis-specified relative to the true data-generating process. Although the technical and computational aspects of this strategy are now well developed, questions remain about how to apply it effectively in practice. In this talk, I will examine two practical questions related to how to build models for meta-analyses involving dependent effect sizes. First, I will illustrate some connections between working models and simpler, ad hoc techniques for dealing with effect size multiplicity, arguing that these connections provide useful heuristics to guide specification of random effects structures in multi-level and multi-variate meta-analysis. Second, I will describe some analytic strategies for conducting equity-related moderator analyses, where predictors involve personal characteristics of the primary study participants that can vary both within and between studies. I distinguish between direct evidence and contextual evidence about equity of impacts and show that the choice of working model can be consequential for analyses involving direct evidence. Throughout, I will highlight some open issues and practical challenges involved in modeling dependent effect sizes."
  },
  {
    "objectID": "publication/AAC-group-and-SCD/index.html",
    "href": "publication/AAC-group-and-SCD/index.html",
    "title": "Systematic review of variables related to instruction in augmentative and alternative communication implementation: Group and single-case design",
    "section": "",
    "text": "Purpose: This article provides a systematic review and analysis of group and single-case studies addressing augmentative and alternative communication (AAC) intervention with school-aged persons having autism spectrum disorder (ASD) and/or intellectual/developmental disabilities resulting in complex communication needs (CCNs). Specifically, we examined participant characteristics in group-design studies reporting AAC intervention outcomes and how these compared to those reported in single-case experimental designs (SCEDs). In addition, we compared the status of intervention features reported in group and SCED studies with respect to instructional strategies utilized.\nParticipants: Participants included school-aged individuals with CCNs who also experienced ASD or ASD with an intellectual delay who utilized aided or unaided AAC.\nMethod: A systematic review using descriptive statistics and effect sizes was implemented.\nResults: Findings revealed that participant features such as race, ethnicity, and home language continue to be underreported in both SCED and group-design studies. Participants in SCED investigations more frequently used multiple communication modes when compared to participants in group studies. The status of pivotal skills such as imitation was sparsely reported in both types of studies. With respect to instructional features, group-design studies were more apt to utilize clinical rather than educational or home settings when compared with SCED studies. In addition, SCED studies were more apt to utilize instructional methods that closely adhered to instructional features more typically characterized as being associated with behavioral approaches.\nConclusion: The authors discuss future research needs, practice implications, and a more detailed specification of treatment intensity parameters for future research."
  },
  {
    "objectID": "publication/AAC-participant-characteristics/index.html",
    "href": "publication/AAC-participant-characteristics/index.html",
    "title": "Participant characteristics predicting communication outcomes in AAC implementation for individuals with ASD and IDD: Meta-analysis",
    "section": "",
    "text": "This meta-analysis examined social communication outcomes in augmentative and alternative communication (AAC) interventions, or those that involved aided (e.g., speech generating devices, picture point systems) or unaided AAC (e.g., gestures, manual sign language) as a component of intervention, and the extent to which communication outcomes were predicted by participant characteristics. Variables of interest included chronological age, communication mode used prior to intervention, number of words produced and imitation skills of participants prior to intervention. Investigators identified 117 primary studies that implemented AAC interventions with school-aged individuals (up to 22 years) with autism spectrum disorder and/or intellectual disability associated with complex communication needs and assessed social-communication outcomes. All included studies involved single-case experimental designs and met basic study design quality standards. We synthesized findings across studies using two complementary effect size indices, Tau(AB) and the log response ratio, and multi-level meta-analysis with robust variance estimation. With Tau(AB), the overall average effect across 338 participants was 0.72, 95% CI [0.67, 0.76], with a high degree of heterogeneity across studies. With the log response ratio, the overall average effect corresponded to a 538% increase from baseline levels of responding, 95% CI [388%, 733%], with a high degree of heterogeneity across studies and contrasts. Moderator analyses detected few differences in effectiveness when comparing across diagnoses, ages, the number and type of communication modes the participants used prior to intervention, the number of words used by the participants prior to intervention, and imitation use prior to intervention."
  },
  {
    "objectID": "publication/ARP-for-behavioral-observation/index.html",
    "href": "publication/ARP-for-behavioral-observation/index.html",
    "title": "Alternating renewal process models for behavioral observation: Simulation methods and validity implications",
    "section": "",
    "text": "Direct observation recording procedures produce reductive summary measurements of an underlying stream of behavior. Previous methodological studies of these recording procedures have employed simulation methods for generating random behavior streams, many of which amount to special cases of a statistical model known as the alternating renewal process. This paper describes the alternating renewal process model in its general form, demonstrates how it provides an organizing framework for most past simulation research on direct observation procedures, and introduces a freely available software package that implements the model. The software can be used to simulate behavior streams as well as data from many common recording procedures, including continuous recording, momentary time sampling, event counting, and interval recording procedures. Several examples illustrate how the software can be used to study the validity and reliability of direct observation data and to develop measurement strategies during the planning phases of empirical studies."
  },
  {
    "objectID": "publication/BC-SMD-primer-and-tutorial/index.html",
    "href": "publication/BC-SMD-primer-and-tutorial/index.html",
    "title": "Between-case standardized mean difference effect sizes for single-case designs: A primer and tutorial using the scdhlm web application",
    "section": "",
    "text": "We describe a standardised mean difference statistic (d) for single-case designs that is equivalent to the usual d in between-groups experiments. We show how it can be used to summarise treatment effects over cases within a study, to do power analyses in planning new studies and grant proposals, and to meta-analyse effects across studies of the same question. We discuss limitations of this d-statistic, and possible remedies to them. Even so, this d-statistic is better founded statistically than other effect size measures for single-case design, and unlike many general linear model approaches such as multilevel modelling or generalised additive models, it produces a standardised effect size that can be integrated over studies with different outcome measures. SPSS macros for both effect size computation and power analysis are available."
  },
  {
    "objectID": "publication/between-groups-d-statistic/index.html",
    "href": "publication/between-groups-d-statistic/index.html",
    "title": "A d-statistic for single-case designs that is equivalent to the usual between-groups d-statistics",
    "section": "",
    "text": "We describe a standardised mean difference statistic (d) for single-case designs that is equivalent to the usual d in between-groups experiments. We show how it can be used to summarise treatment effects over cases within a study, to do power analyses in planning new studies and grant proposals, and to meta-analyse effects across studies of the same question. We discuss limitations of this d-statistic, and possible remedies to them. Even so, this d-statistic is better founded statistically than other effect size measures for single-case design, and unlike many general linear model approaches such as multilevel modelling or generalised additive models, it produces a standardised effect size that can be integrated over studies with different outcome measures. SPSS macros for both effect size computation and power analysis are available."
  },
  {
    "objectID": "publication/competing-approaches-for-cross-classified-data/index.html",
    "href": "publication/competing-approaches-for-cross-classified-data/index.html",
    "title": "Comparison of competing approaches to analyzing cross-classified data: Random effects models, ordinary least squares, or fixed effects with cluster robust standard errors",
    "section": "",
    "text": "Cross-classified random effects modeling (CCREM) is a common approach for analyzing cross-classified data in education. However, when the focus of a study is on the regression coefficients at level one rather than on the random effects, ordinary least squares regression with cluster robust variance estimators (OLS-CRVE) or fixed effects regression with CRVE (FE-CRVE) could be appropriate approaches. These alternative methods may be advantageous because they rely on weaker assumptions than what is required by CCREM. We conducted a Monte Carlo Simulation study to compare the performance of CCREM, OLS-CRVE, and FE-CRVE in models with crossed random effects, including conditions where homoscedasticity assumptions and exogeneity assumptions held and conditions where they were violated. We found that CCREM performed the best when its assumptions are all met. However, when homoscedasticity assumptions are violated, OLS-CRVE and FE-CRVE provided similar or better performance than CCREM. FE-CRVE showed the best performance when the exogeneity assumption is violated. Thus, we recommend two-way FE-CRVE as a good alternative to CCREM, particularly if the homoscedasticity or exogeneity assumptions of the CCREM might be in doubt."
  },
  {
    "objectID": "publication/converting-from-d-to-r-to-z/index.html",
    "href": "publication/converting-from-d-to-r-to-z/index.html",
    "title": "Converting from d to r to z when the design uses extreme groups, dichotomization, or experimental control",
    "section": "",
    "text": "Meta-analyses of the relationship between 2 continuous variables sometimes involves conversions between different effect sizes, but methodological literature offers conflicting guidance about how to make such conversions. This article provides methods for converting from a standardized mean difference to a correlation coefficient (and from there to Fisher’s z) under 3 types of study designs: extreme groups, dichotomization of a continuous variable, and controlled experiments. Also provided are formulas and recommendations regarding how the sampling variance of effect size statistics should be estimated in each of these cases. The conversion formula for extreme groups designs, originally due to Feldt (1961), can be viewed as a generalization of Hunter and Schmidt’s (1990) method for dichotomization designs. A simulation study examines the finite-sample properties of the proposed methods. The conclusion highlights areas where current guidance in the literature should be amended or clarified."
  },
  {
    "objectID": "publication/Decline-effects/index.html",
    "href": "publication/Decline-effects/index.html",
    "title": "High replicability of newly-discovered social-behavioral findings is achievable.",
    "section": "",
    "text": "Failures to replicate evidence of new discoveries have forced scientists to ask whether this unreliability is due to suboptimal implementation of optimal methods or whether presumptively optimal methods are not, in fact, optimal. This paper reports an investigation by four coordinated laboratories of the prospective replicability of 16 novel experimental findings using current optimal practices: high statistical power, preregistration, and complete methodological transparency. In contrast to past systematic replication efforts that reported replication rates averaging 50%, replication attempts here produced the expected effects with significance testing (p&lt;.05) in 86% of attempts, slightly exceeding maximum expected replicability based on observed effect size and sample size. When one lab attempted to replicate an effect discovered by another lab, the effect size in the replications was 97% that of the original study. This high replication rate justifies confidence in rigor enhancing methods and suggests that past failures to replicate may be attributable to departures from optimal procedures."
  },
  {
    "objectID": "publication/effects-of-social-stories-on-challenging-behavior-and-prosocial-skills/index.html",
    "href": "publication/effects-of-social-stories-on-challenging-behavior-and-prosocial-skills/index.html",
    "title": "Examining the effects of social stories on challenging behavior and prosocial skills in young children: A systematic review and meta-analysis",
    "section": "",
    "text": "Social stories are a commonly used intervention practice in early childhood special education. Recent systematic reviews have documented the evidence-base for social stories, but findings are mixed. We examined the efficacy of social stories for young children (i.e., 3-5 years) with challenging behavior across 12 single-case studies, that included 30 participants. The What Works Clearinghouse standards for single case research design were used to evaluate the rigor of studies that included social stories as a primary intervention. For studies meeting standards, we synthesized findings on the efficacy of social stories using meta-analysis techniques and a recently developed parametric effect size measure, the log response ratio. Trends in participants’ response to treatment also were explored. Results indicate variability in rigor and efficacy for the use of social stories as an isolated intervention and in combination with other intervention approaches. Additional studies that investigate the efficacy of social stories as a primary intervention are warranted."
  },
  {
    "objectID": "publication/Equivalences-between-ad-hoc-strategies-and-models/index.html",
    "href": "publication/Equivalences-between-ad-hoc-strategies-and-models/index.html",
    "title": "Equivalences between ad hoc strategies and meta-analytic models for dependent effect sizes",
    "section": "",
    "text": "Meta-analyses of educational research findings frequently involve statistically dependent effect size estimates. Meta-analysts have often addressed dependence issues using ad hoc approaches that involve modifying the data to conform to the assumptions of models for independent effect size estimates, such as aggregating estimates to obtain one summary estimate per study, conducting separate analyses of distinct subgroups of estimates, or combinations thereof. We demonstrate that these ad hoc approaches correspond exactly to certain multivariate models for dependent effect sizes. Specifically, we describe classes of multivariate random effects models that have likelihoods equivalent to those of models for effect sizes that have been averaged by study, classified into subgroups, or both. The equivalence also applies to robust variance estimation methods."
  },
  {
    "objectID": "publication/Four-methods-for-PIR/index.html",
    "href": "publication/Four-methods-for-PIR/index.html",
    "title": "Four methods for analyzing partial interval recording data, with application to single-case research",
    "section": "",
    "text": "Partial interval recording (PIR) is a procedure for collecting measurements during direct observation of behavior. It is used in several areas of educational and psychological research, particularly in connection with single-case research. Measurements collected using partial interval recording suffer from construct invalidity because they are not readily interpretable in terms of the underlying characteristics of the behavior. Using an alternating renewal process model for the behavior under observation, we demonstrate that ignoring the construct invalidity of PIR data can produce misleading inferences, such as inferring that an intervention reduces the prevalence of an undesirable behavior when in fact it has the opposite effect. We then propose four different methods for analyzing PIR summary measurements, each of which can be used to draw inferences about interpretable behavioral parameters. We demonstrate the methods by applying them to data from two single-case studies of problem behavior."
  },
  {
    "objectID": "publication/History-of-meta-regression/index.html",
    "href": "publication/History-of-meta-regression/index.html",
    "title": "A history of meta-regression: Technical, conceptual, and practical developments between 1974 and 2018",
    "section": "",
    "text": "At the beginning of the development of meta‐analysis, understanding the role of moderators was given the highest priority, with meta‐regression provided as a method for achieving this goal. Yet in current practice, meta‐regression is not as commonly used as anticipated. This paper seeks to understand this mismatch by reviewing the history of meta‐regression methods over the past 40 years. We divide this time span into four periods and examine three types of methodological developments within each period: technical, conceptual, and practical. Our focus is broad and includes development of methods in the fields of education, psychology, and medicine. We conclude the paper with a discussion of five consensus points, as well as open questions and areas of research for the future."
  },
  {
    "objectID": "publication/interventions-to-enhance-self-efficacy-in-cancer-patients/index.html",
    "href": "publication/interventions-to-enhance-self-efficacy-in-cancer-patients/index.html",
    "title": "Interventions to enhance self-efficacy in cancer patients and survivors: A meta-analysis of randomized controlled trials",
    "section": "",
    "text": "Objective: Self-efficacy expectations are associated with improvements in problematic outcomes widely considered clinically significant (i.e., emotional distress, fatigue, pain), related to positive health behaviors, and, as a type of personal agency, inherently valuable. Self-efficacy expectancies, estimates of confidence to execute behaviors, are important in that changes in selfefficacy expectations are positively related to future behaviors that promote health and wellbeing. The current meta-analysis investigated the impact of psychological interventions on self-efficacy expectations for a variety of health behaviors among cancer patients. Methods: Ovid Medline, PsycINFO, CINAHL, EMBASE, Cochrane Library, and Web of Science were searched with specific search terms for identifying randomized controlled trials (RCTs) that focused on psychologically-based interventions. Included studies had: 1) an adult cancer sample, 2) a self-efficacy expectation measure of specific behaviors and 3) an RCT design. Standard screening and reliability procedures were used for selecting and coding studies. Coding included theoretically informed moderator variables. Results: Across 79 RCTs, 223 effect sizes, and 8678 participants, the weighted average effect of self-efficacy expectations was estimated as g=0.274 (p&lt;.001). Consistent with Self-Efficacy Theory, the average effect for in-person intervention delivery (g=0.329) was significantly greater than for all other formats (g=0.154, p=.023; e.g., audiovisual, print, telephone, web/internet). Conclusions: The results establish the impact of psychological interventions on self-efficacy expectations as comparable in effect size to commonly reported outcomes (distress, fatigue, pain). Additionally, the result that in-person interventions achieved the largest effect is supported by Social Learning Theory and could inform research related to the development and evaluation of interventions."
  },
  {
    "objectID": "publication/Measurement-comparable-effect-sizes/index.html",
    "href": "publication/Measurement-comparable-effect-sizes/index.html",
    "title": "Measurement-comparable effect sizes for single-case studies of free-operant behavior",
    "section": "",
    "text": "Single-case research comprises a set of designs and methods for evaluating the effects of interventions, practices, or programs on individual cases, through comparison of outcomes measured at different points in time. Although there has long been interest in meta-analytic techniques for synthesizing single-case research, there has been little scrutiny of whether proposed effect sizes remain on a directly comparable metric when outcomes are measured using different operational procedures. Much of single-case research focuses on behavioral outcomes in free-operant contexts, which may be measured using a variety of different direct observation procedures. This article describes a suite of effect sizes for quantifying changes in free-operant behavior, motivated by an alternating renewal process model that allows measurement comparability to be established in precise terms. These effect size metrics have the advantage of comporting with how direct observation data are actually collected and summarized. Effect size estimators are proposed that are applicable when the behavior being measured remains stable within a given treatment condition. The methods are illustrated by 2 examples, including a re-analysis of a systematic review of the effects of choice-making opportunities on problem behavior."
  },
  {
    "objectID": "publication/Meta-analysis-of-SCD/index.html",
    "href": "publication/Meta-analysis-of-SCD/index.html",
    "title": "Research synthesis and meta-analysis of single-case designs",
    "section": "",
    "text": "PDF"
  },
  {
    "objectID": "publication/Power-approximations-for-dependent-effects/index.html",
    "href": "publication/Power-approximations-for-dependent-effects/index.html",
    "title": "Power approximations for overall average effects in meta-analysis of dependent effect sizes",
    "section": "",
    "text": "Meta-analytic models for dependent effect sizes have grown increasingly sophisticated over the last few decades, which has created challenges for a priori power calculations. We introduce power approximations for tests of average effect sizes based upon several common approaches for handling dependent effect sizes. In a Monte Carlo simulation, we show that the new power formulas can accurately approximate the true power of meta-analytic models for dependent effect sizes. Lastly, we investigate the Type I error rate and power for several common models, finding that tests using robust variance estimation provide better Type I error calibration than tests with model-based variance estimation. We consider implications for practice with respect to selecting a working model and an inferential approach."
  },
  {
    "objectID": "publication/psychosocial-interventions-for-positive-affect/index.html",
    "href": "publication/psychosocial-interventions-for-positive-affect/index.html",
    "title": "Psychosocial interventions for cancer survivors: A meta-analysis of effects on positive affect",
    "section": "",
    "text": "Purpose Positive affect has demonstrated unique benefits in the context of health-related stress and is emerging as an important target for psychosocial interventions. The primary objective of this meta-analysis was to determine whether psychosocial interventions increase positive affect in cancer survivors. Methods We coded 28 randomized controlled trials of psychosocial interventions assessing 2082 cancer survivors from six electronic databases. We calculated 76 effect sizes for positive affect and conducted synthesis using random effects models with robust variance estimation. Tests for moderation included demographic, clinical, and intervention characteristics. Results Interventions had a modest effect on positive affect (g = 0.35, 95% CI [0.16, 0.54]) with substantial heterogeneity of effects across studies (\\(\\hat\\tau^2\\) = 0.40; \\(I^2\\) = 78%). Three significant moderators were identified: in-person interventions outperformed remote interventions (P = .046), effects were larger when evaluated against standard of care or wait list control conditions versus attentional, educational, or component controls (P = .009), and trials with survivors of early-stage cancer diagnoses yielded larger effects than those with advanced-stage diagnoses (P = .046). We did not detect differential benefits of psychosocial interventions across samples varying in sex, age, on-treatment versus off-treatment status, or cancer type. Although no conclusive evidence suggested outcome reporting biases (P = .370), effects were smaller in studies with lower risk of bias. Conclusions In-person interventions with survivors of early-stage cancers hold promise for enhancing positive affect, but more methodological rigor is needed. Implications for Cancer Survivors Positive affect strategies can be an explicit target in evidence-based medicine and have a role in patient-centered survivorship care, providing tools to uniquely mobilize human strengths."
  },
  {
    "objectID": "publication/psychosocial-interventions-meaning-and-purpose/index.html",
    "href": "publication/psychosocial-interventions-meaning-and-purpose/index.html",
    "title": "Effects of psychosocial interventions on meaning and purpose in adults with cancer: A systematic review and meta-analysis",
    "section": "",
    "text": "Meaning and purpose in life are associated with the mental and physical health of patients with cancer and survivors and also constitute highly valued outcomes in themselves. Because meaning and purpose are often threatened by a cancer diagnosis and treatment, interventions have been developed to promote meaning and purpose. The present meta-analysis of randomized controlled trials (RCTs) evaluated effects of psychosocial interventions on meaning/purpose in adults with cancer and tested potential moderators of intervention effects. Six literature databases were systematically searched to identify RCTs of psychosocial interventions in which meaning or purpose was an outcome. Using Preferred Reporting Items for Systematic Reviews and Meta‐Analyses guidelines, rater pairs extracted and evaluated data for quality. Findings were synthesized across studies with standard meta‐analytic methods, including meta‐regression with robust variance estimation and risk-of-bias sensitivity analysis. Twenty-nine RCTs were identified, and they encompassed 82 treatment effects among 2305 patients/survivors. Psychosocial interventions were associated with significant improvements in meaning/purpose (g = 0.37; 95% CI, 0.22-0.52; P &lt; .0001). Interventions designed to enhance meaning/purpose (g = 0.42; 95% CI, 0.24-0.60) demonstrated significantly higher effect sizes than those targeting other primary outcomes (g = 0.18; 95% CI, 0.09-0.27; P = .009). Few other intervention, clinical, or demographic characteristics tested were significant moderators. In conclusion, the results suggest that psychosocial interventions are associated with small to medium effects in enhancing meaning/purpose among patients with cancer, and the benefits are comparable to those of interventions designed to reduce depression, pain, and fatigue in patients with cancer. Methodological concerns include small samples and ambiguity regarding allocation concealment. Future research should focus on explicitly meaning-centered interventions and identify optimal treatment or survivorship phases for implementation."
  },
  {
    "objectID": "publication/RASE-special-issue-introduction/index.html",
    "href": "publication/RASE-special-issue-introduction/index.html",
    "title": "Introduction to the special issue on single-case systematic reviews and meta-analysis",
    "section": "",
    "text": "This special issue provides an update on recent conceptual and methodological developments for conducting systematic reviews and meta-analyses of single-case research. In this introductory article, we (a) describe the important role of systematic reviews and meta-analyses within special education; (b) discuss several methodological issues authors must consider when planning and conducting a rigorous single-case review; and (c) summarize current approaches for addressing each of these issues. Following this overview, we describe each article in the special issue, paying particular attention to the methodological areas highlighted. We conclude with recommendations for continued research and development in several areas."
  },
  {
    "objectID": "publication/Religion-spirituality-physical-health/index.html",
    "href": "publication/Religion-spirituality-physical-health/index.html",
    "title": "Religion, spirituality, and physical health in cancer patients: A meta-analysis",
    "section": "",
    "text": "Although religion/spirituality (R/S) is important in its own right for many cancer patients, a large body of research has examined whether R/S is also associated with better physical health outcomes. This literature has been characterized by heterogeneity in sample composition, measures of R/S, and measures of physical health. In an effort to synthesize previous findings, a meta-analysis of the relation between R/S and patient-reported physical health in cancer patients was performed. A search of PubMed, PsycINFO, the Cumulative Index to Nursing and Allied Health Literature, and the Cochrane Library yielded 2073 abstracts, which were independently evaluated by pairs of raters. The meta-analysis was conducted for 497 effect sizes from 101 unique samples encompassing more than 32,000 adult cancer patients. R/S measures were categorized into affective, behavioral, cognitive, and ‘other’ dimensions. Physical health measures were categorized into physical well-being, functional well-being, and physical symptoms. Average estimated correlations (Fisher z scores) were calculated with generalized estimating equations with robust variance estimation. Overall R/S was associated with overall physical health (z=0.153, P&lt;.001); this relation was not moderated by sociodemographic or clinical variables. Affective R/S was associated with physical well-being (z=0.167, P&lt;.001), functional well-being (z=0.343, P&lt;.001), and physical symptoms (z=0.282, P&lt;.001). Cognitive R/S was associated with physical well-being (z=0.079, P&lt;.05) and functional well-being (z=0.090, P&lt;.01). ‘Other’ R/S was associated with functional well-being (z=0.100, P&lt;.05). In conclusion, the results of the current meta-analysis suggest that greater R/S is associated with better patient-reported physical health. These results underscore the importance of attending to patients’ religious and spiritual needs as part of comprehensive cancer care."
  },
  {
    "objectID": "publication/response-guided-designs-in-SCED-baselines/index.html",
    "href": "publication/response-guided-designs-in-SCED-baselines/index.html",
    "title": "The impact of response-guided designs on count outcomes in single-case experimental design baselines",
    "section": "",
    "text": "In single-case experimental design (SCED) research, researchers often choose when to start treatment based on whether the baseline data collected so far are stable, using what is called a response-guided design. There is evidence that response-guided designs are common, and researchers have described a variety of criteria for assessing stability. With many of these criteria, making judgments about stability could yield data with limited variability, which may have consequences for statistical inference and effect size estimates. However, little research has examined the impact of response-guided design on the resulting data. Drawing on both applied and methodological research, we describe several algorithms as models for response-guided design. We use simulation methods to assess how using a response-guided design impacts the baseline data pattern. The simulations generate baseline data in the form of frequency counts, a common type of outcome in SCEDs. Most of the response-guided algorithms we identified lead to baselines with approximately unbiased mean levels, but nearly all of them lead to underestimates in the baseline variance. We discuss implications for the use of response-guided designs in practice and for the plausibility of specific algorithms as representations of actual research practice."
  },
  {
    "objectID": "publication/RVE-in-fixed-effects-models/index.html",
    "href": "publication/RVE-in-fixed-effects-models/index.html",
    "title": "Small sample methods for cluster-robust variance estimation and hypothesis testing in fixed effects models",
    "section": "",
    "text": "In panel data models and other regressions with unobserved effects, fixed effects estimation is often paired with cluster-robust variance estimation (CRVE) to account for heteroscedasticity and un-modeled dependence among the errors. Although asymptotically consistent, CRVE can be biased downward when the number of clusters is small, leading to hypothesis tests with rejection rates that are too high. More accurate tests can be constructed using bias-reduced linearization (BRL), which corrects the CRVE based on a working model, in conjunction with a Satterthwaite approximation for t-tests. We propose a generalization of BRL that can be applied in models with arbitrary sets of fixed effects, where the original BRL method is undefined, and describe how to apply the method when the regression is estimated after absorbing the fixed effects. We also propose a small-sample test for multiple-parameter hypotheses, which generalizes the Satterthwaite approximation for t-tests. In simulations covering a wide range of scenarios, we find that the conventional cluster-robust Wald test can severely over-reject while the proposed small-sample test maintains Type I error close to nominal levels. The proposed methods are implemented in an R package called clubSandwich. This article has online supplementary materials."
  },
  {
    "objectID": "publication/SCD-synthesis-tools-I/index.html",
    "href": "publication/SCD-synthesis-tools-I/index.html",
    "title": "Single-case synthesis tools I: Evaluating the quality and rigor of research on antecedent sensory-based interventions",
    "section": "",
    "text": "Tools for evaluating the quality and rigor of single case research designs (SCD) are often used when conducting SCD syntheses. Preferred components include evaluations of design features related to the internal validity of SCD to obtain quality and/or rigor ratings. Three tools for evaluating the quality and rigor of SCD (Council for Exceptional Children, What Works Clearinghouse, and Single-Case Analysis and Design Framework) were compared to determine if conclusions regarding the effectiveness of antecedent sensory-based interventions for young children changed based on choice of quality evaluation tool. Evaluation of SCD quality differed across tools, suggesting selection of quality evaluation tools impacts evaluation findings. Suggestions for selecting an appropriate quality and rigor assessment tool are provided and across-tool conclusions are drawn regarding the quality and rigor of studies. Finally, authors provide guidance for using quality evaluations in conjunction with outcome analyses when conducting syntheses of interventions evaluated in the context of SCD."
  },
  {
    "objectID": "publication/SCED-MLMA-RVE/index.html",
    "href": "publication/SCED-MLMA-RVE/index.html",
    "title": "Multi-level meta-analysis of single-case experimental designs using robust variance estimation",
    "section": "",
    "text": "Single-case experimental designs (SCEDs) are used to study the effects of interventions on the behavior of individual cases, by making comparisons between repeated measurements of an outcome under different conditions. In research areas where SCEDs are prevalent, there is a need for methods to synthesize results across multiple studies. One approach to synthesis uses a multi-level meta-analysis (MLMA) model to describe the distribution of effect sizes across studies and across cases within studies. However, MLMA relies on having accurate sampling variances of effect size estimates for each case, which may not be possible due to auto-correlation in the raw data series. One possible solution is to combine MLMA with robust variance estimation (RVE), which provides valid assessments of uncertainty even if the sampling variances of effect size estimates are inaccurate. Another possible solution is to forgo MLMA and use simpler, ordinary least squares (OLS) methods with RVE. This study evaluates the performance of effect size estimators and methods of synthesizing SCEDs in the presence of auto-correlation, for several different effect size metrics, via a Monte Carlo simulation designed to emulate the features of real data series. Results demonstrate that the MLMA model with RVE performs properly in terms of bias, accuracy, and confidence interval coverage for estimating overall average log response ratios. The OLS estimator corrected with RVE performs the best in estimating overall average Tau effect sizes. None of the available methods perform adequately for meta-analysis of within-case standardized mean differences."
  },
  {
    "objectID": "publication/selective-reporting-with-dependent-effects/index.html",
    "href": "publication/selective-reporting-with-dependent-effects/index.html",
    "title": "Evaluating meta-analytic methods to detect selective reporting in the presence of dependent effect sizes",
    "section": "",
    "text": "Meta-analysis is a set of statistical tools used to synthesize results from multiple studies evaluating a common research question. Two methodological challenges when conducting meta-analysis include selective reporting and correlated dependent effect sizes. Selective reporting is often a result of selective publication practices based on the statistical significance of study findings, which threatens the validity of meta-analytic results. One of the main sources of dependent effect sizes is the inclusion of multiple outcome measures from a primary study. This violates conventional, univariate meta-analytic techniques. Meta-analysts lack validated methods to detect the presence of selective reporting while incorporating methods to handle dependent effect sizes. This study evaluates currently available univariate selective reporting methods, when ignoring dependence, selecting one effect size per study, or aggregating dependent correlated effect sizes. This study also proposes and examines an Egger’s Regression variant incorporated with Robust Variance Estimation (RVE) to handle within-study dependence. A Monte Carlo simulation study assess the performance of the methods for Type I error rates in the absence of selective reporting, and power to detect selective reporting when introduced. Ignoring dependence inflates Type I error rates for all univariate detection methods. Type I error rates are maintained with regression tests when dependent effect sizes are sampled, aggregated or modeled using RVE. However, all selective reporting methods evaluated in this study have little to no power to detect selection bias, except under strong selection censoring."
  },
  {
    "objectID": "publication/SMD-for-MBD/index.html",
    "href": "publication/SMD-for-MBD/index.html",
    "title": "A standardized mean difference effect size for multiple baseline designs",
    "section": "",
    "text": "Single-case designs are a class of research methods for evaluating treatment effects by measuring outcomes repeatedly over time while systematically introducing different condition (e.g., treatment and control) to the same individual. The designs are used across fields such as behavior analysis, clinical psychology, special education, and medicine. Emerging standards for single-case designs have focused attention on methods for summarizing and meta-analyzing findings and on the need for effect sizes indices that are comparable to those used in between-subjects designs. In the previous work, we discussed how to define and estimate an effect size that is directly comparable to the standardized mean difference often used in between-subjects research based on the data from a particular type of single-case design, the treatment reversal or (AB)k design. This paper extends the effect size measure to another type of single-case study, the multiple baseline design. We propose estimation methods for the effect size and its variance, study the estimators using simulation, and demonstrate the approach in two applications."
  },
  {
    "objectID": "publication/stay-play-talk-meta-analysis/index.html",
    "href": "publication/stay-play-talk-meta-analysis/index.html",
    "title": "Systematic review and meta-analysis of stay-play-talk interventions for improving social behaviors of young children",
    "section": "",
    "text": "Stay-play-talk (SPT) is a peer-mediated intervention which involves training peer implementers to stay in proximity to, play with, and talk to a focal child who has disabilities or lower social competence. This systematic review and meta-analysis investigated the contexts in which SPT interventions have been conducted, the methodological adequacy of the research assessing its effects, and the outcomes for both peer implementers and focal children. Studies have primarily occurred in inclusive preschool settings during free play activities, with researchers serving as facilitators. Average effects were positive for both peer implementers and focal children, although considerable heterogeneity across studies was observed. Additional research is needed to determine what peer implementer and focal child characteristics moderate intervention success, what modifications are needed for children who have complex communication needs, and optimal procedural variations (e.g., group size, training time)."
  },
  {
    "objectID": "publication/Testing-for-funnel-plot-asymmetry-of-SMDs/index.html",
    "href": "publication/Testing-for-funnel-plot-asymmetry-of-SMDs/index.html",
    "title": "Testing for funnel plot asymmetry of standardized mean differences",
    "section": "",
    "text": "Publication bias and other forms of outcome reporting bias are critical threats to the validity of findings from research syntheses. A variety of methods have been proposed for detecting selective outcome reporting in a collection of effect size estimates, including several methods based on assessment of asymmetry of funnel plots, such as Egger’s regression test, the rank correlation test, and the Trim-and-Fill test. Previous research has demonstated that Egger’s regression test is mis-calibrated when applied to log-odds ratio effect size estimates, due to artifactual correlation between the effect size estimate and its standard error. This study examines similar problems that occur in meta-analyses of the standardized mean difference, a ubiquitous effect size measure in educational and psychological research. In a simulation study of standardized mean difference effect sizes, we assess the Type I error rates of conventional tests of funnel plot asymmetry, as well as the likelihood ratio test from a three-parameter selection model. Results demonstrate that the conventional tests have inflated Type I error due to correlation between the effect size estimate and its standard error, while tests based on either a simple modification to the conventional standard error formula or a variance-stabilizing transformation both maintain close-to-nominal Type I error."
  },
  {
    "objectID": "publication/Transition-to-College-Mathematics-Year-1/index.html",
    "href": "publication/Transition-to-College-Mathematics-Year-1/index.html",
    "title": "Evaluating the Transition to College Mathematics Course in Texas high schools: Findings from the first year of implementation",
    "section": "",
    "text": "Texas House Bill 5 introduced requirements that school districts partner with institutions of higher education to provide college preparatory courses in mathematics and English for high school seniors who are not yet college ready. As districts and college partners begin to respond to these provisions, there is a need for empirical research on the effects of different approaches to implementing the college preparatory courses. In response to House Bill 5 requirements, the Charles A. Dana Center has developed a model college preparatory mathematics course, Transition to College Mathematics Course (TCMC), which has been adopted by dozens of school districts across Texas over the past several school years. We examine the effects of TCMC on students’ progress into post-secondary education by comparing students who participated in TCMC during the 2016-17 school year (the first year of implementation) to observationally similar students, either from a previous cohort that did not have access to TCMC or from the same cohort but who did not enroll in the course. We find that, although students who took TCMC graduated at slightly higher rates than comparison students, they had lower rates of enrollment in post-secondary education, driven by lower rates of enrollment in 4-year colleges or universities. Enrollment gradually became more similar over the four semesters following graduation from high school. We find that students who took TCMC were also less likely than students in the comparison group to pass college-level and developmental math courses. Longer-term cumulative outcomes showed stronger reductions in rates of math course passage. However, these results must be interpreted cautiously because we were unable to fully assess and account for students’ college-readiness status at the start of their senior year."
  },
  {
    "objectID": "publication/Transition-to-College-Mathematics-Year-3/index.html",
    "href": "publication/Transition-to-College-Mathematics-Year-3/index.html",
    "title": "Evaluating the Transition to College Mathematics Course in Texas high schools: Examining heterogeneity across schools and student characteristics",
    "section": "",
    "text": "Texas House Bill 5 introduced requirements that school districts partner with institutions of higher education to provide college preparatory courses in mathematics and English language arts for high school seniors who are not yet college ready. In response to House Bill 5 requirements, the Charles A. Dana Center developed a model college preparatory mathematics course, Transition to College Mathematics Course (TCMC), which has been adopted by dozens of school districts across Texas over the past several school years. In prior work, we examined the effects of TCMC on students’ progress into post-secondary education by comparing two student cohorts who participated in TCMC to observationally similar students from the same cohort but who did not enroll in the course. In this report, we investigate the extent of heterogeneity in the effects of participating in TCMC. We find little evidence that the program was differentially effective for students from different socio-economic backgrounds, nor do we find evidence that program effects varied by the number of years that it had been offered. However, for key outcomes such as rates of passing a college-level math course, the effects of participating in TCMC may have varied across the schools that offered the course. Just as in prior work, these findings must be interpreted cautiously because we were unable to fully assess and account for students’ college-readiness status at the start of their senior year."
  },
  {
    "objectID": "software/ARPobservation/index.html",
    "href": "software/ARPobservation/index.html",
    "title": "ARPobservation",
    "section": "",
    "text": "An R package for simulating different methods of recording data based on direct observation of behavior, where behavior is modeled by an alternating renewal process.\n\nAvailable on the Comprehensive R Archive Network\nInstallation instructions\nSource code on Github\nARPsimulator: An interactive web application for simulating systematic direct observation data based on the alternating renewal process model."
  },
  {
    "objectID": "software/POMADE/index.html",
    "href": "software/POMADE/index.html",
    "title": "POMADE",
    "section": "",
    "text": "An R package for computing power levels, minimum detectable effect sizes, and minimum required sample sizes for the test of the overall average effect size in meta-analysis of dependent effect sizes. The package also includes functions for creating plots of power analysis results."
  },
  {
    "objectID": "software/simhelpers/index.html",
    "href": "software/simhelpers/index.html",
    "title": "simhelpers",
    "section": "",
    "text": "Monte Carlo simulations are computer experiments designed to study the performance of statistical methods under known data-generating conditions. The goal of simhelpers is to assist in running such simulation studies. The main tools in the package consist of functions to calculate measures of estimator performance, such as bias, root mean squared error, rejection rates. The functions also calculate the associated Monte Carlo standard errors (MCSE) of the performance measures. The functions use the tidyeval principles, so that they play well with dplyr and fit easily into a %&gt;%-centric workflow.\n\nAvailable on the Comprehensive R Archive Network\nSource code and installation instructions on Github"
  },
  {
    "objectID": "software/wildmeta/index.html",
    "href": "software/wildmeta/index.html",
    "title": "wildmeta",
    "section": "",
    "text": "An R package for conducting hypothesis tests of meta-regression models using cluster wild bootstrapping, based on methods examined in Joshi, Pustejovsky, and Beretvas (2021)."
  },
  {
    "objectID": "posts/Distribution-of-signficant-effects/index.html",
    "href": "posts/Distribution-of-signficant-effects/index.html",
    "title": "Distribution of the number of significant effect sizes",
    "section": "",
    "text": "A while back, I posted the outline of a problem about the number of significant effect size estimates in a study that reports multiple outcomes. This problem interests me because it connects to the issue of selective reporting of study results, which creates problems for meta-analysis. Here, I’ll re-state the problem in slightly more general terms and then make some notes about what’s going on.\nConsider a study that assesses some effect size across \\(m\\) different outcomes. (We’ll be thinking about one study at a time here, so no need to index the study as we would in a meta-analysis problem.) Let \\(T_i\\) denote the effect size estimate for outcome \\(i\\), let \\(V_i\\) denote the sampling variance of the effect size estimate for outcome \\(i\\), and let \\(\\theta_i\\) denote the true effect size parameter for corresponding to outcome \\(i\\). Assume that the study outcomes \\(\\left[T_i\\right]_{i=1}^m\\) follow a correlated-and-hierarchical effects model, in which \\[T_i = \\mu + u + v_i + e_i,\\] where the study-level error \\(u \\sim N\\left(0,\\tau^2\\right)\\), the effect-specific error \\(v_i \\stackrel{iid}{\\sim} N\\left(0, \\omega^2\\right)\\), and the vector of sampling errors \\(\\left[e_i\\right]_{i=1}^m\\) is multivariate normal with mean \\(\\mathbf{0}\\), known variances \\(\\text{Var}(e_i) = \\sigma^2\\), and compound symmetric correlation structure \\(\\text{cor}(e_h, e_i) = \\rho\\).\nDefine \\(A_i\\) as an indicator that is equal to one if \\(T_i\\) is statistically significant at level \\(\\alpha\\) based on a one-sided test, and otherwise equal to zero. (Equivalently, let \\(A_i\\) be equal to one if the effect is statistically significant at level \\(2 \\alpha\\) and in the theoretically expected direction.) Formally, \\[A_i = I\\left(\\frac{T_i}{\\sigma} &gt; q_\\alpha \\right)\\] where \\(q_\\alpha = \\Phi^{-1}(1 - \\alpha)\\) is the critical value from a standard normal distribution (e.g., \\(q_{.05} = 1.645\\), \\(q_{.025} = 1.96\\)). Let \\(N_A = \\sum_{i=1}^m A_i\\) denote the total number of statistically significant effect sizes in the study. The question is: what is the distribution of \\(N_A\\)."
  },
  {
    "objectID": "posts/Distribution-of-signficant-effects/index.html#compound-symmetry-to-the-rescue",
    "href": "posts/Distribution-of-signficant-effects/index.html#compound-symmetry-to-the-rescue",
    "title": "Distribution of the number of significant effect sizes",
    "section": "Compound symmetry to the rescue",
    "text": "Compound symmetry to the rescue\nAs I noted in the previous post, this set-up means that the effect size estimates have a compound symmetric distribution. We can make this a bit more explicit by writing the sampling errors in terms of the sum of a component that’s common acrosss outcomes and a component that’s specific to each outcome. Thus, let \\(e_i = f + g_i\\), where \\(f \\sim N\\left(0, \\rho \\sigma^2 \\right)\\) and \\(g_i \\stackrel{iid}{\\sim} N \\left(0, (1 - \\rho) \\sigma^2\\right)\\). Let me also define \\(\\zeta = \\mu + u + f\\) as the conditional mean of the effects. It then follows that the effect size estimates are conditionally independent, given the common components: \\[\n\\left(T_i | \\zeta \\right) \\stackrel{iid}{\\sim} N\\left(\\zeta, \\omega^2 + (1 - \\rho) \\sigma^2\\right)\n\\] Furthermore, the conditional probability of a significant effect is \\[\n\\text{Pr}(A_i = 1 | \\zeta) = \\Phi\\left(\\frac{\\zeta - q_{\\alpha} \\sigma}{\\sqrt{\\omega^2 + (1 - \\rho)\\sigma^2}}\\right)\n\\] and \\(A_1,...,A_m\\) are mutually independent, conditional on \\(\\zeta\\). Therefore, the conditional distribution of \\(N_A\\) is binomial, \\[\n\\left(N_A | \\zeta\\right) \\sim Bin(m, \\pi)\n\\] where \\[\n\\pi = \\Phi\\left(\\frac{\\zeta - q_{\\alpha} \\sigma}{\\sqrt{\\omega^2 + (1 - \\rho)\\sigma^2}}\\right).\n\\] What about the unconditional distribution?\nTo get rid of the \\(\\zeta\\), we need to integrate over its distribution, which leads to \\[\n\\text{Pr}(N_A = a) = \\text{E}\\left[\\text{Pr}\\left(N_A | \\zeta\\right)\\right] = \\int f_{N_A}\\left(a | \\zeta, \\omega, \\sigma, \\rho, m\\right) \\times f_\\zeta(\\zeta | \\mu, \\tau, \\sigma, \\rho) \\ d \\zeta,\n\\] where \\(f_{N_A}\\left(a | \\zeta, \\omega, \\sigma, \\rho \\right)\\) is a binomial density with size \\(m\\) and probability \\(\\pi = \\pi(\\zeta, \\omega, \\sigma, \\rho)\\) and \\(f_\\zeta(\\zeta | \\mu, \\tau, \\sigma, \\rho)\\) is a normal density with mean \\(\\mu\\) and variance \\(\\tau^2 + \\rho \\sigma^2\\).\nThis distribution is what you might call a binomial-normal convolution or a random-intercept probit model (where the random intercept is \\(\\zeta\\)). As far as I know, the distribution cannot be evaluated analytically but instead must be calculated using some sort of numerical integration routine. Here is an interactive graph of the probability mass function (the probability points are calculated using Gaussian quadrature):\n\n\nCode\nmath = require(\"mathjs\")\nnorm = import('https://unpkg.com/norm-dist@3.1.0/index.js?module')\n\nquad_points = JSON.parse(all_quad_points).at(qp - 1)\n\nsigma = 2 / math.sqrt(ESS)\n\nzeta_sd = math.sqrt(tau**2 + rho * sigma**2)\n\ncrit = norm.icdf(1 - alpha)\n\nbinomial_coefs = Array(m+1).fill(null).map((x,index) =&gt; {\n  return math.combinations(m, index);\n})\n\nprobs = quad_points.map(zeta =&gt; {\n  let Z = (zeta[0] * zeta_sd + mu - crit * sigma) / math.sqrt(omega**2 + (1 - rho) * sigma**2);\n  return [norm.cdf(Z), zeta[1]];\n})\n\np_binom_norm = binomial_coefs.map((coef, a) =&gt; {\n  let p = probs.map((x) =&gt; {\n    return (x[0]**a) * ((1 - x[0])**(m - a)) * x[1];\n  });\n  return coef * math.sum(p);\n})\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nPlot.plot({\n  x: {\n    label: \"Number of significant effect sizes\"\n  },\n  y: {\n    domain: [0, 1],\n    label: \"Probability\"\n  },\n  marks: [\n    Plot.ruleY(0),\n    Plot.barY(p_binom_norm, {\n      fill: \"steelblue\"\n    }),\n  ]\n})\n\n\n\n\n\n\n\n\n\n\n\nCode\nviewof m = Inputs.range(\n  [1, 30], \n  {value: 6, step: 1, label: \"Number of effect sizes (m):\"}\n)\n\nviewof ESS = Inputs.range(\n  [4, 300], \n  {value: 80, step: 1, label: \"Effective sample size:\"}\n)\n\nviewof mu = Inputs.range(\n  [-2, 2], \n  {value: 0.3, step: 0.01, label: \"Average effect size (mu):\"}\n)\n\nviewof tau = Inputs.range(\n  [0, 1], \n  {value: 0.1, step: 0.01, label: \"Between-study SD (tau):\"}\n)\n\nviewof omega = Inputs.range(\n  [0, 1], \n  {value: 0.1, step: 0.01, label: \"Within-study SD (omega):\"}\n)\n\nviewof rho = Inputs.range(\n  [0, 1], \n  {value: 0.6, step: 0.01, label: \"Sampling error correlation (rho):\"}\n)\n\nviewof alpha = Inputs.range(\n  [0.005, 0.995], \n  {value: 0.025, step: .005, label: \"One-sided significance threshold (alpha):\"}\n)\n\nviewof qp = Inputs.range(\n  [1, 30], \n  {value: 21, step: 1, label: \"Number of quadrature points:\"}\n)"
  },
  {
    "objectID": "posts/Distribution-of-signficant-effects/index.html#just-the-moments-please",
    "href": "posts/Distribution-of-signficant-effects/index.html#just-the-moments-please",
    "title": "Distribution of the number of significant effect sizes",
    "section": "Just the moments, please",
    "text": "Just the moments, please\nIf all we care about is the expectation of \\(N_A\\), we don’t need to bother with all the conditioning business and can just look at the marginal distribution of the effect size estimates taken individually. Marginally, \\(T_i\\) is normally distributed with mean \\(\\mu\\) and variance \\(\\tau^2 + \\omega^2 + \\sigma^2\\), so \\(\\text{Pr}(A_i = 1) = \\psi\\), where \\[\n\\psi = \\Phi\\left(\\frac{\\mu - q_{\\alpha} \\sigma}{\\sqrt{\\tau^2 + \\omega^2 + \\sigma^2}}\\right).\n\\] By the linearity of expectations, \\[\n\\text{E}(N_A) = \\sum_{i=1}^m \\text{E}(A_i) = m \\psi.\n\\]\nWe can also get an approximation for the variance of \\(N_A\\) by working with its conditional distribution above. By the rule of variance decomposition, \\[\n\\begin{aligned}\n\\text{Var}(N_A) &= \\text{E}\\left[\\text{Var}\\left(N_A | \\zeta\\right)\\right] + \\text{Var}\\left[\\text{E}\\left(N_A | \\zeta\\right)\\right] \\\\\n&= m \\times \\text{E}\\left[\\pi (1 - \\pi)\\right] + m^2 \\times \\text{Var}\\left[\\pi\\right]\\\\\n&= m \\times \\text{E}\\left[\\pi\\right] \\left(1 - \\text{E}\\left[\\pi\\right]\\right) + m (m - 1) \\times \\text{Var}\\left[\\pi\\right],\n\\end{aligned}\n\\] where \\(\\pi\\) is, as defined above, a function of \\(\\zeta\\) and thus a random variable. Now, \\(\\text{E}(\\pi) = \\psi\\) and we can get something close to \\(\\text{Var}(\\pi)\\) using a first-order approximation: \\[\n\\text{Var}\\left(\\pi\\right) \\approx \\left(\\left.\\frac{\\delta \\pi}{\\delta \\zeta}\\right|_{\\zeta = \\mu}\\right)^2 \\times \\text{Var}\\left(\\zeta\\right) = \\left[\\phi\\left(\\frac{\\mu - q_{\\alpha} \\sigma}{\\sqrt{\\omega^2 + (1 - \\rho)\\sigma^2}}\\right)\\right]^2 \\times \\frac{\\tau^2 + \\rho \\sigma^2}{\\omega^2 + (1 - \\rho)\\sigma^2}.\n\\] Thus, \\[\n\\begin{aligned}\n\\text{Var}(N_A) \\approx m \\times \\psi \\left(1 - \\psi\\right) + m (m - 1) \\times \\left[\\phi\\left(\\frac{\\mu - q_{\\alpha} \\sigma}{\\sqrt{\\omega^2 + (1 - \\rho)\\sigma^2}}\\right)\\right]^2 \\times \\frac{\\tau^2 + \\rho \\sigma^2}{\\omega^2 + (1 - \\rho)\\sigma^2}.\n\\end{aligned}\n\\] If the amount of common variation is small, so \\(\\tau^2\\) is near zero and \\(\\rho\\) is near zero, then the contribution of the second term will be small, and \\(N_A\\) will act more or less like a binomial random variable with size \\(m\\) and probability \\(\\psi\\). On the other hand, if the amount of independent variation in the effect sizes is small, so \\(\\omega^2\\) is near zero and \\(\\rho\\) is near 1, then the term on the right will approach \\(m(m - 1)\\psi(1 - \\psi)\\) and \\(\\text{Var}\\left(N_A\\right)\\) will approach \\(m^2 \\psi(1 - \\psi)\\), or the variance of \\(m\\) times a single Bernoulli variate. So you could say that \\(N_A\\) has anywhere between \\(1\\) and \\(m\\) variate’s worth of information in it, depending on the degree of correlation between the effect size estimates."
  },
  {
    "objectID": "posts/Approximating-cluster-robust-Wald-tests/index.html",
    "href": "posts/Approximating-cluster-robust-Wald-tests/index.html",
    "title": "Approximating the distribution of cluster-robust Wald statistics",
    "section": "",
    "text": "In Tipton and Pustejovsky (2015), we examined several different small-sample approximations for cluster-robust Wald test statistics, which are like \\(F\\) statistics but based on cluster-robust variance estimators. These statistics are, frankly, kind of weird and awkward to work with, and the approximations that we examined were far from perfect. In this post, I will look in detail at the robust Wald statistic for a simple but common scenario: a one-way ANOVA problem with clusters of dependent observations. \\(\\def\\Pr{{\\text{Pr}}}\n\\def\\E{{\\text{E}}}\n\\def\\Var{{\\text{Var}}}\n\\def\\Cov{{\\text{Cov}}}\n\\def\\cor{{\\text{cor}}}\n\\def\\bm{\\mathbf}\n\\def\\bs{\\boldsymbol}\\)\nConsider a setup where clusters can be classified into one of \\(C\\) categories, with each cluster of observations falling into a single category. Let \\(\\bs\\mu = \\left[\\mu_c \\right]_{c=1}^C\\) denote the means of these categories. Suppose we have an estimator of those means \\(\\bs{\\hat\\mu} = \\left[\\hat\\mu_c\\right]_{c=1}^C\\) and a corresponding cluster-robust variance estimator \\(\\bm{V}^R = \\bigoplus_{c=1}^C V^R_c\\). Note that \\(\\bm{V}^R\\) is diagonal because the estimators for each category are independent. Assume that the robust variance estimator is unbiased so \\(\\E\\left(V^R_c\\right) = \\Var\\left( \\hat\\mu_c \\right) = \\psi_c\\) for \\(c = 1,...,C\\). Let \\(\\bs\\Psi = \\bigoplus_{c=1}^C \\psi_c\\).\nSuppose that we want to test the null hypothesis that that means of the categories are all equal, \\(H_0: \\mu_1 = \\mu_2 = \\cdots = \\mu_C\\). We can express this null using a \\(q \\times C\\) contrast matrix \\(\\bm{C} = \\left[-\\bm{1}_q \\ \\bm{I}_q \\right]\\), where \\(q = C - 1\\). The null hypothesis is then \\(\\bm{C} \\bs\\mu = \\bm{0}_q\\). The corresponding cluster-robust Wald statistic is \\[\nQ = \\bs{\\hat\\mu}' \\bm{C}' \\left(\\bm{C} \\bm{V}^R \\bm{C}'\\right)^{-1} \\bm{C} \\bs{\\hat\\mu}.\n\\] Under the null hypothesis, the distribution of \\(Q\\) will converge to a \\(\\chi^2_q\\) as the number of clusters in each category grows large. However, with a limited number of clusters in some of the categories, this approximate reference distribution is not very accurate and tests based on it can have wildly inflated type I error rates.\nIn the paper, we considered several different ways of approximating the distribution of \\(Q\\) that work at smaller sample sizes. One class of approaches to approximating the sampling distribution of \\(Q\\) is to use a Hotelling’s \\(T^2\\) distribution with degrees of freedom \\(\\eta\\). Given the degrees of freedom, Hotelling’s \\(T^2\\) is a multiple of an \\(F\\) distribution: \\[\n\\frac{\\eta - q + 1}{\\eta q} Q \\sim F(q, \\eta - q + 1).\n\\] The question is then how to determine \\(\\eta\\).\nSeveral of the approaches that we considered are based on representing the \\(Q\\) statistic as \\[\nQ = \\bm{z}' \\bm{D}^{-1} \\bm{z},\n\\] where \\(\\bs\\Omega = \\bm{C} \\bs\\Psi \\bm{C}'\\), \\(\\bm{z} = \\bs\\Omega^{-1/2}\\bm{C}\\hat\\mu_c\\), \\(\\bm{G} = \\bs\\Omega^{-1/2} \\bm{C}\\), and \\[\n\\bm{D} = \\bm{G} \\bm{V}^R \\bm{G}'.\n\\] The various approaches we considered involve different ways of approximating the sampling distribution of \\(\\bm{D}\\).\nOne of the approximations involves finding degrees of freedom \\(\\eta\\) by following a strategy suggested by Zhang (2012, 2013). These degrees of freedom are given by \\[\n\\eta_Z = \\frac{q(q + 1)}{\\sum_{s=1}^q \\sum_{t = 1}^q \\Var(d_{st})},\n\\] where \\(d_{st}\\) is the entry in row \\(s\\), column \\(t\\) of \\(\\bm{D}\\). To find \\(\\eta_Z\\), we can compute the denominator using general formulas given in the paper. However, with a bit of analysis we can find a much simpler expression for the special case of one-way ANOVA.\nBefore going further, it’s useful to observe that \\(\\bm{D}\\) is invariant to linear transformations of \\(\\bm{C}\\). In particular, an equivalent way to write the null hypothesis is as \\(H_0: \\bs\\Psi_{\\circ}^{-1/2} \\bm{C} = \\bm{0}_q\\), where \\(\\bs\\Psi_{\\circ} = \\bigoplus_{c=2}^C \\psi_c\\) is the diagonal of the true sampling variances of categories 2 through \\(C\\), omitting the first category. Thus, let me redefine \\[\n\\bs\\Omega = \\bs\\Psi_{\\circ}^{-1/2} \\bm{C} \\bs\\Psi \\bm{C}'\\bs\\Psi_{\\circ}^{-1/2},\n\\] \\(\\bm{z} = \\bs\\Omega^{-1/2}\\bs\\Psi_{\\circ}^{-1/2}\\bm{C}\\hat\\mu_c\\), and \\[\n\\bm{G} = \\bs\\Omega^{-1/2} \\bs\\Psi_{\\circ}^{-1/2} \\bm{C}.\n\\] This transformation of the constraint matrix will make it possible to find a closed-form expression for \\(\\bs\\Omega^{-1/2}\\).\nNow, observe that \\[\n\\begin{aligned}\n\\bs\\Omega &= \\bs\\Psi_{\\circ}^{-1/2} \\bm{C} \\bs\\Psi \\bm{C}'\\bs\\Psi_{\\circ}^{-1/2} \\\\\n&= \\bs\\Psi_{\\circ}^{-1/2} \\left(\\bs\\Psi_{\\circ} + \\psi_1 \\bm{1}_q \\bm{1}_q'\\right)\\bs\\Psi_{\\circ}^{-1/2} \\\\\n&= \\bm{I}_q + \\psi_1 \\bm{f} \\bm{f}',\n\\end{aligned}\n\\] where \\(\\bm{f} = \\bs\\Psi_{\\circ}^{-1/2} \\bm{1}_q = \\left[ \\psi_c^{-1/2}\\right]_{c = 2}^C\\). From the Woodbury identity, \\[\n\\bs\\Omega^{-1} = \\bm{I} - \\frac{1}{W} \\bm{f} \\bm{f}',\n\\] where \\(W = \\sum_{c=1}^C \\frac{1}{\\psi_c}\\).\nFasi, Higham, and Liu (2023) provide formulas for \\(p^{th}\\) roots of low-rank updates to scaled identity matrices. Their results provide a neat closed-form expression for \\(\\bs\\Omega^{-1/2}\\). From their Equation (1.9), \\[\n\\bs\\Omega^{-1/2} = \\mathbf{I}_q - \\kappa \\ \\bm{f} \\bm{f}',\n\\] where \\(\\kappa = \\frac{\\sqrt{\\psi_1}}{W \\sqrt{\\psi_1} + \\sqrt{W}}\\). Further, we can write the \\(q \\times C\\) matrix \\(\\bm{G}\\) as \\[\n\\begin{aligned}\n\\bm{G} &= \\bs\\Omega^{-1/2} \\bs\\Psi_{\\circ}^{-1/2} \\bm{C} \\\\\n&= \\left( \\mathbf{I}_q - \\kappa \\ \\bm{f} \\bm{f}' \\right) \\bs\\Psi_{\\circ}^{-1/2} \\left[-\\bm{1}_q, \\ \\bm{I}_q \\right] \\\\\n&= \\left[\\frac{\\kappa(W \\psi_1 - 1) - \\psi_1}{\\psi_1} \\bm{f},  \\left( \\mathbf{I}_q - \\kappa \\ \\bm{f} \\bm{f}' \\right) \\bs\\Psi_{\\circ}^{-1/2}\\right],\n\\end{aligned}\n\\] with entries given by \\[\ng_{sc} = \\begin{cases}\n\\frac{\\kappa(W \\psi_1 - 1) - \\psi_1}{\\psi_1 \\sqrt{\\psi_{s+1}}} & \\text{if} \\quad c = 1 \\\\\n\\frac{I(s+1 = c)}{\\sqrt{\\psi_{c}}} - \\frac{\\kappa}{\\psi_c \\sqrt{\\psi_{s+1}}} & \\text{if} \\quad c &gt; 1.\n\\end{cases}\n\\] Because \\(\\bm{D} = \\bm{G} \\bm{V}^R \\bm{G}'\\) and \\(\\bm{V}^R\\) is diagonal, we can write the entries of \\(\\bm{D}\\) as \\[\nd_{st} = \\sum_{c=1}^C g_{sc} g_{tc} V^R_c.\n\\] And because the variance estimators for each category are independent, \\[\n\\Var(d_{st}) = \\sum_{c=1}^C g_{sc}^2 g_{tc}^2 \\Var(V^R_c).\n\\] In prior work, we derived expressions for the Satterthwaite degrees of freedom for variances of average effect sizes, and the same formulas can be applied here with the category-specific \\(V^R_c\\). Let me write \\(\\nu_c = 2\\left[\\E(V^R_c)\\right]^2 / \\Var(V^R_c)\\) for the degrees of freedom corresponding to category \\(c\\). Then \\[\n\\Var(d_{st}) = 2 \\sum_{c=1}^C g_{sc}^2 g_{tc}^2 \\frac{\\psi_c^2}{\\nu_c}.\n\\] We can use this to obtain an expression for Zhang’s approximate degrees of freedom: \\[\n\\begin{aligned}\nq(q + 1)\\eta_Z^{-1} &= \\sum_{s=1}^q \\sum_{t = 1}^q \\Var(d_{st}) \\\\\n&= 2\\sum_{s=1}^q \\sum_{t = 1}^q \\sum_{c=1}^C g_{sc}^2 g_{tc}^2 \\frac{\\psi_c^2}{\\nu_c} \\\\\n&= 2\\sum_{c=1}^C \\frac{\\psi_c^2}{\\nu_c} \\left(\\sum_{s=1}^q g_{sc}^2\\right)^2.\n\\end{aligned}\n\\] Now, all we need to do is simplify… \\[\n\\begin{aligned}\n\\sum_{s=1}^q g_{s1}^2 &= \\sum_{s=1}^q \\frac{\\left(\\kappa(W \\psi_1 - 1) - \\psi_1\\right)^2}{\\psi_1^2 \\psi_{s+1}} \\\\\n&= \\frac{\\left(\\kappa(W \\psi_1 - 1) - \\psi_1\\right)^2}{\\psi_1^2} \\sum_{c=2}^C \\frac{1}{\\psi_{s+1}} \\\\\n&= \\frac{\\left(\\kappa(W \\psi_1 - 1) - \\psi_1\\right)^2}{\\psi_1^2} \\frac{(W \\psi_1 - 1)}{\\psi_1} \\\\\n&= \\text{...a bunch of tedious algebra...} \\\\\n&= \\frac{1}{\\psi_1^2} \\left(\\psi_1 - \\frac{1}{W}\\right)\n\\end{aligned}\n\\] and, for \\(c = 2,...,C\\), \\[\n\\begin{aligned}\n\\sum_{s=1}^q g_{sc}^2 &= \\sum_{s=1}^q \\left(\\frac{I(s+1 = c)}{\\sqrt{\\psi_{c}}} - \\frac{\\kappa}{\\psi_c \\sqrt{\\psi_{s+1}}}\\right)^2 \\\\\n&= \\frac{1}{\\psi_c} - \\frac{2 \\kappa}{\\psi_c^2} + \\frac{\\kappa^2}{\\psi_c^2}\\sum_{s=1}^q \\frac{1}{\\psi_{s+1}} \\\\\n&= \\frac{1}{\\psi_c} - \\frac{2 \\kappa}{\\psi_c^2} + \\frac{\\kappa^2}{\\psi_c^2}\\frac{(W \\psi_1 - 1)}{\\psi_1} \\\\\n&= \\text{...a bunch of tedious algebra...} \\\\\n&= \\frac{1}{\\psi_c^2} \\left(\\psi_c - \\frac{1}{W}\\right)\n\\end{aligned}\n\\] Thus, \\[\n\\begin{aligned}\nq(q + 1)\\eta_Z^{-1} &= 2\\sum_{c=1}^C \\frac{\\psi_c^2}{\\nu_c} \\left(\\sum_{s=1}^q g_{sc}^2\\right)^2 \\\\\n&= 2\\sum_{c=1}^C \\frac{1}{\\nu_c \\psi_c^2}\\left(\\psi_c - \\frac{1}{W}\\right)^2 \\\\\n&= 2\\sum_{c=1}^C \\frac{1}{\\nu_c}\\left(1 - \\frac{1}{\\psi_c W}\\right)^2\n\\end{aligned}\n\\] or, rearranging, \\[\n\\eta_Z = \\frac{C(C - 1)}{2 \\sum_{c=1}^C \\frac{1}{\\nu_c}\\left(1 - \\frac{1}{\\psi_c W}\\right)^2}.\n\\] It’s a surprisingly clean formula! Once these degrees of freedom are calculated, the degrees of freedom for the reference \\(F\\) distribution would be \\(q\\) and \\(\\eta_Z - q + 1\\).\nIn the paper, we also considered two other degrees of freedom approximations, which involve not only the variances of \\(d_{st}\\) but also the covariances between entries. In principle, one could follow similar algebra to get expressions for these other degrees of freedom as well. However, our simulations indicated that the other degrees of freedom approximations tend to be overly conservative and produce type-I error rates way below the nominal level (essentially, hardly ever rejecting the null) and less accurate than HTZ. So, there’s not much reason to work through them unless you find algebra enjoyable for its own sake.\nA further question about this cluster-robust Wald statistic is how to approximate its sampling distribution under specific alternative hypotheses. In other words, given a vector of means \\(\\mu_1,...,\\mu_C\\) where the null does not hold, plus some information to determine \\(\\psi_c\\) and \\(\\nu_c\\) for \\(c = 1,...,C\\), how could we approximate the distribution of \\(Q\\)? We need something like a non-central Hotelling’s \\(T^2\\) distribution…\n\n\n\nReuseCC BY 4.0CitationBibTeX citation:@online{pustejovsky2024,\n  author = {Pustejovsky, James E.},\n  title = {Approximating the Distribution of Cluster-Robust {Wald}\n    Statistics},\n  date = {2024-03-24},\n  url = {https://jepusto.com/posts/Approximating-cluster-robust-Wald-tests},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nPustejovsky, James E. 2024. “Approximating the Distribution of\nCluster-Robust Wald Statistics.” March 24, 2024. https://jepusto.com/posts/Approximating-cluster-robust-Wald-tests."
  },
  {
    "objectID": "posts/AERA-2019-SRMA-SIG/index.html",
    "href": "posts/AERA-2019-SRMA-SIG/index.html",
    "title": "Systematic Reviews and Meta-analysis SIG at AERA 2019",
    "section": "",
    "text": "This year, Dr. Laura Dunne and I are serving as program co-chairs for the AERA special interest group on Systematic Reviews and Meta-Analysis, which is a great group of scholars interested in the methodology and application of research synthesis to questions in education and the broader social sciences. We had a strong batch of submissions to the SIG and (since we’re new and still a fairly small group) only a few sessions to fill with them. In assembling this year’s program, Laura and I noted a few common themes that stood out to us. In this post, I’ll highlight a few of them and hopefully whet your appetite to hear more during our sessions at this year’s convention. And if you want to skip the details for now, just take a look at our handy pdf with the full SIG program."
  },
  {
    "objectID": "posts/AERA-2019-SRMA-SIG/index.html#sig-highlights",
    "href": "posts/AERA-2019-SRMA-SIG/index.html#sig-highlights",
    "title": "Systematic Reviews and Meta-analysis SIG at AERA 2019",
    "section": "SIG highlights",
    "text": "SIG highlights\nFirst, two of this year’s presentations deal with network meta-analysis, an approach that goes beyond a single intervention-control comparison, to instead synthesize evidence on the comparative effects of multiple alternative interventions (not just red pill vs blue pill, but also red versus green, green versus blue, etc.). Network meta-analysis is increasingly important in clinical medicine (for example, here’s a recent synthesis examining the relative efficacy of 21 different anti-depressant drugs) but it is still relatively rare in education and other social science meta-analyses. Not in this year’s SIG program though! Both our Sunday morning paper session and Monday round table feature applications of network meta-analysis: one on distance and face-to-face learning, and one on interventions for treatment of post-traumatic stress disorder.\nSecond, publication bias and other forms of outcome reporting bias remain one of the most vexing challenges for meta-analysis. Our Sunday morning paper session includes an innovative methodological study on how to detect selective outcome reporting in multi-level meta-analyses—an important setting where publication bias techniques have yet to be explored. Even with very sophisticated statistical tools, though, the best way to address publication bias is probably to try and prevent it in the first place. To that end, our Monday round table session includes a presentation on locating unreported outcome data for use in meta-analysis.\nThird, the Systematic Reviews and Meta-Analysis SIG has always included a mix of theory and practice. In this year’s program, we’ve tried to preserve that mix within each of our sessions, so that our Sunday paper session and Monday round table each include both methodological research and substantive applications of meta-analysis. We hope that this will promote interesting and valuable dialogues within our community.\nFinally, I am very excited that our business meeting will feature an address by Dr. Rebecca Maynard, who is the University Trustee Chair Professor of Education and Social Policy at the University of Pennsylvania Graduate School of Education, and an influential voice in the use of research synthesis methods to inform education and social policy. She’ll be speaking on Expanded Roles for Meta-Analysis in Supporting Evidence-Based Policy and Practice.\nBe sure to check out the SIG program for more details and other sessions of interest. I look forward to seeing everyone in Toronto!"
  },
  {
    "objectID": "posts/cluster-bootstrap-selection-model/index.html",
    "href": "posts/cluster-bootstrap-selection-model/index.html",
    "title": "Cluster-Bootstrapping a meta-analytic selection model",
    "section": "",
    "text": "Selective reporting of study results is a big concern for meta-analysts. By selective reporting, we mean the phenomenon where affirmative findings—that is, statistically significant findings in the theoretically expected direction—are more likely to be reported and more likely to be available for a systematic review compared to non-affirmative findings. Selective reporting arises due to biases in the publication process, on the part of journals, editors, and reviewers, as well as strategic decisions on part of the authors (Rothstein et al., 2006; Sutton, 2009). Research synthesists worry about selective reporting because it can distort the evidence base available for meta-analysis, almost like a fun-house mirror distorts your appearance, leading to inflation of average effect size estimates and biased estimates of heterogeneity.\nIf you read the meta-analysis methods literature, you will find scores of tools available to investigate and adjust for the biases created by selective reporting. Well known and widely used methods include:\nHowever, nearly all of the statistical methods here have the limitation that they are premised on observing independent effect sizes. That presents a problem for meta-analyses in education, psychology, and other social science fields, where it is very common to have meta-analyses involving dependent effect sizes.\nDependent effects occur in meta-analyses of group comparisons when primary studies report effects for multiple correlated measures of an outcome, at multiple points in time, or for multiple treatment groups compared to the same control group (Becker, 2000). Dependent effects are also common in meta-analyses of correlational effect sizes, where primary studies report more than one relevant correlation coefficient based on the same sample of participants. Methods such as multi-level meta-analysis (Van den Noortgate et al., 2013) and robust variance estimation (Hedges et al., 2010) are available to accommodate dependent effects when summarizing findings across studies or investigating moderators of effect size using meta-regression, but these techniques have yet to be extended to methods for testing or correcting bias due to selective reporting. Consequently, it’s pretty common to see research synthesis papers that use very sophisticated models for part of the analysis, but then use kludgey, awkward, or hacky approaches when it comes time to investigating selective reporting (Rodgers & Pustejovsky, 2020).\nAlong with a group of our colleagues from the American Institutes for Research, we are currently working on a project to develop better methods for investigating selective reporting issues in meta-analyses of dependent effect sizes. In this post, we will share an early peek under the hood at one little piece of what we’re studying, by sketching out what we think is a promising and pragmatic method for examining selective reporting while also accounting for effect size dependency. The method is to use a cluster-level bootstrap, which involves re-sampling clusters of observations (i.e., the set of multiple effect size estimates reported within a given primary study) to approximate the sampling distribution of an estimator (Boos, 2003; Cameron et al., 2008). To illustrate this technique, we will demonstrate how to bootstrap a Vevea-Hedges selection model.\nSelection models comprise a large class of models that have two parts: a model describing the evidence-generation process and a model describing the process by which evidence is reported (Hedges & Vevea, 2005). Vevea-Hedges selection models (Hedges, 1992; Hedges & Vevea, 1996; Vevea & Hedges, 1995) involve a random effects meta-regression model for the evidence-generation process and a step function for the reporting process. With a step function, we assume that the probability that an effect size estimate is observed depends on the range in which its p-value falls. For instance, effects with \\(.01 &lt; p \\leq .05\\) might have some probability \\(\\lambda_1\\) of being reported, effects with \\(.05 &lt; p \\leq .10\\) might have some other probability \\(\\lambda_2\\), and effects with \\(.10 &lt; p\\) might have some other probability \\(\\lambda_3\\).1 Because the Vevea-Hedges model and other selection models separate the data-generation process into these two distinct stages, their parameters have clear interpretations and they can be used to generate bias-adjusted estimates of the distribution of effect sizes and to test for selective reporting issues. The only problem is that available implementations of selection models do not account for effect size dependency—but that’s where cluster bootstrapping could potentially help."
  },
  {
    "objectID": "posts/cluster-bootstrap-selection-model/index.html#disclaimer",
    "href": "posts/cluster-bootstrap-selection-model/index.html#disclaimer",
    "title": "Cluster-Bootstrapping a meta-analytic selection model",
    "section": "Disclaimer",
    "text": "Disclaimer\nTo be clear, this post is based on work in progress. The cluster-bootstrap selection model that we’re going to demonstrate is an experimental and exploratory technique. We’re currently studying its properties and performance using Monte Carlo simulations, but we don’t have formal results to share yet. In the spirit of open and collaborative science, we wrote this post to demonstrate our approach to coding the method, in case others would like to experiment with the technique. Given that there are so few methods available for investigating selective reporting in meta-analyses with dependent effect sizes, we think this method is worth playing with and investigating further, and we would be happy to have others try it out as well. But, if you do so, please treat the results as tentative until we learn more about when the methods work well enough to trust the results."
  },
  {
    "objectID": "posts/cluster-bootstrap-selection-model/index.html#an-example",
    "href": "posts/cluster-bootstrap-selection-model/index.html#an-example",
    "title": "Cluster-Bootstrapping a meta-analytic selection model",
    "section": "An Example",
    "text": "An Example\nFor demonstrating this method, we will use data from a recent meta-analysis by Lehmann and colleagues (2018) that examined the effects of the color red on attractiveness judgments. The data is available via the metadat package (White et al., 2022). The dataset includes 81 effect sizes from 41 unique studies. You can browse the data for yourself here:\n\n\nCode\nlibrary(metadat)   # for the example dataset\nlibrary(tidyverse) # for tidying\nlibrary(janitor)   # for tidying variable names\nlibrary(metafor)   # for meta-analysis\nlibrary(boot)      # for bootstrapping\nlibrary(tictoc)    # for keeping time\n\nlehmann_dat &lt;- \n  dat.lehmann2018 %&gt;%\n  clean_names() %&gt;%\n  mutate(study = str_split_fixed(short_title, pattern = \"-\", n = 2)[, 1]) %&gt;%\n  arrange(study) %&gt;%\n  select(study, presentation = stimuli_presentation, yi, vi, everything())\n\n\n\n\n\n\n\n\n\nstudy\npresentation\nyi\nvi\nshort_title\nfull_citation\nyear\npr_publication\nsource_type\npreregistered\nmoderator_group\ncontext\ngender\ncolor_contrast\ncolor_form\nphoto_type\nphoto_similarity\ndv_type\ndv_items\ndv_scale\ndv_scale_bottom\ndv_scale_top\nlocation\ncontinent\nparticipants\nparticipant_notes\ndesign\neth_majority\neth_majority_detail\neth_stim\neth_match\nred_age\ncontrol_age\ncolor_red\ncolor_control\nred_original\ncolor_match\npresentation_control\nred_n\nred_m\nred_sd\ncontrol_n\ncontrol_m\ncontrol_sd\nsd_diff\nrm_r\ncontrol_attractiveness\nnotes\ntotal_sample_size\npooled\n\n\n\n\nBanas, 2014\nPaper\n0.06\n0.10\nBanas, 2014 - Exp 1\nBanas, K. (2014, July 7). Replication of Elliot et al. (2010) for CREP at the University of Edinburgh. Retrieved from osf.io/cvdpw\n2014.0\nNo\nCREP\nPre-Registered\nNo\nRomantic\nFemales\nGray\nBackground\nBust\nSame between conditions\nPerceived attractiveness\n3\n1-9\n1\n9\nScotland\nEurope\nStudents\nUndergrads\nBetween Subjects\nWhite\nWhite\nLatino\nMis-match\n20.43\n20.95\nLCh(50.0, 59.6, 31.3)\nLCh(50.0, -, 69.1)\nYes\nYes\nYes\n20\n6.05\n1.59\n19\n5.96\n1.49\nNA\nNA\n0.62\nNA\n39\nNA\n\n\nBerthold, 2013\nScreen\n0.55\n0.06\nBerthold, 2013 - Exp 1 - In Group\nBerthold, A. (2013). Unpublished data\n2013.0\nNo\nUnpublished/Online\nNot Pre-Registered\nNo\nRomantic\nFemales\nBlue\nBackground\nBust\nSame between conditions\nPerceived attractiveness\n4\n1-7\n1\n7\nGermany\nEurope\nStudents\nUndergrads\nBetween Subjects\nWhite\nWhite\nWhite\nMatched\n25.00\n25.00\nNo Data\nNo Data\nNo\nNo\nYes\n36\n2.31\n1.29\n33\n1.73\n0.65\nNA\nNA\n0.12\nNA\n69\nNA\n\n\nBigelow et al., 2013\nScreen\n0.31\n0.30\nBigelow et al., 2013 - Exp 1\nBigelow, M.G., Taylor, G. & Underwood, M. (2013). Context-moderated effect of color on physiological and self-report measures of emotional response. UNC Asheville Journal, Undergraduate Research Program, Asheville, NC\n2013.0\nYes\nJournal\nNot Pre-Registered\nNo\nRomantic\nFemales\nBlue\nBackground\nHead Shot\nSame between conditions\nSingle item\n1\n1-9\n1\n9\nUSA\nNorth America\nStudents\nUndergrads\nBetween Subjects\nNA\nNA\nNA\nNA\nNA\nNA\nLab(37.43/59.24/47.63)\nLab(36.68/34.86/87.99)\nNo\nNo\nYes\n6\n5.22\n1.77\n8\n4.56\n2.09\nNA\nNA\n0.44\nNA\n14\nNA\n\n\nBigelow et al., 2013\nScreen\n-0.73\n0.53\nBigelow et al., 2013 - Exp 1\nBigelow, M.G., Taylor, G. & Underwood, M. (2013). Context-moderated effect of color on physiological and self-report measures of emotional response. UNC Asheville Journal, Undergraduate Research Program, Asheville, NC\n2013.0\nYes\nJournal\nNot Pre-Registered\nNo\nRomantic\nMales\nBlue\nBackground\nHead Shot\nSame between conditions\nSingle item\n1\n1-9\n1\n9\nUSA\nNorth America\nStudents\nUndergrads\nBetween Subjects\nNA\nNA\nNA\nNA\nNA\nNA\nLab(37.43/59.24/47.63)\nLab(36.68/34.86/87.99)\nNo\nNo\nYes\n4\n4.65\n2.05\n4\n6.28\n1.81\nNA\nNA\n0.66\nNA\n8\nNA\n\n\nBlech, 2014\nScreen\n0.08\n0.03\nBlech, 2014 - Exp 1\nBlech, C. (2014, August 4). Replication of Elliot et al. (2010). Red, rank, and romance in women viewing men. Retrieved from osf.io/tx2u5\n2014.0\nNo\nUnpublished/Online\nNot Pre-Registered\nNo\nRomantic\nFemales\nWhite\nBackground\nHead Shot\nSame between conditions\nPerceived attractiveness, German translation\n4\n1-9\n1\n9\nGermany\nEurope\nStudents\nUndergrads\nBetween Subjects\nWhite\nWhite\nLatino\nMis-match\nNA\nNA\nNo Data\nNo Data\nNo\nNo\nNo\n71\n5.36\n1.43\n78\n5.25\n1.48\nNA\nNA\n0.53\nNot part of CREP because Used white as control condition, dropped yellow as not an original control color, age not included because separated into categories (&lt;25, 26-40, &gt;=41)\n149\nNA\n\n\nBlech, 2015\nScreen\n-0.35\n0.05\nBlech, 2015 - Class Exp\nBlech, C. (2015). Unpublished data from a class experiment\n2015.0\nNo\nUnpublished/Online\nNot Pre-Registered\nNo\nRomantic\nFemales\nWhite\nClothing\nBust\nSame between conditions\nPerceived attractiveness\n4\n1-9\n1\n9\nGermany\nEurope\nAdults\nNA\nBetween Subjects\nWhite\nWhite\nWhite\nMatched\n31.40\n31.40\nNA\nNA\nNo\nNo\nNo\n37\n3.95\n1.29\n37\n4.45\n1.50\nNA\nNA\n0.43\nNA\n74\nNA\n\n\nBoelk & Madden, 2014\nPaper\n-0.27\n0.06\nBoelk & Madden, 2014 - Exp 1\nBoelk, K., & Madden, W. (2014, August 5). Fork of Elliot, A. J., Niesta Kayser, D., Greitemeyer, T., Lichtenfeld, S., Gramzow, R. H., Maier, M. A., & Liu, H. (2010). Retrieved from osf.io/zf7c9\n2014.0\nNo\nCREP\nPre-Registered\nNo\nRomantic\nFemales\nGray\nBackground\nBust\nSame between conditions\nPerceived attractiveness\n3\n1-9\n1\n9\nUSA\nNorth America\nStudents\nUndergrads\nBetween Subjects\nWhite\nWhite\nLatino\nMis-match\n19.06\n19.35\nLCh(50.0, 59.6, 31.3)\nLCh(50.0, -, 69.1)\nYes\nYes\nYes\n34\n6.03\n1.24\n34\n6.40\n1.49\nNA\nNA\n0.68\nNA\n68\nNA\n\n\nBuechner et al., 2015\nPaper\n0.68\n0.09\nBuechner et al., 2015 - Exp 1 - Prideful Pose\nBuechner, V. L., Maier, M. A., Lichtenfeld, S., & Elliot, A. J. (2015). Emotion Expression and Color: Their Joint Influence on Perceived Attractiveness and Social Position. Current Psychology, 34(2), 422-433. http://doi.org/10.1007/s12144-014-9266-x\n2015.0\nYes\nJournal\nNot Pre-Registered\nNo\nRomantic\nFemales\nBlue\nDot\nBust\nSame between conditions\nPerceived attractiveness\n3\n1-9\n1\n9\nGermany\nEurope\nStudents\nHigh School\nBetween Subjects\nWhite\nWhite\nWhite\nMatched\n16.88\n16.88\nLCh(50.9, 59.7, 25.7)\nLCh(49.2, 60.2, 278.2)\nYes\nYes\nYes\n21\n4.30\n1.65\n29\n3.21\n1.53\nNA\nNA\n0.28\nNA\n50\nNA\n\n\nCostello et al., 2017\nPaper\n-0.12\n0.03\nCostello et al., 2017 - Exp 1\nCostello J., Groeneboom L. & Pollet T. (2017). Romantic red: Do red products enhance the attractiveness of the consumer? Unpublished masters degree manuscript, University of Leiden\n2017.0\nNo\nUnpublished/Online\nNot Pre-Registered\nNo\nRomantic\nMales\nBlue\nItem\nBust\nSame between conditions\nSingle item perceived attractiveness\n1\n1-5\n1\n5\nNetherlands\nEurope\nStudents\nUndergrad\nBetween Subjects\nWhite\nWhite\nAsian\nMis-match\n22.60\nNA\nNA\nNA\nNo\nNo\nYes\n65\n2.25\n1.04\n64\n2.38\n0.95\nNA\nNA\n0.34\nNA\n129\nNA\n\n\nCostello et al., 2017\nPaper\n0.08\n0.02\nCostello et al., 2017 - Exp 2\nCostello J., Groeneboom L. & Pollet T. (2017). Romantic red: Do red products enhance the attractiveness of the consumer? Unpublished masters degree manuscript, University of Leiden\n2017.0\nNo\nUnpublished/Online\nNot Pre-Registered\nNo\nRomantic\nFemales\nBlue/Green\nItem\nHead Shot\nSame between conditions\nSingle item perceived attractiveness\n1\n1-5\n1\n5\nNetherlands\nEurope\nStudents\nUndergrad\nBetween Subjects\nWhite\nWhite\nWhite\nMatched\n21.32\n21.06\nNo Data\nNo Data\nNo\nNo\nYes\n67\n2.60\n1.37\n140\n2.49\n1.38\nNA\nNA\n0.37\nSame filters applied as in Exp 1 (excluded homosexual and preferred not to answer). Control combines blue and green\n207\nNA\n\n\nElliot & Maier, 2013\nPaper\n0.25\n0.03\nElliot & Maier, 2013 - Exp 1\nElliot, A. J., & Maier, M. a. (2013). The red-attractiveness effect, applying the Ioannidis and Trikalinos (2007b) test, and the broader scientific context: a reply to Francis (2013). Journal of Experimental Psychology. General, 142(1), 297-300. http://doi.org/10.1037/a0029592\n2013.0\nYes\nJournal\nNot Pre-Registered\nNo\nRomantic\nFemales\nGray\nBackground\nBust\nSame between conditions\nPerceived attractiveness\n3\n1-9\n1\n9\nUSA\nNorth America\nStudents\nUndergrads\nBetween Subjects\nWhite\nWhite\nLatino\nMis-match\n19.47\n19.47\nLCh(42.6, 45.2, 15.8)\nLCh(43.0, -, 296.7)\nNo\nYes\nYes\n75\n6.29\n1.35\n69\n5.93\n1.49\nNA\nNA\n0.62\nNA\n144\nNA\n\n\nElliot & Niesta, 2008\nPaper\n0.62\n0.14\nElliot & Niesta, 2008 - Exp 4\nElliot, A. J., & Niesta, D. (2008). Romantic red: red enhances men's attraction to women. Journal of Personality and Social Psychology, 95(5), 1150-1164. http://doi.org/10.1037/0022-3514.95.5.1150\n2008.0\nYes\nJournal\nNot Pre-Registered\nNo\nRomantic\nMales\nGreen\nBackground\nBust\nSame between conditions\nPerceived attractiveness\n2\n1-9\n1\n9\nUSA\nNorth America\nStudents\nUndergrads\nBetween Subjects\nWhite\nWhite\nWhite\nMatched\n19.97\n19.97\nLCh(46.1, 51.2, 29.3)\nLCh(46.1, 51.0, 147.6)\nYes\nYes\nYes\n16\n6.29\n0.89\n15\n5.66\n1.09\nNA\nNA\n0.58\nNA\n31\nNA\n\n\nElliot & Niesta, 2008\nPaper\n0.67\n0.11\nElliot & Niesta, 2008 - Exp 3\nElliot, A. J., & Niesta, D. (2008). Romantic red: red enhances men's attraction to women. Journal of Personality and Social Psychology, 95(5), 1150-1164. http://doi.org/10.1037/0022-3514.95.5.1150\n2008.0\nYes\nJournal\nNot Pre-Registered\nNo\nRomantic\nMales\nGray\nBackground\nBust\nSame between conditions\nPerceived attractiveness\n2\n1-9\n1\n9\nUSA\nNorth America\nStudents\nUndergrads\nBetween Subjects\nWhite\nWhite\nWhite\nMatched\n20.00\n20.00\nLCh(50.0, 58.7, 30.3)\nLCh(50.0, -, 52.6)\nYes\nYes\nYes\n20\n6.65\n1.10\n17\n5.91\n1.07\nNA\nNA\n0.61\nNA\n37\nNA\n\n\nElliot & Niesta, 2008\nPaper\n0.84\n0.19\nElliot & Niesta, 2008 - Exp 5\nElliot, A. J., & Niesta, D. (2008). Romantic red: red enhances men's attraction to women. Journal of Personality and Social Psychology, 95(5), 1150-1164. http://doi.org/10.1037/0022-3514.95.5.1150\n2008.0\nYes\nJournal\nNot Pre-Registered\nNo\nRomantic\nMales\nBlue\nClothing\nBust\nSame between conditions\nPerceived attractiveness\n2\n1-9\n1\n9\nUSA\nNorth America\nStudents\nUndergrads\nBetween Subjects\nWhite\nWhite\nWhite\nMatched\n19.77\n19.77\nLCh(45.9, 54.8, 32.5)\nLCh(46.0, 54.9, 283.0)\nYes\nYes\nYes\n12\n7.21\n1.05\n11\n6.09\n1.49\nNA\nNA\n0.64\nNA\n23\nNA\n\n\nElliot & Niesta, 2008\nPaper\n0.75\n0.13\nElliot & Niesta, 2008 - Exp 2\nElliot, A. J., & Niesta, D. (2008). Romantic red: red enhances men's attraction to women. Journal of Personality and Social Psychology, 95(5), 1150-1164. http://doi.org/10.1037/0022-3514.95.5.1150\n2008.0\nYes\nJournal\nNot Pre-Registered\nNo\nRomantic\nMales\nWhite\nBackground\nBust\nSame between conditions\nPerceived attractiveness\n2\n1-9\n1\n9\nUSA\nNorth America\nStudents\nUndergrads\nBetween Subjects\nWhite\nWhite\nWhite\nMatched\n19.27\n19.27\nLCh(55.5, 78.0, 28.0)\nNo Data\nYes\nNo\nYes\n16\n7.07\n0.78\n16\n6.13\n1.53\nNA\nNA\n0.64\nAge includes both male and female participants.\n32\nNA\n\n\nElliot & Niesta, 2008\nPaper\n1.08\n0.17\nElliot & Niesta, 2008 - Exp 1\nElliot, A. J., & Niesta, D. (2008). Romantic red: red enhances men's attraction to women. Journal of Personality and Social Psychology, 95(5), 1150-1164. http://doi.org/10.1037/0022-3514.95.5.1150\n2008.0\nYes\nJournal\nNot Pre-Registered\nNo\nRomantic\nMales\nWhite\nBackground\nBust\nSame between conditions\nPerceived attractiveness\n2\n1-9\n1\n9\nUSA\nNorth America\nStudents\nUndergrads\nBetween Subjects\nWhite\nWhite\nWhite\nMatched\n20.52\n20.52\nLCh(50.3, 58.8, 29.9)\nNo Data\nYes\nNo\nYes\n15\n7.33\n0.90\n12\n6.25\n1.05\nNA\nNA\n0.66\nNA\n27\nNA\n\n\nElliot et al., 2010\nPaper\n0.82\n0.13\nElliot et al., 2010 - Exp 3\nElliot, A. J., Niesta Kayser, D., Greitemeyer, T., Lichtenfeld, S., Gramzow, R. H., Maier, M. A., & Liu, H. (2010). Red, rank, and romance in women viewing men. Journal of Experimental Psychology: General, 139(3), 399-417. http://doi.org/10.1037/a0019689\n2010.0\nYes\nJournal\nNot Pre-Registered\nNo\nRomantic\nFemales\nGray\nBackground\nBust\nSame between conditions\nPerceived attractiveness\n3\n1-9\n1\n9\nUSA\nNorth America\nStudents\nUndergrads\nBetween Subjects\nWhite\nWhite\nLatino\nMis-match\n19.64\n19.64\nLCh(50.0, 59.6, 31.3)\nLCh(50.0, -, 69.1)\nYes\nYes\nYes\n16\n6.69\n1.22\n17\n5.27\n2.04\nNA\nNA\n0.53\ndf doesn't match sample size\n33\nNA\n\n\nElliot et al., 2010\nPaper\n0.61\n0.08\nElliot et al., 2010 - Exp 4\nElliot, A. J., Niesta Kayser, D., Greitemeyer, T., Lichtenfeld, S., Gramzow, R. H., Maier, M. A., & Liu, H. (2010). Red, rank, and romance in women viewing men. Journal of Experimental Psychology: General, 139(3), 399-417. http://doi.org/10.1037/a0019689\n2010.0\nYes\nJournal\nNot Pre-Registered\nNo\nRomantic\nFemales\nGreen\nClothing\nBust\nSame between conditions\nMehrabian & Blum's Perceived attractiveness\n4\n1-9\n1\n9\nChina\nAsia\nStudents\nUndergrads\nBetween Subjects\nChinese\nChinese\nChinese\nMatched\n20.60\n20.60\nLCh(51.3, 51.7, 30.1)\nLCh(51.5, 51.6, 136.6)\nYes\nYes\nYes\n27\n6.32\n1.09\n28\n5.50\n1.50\nNA\nNA\n0.56\nNA\n55\nNA\n\n\nElliot et al., 2010\nPaper\n0.90\n0.21\nElliot et al., 2010 - Exp 1\nElliot, A. J., Niesta Kayser, D., Greitemeyer, T., Lichtenfeld, S., Gramzow, R. H., Maier, M. A., & Liu, H. (2010). Red, rank, and romance in women viewing men. Journal of Experimental Psychology: General, 139(3), 399-417. http://doi.org/10.1037/a0019689\n2010.0\nYes\nJournal\nNot Pre-Registered\nNo\nRomantic\nFemales\nWhite\nBackground\nBust\nSame between conditions\nPerceived attractiveness\n3\n1-9\n1\n9\nUSA\nNorth America\nStudents\nUndergrads\nBetween Subjects\nWhite\nWhite\nWhite\nMatched\n20.19\n20.19\nLCh(49.6, 58.8, 30.4)\nNo Data\nYes\nNo\nYes\n10\n6.79\n1.00\n11\n5.67\n1.34\nNA\nNA\n0.58\ndf doesn't match sample size\n21\nNA\n\n\nElliot et al., 2010\nPaper\n0.82\n0.16\nElliot et al., 2010 - Exp 7\nElliot, A. J., Niesta Kayser, D., Greitemeyer, T., Lichtenfeld, S., Gramzow, R. H., Maier, M. A., & Liu, H. (2010). Red, rank, and romance in women viewing men. Journal of Experimental Psychology: General, 139(3), 399-417. http://doi.org/10.1037/a0019689\n2010.0\nYes\nJournal\nNot Pre-Registered\nNo\nRomantic\nFemales\nBlue\nClothing\nBust\nSame between conditions\nManer et al perceived attractiveness\n1\n1-9\n1\n9\nEngland\nEurope\nStudents\nUndergrads\nBetween Subjects\nWhite\nWhite\nWhite\nMatched\n19.44\n19.44\nLCh(54.8, 43.2, 30.3)\nLCh(55.1, 43.7, 283.0)\nYes\nYes\nYes\n12\n7.50\n1.17\n15\n6.13\n1.89\nNA\nNA\n0.64\nNA\n27\nNA\n\n\nElliot et al., 2010\nPaper\n1.05\n0.15\nElliot et al., 2010 - Exp 2\nElliot, A. J., Niesta Kayser, D., Greitemeyer, T., Lichtenfeld, S., Gramzow, R. H., Maier, M. A., & Liu, H. (2010). Red, rank, and romance in women viewing men. Journal of Experimental Psychology: General, 139(3), 399-417. http://doi.org/10.1037/a0019689\n2010.0\nYes\nJournal\nNot Pre-Registered\nNo\nRomantic\nFemales\nWhite\nBackground\nBust\nSame between conditions\nPerceived attractiveness\n3\n1-9\n1\n9\nUSA\nNorth America\nStudents\nUndergrads\nBetween Subjects\nWhite\nWhite\nWhite\nMatched\n20.46\n20.46\nLCh(49.6, 58.8, 30.4)\nNo Data\nYes\nNo\nYes\n20\n7.15\n0.74\n12\n6.20\n1.08\nNA\nNA\n0.65\nSample size taken from Francis, t(df) seems to be using ANOVA df for post-hoc, age data for both male and female participants (separate was not provided)\n32\nNA\n\n\nElliot et al., 2013\nPaper\n0.64\n0.10\nElliot et al., 2013 - Exp 1\nElliot, A. J., Tracy, J. L., Pazda, A. D., & Beall, A. T. (2013). Red enhances women's attractiveness to men: First evidence suggesting universality. Journal of Experimental Social Psychology, 49(1), 165-168. http://doi.org/10.1016/j.jesp.2012.07.017\n2013.0\nYes\nJournal\nNot Pre-Registered\nNo\nRomantic\nMales\nBlue\nBackground\nHead Shot\nSame between conditions\nPerceived attractiveness\n3\n1-5\n1\n5\nBurkina Faso\nAfrica\nAdults\nNA\nBetween Subjects\nBlack\nBlack\nBlack\nMatched\n26.80\n26.80\nLCh(42.7, 51.5, 20.4)\nLCh(43.4, 51.5, 269.8)\nNo\nYes\nYes\n21\n4.62\n0.59\n21\n4.14\n0.85\nNA\nNA\n0.78\nNA\n42\nNA\n\n\nFrazier, 2014\nPaper\n0.09\n0.06\nFrazier, 2014 - Exp 1\nFrazier, A. (2014, November 13). Fork of Elliot, A. J., Niesta Kayser, D., Greitemeyer, T., Lichtenfeld, S., Gramzow, R. H., Maier, M. A., & Liu, H. (2010). Retrieved from osf.io/u0mig\n2014.0\nNo\nCREP\nPre-Registered\nNo\nRomantic\nFemales\nGray\nBackground\nBust\nSame between conditions\nPerceived attractiveness\n3\n1-9\n1\n9\nUSA\nNorth America\nStudents\nUndergrads\nBetween Subjects\nWhite\nWhite\nLatino\nMis-match\n18.90\n18.90\nLCh(50.0, 59.6, 31.3)\nLCh(50.0, -, 69.1)\nYes\nYes\nYes\n29\n6.09\n1.39\n39\n5.97\n1.26\nNA\nNA\n0.62\nNA\n68\nNA\n\n\nGilston & Privitera, 2016\nScreen\n1.92\n0.04\nGilston & Privitera, 2016 - Exp 1 - Healthy\nGilston, A., & Privitera, G. J. (2015). A 'Healthy' Color: Information About Healthy Eating Attenuates the 'Red Effect.' Global Journal of Health Science, 8(1), 56-61. https://doi.org/10.5539/gjhs.v8n1p56\n2016.0\nYes\nJournal\nNot Pre-Registered\nNo\nRomantic\nMales\nWhite\nClothing\nHead Shot\nSame between conditions\nPerceived attractiveness\n1\n1-7\n1\n7\nUSA\nNorth America\nStudents\nUndergrad\nWithin Subjects\nNA\nNA\nWhite\nNA\n19.85\n19.85\nNo Data\nNo Data\nNo\nNo\nYes\n54\n5.65\n1.51\n54\n3.09\n1.09\nNA\n0.89\n0.35\nNA\n54\n1.32\n\n\nGueguen, 2012\nPaper\n0.74\n0.05\nGueguen, 2012 - Exp 1\nGueguen, N. (2012). Color and Women Attractiveness: When Red Clothed Women Are Perceived to Have More Intense Sexual Intent. The Journal of Social Psychology, 152(3), 261-265. http://doi.org/10.1080/00224545.2011.605398\n2012.0\nYes\nJournal\nNot Pre-Registered\nNo\nRomantic\nMales\nBlue/Green/White\nClothing\nBust\nSame between conditions\nPerceived attractiveness\n1\n1-9\n1\n9\nFrance\nEurope\nStudents\nUndergrads\nBetween Subjects\nNA\nNA\nNA\nNA\n19.20\n19.20\nNo Data\nNo Data\nNo\nNo\nYes\n30\n5.95\n1.24\n90\n5.04\n1.22\nNA\nNA\n0.50\nControl is average of blue, white, and green\n120\nNA\n\n\nHesslinger et al. 2015\nPaper\n0.46\n0.13\nHesslinger et al. 2015 - Exp 2\nHesslinger, V. M., Goldbach, L., Carbon, C.-C., Allgemeine, A., Psychologie, E., & Note, A. (2015). Men in red: A reexamination of the red-attractiveness effect. Psychonomic Bulletin & Review, 55(4), 1-6. http://doi.org/10.3758/s13423-015-0866-8\n2015.0\nYes\nJournal\nNot Pre-Registered\nNo\nRomantic\nFemales\nWhite\nBackground\nFull Body\nSame between conditions\nPerceived attractiveness, German translation\n3\n1-9\n1\n9\nGermany\nEurope\nStudents\nUndergrads\nBetween Subjects\nWhite\nWhite\nWhite\nMatched\n20.20\n20.20\nCIE-Lab(50, 51, 30)\nNo Data\nYes\nNo\nYes\n16\n4.98\n0.70\n16\n4.63\n0.79\nNA\nNA\n0.45\nAge includes both male and female participants.\n32\nNA\n\n\nHesslinger et al. 2015\nPaper\n0.06\n0.06\nHesslinger et al. 2015 - Exp 1\nHesslinger, V. M., Goldbach, L., Carbon, C.-C., Allgemeine, A., Psychologie, E., & Note, A. (2015). Men in red: A reexamination of the red-attractiveness effect. Psychonomic Bulletin & Review, 55(4), 1-6. http://doi.org/10.3758/s13423-015-0866-8\n2015.0\nYes\nJournal\nNot Pre-Registered\nNo\nRomantic\nFemales\nWhite\nBackground\nBust\nSame between conditions\nPerceived attractiveness, German translation\n3\n1-9\n1\n9\nGermany\nEurope\nStudents\nUndergrads\nBetween Subjects\nWhite\nWhite\nWhite\nMatched\n21.20\n22.60\nCIE-Lab(50, 51, 30)\nNo Data\nYes\nNo\nYes\n35\n5.71\n1.65\n37\n5.62\n1.44\nNA\nNA\n0.58\nAge includes both male and female participants.\n72\nNA\n\n\nJohnson et al., 2015\nPaper\n-0.01\n0.05\nJohnson et al., 2015 - Exp 1\nJohnson, K., Meltzer, A., & Grahe, J. E. (2015, October 12). Fork of Elliot, A. J., Niesta Kayser, D., Greitemeyer, T., Lichtenfeld, S., Gramzow, R. H., Maier, M. A., & Liu, H. (2010). Retrieved from osf.io/ictud\n2014.0\nNo\nCREP\nPre-Registered\nNo\nRomantic\nFemales\nGray\nBackground\nBust\nSame between conditions\nPerceived attractiveness\n3\n1-9\n1\n9\nUSA\nNorth America\nStudents\nUndergrads\nBetween Subjects\nWhite\nWhite\nLatino\nMis-match\n18.94\n18.92\nLCh(50.0, 59.6, 31.3)\nLCh(50.0, -, 69.1)\nYes\nYes\nYes\n35\n5.97\n1.60\n38\n5.99\n1.34\nNA\nNA\n0.62\nYellow group also run, but not included\n73\nNA\n\n\nKhislavsky, 2016\nPaper\n0.06\n0.02\nKhislavsky, 2016 - Exp 1\nKhislavsky, A. (2016, March 14). Replication of Elliot et al. (2010). Red, rank, and romance in women viewing men. Retrieved from osf.io/f2udj\n2015.0\nNo\nUnpublished/Online\nNot Pre-Registered\nNo\nRomantic\nFemales\nGray\nBackground\nBust\nSame between conditions\nPerceived attractiveness\n3\n1-9\n1\n9\nUSA\nNorth America\nStudents\nUndergrads\nBetween Subjects\nLatino\nLatino\nLatino\nMatched\n19.91\n20.04\n~LCh(50.0, 59.6, 31.3)\n~LCh(50.0, -, 69.1)\nYes\nYes\nYes\n95\n5.73\n1.76\n92\n5.61\n1.84\nNA\nNA\n0.58\nNot reviewed by CREP\n187\nNA\n\n\nKirsch, 2015\nScreen\n-0.47\n0.03\nKirsch, 2015 - Exp 1 - Heterosexual\nKirsch, F. (2015). Wahrgenommene Attraktivitaet und sexuelle Orientierung: Die Wirkung von Rot und Farbpraeferenzen (Perceived attractiveness and sexual orientation: The effects of red and color preferences). Wiesbaden: Springer. doi: 10.1007/978-3-658-08405-9\n2015.0\nNo\nUnpublished/Online\nNot Pre-Registered\nNo\nRomantic\nFemales\nWhite\nClothing\nBust\nSame between conditions\nPerceived attractiveness\n2\n1-9\n1\n9\nGermany\nEurope\nOnline\nNA\nBetween Subjects\nNA\nNA\nWhite\nNA\n22.45\n22.45\nNo Data\nNo Data\nNo\nNo\nNo\n76\n5.33\n1.83\n85\n6.13\n1.54\nNA\nNA\n0.64\nCross-gender rating (females rating males). Age includes participants in all conditions.\n161\nNA\n\n\nKirsch, 2015\nScreen\n0.32\n0.04\nKirsch, 2015 - Exp 1 - Heterosexual\nKirsch, F. (2015). Wahrgenommene Attraktivitaet und sexuelle Orientierung: Die Wirkung von Rot und Farbpraeferenzen (Perceived attractiveness and sexual orientation: The effects of red and color preferences). Wiesbaden: Springer. doi: 10.1007/978-3-658-08405-9\n2015.0\nNo\nMonograph\nNot Pre-Registered\nNo\nRomantic\nMales\nWhite\nClothing\nBust\nSame between conditions\nPerceived attractiveness\n2\n1-9\n1\n9\nGermany\nEurope\nOnline\nNA\nBetween Subjects\nNA\nNA\nWhite\nNA\n22.45\n22.45\nNo Data\nNo Data\nNo\nNo\nNo\n57\n6.85\n1.44\n48\n6.42\n1.22\nNA\nNA\n0.68\nCross-gender rating (males rating females). Age includes participants in all conditions.\n105\nNA\n\n\nKirsch, 2015\nNA\n0.03\n0.07\nKirsch, 2015 - Exp 2\nKirsch, F. (2015). Wahrgenommene Attraktivitaet und sexuelle Orientierung: Die Wirkung von Rot und Farbpraeferenzen (Perceived attractiveness and sexual orientation: The effects of red and color preferences). Wiesbaden: Springer. doi: 10.1007/978-3-658-08405-9\n2015.0\nNo\nMonograph\nNot Pre-Registered\nNo\nRomantic\nMales\nWhite\nClothing\nBust\nSame between conditions\nPerceived attractiveness\n2\n1-9\n1\n9\nGermany\nEurope\nStudents\nUndergrads\nBetween Subjects\nNA\nNA\nNA\nNA\nNA\nNA\nNo Data\nNo Data\nNo\nNo\nNo\n28\n6.55\n1.17\n28\n6.52\n1.36\nNA\nNA\n0.69\nMeans are only for cross-gender rating (males rating females)\n56\nNA\n\n\nLegate et al., 2015\nPaper\n-0.35\n0.08\nLegate et al., 2015 - Exp 1\nLegate, N., Baciu, C., Horne, L. M., Fiol, S., Paniagua, D., Muqeet, M., & Zachocki, E. (2015, June 27). Replication of Elliot et al. (2010) at IIT. Retrieved from osf.io/zih7c\n2015.0\nNo\nCREP\nPre-Registered\nNo\nRomantic\nFemales\nGray\nBackground\nBust\nSame between conditions\nPerceived attractiveness\n3\n1-9\n1\n9\nUSA\nNorth America\nStudents\nUndergrads\nBetween Subjects\nWhite\nWhite (21 white, 16 asian)\nLatino\nMis-match\n20.73\n20.25\n~LCh(50.0, 59.6, 31.3)\n~LCh(50.0, -, 69.1)\nYes\nYes\nYes\n25\n5.76\n1.61\n23\n6.33\n1.58\nNA\nNA\n0.67\nNA\n48\nNA\n\n\nLehmann & Calin\nScreen\n0.39\n0.07\nLehmann & Calin-Jageman, 2015 - Class Exp\nLehmann & Calin-Jageman (2017) Is red really romantic? Direct replications suggest little to no effect of the color red on perceived attractiveness for men and women. Social Psychology.\n2015.0\nNo\nUnpublished/Online\nNot Pre-Registered\nNo\nRomantic\nFemales\nWhite\nBackground\nBust\nSame between conditions\nPerceived attractiveness\n2\n1-9\n1\n9\nUSA\nNorth America\nOnline\nMturk\nBetween Subjects\nWhite\nWhite\nWhite\nMatched\n31.70\n36.15\nNo Data\nNo Data\nNo\nNo\nNo\n29\n4.34\n1.63\n28\n3.69\n1.65\nNA\nNA\n0.34\nNA\n57\nNA\n\n\nLehmann & Calin\nPaper\n-0.11\n0.03\nLehmann & Calin-Jageman, 2017 - Exp 1\nLehmann & Calin-Jageman (2017) Is red really romantic? Direct replications suggest little to no effect of the color red on perceived attractiveness for men and women. Social Psychology.\n2017.0\nYes\nJournal\nPre-Registered\nNo\nRomantic\nFemales\nWhite\nBackground\nBust\nSame between conditions\nPerceived attractiveness\n2\n1-9\n1\n9\nUSA\nNorth America\nStudents\nUndergrads\nBetween Subjects\nLatino\nMixed\nWhite/Latino\nMatched\n20.41\n21.22\nLCh(51.3, 58.2, 29.5)\nNo Data\nYes\nNo\nYes\n56\n4.67\n2.05\n60\n4.89\n1.80\nNA\nNA\n0.49\nNA\n116\nNA\n\n\nLehmann & Calin\nScreen\n-0.08\n0.02\nLehmann & Calin-Jageman, 2017 - Exp 2\nLehmann & Calin-Jageman (2017) Is red really romantic? Direct replications suggest little to no effect of the color red on perceived attractiveness for men and women. Social Psychology.\n2017.0\nYes\nJournal\nPre-Registered\nNo\nRomantic\nFemales\nGray\nBackground\nBust\nSame between conditions\nPerceived attractiveness\n2\n1-9\n1\n9\nUSA\nNorth America\nOnline\nMturk\nBetween Subjects\nWhite\nWhite\nWhite/Latino\nMatched\n36.05\n38.32\nNo Data\nNo Data\nNo\nNo\nNo\n114\n5.24\n1.58\n130\n5.38\n1.85\nNA\nNA\n0.55\nNA\n244\nNA\n\n\nLehmann & Calin\nScreen\n-0.23\n0.04\nLehmann & Calin-Jageman, 2015 - Class Exp\nLehmann & Calin-Jageman (2017) Is red really romantic? Direct replications suggest little to no effect of the color red on perceived attractiveness for men and women. Social Psychology.\n2015.0\nNo\nUnpublished/Online\nNot Pre-Registered\nNo\nRomantic\nMales\nWhite\nBackground\nBust\nSame between conditions\nPerceived attractiveness\n2\n1-9\n1\n9\nUSA\nNorth America\nOnline\nMturk\nBetween Subjects\nWhite\nWhite\nWhite\nMatched\n33.15\n33.88\nNo Data\nNo Data\nNo\nNo\nNo\n56\n5.24\n1.59\n46\n5.59\n1.34\nNA\nNA\n0.57\nNA\n102\nNA\n\n\nLehmann & Calin\nScreen\n0.10\n0.02\nLehmann & Calin-Jageman, 2017 - Exp 2\nLehmann & Calin-Jageman (2017) Is red really romantic? Direct replications suggest little to no effect of the color red on perceived attractiveness for men and women. Social Psychology.\n2017.0\nYes\nJournal\nPre-Registered\nNo\nRomantic\nMales\nGray\nBackground\nBust\nSame between conditions\nPerceived attractiveness\n2\n1-9\n1\n9\nUSA\nNorth America\nOnline\nMturk\nBetween Subjects\nWhite\nWhite\nWhite\nMatched\n35.78\n35.85\nNo Data\nNo Data\nNo\nNo\nNo\n104\n6.51\n1.32\n106\n6.38\n1.32\nNA\nNA\n0.67\nNA\n210\nNA\n\n\nLehmann & Calin\nPaper\n0.00\n0.14\nLehmann & Calin-Jageman, 2017 - Exp 1\nLehmann & Calin-Jageman (2017) Is red really romantic? Direct replications suggest little to no effect of the color red on perceived attractiveness for men and women. Social Psychology.\n2017.0\nYes\nJournal\nPre-Registered\nNo\nRomantic\nMales\nWhite\nBackground\nBust\nSame between conditions\nPerceived attractiveness\n2\n1-9\n1\n9\nUSA\nNorth America\nStudents\nUndergrads\nBetween Subjects\nMixed\nMixed\nWhite\nMis-match\n20.81\n20.45\nLCh(51.3, 58.2, 29.5)\nNo Data\nYes\nNo\nYes\n21\n6.52\n1.16\n11\n6.52\n1.56\nNA\nNA\n0.69\nNA\n32\nNA\n\n\nLin, 2014\nPaper\n1.53\n0.13\nLin, 2014 - Exp 1\nLin, H. (2014). Red-colored products enhance the attractiveness of women. Displays, 35(4), 202-205. http://doi.org/10.1016/j.displa.2014.05.009\n2014.0\nYes\nJournal\nNot Pre-Registered\nNo\nRomantic\nMales\nBlue\nItem\nBust\nSame between conditions\nSingle item perceived attractiveness\n1\n1-5\n1\n5\nTaiwan\nAsia\nStudents\nUndergrads\nBetween Subjects\nAsian\nNA\nAsian\nMatched\nNA\nNA\nNo Data\nNo Data\nNo\nNo\nYes\n20\n3.50\n0.69\n20\n2.55\n0.51\nNA\nNA\n0.39\nControl condition is blue; silver condition is dropped as it is not an original control color\n40\nNA\n\n\nMaves & Nadler, 2016\nPaper\n-0.12\n0.03\nMaves & Nadler, 2016 - Exp 1\nMaves, M., & Nadler, J. T. (2016, June 2). Data and Analysis. Retrieved from osf.io/9bm8v\n2015.0\nNo\nCREP\nPre-Registered\nNo\nRomantic\nFemales\nGray\nBackground\nBust\nSame between conditions\nPerceived attractiveness\n3\n1-9\n1\n9\nUSA\nNorth America\nStudents\nUndergrads\nBetween Subjects\nWhite\nWhite\nLatino\nMis-match\n21.86\n22.48\n~LCh(50.0, 59.6, 31.3)\n~LCh(50.0, -, 69.1)\nYes\nYes\nYes\n66\n6.07\n1.40\n64\n6.23\n1.38\nNA\nNA\n0.65\nNA\n130\nNA\n\n\nO'Mara & Trujillo, 2015\nScreen\n0.97\n0.10\nO'Mara & Trujillo, 2015 - Exp 1 - Masculine Face\nO'Mara, E. M., & Trujillo, A. (2015), unpublished data.\n2015.0\nNo\nUnpublished/Online\nNot Pre-Registered\nNo\nRomantic\nFemales\nWhite\nBackground\nHead Shot\nSame between conditions\nPerceived attractiveness\n1\n1-9\n1\n9\nUSA\nNorth America\nStudents\nUndergrads\nBetween Subjects\nWhite\nWhite\nWhite\nMatched\n19.10\n19.10\nNA\nNA\nNo\nNo\nYes\n21\n6.11\n1.30\n25\n4.78\n1.38\nNA\nNA\n0.47\nNA\n46\nNA\n\n\nO'Mara & Trujillo, 2015\nScreen\n-0.20\n0.09\nO'Mara & Trujillo, 2015 - Exp 2 - Masculine Face\nO'Mara, E. M., & Trujillo, A. (2015), unpublished data.\n2015.0\nNo\nUnpublished/Online\nNot Pre-Registered\nNo\nRomantic\nFemales\nWhite\nBackground\nHead Shot\nSame between conditions\nPerceived attractiveness\n1\n1-9\n1\n9\nUSA\nNorth America\nOnline\nMturk\nBetween Subjects\nWhite\nWhite\nWhite\nMatched\n28.23\n28.23\nNA\nNA\nNo\nNo\nNo\n24\n4.88\n1.84\n21\n5.23\n1.61\nNA\nNA\n0.53\nNA\n45\nNA\n\n\nO'Mara et al., 2016\nScreen\n0.45\n0.14\nO'Mara et al., 2016 - Exp 2 - Long Shirt\nO'Mara, E. M., Kershaw, C., Receveur, A., Hunt, C., Askar, S., Ballas, T., Farmer, C., O'Koon, B., Stitzel, C., Vavro, C., & Wilhoit, S. (2016), unpublished data.\n2015.0\nNo\nUnpublished/Online\nNot Pre-Registered\nNo\nRomantic\nMales\nBlue\nClothing\nBust\nDifferent between conditions\nPerceived attractiveness\n2\n1-9\n1\n9\nUSA\nNorth America\nStudents\nUndergrads\nBetween Subjects\nWhite\nWhite\nWhite\nMatched\n19.26\n19.26\nNA\nNA\nNo\nNo\nYes\n20\n4.42\n1.37\n12\n3.79\n1.41\nNA\nNA\n0.35\nNA\n32\nNA\n\n\nO'Mara et al., 2016\nScreen\n-0.32\n0.14\nO'Mara et al., 2016 - Exp 2 - Tank Top\nO'Mara, E. M., Kershaw, C., Receveur, A., Hunt, C., Askar, S., Ballas, T., Farmer, C., O'Koon, B., Stitzel, C., Vavro, C., & Wilhoit, S. (2016), unpublished data.\n2015.0\nNo\nUnpublished/Online\nNot Pre-Registered\nNo\nRomantic\nMales\nBlue\nClothing\nBust\nDifferent between conditions\nPerceived attractiveness\n2\n1-9\n1\n9\nUSA\nNorth America\nStudents\nUndergrads\nBetween Subjects\nWhite\nWhite\nWhite\nMatched\n19.26\n19.26\nNA\nNA\nNo\nNo\nYes\n17\n3.91\n1.09\n12\n4.33\n1.51\nNA\nNA\n0.42\nNA\n29\nNA\n\n\nO'Mara et al., 2016\nScreen\n0.47\n0.07\nO'Mara et al., 2016 - Exp 1 - Long Shirt\nO'Mara, E. M., Kershaw, C., Receveur, A., Hunt, C., Askar, S., Ballas, T., Farmer, C., O'Koon, B., Stitzel, C., Vavro, C., & Wilhoit, S. (2016), unpublished data.\n2015.0\nNo\nUnpublished/Online\nNot Pre-Registered\nNo\nRomantic\nMales\nWhite\nClothing\nBust\nDifferent between conditions\nPerceived attractiveness\n2\n1-9\n1\n9\nUSA\nNorth America\nOnline\nMturk\nBetween Subjects\nWhite\nWhite\nWhite\nMatched\n34.56\n34.56\nNA\nNA\nNo\nNo\nNo\n28\n6.07\n2.14\n35\n5.12\n1.89\nNA\nNA\n0.52\nNA\n63\nNA\n\n\nO'Mara et al., 2016\nScreen\n0.10\n0.06\nO'Mara et al., 2016 - Exp 1 - Tank Top\nO'Mara, E. M., Kershaw, C., Receveur, A., Hunt, C., Askar, S., Ballas, T., Farmer, C., O'Koon, B., Stitzel, C., Vavro, C., & Wilhoit, S. (2016), unpublished data.\n2015.0\nNo\nUnpublished/Online\nNot Pre-Registered\nNo\nRomantic\nMales\nWhite\nClothing\nBust\nDifferent between conditions\nPerceived attractiveness\n2\n1-9\n1\n9\nUSA\nNorth America\nOnline\nMturk\nBetween Subjects\nWhite\nWhite\nWhite\nMatched\n34.56\n34.56\nNA\nNA\nNo\nNo\nNo\n34\n5.53\n1.96\n35\n5.33\n2.11\nNA\nNA\n0.54\nNA\n69\nNA\n\n\nO'Mara et al., 2016\nScreen\n-0.12\n0.04\nO'Mara et al., 2016 - Exp 3 - Tank Top\nO'Mara, E. M., Kershaw, C., Receveur, A., Hunt, C., Askar, S., Ballas, T., Farmer, C., O'Koon, B., Stitzel, C., Vavro, C., & Wilhoit, S. (2016), unpublished data.\n2016.0\nNo\nUnpublished/Online\nNot Pre-Registered\nNo\nRomantic\nMales\nWhite\nClothing\nBust\nDifferent between conditions\nPerceived attractiveness\n2\n1-9\n1\n9\nUSA\nNorth America\nStudents\nUndergrads\nBetween Subjects\nWhite\nWhite\nWhite\nMatched\n19.54\n19.54\nNA\nNA\nNo\nNo\nYes\n51\n6.70\n1.14\n55\n6.83\n1.00\nNA\nNA\n0.73\nNA\n106\nNA\n\n\nO'Mara et al., 2016\nScreen\n-0.14\n0.03\nO'Mara et al., 2016 - Exp 3 - Long Shirt\nO'Mara, E. M., Kershaw, C., Receveur, A., Hunt, C., Askar, S., Ballas, T., Farmer, C., O'Koon, B., Stitzel, C., Vavro, C., & Wilhoit, S. (2016), unpublished data.\n2016.0\nNo\nUnpublished/Online\nNot Pre-Registered\nNo\nRomantic\nMales\nWhite\nClothing\nBust\nDifferent between conditions\nPerceived attractiveness\n2\n1-9\n1\n9\nUSA\nNorth America\nStudents\nUndergrads\nBetween Subjects\nWhite\nWhite\nWhite\nMatched\n19.54\n19.54\nNA\nNA\nNo\nNo\nYes\n63\n6.97\n1.27\n57\n7.12\n0.93\nNA\nNA\n0.77\nNA\n120\nNA\n\n\nPazda et al., 2012\nPaper\n0.63\n0.09\nPazda et al., 2012 - Exp 2\nPazda, A. D., Elliot, A. J., & Greitemeyer, T. (2012). Sexy red: Perceived sexual receptivity mediates the red-attraction relation in men viewing woman. Journal of Experimental Social Psychology, 48(3), 787-790. http://doi.org/10.1016/j.jesp.2011.12.009\n2012.0\nYes\nJournal\nNot Pre-Registered\nNo\nRomantic\nMales\nGreen\nClothing\nBust\nSame between conditions\nPerceived attractiveness\n2\n1-9\n1\n9\nAustria\nEurope\nStudents\nUndergrads\nBetween Subjects\nWhite\nWhite\nWhite\nMatched\n23.50\n23.50\nLCh(40.6, 40.4, 20.1)\nLCh(40.3, 41.2, 146.8)\nNo\nYes\nYes\n27\n6.07\n1.17\n22\n5.00\n2.13\nNA\nNA\n0.50\nNA\n49\nNA\n\n\nPazda et al., 2014\nScreen\n0.32\n0.02\nPazda et al., 2014 - Exp 1\nPazda, A. D., Elliot, A. J., & Greitemeyer, T. (2014). Perceived sexual receptivity and fashionableness: Separate paths linking red and black to perceived attractiveness. Color Research & Application, 39(2), 208-212. http://doi.org/10.1002/col.21804\n2014.0\nYes\nJournal\nNot Pre-Registered\nNo\nRomantic\nMales\nWhite\nClothing\nFull Body\nSame between conditions\nPerceived attractiveness\n2\n1-9\n1\n9\nNot specified\nUnknown\nOnline\nMturk\nBetween Subjects\nWhite\nWhite (170 white, 142 Asian)\nWhite\nMatched\n24.50\n24.50\nNo Data\nNo Data\nNo\nNo\nNo\n109\n6.03\n1.78\n125\n5.46\n1.77\nNA\nNA\n0.56\nNA\n234\nNA\n\n\nPazda et al., 2017\nScreen\n0.17\n0.00\nPazda et al., 2017 - Exp 1\nPazda, A., Thorstenson & Elliot, A. (2017). Unpublished data.\n2017.0\nNo\nUnpublished/Online\nNot Pre-Registered\nNo\nRomantic\nMales\nGreen\nClothing\nFull Body\nSame between conditions\nSingle Item\n1\n1-9\n1\n9\nUSA\nNorth America\nStudents\nLab study\nWithin Subjects\nNA\nNA\nWhite\nNA\n19.76\nNA\nred LCh(42.00, 57.92, 348.89), green LCh(41.50, 56.71, 92.17)\nNA\nNo\nYes\nYes\n115\n7.81\n1.21\n115\n7.58\n1.41\nNA\n0.84\n0.82\nNA\n115\n1.31\n\n\nPazda et al., 2017\nScreen\n0.10\n0.00\nPazda et al., 2017 - Exp 2\nPazda, A., Thorstenson & Elliot, A. (2017). Unpublished data.\n2017.0\nNo\nUnpublished/Online\nPre-Registered\nNo\nRomantic\nMales\nGreen\nClothing\nFull Body\nSame between conditions\nSingle Item\n1\n1-9\n1\n9\nUSA\nNorth America\nOnline\nMturk\nWithin Subjects\nWhite\nNA\nWhite\nMatched\n37.43\nNA\nred LCh(42.00, 57.92, 348.89), green LCh(41.50, 56.71, 92.17)\nNA\nNo\nNo\nNo\n228\n8.08\n1.28\n228\n7.95\n1.39\nNA\n0.80\n0.87\nNA\n228\n1.34\n\n\nPeperkoorn et al., 2016\nPaper\n-0.43\n0.06\nPeperkoorn et al., 2016 - Exp 1 - Short Term\nPeperkoorn, L. S., Roberts, S. C., & Pollet, T. V. (2016). Revisiting the Red Effect on Attractiveness and Sexual Receptivity: No Effect of the Color Red on Human Mate Preferences. Evolutionary Psychology, 14(4). http://doi.org/10.1177/1474704916673841\n2016.0\nYes\nJournal\nNot Pre-Registered\nNo\nRomantic\nMales\nWhite\nClothing\nBust\nSame between conditions\nPerceived attractiveness\n2\n1-11\n2\n22\nNetherlands\nEurope\nStudents\nUndergrads\nBetween Subjects\nWhite\nWhite\nWhite\nMatched\n23.67\n23.67\nNA\nNA\nNo\nNo\nYes\n34\n14.26\n3.31\n34\n15.47\n2.18\nNA\nNA\n0.67\nNA\n68\nNA\n\n\nPeperkoorn et al., 2016\nScreen\n-0.10\n0.06\nPeperkoorn et al., 2016 - Exp 2 - Short Term\nPeperkoorn, L. S., Roberts, S. C., & Pollet, T. V. (2016). Revisiting the Red Effect on Attractiveness and Sexual Receptivity: No Effect of the Color Red on Human Mate Preferences. Evolutionary Psychology, 14(4). http://doi.org/10.1177/1474704916673841\n2016.0\nYes\nJournal\nNot Pre-Registered\nNo\nRomantic\nMales\nWhite\nClothing\nBust\nSame between conditions\nPerceived attractiveness\n2\n1-11\n2\n22\nUSA\nNorth America\nOnline\nMturk\nBetween Subjects\nWhite\nWhite\nWhite\nMatched\n30.13\n30.13\nNA\nNA\nNo\nNo\nNo\n33\n15.06\n4.14\n36\n15.47\n3.61\nNA\nNA\n0.67\nNA\n69\nNA\n\n\nPollet, 2013\nScreen\n0.16\n0.08\nPollet, 2013 - Exp 1\nPollet T. (2013). Unpublished data\n2013.0\nNo\nUnpublished/Online\nNot Pre-Registered\nNo\nRomantic\nFemales\nBlue\nClothing\nFull Body\nSame between conditions\nSingle item\n1\n1-7\n1\n7\nNetherlands\nEurope\nStudents\nNA\nBetween Subjects\nWhite\nPrimarily dutch nationality\nWhite\nMatched\n26.60\n28.40\nNo Data\nNo Data\nNo\nNo\nNo\n26\n3.15\n1.22\n23\n2.96\n1.19\nNA\nNA\n0.33\nOnly blue control color used as other control colors were not in original\n49\nNA\n\n\nPurdy, 2009\nScreen\n0.30\n0.16\nPurdy, 2009 - Exp 1 - High Arousal\nPurdy, M. A. (2009). The influence of the amygdala and color on judgments of attractiveness. UNC Asheville Journal, Undergraduate Research Program, Asheville, NC\n2009.0\nYes\nJournal\nNot Pre-Registered\nNo\nRomantic\nMales\nGray\nBackground\nBust\nSame between conditions\nPerceived attractiveness\n3\n1-9\n1\n9\nUSA\nNorth America\nStudents\nUndergrads\nBetween Subjects\nNA\nNA\nWhite\nNA\n21.00\n21.00\nRGB(185, 26, 23)\nRGB(216,216,216)\nNo\nNo\nYes\n13\n7.26\n3.27\n13\n6.24\n3.27\nNA\nNA\n0.66\nNA\n26\nNA\n\n\nPurdy, 2009\nScreen\n0.01\n0.15\nPurdy, 2009 - Exp 1 - Low Arousal\nPurdy, M. A. (2009). The influence of the amygdala and color on judgments of attractiveness. UNC Asheville Journal, Undergraduate Research Program, Asheville, NC\n2009.0\nYes\nJournal\nNot Pre-Registered\nNo\nRomantic\nMales\nGray\nBackground\nBust\nSame between conditions\nPerceived attractiveness\n3\n1-9\n1\n9\nUSA\nNorth America\nStudents\nUndergrads\nBetween Subjects\nNA\nNA\nWhite\nNA\n21.00\n21.00\nRGB(185, 26, 23)\nRGB(216,216,216)\nNo\nNo\nYes\n13\n6.37\n3.31\n13\n6.33\n3.27\nNA\nNA\n0.67\nNA\n26\nNA\n\n\nRoberts et al., 2010\nScreen\n0.33\n0.05\nRoberts et al., 2010 - Exp 2\nRoberts, S. C., Owen, R. C., & Havlicek, J. (2010). Distinguishing between Perceiver and Wearer Effects in Clothing Color-Associated Attributions. Evolutionary Psychology, 8(3), 350-364. http://doi.org/10.1177/147470491000800304\n2010.1\nYes\nJournal\nNot Pre-Registered\nNo\nRomantic\nFemales\nBlue/Green/White\nClothing\nBust\nDifferent between conditions\nSingle item rating of attractiveness\n1\n1-10\n1\n10\nEngland\nEurope\nStudents\nUndergrads\nWithin Subjects\nWhite\nNoted in manuscript all caucasian\nWhite\nMatched\nNA\nNA\nNo Data\nNo Data\nNo\nNo\nYes\n15\n3.66\n0.60\n15\n3.45\n0.61\nNA\n0.66\n0.27\nAge range includes both male and female participants.\n15\n0.60\n\n\nRoberts et al., 2010\nScreen\n0.40\n0.02\nRoberts et al., 2010 - Exp 2\nRoberts, S. C., Owen, R. C., & Havlicek, J. (2010). Distinguishing between Perceiver and Wearer Effects in Clothing Color-Associated Attributions. Evolutionary Psychology, 8(3), 350-364. http://doi.org/10.1177/147470491000800304\n2010.1\nYes\nJournal\nNot Pre-Registered\nNo\nRomantic\nMales\nBlue/Green/White\nClothing\nBust\nDifferent between conditions\nSingle item rating of attractiveness\n1\n1-10\n1\n10\nEngland\nEurope\nStudents\nUndergrads\nWithin Subjects\nWhite\nNoted in manuscript all caucasian\nWhite\nMatched\nNA\nNA\nNo Data\nNo Data\nNo\nNo\nYes\n15\n3.65\n0.45\n15\n3.46\n0.48\nNA\n0.92\n0.27\nAge range includes both male and female participants.\n15\n0.47\n\n\nRoberts et al., 2010\nScreen\n0.19\n0.02\nRoberts et al., 2010 - Exp 1\nRoberts, S. C., Owen, R. C., & Havlicek, J. (2010). Distinguishing between Perceiver and Wearer Effects in Clothing Color-Associated Attributions. Evolutionary Psychology, 8(3), 350-364. http://doi.org/10.1177/147470491000800304\n2010.1\nYes\nJournal\nNot Pre-Registered\nNo\nRomantic\nFemales\nBlue/Green/White\nClothing\nBust\nDifferent between conditions\nSingle item rating of attractiveness\n1\n1-10\n1\n10\nEngland\nEurope\nStudents\nUndergrads\nWithin Subjects\nWhite\nNoted in manuscript all caucasian\nWhite\nMatched\nNA\nNA\nNo Data\nNo Data\nNo\nNo\nYes\n30\n4.50\n1.23\n30\n4.26\n1.18\nNA\n0.66\n0.36\nAge range includes both male and female participants.\n30\n1.21\n\n\nRoberts et al., 2010\nScreen\n0.19\n0.01\nRoberts et al., 2010 - Exp 1\nRoberts, S. C., Owen, R. C., & Havlicek, J. (2010). Distinguishing between Perceiver and Wearer Effects in Clothing Color-Associated Attributions. Evolutionary Psychology, 8(3), 350-364. http://doi.org/10.1177/147470491000800304\n2010.1\nYes\nJournal\nNot Pre-Registered\nNo\nRomantic\nMales\nBlue/Green/White\nClothing\nBust\nDifferent between conditions\nSingle item rating of attractiveness\n1\n1-10\n1\n10\nEngland\nEurope\nStudents\nUndergrads\nWithin Subjects\nWhite\nNoted in manuscript all caucasian\nWhite\nMatched\nNA\nNA\nNo Data\nNo Data\nNo\nNo\nYes\n30\n4.60\n1.10\n30\n4.40\n0.92\nNA\n0.92\n0.38\nAge range includes both male and female participants.\n30\n1.01\n\n\nRoberts et al., 2010\nScreen\n-0.08\n0.01\nRoberts et al., 2010 - Exp 3\nRoberts, S. C., Owen, R. C., & Havlicek, J. (2010). Distinguishing between Perceiver and Wearer Effects in Clothing Color-Associated Attributions. Evolutionary Psychology, 8(3), 350-364. http://doi.org/10.1177/147470491000800304\n2010.1\nYes\nJournal\nNot Pre-Registered\nNo\nRomantic\nMales\nWhite\nClothing\nBust\nSame between conditions\nSingle item rating of attractiveness\n1\n1-10\n1\n10\nEngland\nEurope\nStudents\nUndergrads\nWithin Subjects\nWhite\nNoted in manuscript all caucasian\nWhite\nMatched\nNA\nNA\nNo Data\nNo Data\nNo\nNo\nYes\n25\n4.82\n0.90\n25\n4.88\n0.85\n0.36\n0.92\n0.43\nAge range includes both male and female participants.\n25\n0.88\n\n\nRoberts et al., 2010\nScreen\n-0.05\n0.03\nRoberts et al., 2010 - Exp 3\nRoberts, S. C., Owen, R. C., & Havlicek, J. (2010). Distinguishing between Perceiver and Wearer Effects in Clothing Color-Associated Attributions. Evolutionary Psychology, 8(3), 350-364. http://doi.org/10.1177/147470491000800304\n2010.1\nYes\nJournal\nNot Pre-Registered\nNo\nRomantic\nFemales\nWhite\nClothing\nBust\nSame between conditions\nSingle item rating of attractiveness\n1\n1-10\n1\n10\nEngland\nEurope\nStudents\nUndergrads\nWithin Subjects\nWhite\nNoted in manuscript all caucasian\nWhite\nMatched\nNA\nNA\nNo Data\nNo Data\nNo\nNo\nYes\n23\n4.87\n0.75\n23\n4.90\n0.73\n0.61\n0.66\n0.43\nAge range includes both male and female participants.\n23\n0.74\n\n\nSchwarz & Singer, 2013\nPaper\n0.19\n0.07\nSchwarz & Singer, 2013 - Exp 1 - Adults Rate Young\nSchwarz, S., & Singer, M. (2013). Romantic red revisited: Red enhances men's attraction to young, but not menopausal women. Journal of Experimental Social Psychology, 49(1), 161-164. http://doi.org/10.1016/j.jesp.2012.08.004\n2013.0\nYes\nJournal\nNot Pre-Registered\nNo\nRomantic\nMales\nWhite\nBackground\nBust\nSame between conditions\nPerceived attractiveness\n1\n1-9\n1\n9\nGermany\nEurope\nAdults\nNA\nBetween Subjects\nNA\nNA\nWhite\nNA\n53.47\n53.47\nLum: 35.2, Chroma: 39.3, Hue no specified\nNo Data\nNo\nNo\nYes\n30\n6.53\n1.96\n30\n6.13\n2.17\nNA\nNA\n0.64\nNA\n60\nNA\n\n\nSchwarz & Singer, 2013\nPaper\n-0.15\n0.07\nSchwarz & Singer, 2013 - Exp 1 - UGrads Rate Young\nSchwarz, S., & Singer, M. (2013). Romantic red revisited: Red enhances men's attraction to young, but not menopausal women. Journal of Experimental Social Psychology, 49(1), 161-164. http://doi.org/10.1016/j.jesp.2012.08.004\n2013.0\nYes\nJournal\nNot Pre-Registered\nNo\nRomantic\nMales\nWhite\nBackground\nBust\nSame between conditions\nPerceived attractiveness\n1\n1-9\n1\n9\nGermany\nEurope\nStudents\nUndergrads\nBetween Subjects\nNA\nNA\nWhite\nNA\n24.67\n24.67\nLum: 35.2, Chroma: 39.3, Hue no specified\nNo Data\nNo\nNo\nYes\n30\n6.20\n1.42\n30\n6.40\n1.18\nNA\nNA\n0.68\nNA\n60\nNA\n\n\nSeibt & Klement, 2015\nScreen\n0.22\n0.05\nSeibt & Klement, 2015 - Exp 1\nSeibt, T., & Klement, V. (2015). The Impact of the Colour Red on Attractiveness Perception. In 4th Advanced Research in Scientific Areas (pp. 20-24). http://doi.org/10.18638/arsa.2015.4.1.799\n2015.0\nNo\nConference Proceedings\nNot Pre-Registered\nNo\nRomantic\nFemales\nGreen\nBackground\nNA\nSame between conditions\nPerceived Attractiveness, subscale of Haselton und Gangestad (2006)\nNA\n1-9\n1\n9\nGermany\nEurope\nStudents\nUndergrads and Grads\nBetween Subjects\nNA\nNA\nNA\nNA\n26.70\n26.70\nRGB(255.0.0)\nRGB(0.139.0)\nNo\nNo\nNo\n54\n3.19\n0.53\n31\n3.07\n0.55\nNA\nNA\n0.26\nNA\n85\nNA\n\n\nSeibt & Klement, 2015\nScreen\n0.20\n0.16\nSeibt & Klement, 2015 - Exp 1\nSeibt, T., & Klement, V. (2015). The Impact of the Colour Red on Attractiveness Perception. In 4th Advanced Research in Scientific Areas (pp. 20-24). http://doi.org/10.18638/arsa.2015.4.1.799\n2015.0\nNo\nConference Proceedings\nNot Pre-Registered\nNo\nRomantic\nMales\nGreen\nBackground\nNA\nSame between conditions\nPerceived Attractiveness, subscale of Haselton und Gangestad (2006)\nNA\n1-9\n1\n9\nGermany\nEurope\nStudents\nUndergrads and Grads\nBetween Subjects\nNA\nNA\nNA\nNA\n26.70\n26.70\nRGB(255.0.0)\nRGB(0.139.0)\nNo\nNo\nNo\n12\n3.40\n0.43\n14\n3.30\n0.54\nNA\nNA\n0.29\nNA\n26\nNA\n\n\nSeibt, 2015\nScreen\n0.20\n0.05\nSeibt, 2015 - Exp 1\nSeibt, T. (2015). Romantic Red Effect in the Attractiveness Perception. In Hassacc (pp. 31-34). http://doi.org/10.18638/hassacc.2015.3.1.186\n2015.0\nNo\nConference Proceedings\nNot Pre-Registered\nNo\nRomantic\nMales\nGreen\nBackground\nNA\nSame between conditions\nPerceived Attractiveness\nNA\n1-5\n1\n5\nGermany\nEurope\nStudents\nUndergrads and Grads\nBetween Subjects\nNA\nNA\nNA\nNA\n25.50\n25.50\nNo Data\nNo Data\nNo\nNo\nNo\n46\n2.80\n0.49\n41\n2.70\n0.52\nNA\nNA\n0.42\nNA\n87\nNA\n\n\nStefan & Gueguen, 2013\nPaper\n0.62\n0.12\nStefan & Gueguen, 2013 - Exp 1\nStefan J. & Gueguen, N. (2013). Unpublished data.\n2013.0\nNo\nUnpublished/Online\nNot Pre-Registered\nNo\nRomantic\nMales\nWhite\nClothing\nFull Body\nSame between conditions\nSingle item\n1\n1-100\n1\n100\nFrance\nEurope\nStudents\nUndergrads and Grads\nBetween Subjects\nWhite\nWhite\nWhite\nMatched\n22.11\n22.11\nNA\nNA\nNo\nNo\nYes\n18\n81.33\n16.30\n16\n67.50\n26.87\nNA\nNA\n0.67\nData provided by Elliot\n34\nNA\n\n\nSullivan et al., 2016\nScreen\n0.24\n0.04\nSullivan et al., 2016 - Exp 1\nSullivan, J., Amaral Lavoie, E., Bays, R. B., Fontana, S., Goodkind, R., Johnson, R., ... Lavoie, M. (2016, April 4). Replication of Elliot et al., 2010: Red, Rank, and Romance. Retrieved from osf.io/pm7fx\n2015.0\nNo\nUnpublished/Online\nNot Pre-Registered\nNo\nRomantic\nFemales\nGray\nBackground\nBust\nSame between conditions\nPerceived attractiveness\n3\n1-9\n1\n9\nUSA\nNorth America\nStudents\nUndergrads\nBetween Subjects\nWhite\nWhite\nLatino\nMis-match\n19.96\n19.45\nNo Data\nNo Data\nNo\nNo\nYes\n49\n5.60\n1.37\n53\n5.26\n1.42\nNA\nNA\n0.53\nTwo-year class project posted on OSF\n102\nNA\n\n\nSullivan et al., 2016\nScreen\n-0.34\n0.06\nSullivan et al., 2016 - Exp 2\nSullivan, J., Amaral Lavoie, E., Bays, R. B., Fontana, S., Goodkind, R., Johnson, R., ... Lavoie, M. (2016, April 4). Replication of Elliot et al., 2010: Red, Rank, and Romance. Retrieved from osf.io/pm7fx\n2016.0\nNo\nUnpublished/Online\nNot Pre-Registered\nNo\nRomantic\nFemales\nGray\nBackground\nBust\nSame between conditions\nPerceived attractiveness\n3\n1-9\n1\n9\nUSA\nNorth America\nStudents\nUndergrads\nBetween Subjects\nWhite\nWhite\nLatino\nMis-match\nNA\nNA\nNo Data\nNo Data\nNo\nNo\nYes\n38\n4.85\n1.65\n27\n5.37\n1.38\nNA\nNA\n0.55\nAge recorded as dichotomy (younger then 20, older than 20) so not included\n65\nNA\n\n\nSullivan et al., 2016\nScreen\n0.42\n0.04\nSullivan et al., 2016 - Exp 1\nSullivan, J., Amaral Lavoie, E., Bays, R. B., Fontana, S., Goodkind, R., Johnson, R., ... Lavoie, M. (2016, April 4). Replication of Elliot et al., 2010: Red, Rank, and Romance. Retrieved from osf.io/pm7fx\n2015.0\nNo\nUnpublished/Online\nNot Pre-Registered\nNo\nRomantic\nMales\nGray\nBackground\nBust\nSame between conditions\nPerceived attractiveness\n3\n1-9\n1\n9\nUSA\nNorth America\nStudents\nUndergrads\nBetween Subjects\nWhite\nWhite\nWhite\nMatched\n19.62\n19.75\nNo Data\nNo Data\nNo\nNo\nYes\n45\n6.21\n1.38\n48\n5.58\n1.56\nNA\nNA\n0.57\nNA\n93\nNA\n\n\nSullivan et al., 2016\nScreen\n0.02\n0.12\nSullivan et al., 2016 - Exp 2\nSullivan, J., Amaral Lavoie, E., Bays, R. B., Fontana, S., Goodkind, R., Johnson, R., ... Lavoie, M. (2016, April 4). Replication of Elliot et al., 2010: Red, Rank, and Romance. Retrieved from osf.io/pm7fx\n2016.0\nNo\nUnpublished/Online\nNot Pre-Registered\nNo\nRomantic\nMales\nGray\nBackground\nBust\nSame between conditions\nPerceived attractiveness\n3\n1-9\n1\n9\nUSA\nNorth America\nStudents\nUndergrads\nBetween Subjects\nWhite\nWhite\nWhite\nMatched\nNA\nNA\nNo Data\nNo Data\nNo\nNo\nYes\n21\n6.24\n1.29\n14\n6.21\n1.45\nNA\nNA\n0.65\nAge recorded as dichotomy (younger then 20, older than 20) so not included\n35\nNA\n\n\nWartenberg et al., 2011\nScreen\n0.20\n0.01\nWartenberg et al., 2011 - Exp 1 - In Group\nWartenberg, W., Hoepfner, T., Potthast, P., & Mirau, A. (2011). If you wear red on a date, you will please your mate. Proceedings of Empiriepraktikumskongress, 6th, Aug. 7, pp 26-27. University of Jena, Germany.\n2011.0\nNo\nConference Proceedings\nNot Pre-Registered\nNo\nRomantic\nFemales\nBlue\nBackground\nBust\nSame between conditions\nPerceived attractiveness\n4\n1-7\n1\n7\nGermany\nEurope\nStudents\nUndergrads\nWithin Subjects\nWhite\nWhite\nWhite\nMatched\n21.47\n21.47\nNo Data\nNo Data\nNo\nNo\nYes\n39\n3.00\n1.18\n39\n2.76\n1.13\n0.63\n0.86\n0.29\nTranslation of summary provided by Elliot; within subjects info still needed\n39\n1.16\n\n\nWen et al., 2014\nPaper\n0.16\n0.07\nWen et al., 2014 - Exp 1 - Feminine Females\nWen, F., Zuo, B., Wu, Y., Sun, S., & Liu, K. (2014). Red is Romantic, but Only for Feminine Females: Sexual Dimorphism Moderates Red Effect on Sexual Attraction. Evolutionary Psychology, 12(4), 719-735. http://doi.org/10.1177/147470491401200404\n2014.0\nYes\nJournal\nNot Pre-Registered\nNo\nRomantic\nMales\nBlue/White\nClothing\nBust\nSame between conditions\nPerceived attractiveness, extracted factor\n3\n1-7\n1\n7\nChina\nAsia\nStudents\nUndergrads\nBetween Subjects\nAsian\nAsian\nAsian\nMatched\n20.95\n20.95\nLCh(51.1, 57.7, 27.8)\nLCh(51.6, 57.6, 278.3)\nYes\nYes\nYes\n22\n0.24\n0.93\n44\n0.10\n0.81\nNA\nNA\nNA\nControl is average of blue and white conditions; only normalized scores provided. Age range includes participants in all conditions.\n66\nNA\n\n\nWen et al., 2014\nPaper\n-0.08\n0.07\nWen et al., 2014 - Exp 1 - Masculine Males\nWen, F., Zuo, B., Wu, Y., Sun, S., & Liu, K. (2014). Red is Romantic, but Only for Feminine Females: Sexual Dimorphism Moderates Red Effect on Sexual Attraction. Evolutionary Psychology, 12(4), 719-735. http://doi.org/10.1177/147470491401200404\n2014.0\nYes\nJournal\nNot Pre-Registered\nNo\nRomantic\nFemales\nBlue/White\nClothing\nBust\nSame between conditions\nPerceived attractiveness, extracted factor\n3\n1-7\n1\n7\nChina\nAsia\nStudents\nUndergrads\nBetween Subjects\nAsian\nAsian\nAsian\nMatched\n20.95\n20.95\nLCh(51.1, 57.7, 27.8)\nLCh(51.6, 57.6, 278.3)\nYes\nYes\nYes\n21\n-0.10\n0.93\n49\n-0.03\n0.85\nNA\nNA\nNA\nControl is average of blue and white conditions; only normalized scores provided. Age range includes participants in all conditions.\n70\nNA\n\n\nWilliams & Neelon, 2013\nScreen\n0.58\n0.13\nWilliams & Neelon, 2013 - Exp 1 - All\nWilliams, C. L. & Neelon, M. (2013). Conditional beauty: The impact of emotionally linked images on the red effect in sexual attraction. Psi Chi Journal of Psychological Research, 18(1), 10-19.\n2013.0\nYes\nJournal\nNot Pre-Registered\nNo\nRomantic\nMales\nBlue\nBackground\nBust\nSame between conditions\nPerceived attractiveness\n2\n1-9\n1\n9\nUSA\nNorth America\nStudents\nUndergrads\nBetween Subjects\nWhite\nWhite (noted by authors)\nWhite\nMatched\n22.50\n22.50\nRGB(183,70,60)\nRGB(76,105,200)\nNo\nNo\nYes\n16\n6.81\n1.03\n15\n6.15\n1.20\nNA\nNA\n0.64\nData provided by Elliot\n31\nNA\n\n\nWilliams & Neelon, 2013\nScreen\n-0.13\n0.08\nWilliams & Neelon, 2013 - Exp 2 - Positive and Neutral\nWilliams, C. L. & Neelon, M. (2013). Conditional beauty: The impact of emotionally linked images on the red effect in sexual attraction. Psi Chi Journal of Psychological Research, 18(1), 10-19.\n2013.0\nYes\nJournal\nNot Pre-Registered\nNo\nRomantic\nMales\nBlue\nBackground\nBust\nSame between conditions\nPerceived attractiveness\n2\n1-9\n1\n9\nUSA\nNorth America\nStudents\nUndergrads\nBetween Subjects\nWhite\nWhite (noted by authors)\nWhite\nMatched\n21.50\n21.50\nRGB(183,70,60)\nRGB(76,105,200)\nNo\nNo\nYes\n26\n6.40\n1.28\n26\n6.56\n1.04\nNA\nNA\n0.69\nData provided by Elliot\n52\nNA\n\n\nYoung, 2015\nScreen\n0.14\n0.00\nYoung, 2015 - Exp 1 - More Attractive\nYoung, S. G. (2015). The effect of red on male perceptions of female attractiveness: Moderation by baseline attractiveness of female faces. European Journal of Social Psychology, 45(2), 146-151. http://doi.org/10.1002/ejsp.2098\n2015.0\nYes\nJournal\nNot Pre-Registered\nNo\nRomantic\nMales\nGray\nBackground\nHead Shot\nSame between conditions\nSingle item\n1\n1-7\n1\n7\nUSA\nNorth America\nStudents\nUndergrads\nWithin Subjects\nMixed\nMixed\nWhite\nMis-match\n19.90\n19.90\nLCh(62.7, 84.6, 34.1)\nLCh(62.6, -, 265.5)\nNo\nYes\nYes\n19\n4.29\n0.86\n19\n4.17\n0.81\n0.23\n0.96\n0.53\nNA\n19\n0.84\n\n\nYoung, 2015\nScreen\n0.10\n0.00\nYoung, 2015 - Exp 2 - More Attractive\nYoung, S. G. (2015). The effect of red on male perceptions of female attractiveness: Moderation by baseline attractiveness of female faces. European Journal of Social Psychology, 45(2), 146-151. http://doi.org/10.1002/ejsp.2098\n2015.0\nYes\nJournal\nNot Pre-Registered\nNo\nRomantic\nMales\nBlue/Gray\nBackground\nHead Shot\nSame between conditions\nSingle item\n1\n1-7\n1\n7\nUSA\nNorth America\nStudents\nUndergrads\nWithin Subjects\nMixed\nMixed\nWhite\nMis-match\n21.10\n21.10\nLCh(62.7, 84.6, 34.1)\nLCh(62.7, 84.6, 34.1)\nNo\nYes\nYes\n46\n4.30\n1.11\n46\n4.19\n1.07\n0.25\n0.98\n0.53\nBlue and Gray ratings averaged, then compared to red; original data provided\n46\n1.09\n\n\n\n\n\n\n\n\n\nPreliminary Analysis\nAs a little warm-up exercise, here is a basic random effects meta-analysis of these data, fit via the metafor package (Viechtbauer, 2010). We use cluster-robust standard errors to account for effect size dependency.\n\n\nCode\n# Estimate random effects model\nRE_mod &lt;- rma.uni(yi, vi = vi, data = lehmann_dat, method = \"ML\")\n\n# Calculate cluster-robust standard errors\nRE_robust &lt;- robust(RE_mod, cluster = study, clubSandwich = TRUE)\nRE_robust\n\n\n\nRandom-Effects Model (k = 81; tau^2 estimator: ML)\n\ntau^2 (estimated amount of total heterogeneity): 0.1009 (SE = 0.0245)\ntau (square root of estimated tau^2 value):      0.3176\nI^2 (total heterogeneity / total variability):   81.54%\nH^2 (total variability / sampling variability):  5.42\n\nTest for Heterogeneity:\nQ(df = 80) = 246.9683, p-val &lt; .0001\n\nNumber of estimates:   81\nNumber of clusters:    41\nEstimates per cluster: 1-6 (mean: 1.98, median: 1)\n\nModel Results:\n\nestimate      se¹    tval¹     df¹    pval¹   ci.lb¹   ci.ub¹     \n  0.2069  0.0569   3.6331   23.41   0.0014   0.0892   0.3245   ** \n\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n1) results based on cluster-robust inference (var-cov estimator: CR2,\n   approx t-test and confidence interval, df: Satterthwaite approx)\n\n\nThe random effects model indicates an average effect size of about 0.21 standard deviations and substantial heterogeneity, with \\(\\hat\\tau = 0.32\\). The cluster-robust standard error is about 27% bigger than the model-based standard error (not shown) because the latter does not account for dependent effect sizes.\nHere is a contour-enhanced funnel plot of the data:\n\n\nCode\nfunnel(RE_mod, refline = 0, level=c(90, 95, 99), shade=c(\"white\", \"gray55\", \"gray75\"))\n\n\n\n\n\n\n\n\n\nThe funnel plot shows some asymmetry, suggesting that there is reason to be concerned about selective reporting bias in these data.\n\n\nA Selection Model\nFor starters, we will fit a very simple selection model, with a single step in the probability of reporting at \\(\\alpha = .025\\). This is what’s come to be called the three-parameter selection model (McShane et al., 2016). The step is defined in terms of a one-sided p-value, so \\(\\alpha = .025\\) corresponds to the point where an effect size estimate in the theoretically expected direction would have a regular, two-sided p-value of .05, right at the mystical threshold of statistical significance. We fit the model using metafor’s selmodel() function:\n\n\nCode\nRE_sel &lt;- selmodel(RE_mod, type = \"stepfun\", steps = .025)\nRE_sel\n\n\n\nRandom-Effects Model (k = 81; tau^2 estimator: ML)\n\ntau^2 (estimated amount of total heterogeneity): 0.0811 (SE = 0.0261)\ntau (square root of estimated tau^2 value):      0.2848\n\nTest for Heterogeneity:\nLRT(df = 1) = 37.3674, p-val &lt; .0001\n\nModel Results:\n\nestimate      se    zval    pval   ci.lb   ci.ub    \n  0.1328  0.0655  2.0267  0.0427  0.0044  0.2612  * \n\nTest for Selection Model Parameters:\nLRT(df = 1) = 1.7646, p-val = 0.1840\n\nSelection Model Results:\n\n                     k  estimate      se     zval    pval   ci.lb   ci.ub    \n0     &lt; p &lt;= 0.025  25    1.0000     ---      ---     ---     ---     ---    \n0.025 &lt; p &lt;= 1      56    0.5485  0.2495  -1.8097  0.0703  0.0594  1.0375  . \n\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nThe selection parameter represents the probability that an effect size estimate that is not in the theoretically expected direction or not statistically significant at the conventional level would be included in the synthesis, relative to the probability that an affirmative, statistically significant effect size estimate would be included. In this example, the probability of selection is estimated as 0.55, meaning only about 55% of the non-significant results that we would expect were generated are actually reported. Adjusting for this selection bias, the model estimates an overall average effect size of 0.13 SD—smaller than the unadjusted random effects estimate—with heterogeneity of \\(\\hat\\tau = 0.28\\).\nThe problem with this analysis is that the selection model is set up under the assumption that the effect size estimates are all independent. As a result, the reported standard errors are probably smaller than they should be and the confidence intervals are narrower than they should be. We’ll use cluster bootstrapping to get standard errors and confidence intervals that should better account for effect size dependency."
  },
  {
    "objectID": "posts/cluster-bootstrap-selection-model/index.html#cluster-bootstrapping-a-selection-model",
    "href": "posts/cluster-bootstrap-selection-model/index.html#cluster-bootstrapping-a-selection-model",
    "title": "Cluster-Bootstrapping a meta-analytic selection model",
    "section": "Cluster Bootstrapping a Selection Model",
    "text": "Cluster Bootstrapping a Selection Model\nIn R, the boot package provides tools for running a variety of different bootstrap techniques and obtaining confidence intervals based on bootstrap distributions (Canty & Ripley, 2021). It’s been around for ages and has some very nice features, but it requires a bit of trickery to use it for cluster bootstrapping. The main challenge is that the package functionality is set up under the assumption that every row of the dataset should be treated as an independent observation. To make it work for cluster bootstrapping, we will need a function to fit the selection model, which takes in a dataset with one row per cluster and returns a vector of parameter estimates. The function also has to have an index argument which is a vector of row indexes used to create the bootstrap sample. Here is a skeleton for such a function:\n\n\nCode\nfit_selmodel &lt;- function(\n    dat,   # dataset with one row per cluster\n    index, # vector of indexes used to create the bootstrap sample\n    ...    # any further arguments\n) { \n  \n  # take subset of data\n  boot_dat &lt;- dat[index,]\n  \n  # fit selection model\n  \n  # compile parameter estimates into a vector\n  \n}\n\n\n\nClustering and unclustering\nThe Lehmann dataset has one row per effect size, sometimes with multiple rows per study, so we need to modify the data to have one row per study. There are at least two ways to accomplish this. One option is to create a dataset consisting only of study-level IDs, then merge it back on to the full data to get the effect-size level data:\n\n\nCode\n# Make a dataset of study IDs\ncluster_IDs &lt;- data.frame(study = unique(lehmann_dat$study))\n\nglimpse(cluster_IDs)\n\n\nRows: 41\nColumns: 1\n$ study &lt;chr&gt; \"Banas, 2014 \", \"Berthold, 2013 \", \"Bigelow et al., 2013 \", \"Ble…\n\n\nCode\n# Merge with full data\nfull_dat &lt;- merge(cluster_IDs, lehmann_dat, by = \"study\")\n\nfull_dat %&gt;% select(study, yi, vi) %&gt;% glimpse()\n\n\nRows: 81\nColumns: 3\n$ study &lt;chr&gt; \"Banas, 2014 \", \"Berthold, 2013 \", \"Bigelow et al., 2013 \", \"Big…\n$ yi    &lt;dbl&gt; 0.05716727, 0.55411916, 0.31467980, -0.73259462, 0.07921700, -0.…\n$ vi    &lt;dbl&gt; 0.10267348, 0.06030579, 0.29520322, 0.53354343, 0.02692608, 0.05…\n\n\nAnother option is to use the dplyr::nest_by() function to nest the data by cluster (Wickham et al., 2019). Then, we can use tidyr::unnest() to recover the effect size level data (Wickham et al., 2019). Like so:\n\n\nCode\n# Nest the data for each study\nlehmann_nested &lt;- nest_by(lehmann_dat, study, .key = \"data\")\n\nglimpse(lehmann_nested)\n\n\nRows: 41\nColumns: 2\nRowwise: study\n$ study &lt;chr&gt; \"Banas, 2014 \", \"Berthold, 2013 \", \"Bigelow et al., 2013 \", \"Ble…\n$ data  &lt;list&lt;tibble[,49]&gt;&gt; [&lt;tbl_df[1 x 49]&gt;], [&lt;tbl_df[1 x 49]&gt;], [&lt;tbl_df[2…\n\n\nCode\n# Recover the full dataset\nfull_dat &lt;-\n  lehmann_nested %&gt;%\n  unnest(data)\n\nfull_dat %&gt;% select(study, yi, vi) %&gt;% glimpse()\n\n\nRows: 81\nColumns: 3\nGroups: study [41]\n$ study &lt;chr&gt; \"Banas, 2014 \", \"Berthold, 2013 \", \"Bigelow et al., 2013 \", \"Big…\n$ yi    &lt;dbl&gt; 0.05716727, 0.55411916, 0.31467980, -0.73259462, 0.07921700, -0.…\n$ vi    &lt;dbl&gt; 0.10267348, 0.06030579, 0.29520322, 0.53354343, 0.02692608, 0.05…\n\n\nWe will follow the latter strategy for the remainder of our example.\nWith this nest-and-unnest approach, we can fill in a little bit more of our function skeleton:\n\n\nCode\nfit_selmodel &lt;- function(\n    dat,   # dataset with one row per cluster\n    index, # vector of indexes used to create the bootstrap sample\n    ...    # any further arguments\n) { \n  \n  # take subset of data\n  boot_dat_cluster &lt;- dat[index, ]\n  \n  # expand to one row per effect size\n  boot_dat &lt;- tidyr::unnest(boot_dat_cluster, data)\n  \n  # fit selection model\n  \n  # compile parameter estimates into a vector\n  \n}\n\n\n\n\nSelection model function\nNext, we need to complete the function by writing code to fit the selection model. This is a little bit involved because of the way the metafor package implements the Vevea-Hedges selection model. We first need to fit a regular random effects model using metafor::rma.uni(), then pass the result to the metafor::selmodel() function to fit a selection model, and then pull out the parameter estimates as a vector. To make the code clearer, we will move this step out into its own function:\n\n\nCode\nrun_sel_model &lt;- function(dat, steps = .025) {\n  \n  # initial random effects model\n  RE_mod &lt;- metafor::rma.uni(\n      yi = yi, vi = vi, data = dat, method = \"ML\"\n  )\n  \n  # fit selection model\n  res &lt;- metafor::selmodel(\n    RE_mod, type = \"stepfun\", steps = steps,\n    skiphes = TRUE, # turn off SE calculation\n    skiphet = TRUE # turn off heterogeneity test\n  )\n  \n  # compile parameter estimates into a vector\n  c(beta = res$beta[,1], tau = sqrt(res$tau2), delta = res$delta[-1])\n  \n}\n\n\nNote the use of skiphes and skiphet arguments in the selmodel() call, which skip the calculation of standard errors and skip the calculation of the test for heterogeneity. We don’t need the standard errors here because we’re going to use bootstrapping instead, and we’re not interested in the heterogeneity test. Turning off these calculations saves computational time.\nA further complication with fitting a selection model is that the parameter estimates are obtained by maximum likelihood, using an iterative optimization algorithm that sometimes fails to converge. To handle non-convergence, we will pass our function through purrr::possibly() so that errors are suppressed, rather than causing everything to grind to a halt (Wickham et al., 2019). We set the otherwise argument so that non-convergent results are returned as NA values:\n\n\nCode\nrun_sel_model &lt;- purrr::possibly(run_sel_model, otherwise = rep(NA_real_, 3))\n\n\n\n\nA first bootstrap\nHere is the completed fitting function called fit_selmodel():\n\n\nCode\nfit_selmodel &lt;- function(dat, \n                         index = 1:nrow(dat), \n                         steps = 0.025) {\n  \n  # take subset of data\n  boot_dat_cluster &lt;- dat[index, ]\n  \n  # expand to one row per effect size\n  boot_dat &lt;- tidyr::unnest(boot_dat_cluster, data)\n  \n  # fit selection model, return vector\n  run_sel_model(boot_dat, steps = steps)\n  \n}\n\n\nFirst, we take a subset of the data based on the index argument. This generates a bootstrap sample from the original data based on re-sampled clusters. We then use tidyr::unnest() to get the effect size level data for those re-sampled clusters. We then re-fit the model using our run_sel_model() function.\nNow let’s apply our function to the Lehmann dataset. We will first need to create a nested version of the dataset, with one row per study:\n\n\nCode\nlehmann_nested &lt;- nest_by(lehmann_dat, study, .key = \"data\")\n\n\nNow we can fit the selection model using fit_sel_model():\n\n\nCode\nfit_selmodel(lehmann_nested)\n\n\nbeta.intrcpt          tau        delta \n   0.1327996    0.2848304    0.5484541 \n\n\nThe results reproduce what we saw earlier when we estimated the three parameter selection model.\nNow we can bootstrap using the boot::boot() function. The inputs to boot() are the nested dataset, the function to fit the selection model, and then any additional arguments passed to fit_selmodel()—here we include an argument for steps—and finally, the number of bootstrap replications:\n\n\nCode\n# Generate bootstrap\nset.seed(20230321)\n\ntic()\n\nboots &lt;- boot(\n  data = lehmann_nested,            # nested dataset\n  statistic = fit_selmodel,         # function for fitting selection model\n  steps = .025,                     # further arguments to the fitting function\n  R = 1999                          # number of bootstraps\n)\n\ntime_seq &lt;- toc()\n\n\n141.64 sec elapsed\n\n\nThis code takes a while to run, but we can speed it up with parallel processing.\n\n\nParallel processing\nThe boot package has some handy parallel processing features, but they can be a bit finicky to use here because of how R manages environments across multiple processes. With the above code, we can’t simply turn on parallel processing because the worker processes won’t know where to find the run_sel_model() function that gets called inside fit_selmodel(). To fix this, we include the run_sel_model() function inside our fitting function, as follows:\n\n\nCode\nfit_selmodel &lt;- function(dat, \n                         index = 1:nrow(dat), \n                         steps = 0.025) {\n  \n  # take subset of data\n  boot_dat_cluster &lt;- dat[index, ]\n  \n  # expand to one row per effect size\n  boot_dat &lt;- tidyr::unnest(boot_dat_cluster, data)\n  \n  # build run_selmodel\n  run_sel_model &lt;- function(dat, steps = .025) {\n  \n    # initial random effects model\n    RE_mod &lt;- metafor::rma.uni(\n      yi = yi, vi = vi, data = dat, method = \"ML\"\n    )\n    \n    # fit selection model\n    res &lt;- metafor::selmodel(\n      RE_mod, type = \"stepfun\", steps = steps,\n      skiphes = TRUE, # turn off SE calculation\n      skiphet = TRUE # turn off heterogeneity test\n    )\n    \n    # compile parameter estimates into a vector\n    c(beta = res$beta[,1], tau = sqrt(res$tau2), delta = res$delta[-1])\n    \n  }\n  \n  p &lt;- 2L + length(steps)  # calculate total number of model parameters\n  \n  # error handling for run_sel_model\n  run_sel_model &lt;- purrr::possibly(run_sel_model, otherwise = rep(NA_real_, p))\n  \n  # fit selection model, return vector of parameter estimates\n  run_sel_model(boot_dat, steps = steps)\n  \n}\n\n\nNow we can call boot() with options for parallel processing. The machine we used to compile this post has 8 cores. We will use half of the available cores for parallel processing. The configuration of parallel processing will depend on your operating system (different options are available for Mac), so you may need to adapt this code a bit.\n\n\nCode\nncpus &lt;- parallel::detectCores() / 2\n\n# Generate bootstrap\nset.seed(20230321)\n\ntic()\n\nboots &lt;- boot(\n  data = lehmann_nested,\n  statistic = fit_selmodel, steps = .025,\n  R = 1999,\n  parallel = \"snow\", ncpus = ncpus # parallel processing options\n)\n\ntime_par &lt;- toc()\n\n\n49.61 sec elapsed\n\n\nParallel processing is really helpful here. We get 2000 bootstraps in 50 seconds, 2.9 times faster than sequential processing.\n\n\nStandard Errors\nThe standard deviations of the bootstrapped parameter estimates can be interpreted as standard errors for the parameter estimates that take into account the dependence structure of the effect size estimates. Here is a table comparing the cluster-bootstrapped standard errors to the model-based standard errors generated by selmodel():\n\n\nCode\nest &lt;- boots$t0 # original parameter estimates\n\n# calculate bootstrap SEs\nboot_SE &lt;- apply(boots$t, 2, sd, na.rm = TRUE)  \n\n# calculate model-based SEs\nmodel_SE &lt;- with(RE_sel, c(se, se.tau2 / (2 * sqrt(tau2)), se.delta[-1]))\n\n# make a table\nres &lt;- tibble(\n  Parameter = names(est),\n  Est = est,\n  `SE(bootstrap)` = boot_SE,\n  `SE(model)` = model_SE,\n  `SE(bootstrap) / SE(model)` = boot_SE / model_SE\n)\n\n\n\n\n\n\n\n\nParameter\nEst\nSE(bootstrap)\nSE(model)\nSE(bootstrap) / SE(model)\n\n\n\n\nbeta.intrcpt\n0.133\n0.113\n0.066\n1.732\n\n\ntau\n0.285\n0.148\n0.046\n3.234\n\n\ndelta\n0.548\n0.780\n0.250\n3.127\n\n\n\n\n\n\n\n\nThe standard errors based on cluster bootstrapping are all substantially larger than the model-based standard errors, which don’t account for dependence.\n\n\nConfidence Intervals\nFor reporting results from this sort of analysis, it is useful to provide confidence intervals along with the model parameter estimates and standard errors. These can be calculated using the boot::boot_ci() function. This function provides several different types of confidence intervals; for illustration, we will stick with simple percentile confidence intervals, which are calculated by taking percentiles of the bootstrap distribution of each parameter estimate. To use the function, we’ll specify the type of confidence interval and the index of the parameter we want. An index of 1 is for the overall average effect size:\n\n\nCode\nboot.ci(boots, type = \"perc\", index = 1) # For overall average ES\n\n\nBOOTSTRAP CONFIDENCE INTERVAL CALCULATIONS\nBased on 1995 bootstrap replicates\n\nCALL : \nboot.ci(boot.out = boots, type = \"perc\", index = 1)\n\nIntervals : \nLevel     Percentile     \n95%   (-0.0038,  0.4171 )  \nCalculations and Intervals on Original Scale\n\n\nHere is the confidence interval for between-study heterogeneity:\n\n\nCode\nboot.ci(boots, type = \"perc\", index = 2) # For heterogeneity\n\n\nBOOTSTRAP CONFIDENCE INTERVAL CALCULATIONS\nBased on 1995 bootstrap replicates\n\nCALL : \nboot.ci(boot.out = boots, type = \"perc\", index = 2)\n\nIntervals : \nLevel     Percentile     \n95%   ( 0.001,  0.489 )  \nCalculations and Intervals on Original Scale\n\n\nAnd for the selection weight:\n\n\nCode\nboot.ci(boots, type = \"perc\", index = 3) # For selection weight\n\n\nBOOTSTRAP CONFIDENCE INTERVAL CALCULATIONS\nBased on 1995 bootstrap replicates\n\nCALL : \nboot.ci(boot.out = boots, type = \"perc\", index = 3)\n\nIntervals : \nLevel     Percentile     \n95%   ( 0.0574,  2.7536 )  \nCalculations and Intervals on Original Scale\n\n\nPercentile confidence intervals can be asymmetric, and here the confidence interval for the selection weight parameter is notably asymmetric. The end-points of the confidence interval range from 0.057 (which represents very strong selective reporting, with only 6% of non-significant results reported) to 2.754 (which represents very strong selection against affirmative results).2 So, overall we can’t really draw any strong conclusions about the strength of selective reporting."
  },
  {
    "objectID": "posts/cluster-bootstrap-selection-model/index.html#conclusion",
    "href": "posts/cluster-bootstrap-selection-model/index.html#conclusion",
    "title": "Cluster-Bootstrapping a meta-analytic selection model",
    "section": "Conclusion",
    "text": "Conclusion\nIn this post, we’ve demonstrated how to code a cluster-level bootstrap for a three parameter version of the Vevea-Hedges selection model. We think this cluster-bootstrapping technique is interesting and promising because it can be applied with a very broad swath of models and methods to investigate selective reporting. For instance, the code we’ve demonstrated could be modified by:\n\nusing a meta-regression model instead of just a summary meta-analysis;\nusing a different form of selection function such as the beta-weight model proposed by Citkowicz and Vevea (2017) or a more elaborate step function with multiple steps; or\nusing a step function model applied across subsets of effect sizes, as in Coburn & Vevea (2015).\n\nThe metafor package implements an expansive set of selection models with the selmodel function, so one could really just swap specifications in and out. In principle, the cluster-level bootstrap could also be used in combination with other forms of selective reporting analysis such as PET-PEESE (although with such regression adjustments, cluster-robust variance estimation is also an option, see Rodgers & Pustejovsky, 2020) or Copas-style sensitivity analyses (Copas & Shi, 2001).\nWe are currently studying the performance of bootstrapping a three parameter selection model in some big Monte Carlo simulations. Based on some very preliminary results, it looks like the cluster bootstrapped selection model provides confidence intervals with reasonable coverage levels. We have more to do before we share these results, so again we want to emphasize that what we have demonstrated in this post is experimental and exploratory.\nFurther questions we need to investigate are how things work if we include covariates in the selection model, whether there are better variations of the bootstrap than what we have demonstrated here (e.g., the fractionally weighted bootstrapping, Xu et al., 2020), and the limits of this method in terms of the number of studies needed for adequate performance. If things pan out, we also plan to turn the workflow we’ve demonstrated here into some more user-friendly functions, perhaps as part of the wildmeta package."
  },
  {
    "objectID": "posts/cluster-bootstrap-selection-model/index.html#footnotes",
    "href": "posts/cluster-bootstrap-selection-model/index.html#footnotes",
    "title": "Cluster-Bootstrapping a meta-analytic selection model",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nTechnically, these parameters \\(\\lambda_1,\\lambda_2,\\lambda_3\\) are not absolute probabilities but instead relative risks of being reported, compared to a reference range of \\(p\\)-values. In the above example, they would be defined relative to the probability of being reported for an effect size estimate with \\(p \\leq .01\\).↩︎\nCompare this to the model-based confidence interval of \\([0.059, 1.037]\\).↩︎"
  },
  {
    "objectID": "posts/What-is-Tau-U/index.html",
    "href": "posts/What-is-Tau-U/index.html",
    "title": "What is Tau-U?",
    "section": "",
    "text": "Parker, Vannest, Davis, and Sauber (2011) proposed the Tau-U index—actually several indices, rather—as effect size measures for single-case designs. The original paper describes several different indices that involve corrections for trend during the baseline phase, treatment phase, both phases, or neither phase. Without correcting for trends in either phase, the index is equal to the Mann-Whitney \\(U\\) statistic calculated by comparing every pair of observations containing one point from each phase, scaled by the total number of such pairs. This version, which I’ll call just “Tau”, is simply a linear re-scaling of the NAP statistic to the range [-1,1].\nTo correct for baseline trend, the original paper proposes to calculate Kendall’s rank correlation (\\(\\tau_A\\)) between the phase A outcome data and the session numbers and use the result to make an adjustment to Tau. The other analyses presented in the original paper (incorporating adjustments for time trends during the treatment phase) are not presented in subsequent review papers, nor are they implemented in the web-calculator created by the authors, and so I won’t discuss them further here. Instead, in this post I will examine the calculation of the version of Tau-U that incorporates a baseline trend correction. This version seems to be the most widely applied in practice (likely due to the availability of the web-calculator) and is presented in several review papers by the same authors. It turns out though, that the definition of the index has shifted from the original paper to subsequent presentations.\nTo make this concrete, let me first define a couple of things. Suppose that we have data from the baseline and treatment phases for a single case, where the baseline phase has \\(m\\) observations and treatment phase has \\(n\\) observations. Let \\(y^A_1,...,y^A_m\\) denote the baseline phase data and \\(y^B_1,...,y^B_n\\) denote the treatment phase data. Let \\(S_P\\) denote Kendall’s S statistic calculated for the comparison between phases and \\(S_A\\) denote Kendall’s S statistic calculated on the baseline trend. More precisely,\n\\[\n\\begin{aligned}\nS_P &= \\sum_{i=1}^m \\sum_{j=1}^n \\left[I\\left(y^B_j &gt; y^A_i\\right) - I\\left(y^B_j &lt; y^A_i\\right)\\right] \\\\\nS_A &= \\sum_{i=1}^{m - 1} \\sum_{j = i + 1}^m \\left[I\\left(y^A_j &gt; y^A_i\\right) - I\\left(y^A_j &lt; y^A_i\\right)\\right].\n\\end{aligned}\n\\]\nNote that \\(S_P\\) is calculated from \\(m \\times n\\) pairs of observations, and Tau (without trend correction) is equal to \\(\\text{Tau} = S_P / (m n)\\). Furthermore, \\(S_A\\) is calculated from \\(m (m - 1) / 2\\) pairs of observations and Kendall’s rank correlation coefficient for the baseline phase observations is \\(t_A = S_A / [m (m - 1) / 2]\\)."
  },
  {
    "objectID": "posts/What-is-Tau-U/index.html#the-original-version",
    "href": "posts/What-is-Tau-U/index.html#the-original-version",
    "title": "What is Tau-U?",
    "section": "The original version",
    "text": "The original version\nIn the original paper, the authors explain that values of Tau-U can be calculated by adding or substracting values of \\(\\tau\\), weighted by the corresponding number of pairs. Thus, Tau-U would be calculated as\n\\[\n\\text{Tau-U} = \\frac{S_P - S_A}{mn + m(m - 1) / 2} = \\frac{2n}{2n + m - 1} \\text{Tau} - \\frac{m - 1}{2n + m - 1} t_A.\n\\]\nBoth \\(\\text{Tau}\\) and \\(t_A\\) have range [-1,1], and so Tau-U has the same range. This version of Tau-U can be calculated using this web app by Rumen Manolov, which is based on this R code by Kevin Tarlow. (The app and the R script also provide the other variants of Tau-U described in Parker, Vannest, Davis, and Sauber (2011).)"
  },
  {
    "objectID": "posts/What-is-Tau-U/index.html#the-revised-version",
    "href": "posts/What-is-Tau-U/index.html#the-revised-version",
    "title": "What is Tau-U?",
    "section": "The revised (?) version",
    "text": "The revised (?) version\nParker, Vannest, and Davis (2011) reviewed nine different non-overlap indices for use with data from single-case designs, including Tau-U. Rather than describing all four variations from the original paper, the authors define the index as follows:\n\nTau-U (Parker et al., in press) extends [Tau] to control for undesirable positive baseline trend (monotonic trend). Monotonic trend is the upward progression of data points in any configuration, whether linear, curvilinear, or even in a mixed pattern of “fits and starts” (p. 11).\n\nIn this and subsequent review articles, Tau-U seems to refer exclusively to the variant involving comparison between phases A and B, with an adjustment for phase A trend. That seems a sensible enough choice, which could have been due to space limitations, guidance from the journal editor, or further refinement of the methods (i.e., recognizing which of the variants would be most useful in application). However, the presentation of Tau-U in this article involved more than a change in emphasis—the definition of the index also changed. Following the notation above, Tau-U was now defined as\n\\[\n\\text{Tau-U} = \\frac{S_P - S_A}{mn} = \\text{Tau} - \\frac{m - 1}{2n} t_A.\n\\]\nThe logical range of this version of the index is from \\(-(2n + m - 1) / (2n)\\) to \\((2n + m - 1) / (2n)\\).\nThis is the version of Tau-U implemented in the singlecaseresearch.org web calculator. It is also the version described in a later chapter by the same authors (Parker, Vannest, & Davis, 2014) and a review article by Rakap (2015). My previous post about Tau-U also presented this version of the index and noted that its magnitude is sensitive to the lengths of the baseline and treatment phases, which makes it rather difficult to interpret the Tau-U index as a measure of treatment effect magnitude."
  },
  {
    "objectID": "posts/What-is-Tau-U/index.html#comparison",
    "href": "posts/What-is-Tau-U/index.html#comparison",
    "title": "What is Tau-U?",
    "section": "Comparison",
    "text": "Comparison\nHere is an R function for calculating the original or revised versions of Tau-U:\n\nTau_U &lt;- function(A_data, B_data, version = \"revised\") {\n    m &lt;- length(A_data)\n    n &lt;- length(B_data)\n    Q_A &lt;- sapply(A_data, function(j) (j &gt; A_data) - (j &lt; A_data))\n    Q_P &lt;- sapply(B_data, function(j) (j &gt; A_data) - (j &lt; A_data))\n    \n    if (version==\"original\") {\n      (sum(Q_P) - sum(Q_A[upper.tri(Q_A)])) / (m * n + m * (m - 1) / 2)\n    } else {\n      (sum(Q_P) - sum(Q_A[upper.tri(Q_A)])) / (m * n)\n    }\n}\n\nThe papers I’ve mentioned above all provide examples of the calculation of Tau-U. The following table reports the data from each of these examples (Parker, Vannest, Davis, and Sauber, 2011a; Parker, Vannest, and Davis, 2011b; Parker, Vannest, & Davis, 2014), along with the value of Tau-U based on the original and revised formulas. The differences in magnitude are non-trivial.\n\n\n\n\n\n\n\n\n\n\n\n\nSource\nPhase A data\nPhase B data\noriginal\nrevised\n\n\n\n\n2011a, Figure 2\n2, 3, 5, 3\n4, 5, 5, 7, 6\n0.5000000\n0.6500000\n\n\n2011b, Figure 1\n20, 20, 26, 25, 22, 23\n28, 25, 24, 27, 30, 30, 29\n0.5438596\n0.7380952\n\n\n2011b, Table 1\n3, 3, 4, 5\n4, 5, 6, 7, 7\n0.4230769\n0.5500000\n\n\n2014, Figure 4.1\n22, 21, 23, 23, 23, 22\n24, 22, 23, 23, 24, 26, 25\n0.4385965\n0.5952381"
  },
  {
    "objectID": "posts/What-is-Tau-U/index.html#implications",
    "href": "posts/What-is-Tau-U/index.html#implications",
    "title": "What is Tau-U?",
    "section": "Implications",
    "text": "Implications\nRather than one effect size index called “Tau-U”, there are instead two different definitions, which can lead to quite different values of the index. Given this, researchers who apply Tau-U should endeavor to be clear and unambiguous about which version of the index they use. This can be done by stating exactly which software routine, web-app, or formula was used in making the calculations. If the calculations are done using a computer script, then the script should be made available (e.g., through the Open Science Framework) so that other researchers can replicate the calculations.\nFurthermore, researchers need to be careful about applying interpretive guidelines for Tau-U, since those guidelines will not apply uniformly across the different versions of the index.\nFinally, I would recommend that any researchers who conduct a meta-analysis of single-case research make available the raw data used for effect size calculations, so that other researchers can scrutinize, replicate, and extend their analyses. The whole enterprise of research synthesis rests on the availability of data from primary studies (at least in summary form). It seems to me that meta-analysts thus have a duty to make the data that they assemble and organize readily accessible for others to use. Particularly in the context of meta-analysis of single-case research—where new methods are developing rapidly and there is not currently consensus around best practices—it seems especially appropriate and prudent to make one’s data available for future re-analysis."
  },
  {
    "objectID": "posts/weighting-in-multivariate-meta-analysis/index.html",
    "href": "posts/weighting-in-multivariate-meta-analysis/index.html",
    "title": "Weighting in multivariate meta-analysis",
    "section": "",
    "text": "One common question about multivariate/multi-level meta-analysis is how such models assign weight to individual effect size estimates. When a version of the question came up recently on the R-sig-meta-analysis listserv, Dr. Wolfgang Viechtbauer offered a whole blog post in reply, demonstrating how weights work in simpler fixed effect and random effects meta-analysis and then how things get more complicated in multivariate models. I started thumb-typing my own reply as well, but then decided it would be better to write up a post so that I could use a bit of math notation (and to give my thumbs a break). So, in this post I’ll try to add some further intuition on how weights work in certain multivariate meta-analysis models. Most of the discussion will apply to models that include multiple level of random effects, but no predictors. I’ll also comment briefly on meta-regression models with only study-level predictor variables, and finally give some pointers to work on more complicated models."
  },
  {
    "objectID": "posts/weighting-in-multivariate-meta-analysis/index.html#a-little-background",
    "href": "posts/weighting-in-multivariate-meta-analysis/index.html#a-little-background",
    "title": "Weighting in multivariate meta-analysis",
    "section": "0.1 A little background",
    "text": "0.1 A little background\nIt’s helpful to start by looking briefly at the basic fixed effect and random effects models, assuming that we’ve got a set of studies that each contribute a single effect size estimate so everything’s independent. Letting \\(T_j\\) be the effect size from study \\(j\\), with sampling variance \\(V_j\\), both for \\(j = 1,...,k\\), the basic random effects model is: \\[\nT_j = \\mu + \\eta_j + e_j\n\\] where \\(\\mu\\) is the overall average effect size, \\(v_j\\) is a random effect with variance \\(\\text{Var}(\\eta_j) = \\tau^2\\) and \\(e_j\\) is a sampling error with known variance \\(V_j\\). The first step in estimating this model is to estimate \\(\\tau^2\\). There’s lots of methods for doing so, but let’s not worry about those details—just pick one and call the estimate \\(\\hat\\tau^2\\). Then, to estimate \\(\\mu\\), we take a weighted average of the effect size estimates: \\[\n\\hat\\mu = \\frac{1}{W} \\sum_{j=1}^k w_j T_j, \\qquad \\text{where} \\quad W = \\sum_{j=1}^k w_j.\n\\] The weights used in the weighted average are chosen to make the overall estimate as precise as possible (i.e., having the smallest possible sampling variance or standard error). Mathematically, the best possible weights are inverse variance weights, that is, setting the weight for each effect size estimate proportional to the inverse of how much variance there is in each estimate. With inverse variance weights, larger studies with more precise effect size estimates will tend to get more weight and smaller, noisier studies will tend to get less weight.\nIn the basic random effects model, the weights for each study are proportional to \\[\nw_j = \\frac{1}{\\hat\\tau^2 + V_j},\n\\] for \\(j = 1,...,k\\). The denominator term here includes both the (estimated) between-study heterogeneity and the sampling variance because both terms contribute to how noisy the effect size estimate is. In the fixed effect model, we ignore between-study heterogeneity so the weights are inversely proportional to the sampling variances, with \\(w_j = 1 / V_j\\). In the random effects model, larger between-study heterogeneity will make the weights closer to equal, while smaller between-study heterogeneity will lead to weights that tend to emphasize larger studies with more precise estimates. In the remainder, I’ll show that there are some similar dynamics at work in a more complicated, multivariate meta-analysis model."
  },
  {
    "objectID": "posts/weighting-in-multivariate-meta-analysis/index.html#a-multivariate-meta-analysis",
    "href": "posts/weighting-in-multivariate-meta-analysis/index.html#a-multivariate-meta-analysis",
    "title": "Weighting in multivariate meta-analysis",
    "section": "0.2 A multivariate meta-analysis",
    "text": "0.2 A multivariate meta-analysis\nNow let’s consider the case where some or all studies in our synthesis contribute more than one effect size estimate. Say that we have effect sizes \\(T_{ij}\\), where \\(i = 1,...,n_j\\) indexes effect size estimates within study \\(j\\) and \\(j\\) indexes studies, for \\(j = 1,...,k\\). Say that effect size estimate \\(T_{ij}\\) has sampling variance \\(V_{ij}\\), and there is some sampling correlation between effect sizes \\(h\\) and \\(i\\) within study \\(j\\), denoted \\(r_{hij}\\).\nThere are many models that a meta-analyst might consider for this data structure. A fairly common one would be a model that includes random effects not only for between-study heterogeneity (as in the basic random effects model) but also random effects capturing within-study heterogeneity in true effect sizes. Let me write this model heirarchically, as \\[\n\\begin{align}\nT_{ij} &= \\theta_j + \\nu_{ij} + e_{ij} \\\\\n\\theta_j &= \\mu + \\eta_j\n\\end{align}\n\\] In the first line of the model, \\(\\theta_j\\) denotes the average effect size parameter for study \\(j\\), \\(\\nu_{ij}\\) captures within-study heterogeneity in the true effect size parameters and \\(e_{ij}\\) is a sampling error. Above, I’ve assumed that we know the structure of the sampling errors, so \\(\\text{Var}(e_{ij}) = V_{ij}\\) and \\(\\text{Cov}(e_{hj}, e_{ij}) = r_{hij} \\sqrt{V_{hj} V_{ij}}\\). Let’s also denote the within-study variance as \\(\\omega^2\\), so \\(\\text{Var}(\\nu_{ij}) = \\omega^2\\). In the second line of the model, \\(\\mu\\) is still the overall average effect size across all studies and effect sizes within studies and \\(\\eta_j\\) is a between-study error, with \\(\\text{Var}(\\eta_j) = \\tau^2\\), capturing the degree of heterogeneity in the average effect sizes (the \\(\\theta_j\\)’s) across studies.1\nOne thing to note about this model is that it treats all of the effect sizes as coming from a population with a common mean \\(\\mu\\). Some statisticians might object to calling it a multivariate model because we’re not distinguishing averages for different dimensions (or variates) of the effect sizes. To this I say: whatev’s, donkey! I’m calling it multivariate because you have to use the rma.mv() function from the metafor package to estimate it. I will acknowledge, though, that there will often be reason to use more complicated models, for example by replacing the overall average \\(\\mu\\) with some meta-regression \\(\\mathbf{x}_{ij} \\boldsymbol\\beta\\). That’s a discussion for another day. For now, we’re only going to consider the model with an overall average effect size parameter \\(\\mu\\). The question is, how do the individual effect size estimates \\(T_{ij}\\) contribute to the estimate of this overall average effect?"
  },
  {
    "objectID": "posts/weighting-in-multivariate-meta-analysis/index.html#equally-precise-effect-size-estimates",
    "href": "posts/weighting-in-multivariate-meta-analysis/index.html#equally-precise-effect-size-estimates",
    "title": "Weighting in multivariate meta-analysis",
    "section": "0.3 Equally precise effect size estimates",
    "text": "0.3 Equally precise effect size estimates\nTo make some headway, it is helpful to first consider an even more specific model where, within a given study, all effect size estimates are equally precise and equally correlated. In particular, let’s assume that for each study \\(j\\), the sampling variances are all equal, with \\(V_{ij} = V_j\\) for \\(i = 1,...,n_j\\), and the correlations between the sampling errors are also all equal, with \\(r_{hij} = r_j\\) for \\(h,i = 1,...,n_j\\).\nThese assumptions might not be all that far-fetched. Within a given study, if the effect size estimates are for different measures of a common construct, it’s not unlikely that they would all be based on similar sample sizes (+/- a bit of item non-response). It might be a bit less likely if the effect size estimates are for treatment effects from different follow-up times (since drop-out/non-response tends to increase over time) or different treatment groups compared to a common control group—but still perhaps not entirely unreasonable. Further, it’s rather uncommon to have good information about the correlations between effect size estimates from a given study (because primary studies don’t often report all of the information needed to calculate these correlations). In practice, meta-analysts might need to simply make a rough guess about the correlations and then use robust variance estimation and/or sensitivity analysis to check themselves. And if we’re just ball-parking, then we’ll probably assume a single correlation for all of the studies.\nThe handy thing about this particular scenario is that, because all of the effect size estimates within a study are equally precise and equally correlated, the most efficient way to estimate an average effect for a given study is to just take the simple average (and, intuitively, this seems like the only sensible thing to do). To be precise, consider how we would estimate \\(\\theta_j\\) for a given study \\(j\\). The most precise possible estimate is simply \\[\n\\hat\\theta_j = \\frac{1}{n_j} \\sum_{i=1}^{n_j} T_{ij}.\n\\] And we could do the same for each of the other studies, \\(j = 1,...,k\\). It turns out that the estimate of the overall average effect size is a weighted average of these study-specific average effect sizes: \\[\n\\hat\\mu = \\frac{1}{W} \\sum_{j=1}^k w_j \\hat\\theta_j,\n\\] for some weights \\(w_1,...,w_k\\). But what are these weights? Just like in the basic random effects model, they are inverse-variance weights. It’s just that the variance is a little bit more complicated.\nConsider how precise each of the study-specific estimates are, relative to the true effects in their respective studies. Conditional on the true effect \\(\\theta_j\\), \\[\n\\text{Var}(\\hat\\theta_j | \\theta_j) = \\frac{1}{n_j}\\left(\\omega^2 + (n_j - 1) r_j V_j + V_j\\right).\n\\] Without conditioning on \\(\\theta_j\\), the variance of the \\(\\hat\\theta_j\\) estimates also includes a term for variation in the true study-specific average effect sizes, becoming \\[\n\\text{Var}(\\hat\\theta_j) = \\tau^2 + \\frac{1}{n_j}\\left(\\omega^2 + (n_j - 1) r_j V_j + V_j\\right).\n\\] The weights used in estimating \\(\\mu\\) are the inverse of this quantity: \\[\nw_j = \\frac{1}{\\tau^2 + \\frac{1}{n_j}\\left(\\omega^2 + (n_j - 1) r_j V_j + V_j\\right)}.\n\\] Within a study, each individual effect size gets an \\(n_j^{th}\\) of this study-level weight. We can therefore write the overall average as \\[\n\\hat\\mu = \\frac{1}{W} \\sum_{j=1}^k \\sum_{i=1}^{n_j} w_{ij} T_{ij},\n\\] where \\[\nw_{ij} = \\frac{1}{n_j \\tau^2 + \\omega^2 + (n_j - 1) r_j V_j + V_j}.\n\\]\nThere are several things worth noting about this expression for the weights. First, suppose that there is little between-study or within-study heterogeneity, so \\(\\tau^2\\) and \\(\\omega^2\\) are both close to zero. Then the weights are driven by the number of effect sizes within the study (\\(n_j\\)), the sampling variance of those effect sizes (\\(V_j\\)) and their correlation \\(r_j\\). If \\(r_j\\) is near one, then averaging together a bunch of highly correlated estimates doesn’t improve precision much, relative to just using one of the effect sizes. The study-specific average effect estimate will therefore have variance close to \\(V_j\\) (i.e., the variance of a single effect size estimate). If \\(r_j\\) is below one, then averaging yields a more precise estimate than any of the individual effect sizes, and averaging together more effect sizes will yield a more precise estimate at the study level. If the assumed correlations are reasonably accurate, the weights used in the multi-variate meta-analysis will appropriately take into account the number of effect sizes within each study and the precision of those effect sizes.\nSecond, now suppose that there is no between-study heterogeneity (\\(\\tau^2 = 0\\)) but there is positive within-study heterogeneity. Larger degrees of within-study heterogeneity will tend to equalize the weights at the effect size level, regardless of how effect size estimates are nested within studies. When there is within-study heterogeneity, averaging together a bunch of estimates will yield a more precise estimate of study-specific average effects. Therefore, when \\(\\omega^2\\) is larger, studies with more effect sizes will tend to get a relatively larger share of the weight.2\nThird and finally, between-study heterogeneity will tend to equalize the weights at the study level, so that the overall average is pulled closer to a simple average of the study-specific average effects. This works very much like in basic random effects meta-analysis, where increased heterogeneity will lead to weights that are closer to equal and an average effect size estimate that is closer to a simple average."
  },
  {
    "objectID": "posts/weighting-in-multivariate-meta-analysis/index.html#a-computational-example",
    "href": "posts/weighting-in-multivariate-meta-analysis/index.html#a-computational-example",
    "title": "Weighting in multivariate meta-analysis",
    "section": "0.4 A computational example",
    "text": "0.4 A computational example\nI think it’s useful to verify algebraic results like the ones I’ve given above by checking that you can reproduce them with real data. I’ll use the corrdat dataset from the robumeta package for illustration. The dataset has one duplicated row in it (I have no idea why!), which I’ll remove before analyzing further.\n\nlibrary(dplyr)\n\ndata(corrdat, package = \"robumeta\")\n\ncorrdat &lt;- \n  corrdat %&gt;%\n  distinct(studyid, esid, .keep_all = TRUE)\n\nThis dataset included a total of 171 effect size estimates from 39 unique studies. For each study, between 1 and 18 eligible effect size estimates were reported. Here is a histogram depicting the number of studies by the number of reported effect size estimates:\n\n\n\n\n\n\n\n\n\nHere is the plot of the variances of each effect size versus the study IDs:\n\n\n\n\n\n\n\n\n\nFor most of the studies, the effect sizes have very similar sampling variances. One exception is study 9, where two of the effect sizes have variances of under 0.20 and the other two effect sizes have variances in excess of 0.35. Another exception is study 30, which has one effect size with much larger variance than the others.\nJust for sake of illustration, I’m going to enforce my assumption that effect sizes have equal variances within each study by recomputing the sampling variances as the average sampling variance within each study. I will then impute a sampling variance-covariance matrix for the effect sizes, assuming a correlation of 0.7 for effects from the same study:\n\nlibrary(clubSandwich)\n\ncorrdat &lt;- \n  corrdat %&gt;%\n  group_by(studyid) %&gt;%\n  mutate(V_bar = mean(var)) %&gt;%\n  ungroup()\n\nV_mat &lt;- impute_covariance_matrix(vi = corrdat$V_bar, \n                                  cluster = corrdat$studyid,\n                                  r = 0.7)\n\nWith this variance-covariance matrix, I can then estimate the multivariate meta-analysis model:\n\nlibrary(metafor)\n\nMVMA_fit &lt;- rma.mv(yi = effectsize, V = V_mat, \n                   random = ~ 1 | studyid / esid,\n                   data = corrdat)\n\nsummary(MVMA_fit)\n\n\nMultivariate Meta-Analysis Model (k = 171; method: REML)\n\n  logLik  Deviance       AIC       BIC      AICc   \n-94.7852  189.5703  195.5703  204.9777  195.7149   \n\nVariance Components:\n\n            estim    sqrt  nlvls  fixed        factor \nsigma^2.1  0.0466  0.2159     39     no       studyid \nsigma^2.2  0.1098  0.3314    171     no  studyid/esid \n\nTest for Heterogeneity:\nQ(df = 170) = 1141.4235, p-val &lt; .0001\n\nModel Results:\n\nestimate      se    zval    pval   ci.lb   ci.ub      \n  0.2263  0.0589  3.8413  0.0001  0.1108  0.3417  *** \n\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nBased on this model, between-study heterogeneity is estimated as \\(\\hat\\tau = 0.216\\) and within-study heterogeneity is estimated as \\(\\hat\\omega = 0.331\\), both of which are quite high. The overall average effect size estimate is 0.226, with a standard error of 0.059.\nI’ll first get the weights used in rma.mv to compute the overall average. The weights are represented as an \\(N \\times N\\) matrix. Taking the row or column sums, then rescaling by the total, gives the weight assigned to each effect size estimate:\n\nW_mat &lt;- weights(MVMA_fit, type = \"matrix\")\ncorrdat$w_ij_metafor &lt;- colSums(W_mat) / sum(W_mat)\n\nTo verify that the formulas above are correct, I’ll use them to directly compute weights:\n\nr &lt;- 0.7\ntau_sq &lt;- MVMA_fit$sigma2[1]\nomega_sq &lt;- MVMA_fit$sigma2[2]\n\ncorrdat_weights &lt;- \n  corrdat %&gt;%\n  group_by(studyid) %&gt;%\n  mutate(\n    n_j = n(),\n    w_ij = 1 / (n_j * tau_sq + omega_sq + (n_j - 1) * r * V_bar + V_bar)\n  ) %&gt;%\n  ungroup() %&gt;%\n  mutate(\n    w_ij = w_ij / sum(w_ij)\n  )\n\nThe weights I computed are perfectly correlated with the weights used rma.mv, as can be seen in the plot below.\n\nggplot(corrdat_weights, aes(w_ij, w_ij_metafor)) + \n  geom_point() + \n  theme_minimal()\n\n\n\n\n\n\n\n\nIf we remove the within-study random effect term from the model, the weights will be equivalent to setting \\(\\omega^2\\) to zero, but with a different estimate of \\(\\tau^2\\).\n\nMVMA_no_omega &lt;- rma.mv(yi = effectsize, V = V_mat, \n                        random = ~ 1 | studyid,\n                        data = corrdat)\nMVMA_no_omega\n\n\nMultivariate Meta-Analysis Model (k = 171; method: REML)\n\nVariance Components:\n\n            estim    sqrt  nlvls  fixed   factor \nsigma^2    0.0951  0.3084     39     no  studyid \n\nTest for Heterogeneity:\nQ(df = 170) = 1141.4235, p-val &lt; .0001\n\nModel Results:\n\nestimate      se    zval    pval   ci.lb   ci.ub      \n  0.2235  0.0619  3.6122  0.0003  0.1022  0.3448  *** \n\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nRe-fitting the model with rma.mv() gives an between-study heterogeneity estimate of \\(\\hat\\tau = 0.308\\) and an overall average effect size estimate of \\(\\hat\\mu = 0\\). Using this estimate, I’ll compute the weights based on the formula and then use those weights to determine the overall average effect size estimate.\n\ntau_sq &lt;- MVMA_no_omega$sigma2\n\ncorrdat_weights &lt;- \n  corrdat_weights %&gt;%\n  mutate(\n    w_ij_no_omega = 1 / (n_j * tau_sq + (n_j - 1) * r * V_bar + V_bar),\n    w_ij_no_omega = w_ij_no_omega / sum(w_ij_no_omega)\n  )\n\nwith(corrdat_weights, weighted.mean(effectsize, w = w_ij_no_omega))\n\n[1] 0.2235231\n\n\nThis matches the output of rma.mv().\nHere is a plot showing the weights of individual effect sizes for each study. In blue are the weights under the assumption that \\(\\omega^2 = 0\\). In green are the weights allowing for \\(\\omega^2 &gt; 0\\). It’s notable here that introducing the within-study heterogeneity term leads to pretty big changes in the weights for some studies. In particular, studies that have only a single effect size estimate (e.g., studys 7, 8, 22, 25, 28) lose a lot of weight when \\(\\omega^2 &gt; 0\\). That’s partially because \\(\\omega^2\\) tends to pull weight towards studies with more effect sizes, and partially because of the change in the estimate of \\(\\tau^2\\), which tends to equalize the weight assigned to each study.\n\n\n\n\n\n\n\n\n\nBelow is a plot illustrating the changes in study-level weights (i.e., aggregating the weight assigned to each study). The bar color corresponds to the number of effect size estimates in each study; light grey studies have just one effect size, while studies with more effect sizes are more intensly purple. The notable drops in weight for studies with a single effect size estimate (light grey) are visible here too. Studies with more effect sizes (e.g., studies 2, 15, 30, with dark purple bars) gain weight when we allow \\(\\omega^2\\) to be greater than zero.\n\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\nWarning: The `guide` argument in `scale_*()` cannot be `FALSE`. This was deprecated in\nggplot2 3.3.4.\nℹ Please use \"none\" instead."
  },
  {
    "objectID": "posts/weighting-in-multivariate-meta-analysis/index.html#now-without-compound-symmetry",
    "href": "posts/weighting-in-multivariate-meta-analysis/index.html#now-without-compound-symmetry",
    "title": "Weighting in multivariate meta-analysis",
    "section": "0.5 Now without compound symmetry",
    "text": "0.5 Now without compound symmetry\nIf we remove the restrictions that effect sizes from the same study have the same sampling variance and are equi-correlated, then the weights get a little bit more complicated. However, the general intuitions carry through. Let’s now consider the model with arbitrary sampling variance \\(V_{ij}\\) and sampling correlations within studies \\(r_{hij}\\). The most efficient estimate of the study-specific average effect is now a weighted average, with weights that depend on both the variances and covariances of the effect size estimates within each study. Let \\[\n\\boldsymbol{\\hat\\Sigma}_j = \\hat\\omega^2 \\mathbf{I}_j + \\mathbf{V}_j,\n\\] where \\(\\mathbf{I}_j\\) is an \\(n_j \\times n_j\\) identity matrix and \\(\\mathbf{V}_j\\) is the sampling variance-covariance matrix of the effect size estimates, with entry \\((h,i)\\) equal to \\(\\left[\\mathbf{V}_j\\right]_{h,i} = r_{hij} \\sqrt{V_{hj} V_{ij}}\\). The estimate of the study-specific average effect size for study \\(j\\) is still a weighted average: \\[\n\\hat\\theta_j = \\frac{\\sum_{i=1}^{n_j} s_{ij} T_{ij}}{\\sum_{i=1}^{n_j} s_{ij}},\n\\] where \\[\ns_{ij} = \\displaystyle{\\sum_{h=1}^{n_j} \\left[\\boldsymbol{\\hat\\Sigma}^{-1}\\right]_{hi}},\n\\] and \\(\\left[\\boldsymbol{\\hat\\Sigma}^{-1}\\right]_{hi}\\) denotes entry \\((h,i)\\) in the inverse of the matrix \\(\\boldsymbol{\\hat\\Sigma}\\). Let \\(V^C_j\\) denote the variance of the study-specific average effect size estimate, conditional on the true \\(\\theta_j\\): \\[\nV^C_j = \\text{Var}(\\hat\\theta_j | \\theta_j) = \\left(\\sum_{i=1}^{n_j} s_{ij} \\right)^{-1}\n\\] The unconditional variance of \\(\\hat\\theta_j\\) is then \\[\n\\text{Var}(\\hat\\theta_j) = \\tau^2 + V^C_j.\n\\] Because the overall average effect size estimate is (still) the inverse-variance weighted average, the weight assigned at the study level is equal to \\[\nw_j = \\frac{1}{\\hat\\tau^2 + V^C_j}\n\\] and the weight assigned to individual effect sizes is \\[\nw_{ij} = \\frac{s_{ij} V^C_j}{\\hat\\tau^2 + V^C_j}.\n\\] How do \\(\\omega^2\\) and \\(\\tau^2\\) affect these more general weights? The intuitions that I described earlier still mostly hold. Increasing \\(\\omega^2\\) will tend to equalize the weights at the effect size level (i.e., equalize the \\(s_{ij}\\) across \\(i\\) and \\(j\\)), pulling weight towards studies with more effect size estimates. Increasing \\(\\tau^2\\) will tend to equalize the weights at the study-level.\nOne wrinkle with the more general form of the weights is that the effect-size level weights can sometimes be negative (i.e., negative \\(s_{ij}\\)). This will tend to happen when the sampling variances within a study are discrepant, such as when one \\(V_{ij}\\) is much smaller than the others in study \\(j\\), when the (assumed or estimated) sampling correlation is high, and when \\(\\omega^2\\) is zero or small. This is something that warrants further investigation."
  },
  {
    "objectID": "posts/weighting-in-multivariate-meta-analysis/index.html#what-about-meta-regression",
    "href": "posts/weighting-in-multivariate-meta-analysis/index.html#what-about-meta-regression",
    "title": "Weighting in multivariate meta-analysis",
    "section": "0.6 What about meta-regression?",
    "text": "0.6 What about meta-regression?\nSome of the foregoing analysis also applies to models that include predictors. In particular, the formulas I’ve given for the weights will still hold for meta-regression models that include only study-level predictors. In other words, they work for models of the following form: \\[\nT_{ij} = \\mathbf{x}_j \\boldsymbol\\beta + \\eta_j + \\nu_{ij} + e_{ij},\n\\] where \\(\\mathbf{x}_j\\) is a row-vector of one or more predictors for study \\(j\\) (including a constant intercept). Introducing these predictors will alter the variance component estimates \\(\\hat\\tau^2\\) and \\(\\hat\\omega^2\\), but the form of the weights will remain the same as above, and the intuitions still hold. This is because, for purposes of estimating \\(\\boldsymbol\\beta\\), the model is essentially the same as a meta-regression at the study level, using the study-specific average effect size estimates as input: \\[\n\\hat\\theta_j = \\mathbf{x}_j \\boldsymbol\\beta + \\eta_j + \\tilde{e}_j\n\\] where \\(\\text{Var}(\\tilde{e}_j) = \\text{Var}(\\hat\\theta_j | \\theta_j)\\).3\nHere is an illustration with the corrdat meta-analysis. In these data, the variable college indicates whether the effect size comes from a college-age sample; it varies only at the study level. The variable males, binge, and followup have some within-study variation, which I’ll by taking the average of each of these predictors at the study level:\n\ncorrdat &lt;- \n  corrdat %&gt;%\n  group_by(studyid) %&gt;%\n  mutate(\n    males_M = mean(males),\n    binge_M = mean(binge),\n    followup_M = mean(followup)\n  )\n\nNow let’s fit a meta-regression model using all of the study-level predictors:\n\nMVMR_fit &lt;- rma.mv(yi = effectsize, V = V_mat,\n                   mods = ~ college + males_M + binge_M + followup_M,  \n                   random = ~ 1 | studyid / esid,\n                   data = corrdat)\n\nsummary(MVMR_fit)\n\n\nMultivariate Meta-Analysis Model (k = 171; method: REML)\n\n  logLik  Deviance       AIC       BIC      AICc   \n-86.6244  173.2488  187.2488  209.0327  187.9577   \n\nVariance Components:\n\n            estim    sqrt  nlvls  fixed        factor \nsigma^2.1  0.0297  0.1723     39     no       studyid \nsigma^2.2  0.1068  0.3268    171     no  studyid/esid \n\nTest for Residual Heterogeneity:\nQE(df = 166) = 1083.6655, p-val &lt; .0001\n\nTest of Moderators (coefficients 2:5):\nQM(df = 4) = 13.0787, p-val = 0.0109\n\nModel Results:\n\n            estimate      se     zval    pval    ci.lb    ci.ub    \nintrcpt      -0.0361  0.3678  -0.0982  0.9218  -0.7571   0.6849    \ncollege       0.2660  0.1384   1.9215  0.0547  -0.0053   0.5373  . \nmales_M       0.0023  0.0048   0.4753  0.6346  -0.0072   0.0118    \nbinge_M       0.3441  0.1570   2.1927  0.0283   0.0365   0.6518  * \nfollowup_M   -0.0023  0.0011  -2.0379  0.0416  -0.0044  -0.0001  * \n\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nAs you might expect, between-study heterogeneity is reduced a bit by the inclusion of these predictors.4\nWe can check my claim of computational equivalence by fitting the meta-regression model at the study level. Here I’ll aggregate everything up to the study level and compute the study-level weights:\n\ntau_sq_reg &lt;- MVMR_fit$sigma2[1]\nomega_sq_reg &lt;- MVMR_fit$sigma2[2]\n\ncorrdat_studylevel &lt;- \n  corrdat %&gt;%\n  group_by(studyid) %&gt;%\n  mutate(n_j = n()) %&gt;%\n  summarize_at(vars(effectsize, n_j, V_bar, college, binge_M, followup_M, males_M), mean\n  ) %&gt;%\n  mutate(\n    V_cond = (omega_sq_reg + (n_j - 1) * r * V_bar + V_bar) / n_j,\n    w_j = 1 / (tau_sq_reg + V_cond)\n  )\n\nNow I can fit a study-level meta-regression model. I use the weights argument to ensure that the meta-regression is estimated using the \\(w_j\\) weights:\n\nMR_study_fit &lt;- rma(yi = effectsize, vi = V_cond, \n                    mods = ~ college + males_M + binge_M + followup_M, \n                    weights = w_j, data = corrdat_studylevel)\nsummary(MR_study_fit)\n\n\nMixed-Effects Model (k = 39; tau^2 estimator: REML)\n\n  logLik  deviance       AIC       BIC      AICc   \n-13.0651   26.1303   38.1303   47.2884   41.2414   \n\ntau^2 (estimated amount of residual heterogeneity):     0.0297 (SE = 0.0264)\ntau (square root of estimated tau^2 value):             0.1723\nI^2 (residual heterogeneity / unaccounted variability): 26.89%\nH^2 (unaccounted variability / sampling variability):   1.37\nR^2 (amount of heterogeneity accounted for):            37.90%\n\nTest for Residual Heterogeneity:\nQE(df = 34) = 46.5050, p-val = 0.0748\n\nTest of Moderators (coefficients 2:5):\nQM(df = 4) = 13.0787, p-val = 0.0109\n\nModel Results:\n\n            estimate      se     zval    pval    ci.lb    ci.ub    \nintrcpt      -0.0361  0.3678  -0.0982  0.9218  -0.7571   0.6849    \ncollege       0.2660  0.1384   1.9215  0.0547  -0.0053   0.5373  . \nmales_M       0.0023  0.0048   0.4753  0.6346  -0.0072   0.0118    \nbinge_M       0.3441  0.1570   2.1927  0.0283   0.0365   0.6518  * \nfollowup_M   -0.0023  0.0011  -2.0379  0.0416  -0.0044  -0.0001  * \n\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nThe meta-regression coefficient estimates are essentially identical to those from the multi-variate meta-regression, although the between-study heterogeneity estimate differs slightly because it is based on maximizing the single-level model, conditional on an estimate of \\(\\omega^2\\)."
  },
  {
    "objectID": "posts/weighting-in-multivariate-meta-analysis/index.html#footnotes",
    "href": "posts/weighting-in-multivariate-meta-analysis/index.html#footnotes",
    "title": "Weighting in multivariate meta-analysis",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nNote that this model also encompasses the multi-level meta-analysis described by Konstantopoulos (2011) and Van den Noortgate, et al. (2013) as a special case, with \\(r_{hij} = 0\\) for all \\(h,i=1,...,n_j\\) and \\(j = 1,...,k\\).↩︎\nPerhaps that makes sense, if you’ve carefully selected the set of effect sizes for inclusion in your meta-analysis. However, it seems to me that it could sometimes lead to perverse results. Say that all studies but one include just a single effect size estimate, each using the absolute gold standard approach to assessing the outcome, but that one study took a “kitchen sink” approach and assessed the outcome a bunch of different ways, including the gold standard plus a bunch of junky scales. Inclusion of the junky scales will lead to within-study heterogeneity, which in turn will pull the overall average effect size towards this study—the one with all the junk! That seems less than ideal, and the sort of situation where it would be better to select from the study with multiple outcomes the single effect size estimate based on the outcome assessment that most closely aligns with the other studies.↩︎\nThings get even simpler if the model does not include within-study random effects, as I discussed in a previous post.↩︎\nHowever, this need not be the case—it’s possible that introducing between-study predictors could increase the estimate of between-study heterogeneity. Yes, that’s totally counter-intuitive. Multi-level models can be weird.↩︎"
  },
  {
    "objectID": "posts/variance-of-r-in-two-level-design/index.html",
    "href": "posts/variance-of-r-in-two-level-design/index.html",
    "title": "Sampling variance of Pearson r in a two-level design",
    "section": "",
    "text": "Consider Pearson’s correlation coefficient, \\(r\\), calculated from two variables \\(X\\) and \\(Y\\) with population correlation \\(\\rho\\). If one calculates \\(r\\) from a simple random sample of \\(N\\) observations, then its sampling variance will be approximately\n\\[\n\\text{Var}(r) \\approx \\frac{1}{N}\\left(1 - \\rho^2\\right)^2.\n\\]\nBut what if the observations are drawn from a multi-stage sample? If one uses the raw correlation between the observations (ignoring the multi-level structure), then the \\(r\\) will actually be a weighted average of within-cluster and between-cluster correlations (see Snijders & Bosker, 2012). Intuitively, I would expect that the sampling variance of the between-cluster correlation will be a function of the number of clusters (regardless of the number of observations per cluster), so the variance of \\(r\\) from a multi-stage sample would not necessarily be the same as that from a simple random sample. What is the sampling variance of \\(r\\) in this design?\nLet me be more precise here by formalizing the sampling process. Suppose that we have a sample with \\(m\\) clusters, \\(n_j\\) observations in cluster \\(j\\), and total sample size \\(N = \\sum_{j=1}^m n_j\\). Assume that\n\\[\n\\begin{aligned}\nX_{ij} &= \\mu_x + v^x_j + e^x_{ij} \\\\\nY_{ij} &= \\mu_y + v^y_j + e^y_{ij},\n\\end{aligned}\n\\]\nfor \\(i=1,...,n_j\\) and \\(j=1,...,m\\), where\n\\[\n\\begin{aligned}\n\\left[\\begin{array}{c} v^x_j \\\\ v^y_j \\end{array}\\right] &\\sim N\\left(\\left[\\begin{array}{c}0 \\\\ 0 \\end{array}\\right], \\left[\\begin{array}{cc}\\omega_x^2 & \\phi \\omega_x \\omega_y \\\\ \\phi \\omega_x \\omega_y & \\omega_y^2\\end{array}\\right]\\right) \\\\\n\\left[\\begin{array}{c} e^x_{ij} \\\\ e^y_{ij} \\end{array}\\right] &\\sim N\\left(\\left[\\begin{array}{c}0 \\\\ 0 \\end{array}\\right], \\left[\\begin{array}{cc}\\sigma_x^2 & \\rho \\sigma_x \\sigma_y \\\\ \\rho \\sigma_x \\sigma_y & \\sigma_y^2\\end{array}\\right]\\right)\n\\end{aligned}\n\\]\nand the error terms are mutually independent unless otherwise noted. The raw Pearson’s \\(r\\) is calculated using the total sums of squares and cross-products:\n\\[\nr = \\frac{SS_{xy}}{\\sqrt{SS_{xx} SS_{yy}}},\n\\]\nwhere\n\\[\n\\begin{aligned}\nSS_{xx} &= \\sum_{j=1}^m \\sum_{i=1}^{n_j} \\left(X_{ij} - \\bar{\\bar{x}}\\right)^2, \\qquad \\bar{\\bar{x}} = \\frac{1}{N} \\sum_{j=1}^m \\sum_{i=1}^{n_j} X_{ij} \\\\\nSS_{xy} &= \\sum_{j=1}^m \\sum_{i=1}^{n_j} \\left(Y_{ij} - \\bar{\\bar{y}}\\right)^2, \\qquad \\bar{\\bar{y}} = \\frac{1}{N} \\sum_{j=1}^m \\sum_{i=1}^{n_j} Y_{ij} \\\\\nSS_{xy} &= \\sum_{j=1}^m \\sum_{i=1}^{n_j} \\left(X_{ij} - \\bar{\\bar{x}}\\right) \\left(Y_{ij} - \\bar{\\bar{y}}\\right).\n\\end{aligned}\n\\]\n\nCommon correlation and ICC\nThe distribution of the total correlation seems to be pretty complicated. So far, I’ve been able to obtain the variance of \\(r\\) for a special case that makes some further, fairly restrictive assumptions. Specifically, assume that the correlation is constant across the two levels, so that \\(\\phi = \\rho\\), and that the intra-class correlation of \\(X\\) is the same as that of \\(Y\\). Let \\(k = \\omega_x^2 / \\sigma_x^2 = \\omega_y^2 / \\sigma_y^2\\) and \\(\\psi = k / (k + 1) = \\omega_x^2 / (\\omega_x^2 + \\sigma_x^2)\\). Then\n\\[\n\\text{Var}(r) \\approx \\frac{(1 - \\rho^2)^2}{\\tilde{N}},\n\\]\nwhere\n\\[\n\\tilde{N} = \\frac{N[g_1 k + 1]^2}{g_2 k^2 + 2 g_1 k + 1} \\approx \\frac{N}{1 + (g_2 - g_1^2)\\psi^2},\n\\]\nwith \\(\\displaystyle{g_1 = 1 - \\frac{1}{N^2}\\sum_{j=1}^m n_j^2}\\), and \\(\\displaystyle{g_2 = \\frac{1}{N}\\sum_{j=1}^m n_j^2 - \\frac{2}{N^2}\\sum_{j=1}^m n_j^3 + \\frac{1}{N^3} \\left(\\sum_{j=1}^m n_j^2 \\right)^2}\\).\nIf the clusters are all of equal size \\(n\\), then\n\\[\n\\tilde{N} = \\frac{nm[k(m - 1) / m + 1]^2}{k^2 n (m - 1)/m + 2 k (m - 1) / m + 1} \\approx \\frac{N}{1 + (n - 1) \\psi^2},\n\\]\nThe right-hand expression is a further approximation that will be very close to right so long as \\(m\\) is not too too small.\n\n\nZ-transformation\nUnder the (restrictive) assumptions of common correlation and equal ICCs, Fisher’s z transformation is variance-stabilizing (as it is under simple random sampling), so it seems reasonable to use\n\\[\n\\text{Var}\\left(z(r)\\right) \\approx \\frac{1}{\\tilde{N} - 3}.\n\\]\n\n\nDesign effect\nThe design effect (\\(DEF\\)) is the ratio of the actual sampling variance of \\(r\\) to the sampling variance in a simple random sample of the same size. For the special case that I’ve described,\n\\[\nDEF = \\frac{N}{\\tilde{N}} = 1 + (g_2 - g_1^2) \\psi^2,\n\\]\nor with equal cluster-sizes, \\(DEF = 1 + (n - 1)\\psi^2\\). These expressions make it clear that the design effect for the correlation is not equivalent to the well-known design effect for means or mean differences in cluster-randomized designs, which is \\(1 + (n - 1)\\psi\\). We need to take the square of the ICC here, which will make the design effect for \\(r\\) smaller than the design effect for a mean (or difference in means) based on the same sample.\n\n\nOther special cases\nThere are some further special cases that are not to hard to work out and could be useful as rough approximations at least. One is if the within-cluster correlation is zero \\((\\rho = 0)\\) and we’re interested in the between-cluster correlation \\(\\phi\\). Then the total correlation can be corrected for what is essentially measurement error using formulas from Hunter and Schmidt (2004). A further specialization is if \\(X\\) is a cluster-level measure, so that \\(\\sigma_x^2 = 0\\). I’ll consider these in a later post, perhaps.\n\n\n\n\n\n\nReuseCC BY 4.0CitationBibTeX citation:@online{pustejovsky2018,\n  author = {Pustejovsky, James E.},\n  title = {Sampling Variance of {Pearson} r in a Two-Level Design},\n  date = {2018-04-19},\n  url = {https://jepusto.com/posts/variance-of-r-in-two-level-design},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nPustejovsky, James E. 2018. “Sampling Variance of Pearson r in a\nTwo-Level Design.” April 19, 2018. https://jepusto.com/posts/variance-of-r-in-two-level-design."
  },
  {
    "objectID": "posts/using-response-ratios-paper/index.html",
    "href": "posts/using-response-ratios-paper/index.html",
    "title": "New paper: Using response ratios for meta-analyzing SCDs with behavioral outcomes",
    "section": "",
    "text": "I’m pleased to announce that my article “Using response ratios for meta-analyzing SCDs with behavioral outcomes” has been accepted at Journal of School Psychology. There’s a multitude of ways that you can access this work:\n\nFor the next 6 weeks or so, the published version of the article will be available at the journal website.\nThe pre-print will always remain available at PsyArXiv.\nSome supporting materials and replication code are available on the Open Science Framework.\n\nHere’s the abstract of the paper:\n\nMethods for meta-analyzing single-case designs (SCDs) are needed to inform evidence-based practice in clinical and school settingsand to draw broader and more defensible generalizations in areas where SCDs comprise a large part of the research base. The most widely used outcomesin single-case research are measures of behavior collected using systematic direct observation, which typically take the form of rates or proportions. For studies that use such measures, one simple and intuitive way to quantify effect sizes is in terms of proportionate change from baseline, using an effect size known as the log response ratio. This paper describes methods for estimating log response ratios and combining the estimates using meta-analysis. The methods are based on a simple model for comparing two phases, where the level of the outcome is stable within each phase and the repeated outcome measurements are independent. Although auto-correlation will lead to biased estimates of the sampling variance of the effect size, meta-analysis of response ratios can be conducted with robust variance estimation procedures that remain valid even when sampling variance estimates are biased. The methods are demonstrated using data from a recent meta-analysis on group contingency interventions for student problem behavior.\n\n\n\n\n\n\nReuseCC BY 4.0CitationBibTeX citation:@online{pustejovsky2018,\n  author = {Pustejovsky, James E.},\n  title = {New Paper: {Using} Response Ratios for Meta-Analyzing {SCDs}\n    with Behavioral Outcomes},\n  date = {2018-03-16},\n  url = {https://jepusto.com/posts/using-response-ratios-paper},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nPustejovsky, James E. 2018. “New Paper: Using Response Ratios for\nMeta-Analyzing SCDs with Behavioral Outcomes.” March 16, 2018. https://jepusto.com/posts/using-response-ratios-paper."
  },
  {
    "objectID": "posts/unlucky-randomization/index.html",
    "href": "posts/unlucky-randomization/index.html",
    "title": "Unlucky randomization",
    "section": "",
    "text": "A colleague asked me the other day:\n\nI wonder if you have any suggestions for what to do if random assignment results in big group differences on the pre-test scores of the main outcome measure? My default is just to shrug, use the pretest scores as a covariate and interpret with caution, but if there’s a suggestion you have I’d be most grateful for being pointed in the right direction. These are paid participants (otherwise I’d ask the student to collect more data), 25 and 28 in each group, randomization done by Qualtrics survey program, pretest differences are pronounced (p = .002) and NOT attributable to outliers.\n\nI would guess that many statisticians probably get a question along these lines on a pretty regular basis. So as not to repeat myself in the future, I’m posting my response here.\nThese sorts of things happen just by dumb luck sometimes, and the possibility of unlucky randomizations like this are one of the primary reasons to collect pre-test data and use it in the analysis. My main advice would therefore be to do just as you’ve described: control for the covariate just as you would otherwise. There are certainly other analyses you could run (such as using propensity scores to re-balance the data), but whatever advantages they offer might well be offset by the cost of 1) deviating from your initial protocol and 2) having to explain a less familiar and more complicated analysis.\nIf I were analyzing these data, I would do the following:\n\nCheck that the randomization software was actually working correctly, and that the unbalanced data wasn’t the result of a glitch in Qualtrics or something like that.\nLook at histograms of the pretest scores for each group to get a sense of how big the difference in the distributions is.\nIf there are other baseline variables, check to see whether there are big group differences on any of those as well.\nEnsure that the write-up characterizes the magnitude of observed differences on the pre-test and any other baseline variables (i.e., report an effect size like the standardized mean difference, in addition to the p-value).\nLarger baseline differences tend to make the results more sensitive to how the data are analyzed. As a result, I would be extra thorough in checking the required assumptions for an analysis of covariance—especially linearity and homogeneity of slopes—and would examine whether the treatment effect estimates are sensitive to including a pretest-by-treatment interaction.\nFor future studies, investigate whether it would be possible to block-randomize (e.g., block by low/middle/high scores on the pretest) in order to insure against the possibility of getting big baseline differences.\n\n\n\n\n\n\nReuseCC BY 4.0CitationBibTeX citation:@online{pustejovsky2016,\n  author = {Pustejovsky, James E.},\n  title = {Unlucky Randomization},\n  date = {2016-05-11},\n  url = {https://jepusto.com/posts/unlucky-randomization},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nPustejovsky, James E. 2016. “Unlucky Randomization.” May\n11, 2016. https://jepusto.com/posts/unlucky-randomization."
  },
  {
    "objectID": "posts/SPED-Pro-Sem-again/index.html",
    "href": "posts/SPED-Pro-Sem-again/index.html",
    "title": "Special Education Pro-Sem",
    "section": "",
    "text": "Yesterday evening I again had the pleasure of visiting Dr. Barnes’ pro seminar for first year students in Special Education, where I shared some of my work on research synthesis and meta-analysis of single-case research. Here are the slides from my presentation.\n\n\n\n\n\nReuseCC BY 4.0CitationBibTeX citation:@online{pustejovsky2015,\n  author = {Pustejovsky, James E.},\n  title = {Special {Education} {Pro-Sem}},\n  date = {2015-11-24},\n  url = {https://jepusto.com/posts/SPED-Pro-Sem-again},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nPustejovsky, James E. 2015. “Special Education Pro-Sem.”\nNovember 24, 2015. https://jepusto.com/posts/SPED-Pro-Sem-again."
  },
  {
    "objectID": "posts/Sometimes-aggregating-effect-sizes-is-fine/index.html",
    "href": "posts/Sometimes-aggregating-effect-sizes-is-fine/index.html",
    "title": "Sometimes, aggregating effect sizes is fine",
    "section": "",
    "text": "In meta-analyses of psychology, education, and other social science research, it is very common that some of the included studies report more than one relevant effect size. For example, in a meta-analysis of intervention effects on reading outcomes, some studies may have used multiple measures of reading outcomes (each of which meets inclusion criteria), or may have measured outcomes at multiple follow-up times; some studies might have also investigated more than one version of an intervention, and it might be of interest to include effect sizes comparing each version to the no-intervention control condition; and it’s even possible that some studies may have all of these features, potentially contributing lots of effect size estimates.\nThese situations create a technical challenge for conducting a meta-analysis. Because effect size estimates from the same study are correlated, it’s not usually reasonable to use methods that are premised on each effect size estimate being independent (i.e., univariate methods). Instead, the analyst needs to apply methods that take into account the dependencies among estimates coming from the same study. It used to be common to use ad hoc approaches for handling dependence, such as averaging the estimates together or selecting one estimate per study and then using univariate methods (cf. Becker, 2000). More sophisticated, multivariate meta-analysis (MVMA) models that directly account for correlations among the effect size estimates had been developed (Kalaian & Raudenbush, 1996) but were challenging to implement and so rarely used (at least, that’s my impression). More recently, techniques such as multi-level meta-analysis [MLMA; Van den Noortgate et al. (2013); Van den Noortgate et al. (2015)] and robust variance estimation [RVE; Hedges et al. (2010)] have emerged, which account for dependencies while using all available effect size estimates and still being feasible to implement. These new techniques of MLMA and RVE are starting to be more widely adopted in practice, and it is not implausible that they will become the standard approach in psychological and educational meta-analysis within a few years.\nGiven the extent of interest in MLMA and RVE, one might wonder: are the older ad hoc approaches ever reasonable or appropriate? I think that some are, under certain circumstances. In this post I’ll highlight one such circumstance, where aggregating effect size estimates is not only reasonable but leads to exactly the same results as a multivariate model. This occurs when two conditions are met:\nIn short, if all we care about is understanding between-study variation in effect sizes, then it is fine to aggregate them up to the study level."
  },
  {
    "objectID": "posts/Sometimes-aggregating-effect-sizes-is-fine/index.html#full-likelihood",
    "href": "posts/Sometimes-aggregating-effect-sizes-is-fine/index.html#full-likelihood",
    "title": "Sometimes, aggregating effect sizes is fine",
    "section": "Full likelihood",
    "text": "Full likelihood\nFor the univariate model, the log-likelihood contribution of study \\(k\\): \\[\nl^{U}_k\\left(\\boldsymbol\\beta, \\tau^2\\right) = -\\frac{1}{2} \\log\\left(\\tau^2 + V_k\\right) - \\frac{1}{2} \\frac{\\left(\\bar{T}_k - \\mathbf{x}_k \\boldsymbol\\beta\\right)^2}{\\tau^2 + V_k}.\n\\] For the multivariate model, the log-likelihood contribution of study \\(k\\) is: \\[\nl^{MV}_k\\left(\\boldsymbol\\beta, \\tau^2\\right) = -\\frac{1}{2} A -\\frac{1}{2} B\n\\] where \\[\nA = \\log\\left|\\tau^2\\mathbf{1}_k\\mathbf{1}_k' + \\mathbf{S}_k\\right|\n\\] and \\[\nB = \\left(\\mathbf{T}_k - \\mathbf{x}_k \\boldsymbol\\beta \\mathbf{1}_k\\right)' \\left(\\tau^2\\mathbf{1}_k\\mathbf{1}_k' + \\mathbf{S}_k\\right)^{-1} \\left(\\mathbf{T}_k - \\mathbf{x}_k \\boldsymbol\\beta \\mathbf{1}_k\\right).\n\\] The term \\(A\\) can be rearranged as \\[\nA = \\log\\left|\\left(\\tau^2\\mathbf{1}_k\\mathbf{1}_k'\\mathbf{S}_k^{-1} + \\mathbf{I}_k\\right) \\mathbf{S}_k\\right|\n\\] where \\(\\mathbf{I}_k\\) is a \\(J_k \\times J_k\\) identity matrix. One of the properties of determinants is that the determinant of a product of two matrices is equal to the product of the determinants. Another is that, for two vectors \\(\\mathbf{u}\\) and \\(\\mathbf{v}\\), \\(\\left|\\mathbf{I} + \\mathbf{u}\\mathbf{v}'\\right| = 1 + \\mathbf{v}'\\mathbf{u}\\). Applying both of these properties, it follows that \\[\n\\begin{aligned}\nA &= \\log\\left|\\left(\\tau^2\\mathbf{1}_k\\mathbf{1}_k'\\mathbf{S}_k^{-1} + \\mathbf{I}_k\\right) \\mathbf{S}_k\\right| \\\\\n&= \\log \\left( \\left|\\mathbf{I}_k + \\tau^2\\mathbf{1}_k\\mathbf{1}_k'\\mathbf{S}_k^{-1}\\right| \\left|\\mathbf{S}_k\\right|\\right) \\\\\n&= \\log \\left(1 + \\frac{\\tau^2}{V_k}\\right) + \\log \\left|\\mathbf{S}_k\\right| \\\\\n&= \\log(\\tau^2 + V_k) - \\log(V_k) + \\log \\left|\\mathbf{S}_k\\right|.\n\\end{aligned}\n\\] The \\(B\\) term takes a little more work. From the Sherman-Morrison identity, we have that: \\[\n\\left(\\tau^2\\mathbf{1}_k\\mathbf{1}_k' + \\mathbf{S}_k\\right)^{-1} = \\mathbf{S}_k^{-1} - \\mathbf{S}_k^{-1} \\mathbf{1}_k \\left(\\frac{1}{\\tau^2} + \\frac{1}{V_k}\\right)^{-1} \\mathbf{1}_k'\\mathbf{S}_k^{-1},\n(\\#eq:Sherman)\n\\] by which it follows that \\[\n\\mathbf{1}_k'\\left(\\tau^2\\mathbf{1}_k\\mathbf{1}_k' + \\mathbf{S}_k\\right)^{-1}\\mathbf{1}_k = \\frac{1}{\\tau^2 + V_k}.\n(\\#eq:inversevariance)\n\\] Now, rearrange the \\(B\\) term to get \\[\n\\begin{aligned}\nB &= \\left[\\mathbf{T}_k - \\bar{T}_k \\mathbf{1}_k + \\left(\\bar{T}_k - \\mathbf{x}_k \\boldsymbol\\beta\\right) \\mathbf{1}_k\\right]' \\left(\\tau^2\\mathbf{1}_k\\mathbf{1}_k' + \\mathbf{S}_k\\right)^{-1} \\left[\\mathbf{T}_k - \\bar{T}_k \\mathbf{1}_k + \\left(\\bar{T}_k - \\mathbf{x}_k \\boldsymbol\\beta\\right) \\mathbf{1}_k\\right] \\\\\n&= B_1 + 2 B_2 + B_3\n\\end{aligned}\n\\] where \\[\n\\begin{aligned}\nB_1 &= \\left(\\mathbf{T}_k - \\bar{T}_k \\mathbf{1}_k\\right)' \\left(\\tau^2\\mathbf{1}_k\\mathbf{1}_k' + \\mathbf{S}_k\\right)^{-1} \\left(\\mathbf{T}_k - \\bar{T}_k \\mathbf{1}_k\\right) \\\\\nB_2 &= \\left(\\mathbf{T}_k - \\bar{T}_k \\mathbf{1}_k\\right)' \\left(\\tau^2\\mathbf{1}_k\\mathbf{1}_k' + \\mathbf{S}_k\\right)^{-1} \\mathbf{1}_k \\left(\\bar{T}_k - \\mathbf{x}_k \\boldsymbol\\beta\\right) \\\\\nB_3 &= \\left(\\bar{T}_k - \\mathbf{x}_k \\boldsymbol\\beta\\right) \\mathbf{1}_k' \\left(\\tau^2\\mathbf{1}_k\\mathbf{1}_k' + \\mathbf{S}_k\\right)^{-1} \\mathbf{1}_k \\left(\\bar{T}_k - \\mathbf{x}_k \\boldsymbol\\beta\\right)\n\\end{aligned}\n\\] Applying @ref(eq:Sherman) to \\(B_1\\), \\[\n\\begin{aligned}\nB_1 &= \\left(\\mathbf{T}_k - \\bar{T}_k \\mathbf{1}_k\\right)' \\left[\\mathbf{S}_k^{-1} - \\mathbf{S}_k^{-1} \\mathbf{1}_k \\left(\\frac{1}{\\tau^2} + \\frac{1}{V_k}\\right)^{-1} \\mathbf{1}_k'\\mathbf{S}_k^{-1}\\right] \\left(\\mathbf{T}_k - \\bar{T}_k \\mathbf{1}_k\\right) \\\\\n&= \\left(\\mathbf{T}_k - \\bar{T}_k \\mathbf{1}_k\\right)' \\mathbf{S}_k^{-1}\\left(\\mathbf{T}_k - \\bar{T}_k \\mathbf{1}_k\\right) - \\left(\\mathbf{T}_k - \\bar{T}_k \\mathbf{1}_k\\right)' \\mathbf{S}_k^{-1} \\mathbf{1}_k \\left(\\frac{1}{\\tau^2} + \\frac{1}{V_k}\\right)^{-1} \\mathbf{1}_k'\\mathbf{S}_k^{-1}\\left(\\mathbf{T}_k - \\bar{T}_k \\mathbf{1}_k\\right) \\\\\n&= \\left(\\mathbf{T}_k - \\bar{T}_k \\mathbf{1}_k\\right)' \\mathbf{S}_k^{-1}\\left(\\mathbf{T}_k - \\bar{T}_k \\mathbf{1}_k\\right).\n\\end{aligned}\n\\] The second term drops out because \\(\\left(\\mathbf{T}_k - \\bar{T}_k \\mathbf{1}_k\\right)' \\mathbf{S}_k^{-1} \\mathbf{1}_k = \\bar{T}_k / V_k - \\bar{T}_k / V_k = 0\\). Along similar lines, \\[\n\\begin{aligned}\nB_2 &= \\left(\\mathbf{T}_k - \\bar{T}_k \\mathbf{1}_k\\right)' \\left[\\mathbf{S}_k^{-1} - \\mathbf{S}_k^{-1} \\mathbf{1}_k \\left(\\frac{1}{\\tau^2} + \\frac{1}{V_k}\\right)^{-1} \\mathbf{1}_k'\\mathbf{S}_k^{-1}\\right] \\mathbf{1}_k \\left(\\bar{T}_k - \\mathbf{x}_k \\boldsymbol\\beta\\right) \\\\\n&= \\left(\\mathbf{T}_k - \\bar{T}_k \\mathbf{1}_k\\right)' \\mathbf{S}_k^{-1}\\mathbf{1}_k \\left(\\bar{T}_k - \\mathbf{x}_k \\boldsymbol\\beta\\right) - \\left(\\mathbf{T}_k - \\bar{T}_k \\mathbf{1}_k\\right)' \\mathbf{S}_k^{-1} \\mathbf{1}_k \\left(\\frac{1}{\\tau^2} + \\frac{1}{V_k}\\right)^{-1} \\mathbf{1}_k'\\mathbf{S}_k^{-1}\\mathbf{1}_k \\left(\\bar{T}_k - \\mathbf{x}_k \\boldsymbol\\beta\\right) \\\\\n&= 0.\n\\end{aligned}\n\\] Finally, the third term simplifies using @ref(eq:inversevariance): \\[\nB_3 = \\frac{\\left(\\bar{T}_k - \\mathbf{x}_k \\boldsymbol\\beta\\right)^2}{\\tau^2 + V_k}.\n\\] Thus, the full \\(B\\) term reduces to \\[\nB = \\left(\\mathbf{T}_k - \\bar{T}_k \\mathbf{1}_k\\right)' \\mathbf{S}_k^{-1}\\left(\\mathbf{T}_k - \\bar{T}_k \\mathbf{1}_k\\right) + \\frac{\\left(\\bar{T}_k - \\mathbf{x}_k \\boldsymbol\\beta\\right)^2}{\\tau^2 + V_k}\n\\] and the multivariate log likelihood contribution is \\[\n\\begin{aligned}\nl^{MV}_k\\left(\\boldsymbol\\beta, \\tau^2\\right) &= -\\frac{1}{2} \\log(\\tau^2 + V_k) + \\frac{1}{2} \\log(V_k) - \\frac{1}{2}\\log \\left|\\mathbf{S}_k\\right| - \\frac{1}{2} \\left(\\mathbf{T}_k - \\bar{T}_k \\mathbf{1}_k\\right)' \\mathbf{S}_k^{-1}\\left(\\mathbf{T}_k - \\bar{T}_k \\mathbf{1}_k\\right) -\\frac{1}{2} \\frac{\\left(\\bar{T}_k - \\mathbf{x}_k \\boldsymbol\\beta\\right)^2}{\\tau^2 + V_k} \\\\\n&= l^U_k\\left(\\boldsymbol\\beta, \\tau^2\\right) + \\frac{1}{2} \\log(V_k) - \\frac{1}{2}\\log \\left|\\mathbf{S}_k\\right| - \\frac{1}{2} \\left(\\mathbf{T}_k - \\bar{T}_k \\mathbf{1}_k\\right)' \\mathbf{S}_k^{-1}\\left(\\mathbf{T}_k - \\bar{T}_k \\mathbf{1}_k\\right).\n\\end{aligned}\n\\] The last three terms depend on the data (\\(\\mathbf{T}_k\\) and \\(\\mathbf{S}_k\\)) but not on the parameters \\(\\boldsymbol\\beta\\) or \\(\\tau^2\\). Therefore, the univariate and multivariate likelihoods will be maximized at the same parameter values, i.e., the FML estimators are identical."
  },
  {
    "objectID": "posts/Sometimes-aggregating-effect-sizes-is-fine/index.html#restricted-likelihood",
    "href": "posts/Sometimes-aggregating-effect-sizes-is-fine/index.html#restricted-likelihood",
    "title": "Sometimes, aggregating effect sizes is fine",
    "section": "Restricted likelihood",
    "text": "Restricted likelihood\nIn practice, it is more common to use RML estimation rather than FML. The RML estimators maximize a different objective function that includes the full likelihood, plus an additional term. The RML objective function for the univariate model is \\[\n\\sum_{k=1}^K l^U_k(\\boldsymbol\\beta, \\tau^2) - \\frac{1}{2} R^U(\\tau^2)\n\\] where \\[\nR^U(\\tau^2) = \\log \\left|\\sum_{k=1}^k\\frac{\\mathbf{x}_k' \\mathbf{x}_k}{\\tau^2 + V_k} \\right|.\n\\] For the multivariate model, the RML objective is \\[\n\\sum_{k=1}^K l^{MV}_k(\\boldsymbol\\beta, \\tau^2) - \\frac{1}{2} R^{MV}(\\tau^2).\n\\] where \\[\n\\begin{aligned}\nR^{MV}(\\tau^2) &= \\log \\left|\\sum_{k=1}^k \\mathbf{x}_k'\\mathbf{1}_k'\\left(\\tau^2\\mathbf{1}_k\\mathbf{1}_k' + \\mathbf{S}_k\\right)^{-1}\\mathbf{1}_k \\mathbf{x}_k \\right|\\\\\n&= \\log \\left|\\sum_{k=1}^k\\frac{\\mathbf{x}_k' \\mathbf{x}_k}{\\tau^2 + V_k} \\right| \\\\\n&= R^U(\\tau^2)\n\\end{aligned}\n\\] because of @ref(eq:inversevariance). Thus, the univariate and multivariate models also have the same RML estimators."
  },
  {
    "objectID": "posts/Sometimes-aggregating-effect-sizes-is-fine/index.html#footnotes",
    "href": "posts/Sometimes-aggregating-effect-sizes-is-fine/index.html#footnotes",
    "title": "Sometimes, aggregating effect sizes is fine",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nA common special case is that the sampling variances for effect sizes within a given study \\(k\\) are all equal, so that \\(S_{ik} = s_{jk} = S_k\\) for \\(i,j = 1,...,J_ik\\) and \\(k = 1,...,K\\). We might further posit that there is a constant sampling correlation between every pair of effect sizes within a given study, so that \\(\\rho_{ijk} = \\rho_k\\) for \\(i,j = 1,...,J_ik\\) and \\(k = 1,...,K\\). If both of these conditions hold, then the inverse-variance weighted average effect size simplifies to the arithmetic average \\[\n\\bar{T}_k = \\frac{1}{J_k} \\sum_{j=1}^{J_k} T_{jk}\n\\] with sampling variance \\[\nV_k = \\frac{(J_k - 1)\\rho_k + 1}{J} \\times S_k^2\n\\] (cf. Borenstein et al., 2009, Eq. (24.6), p. 230).↩︎\nThe same thing holds if we use FML rather than RML estimation—try it for yourself and see!↩︎\nAs RVE and MLMA become more wide-spread, I could imagine it happening that a meta-analyst who uses aggregation and a univariate model might get push-back from a reviewer, who uncritically recommends using a “more advanced” method to handle dependence. The results in this post provide a way for the meta-analyst to establish that doing so would be unnecessary.↩︎\nHere’s verification with the computational example from above:\n\n# multivariate CR2\ncoef_test(MV_fit, vcov = \"CR2\")\n\n   Coef. Estimate      SE t-stat d.f. (Satt) p-val (Satt) Sig.\n intrcpt  0.64656 0.17647   3.66        11.5      0.00345   **\n college  0.37027 0.18648   1.99        11.9      0.07053    .\n   males -0.00763 0.00287  -2.66        14.5      0.01826    *\n\n# univariate HC2\ncoef_test(uni_fit, vcov = \"CR2\", cluster = corrdat_agg$studyid)\n\n   Coef. Estimate      SE t-stat d.f. (Satt) p-val (Satt) Sig.\n intrcpt  0.64656 0.17622   3.67        11.5      0.00342   **\n college  0.37027 0.18597   1.99        11.9      0.06985    .\n   males -0.00763 0.00287  -2.66        14.5      0.01808    *\n\n\n↩︎"
  },
  {
    "objectID": "posts/Siren-song-of-significance/index.html",
    "href": "posts/Siren-song-of-significance/index.html",
    "title": "The siren song of significance",
    "section": "",
    "text": "How is statistical analysis like the Odyssey? Here’s an analogy that I used in my research methods course last semester to explain the purpose of study pre-registration. If you’ve ever read the Odyssey, you’ll recall the story of the Sirens, the enchanting lady-monsters whose singing lures to certain death any sailor who hears them. (See Wikipedia for crib notes.) On the advise of his witch-friend Circe, Odysseus pulls a stunt so that he can hear the song of the Sirens while still making it safely past. He instructs his crew to plug their ears with beeswax and then lash him to the mast of his boat. As they sail past the Sirens, Odysseus hears the beautiful voices come-hithering and begs his men to free him, but they tie him up tighter until they are all safely out of ear-shot.\nI think this is a good analogy for the benefits of pre-registering your experiments. Statistical significance testing is an alluring thing. It provides us, as data-analysts, with a way of drawing a definitive conclusion about whatever phenomenon we’re studying—“This effect is non-zero!”—and thus to write compelling articles and get published and get tenure and so on. But we also now know that statistical significance testing is easily abused. Using flexible data analysis procedures, it is easy to obtain statistically significant results from totally meaningless data. And statistical significance is so alluring, why should any scholar believe that you haven’t hacked your way to get there? Is there any way to conduct hypothesis tests and actually believe (and convince others) that you’ve ruled out a null at the end of it?\nThat’s where study pre-registration comes in. Pre-registration involves creating a public record of the exact plans you intend to follow when collecting and analyzing data, in advance of conducting the study. It is like tying your arms and legs to the mast of your ship as you sail through the straights of data collection and analysis. No matter how tempting it is to control for a couple of other covariates…no matter how much cleaner that log-transformation looks…your pre-registered protocol keeps you tied down, preventing you from throwing yourself overboard into the sea of questionable research practices. And as you come out the other side, you can actually put stock in your findings, having heard the siren song of statistical significance and lived to tell the tale.\n\n\n\n\n\nReuseCC BY 4.0CitationBibTeX citation:@online{pustejovsky2017,\n  author = {Pustejovsky, James E.},\n  title = {The Siren Song of Significance},\n  date = {2017-06-19},\n  url = {https://jepusto.com/posts/Siren-song-of-significance},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nPustejovsky, James E. 2017. “The Siren Song of\nSignificance.” June 19, 2017. https://jepusto.com/posts/Siren-song-of-significance."
  },
  {
    "objectID": "posts/Simulating-correlated-SMDs/index.html",
    "href": "posts/Simulating-correlated-SMDs/index.html",
    "title": "Simulating correlated standardized mean differences for meta-analysis",
    "section": "",
    "text": "As I’ve discussed in previous posts, meta-analyses in psychology, education, and other areas often include studies that contribute multiple, statistically dependent effect size estimates. I’m interested in methods for meta-analyzing and meta-regressing effect sizes from data structures like this, and studying this sort of thing often entails conducting Monte Carlo simulations. Monte Carlo simulations involve generating artificial data—in this case, a set of studies, each of which has one or more dependent effect size estimates—that follows a certain distributional model, applying different analytic methods to the artificial data, and then repeating the process a bunch of times. Because we know the true parameters that govern the data-generating process, we can evaluate the performance of the analytic methods in terms of bias, accuracy, hypothesis test calibration and power, confidence interval coverage, and the like.\nIn this post, I’ll discuss two alternative methods to simulate meta-analytic datasets that include studies with multiple, dependent effect size estimates: simulating individual participant-level data or simulating summary statistics. I’ll focus on the case of the standardized mean difference (SMD) because it is so common in meta-analyses of intervention studies. For simplicity, I’ll assume that the effect sizes all come from simple, two-group comparisons (without any covariate adjustment or anything like that) and that the individual observations are multi-variate normally distributed within each group. Our goal will be to simulate a set of \\(K\\) studies, where study \\(k\\) is based on measuring \\(J_k\\) outcomes on a sample of \\(N_k\\) participants, all for \\(k = 1,...,K\\). Let \\(\\boldsymbol\\delta_k = (\\delta_{1k} \\cdots \\delta_{J_k k})'\\) be the \\(J_k \\times 1\\) vector of true standardized mean differences for study \\(k\\). I’ll assume that we know these true effect size parameters for all \\(K\\) studies, so that I can avoid committing to any particular form of random effects model."
  },
  {
    "objectID": "posts/Simulating-correlated-SMDs/index.html#exercises",
    "href": "posts/Simulating-correlated-SMDs/index.html#exercises",
    "title": "Simulating correlated standardized mean differences for meta-analysis",
    "section": "Exercises",
    "text": "Exercises\nThe function above is serviceable but quite basic. I can think of several additional features that one might like to have for use in research simulations, but I’m feeling both cheeky and lazy at the moment, so I’ll leave them for you, dear reader. Here are some suggested exercises:\n\nAdd an argument to the function, Hedges_g = TRUE, which controls where the simulated effect size is Hedges’ \\(g\\) or Cohen’s \\(d\\). If it is Hedges’ g, make sure that the standard error is corrected too.\nAdd an argument to the function, p_val = TRUE, which allows the user to control whether or not to return \\(p\\)-values from the test of mean differences for each outcome. Note that the p-values should be for a test of the raw mean differences between groups, rather than a test of the effect size \\(\\delta_{jk} = 0\\).\nAdd an argument to the function, corr_mat = FALSE, which controls whether the function returns just the simulated effect sizes and SEs or both the simulated effect sizes and the full sampling variance-covariance matrix of the effect sizes. See here for the relevant formulas."
  },
  {
    "objectID": "posts/Simulating-correlated-SMDs/index.html#exercises-1",
    "href": "posts/Simulating-correlated-SMDs/index.html#exercises-1",
    "title": "Simulating correlated standardized mean differences for meta-analysis",
    "section": "Exercises",
    "text": "Exercises\nOnce again, I’ll leave it to you, dear reader, to do the fun programming bits:\n\nCreate a modified version of the function r_SMDs_raw that simulates summary statistics instead of raw data (Call it r_SMDs_stats).\nUse the microbenchmark package (or your preferred benchmarking tool) to compare the computational efficiency of both versions of the function.\nCheck your work! Verify that both versions of the function generate the same distributions if the same parameters are used as input."
  },
  {
    "objectID": "posts/Simulating-correlated-SMDs/index.html#exercises-2",
    "href": "posts/Simulating-correlated-SMDs/index.html#exercises-2",
    "title": "Simulating correlated standardized mean differences for meta-analysis",
    "section": "Exercises",
    "text": "Exercises\n\nModify r_meta so that it uses r_SMDs_stats.\nAdd options to r_meta for Hedges_g, p_val = TRUE, and corr_mat = FALSE and ensure that these get passed along to the r_SMDs function.\nOne way to check that the r_meta function is working properly is to generate a very large meta-analytic dataset, then to verify that the generated distributions align with expectations. Here’s a very large meta-analytic dataset:\n::: {.cell}\nmeta_data &lt;- \n  r_meta(100000, mu = 0.2, tau = 0.05, \n         J_mean = 5, N_mean = 40, \n         rho = 0.6, nu = 39)\n:::\nCompare the distribution of the simulated dataset against what you would expect to get based on the input parameters.\nModify the r_meta function so that \\(J_k\\) and \\(N_k\\) are correlated, according to \\[\n\\begin{align}\nJ_k &\\sim 2 + Poisson(\\mu_J - 2) \\\\\nN_k &\\sim 20 + 2 \\times Poisson\\left(\\frac{1}{2}(\\mu_N - 20) + \\alpha (J_k - \\mu_J) \\right)\n\\end{align}\n\\] for user-specified values of \\(\\mu_J\\) (the average number of outcomes per study), \\(\\mu_N\\) (the average total sample size per study), and \\(\\alpha\\), which controls the degree of dependence between \\(J_k\\) and \\(N_k\\)."
  },
  {
    "objectID": "posts/Simulating-correlated-SMDs/index.html#a-challenge",
    "href": "posts/Simulating-correlated-SMDs/index.html#a-challenge",
    "title": "Simulating correlated standardized mean differences for meta-analysis",
    "section": "A challenge",
    "text": "A challenge\nThe meta-analytic model that we’re using here is quite simple—simplistic, even—and for some simulation studies, something more complex might be needed. For example, we might need to generate data from a model that includes within-study random effects, as in: \\[\n\\delta_{jk} = \\mu + u_k + v_{jk}, \\quad \\text{where}\\quad u_k \\sim N(0, \\tau^2), \\quad v_{jk} \\sim N(0, \\omega^2).\n\\] Even more complex would be to simulate from a multi-level meta-regression model \\[\n\\delta_{jk} = \\mathbf{x}_{jk} \\boldsymbol\\beta + u_k + v_{jk}, \\quad \\text{where}\\quad u_k \\sim N(0, \\tau^2), \\quad v_{jk} \\sim N(0, \\omega^2),\n\\] where \\(\\mathbf{x}_{jk}\\) is a \\(1 \\times p\\) row-vector of covariates describing outcome \\(j\\) in study \\(k\\) and \\(\\boldsymbol\\beta\\) is a \\(p \\times 1\\) vector of meta-regression coefficients. In past work, I’ve done this by writing a data-generating function that takes a fixed design matrix \\(\\mathbf{X} = \\left(\\mathbf{x}_{11}' \\cdots \\mathbf{x}_{J_K K}'\\right)'\\) as an input argument, along with \\(\\boldsymbol\\beta\\). The design matrix would also include an identifier for each unique study. There are surely better (simpler, easier to follow) ways to implement the multi-level meta-regression model. I’ll once again leave it to you to work out an approach."
  },
  {
    "objectID": "posts/scdhlm-tutorial/index.html",
    "href": "posts/scdhlm-tutorial/index.html",
    "title": "New tutorial paper on BC-SMD effect sizes",
    "section": "",
    "text": "I’m pleased to announce that the Campbell Collaboration has just published a new discussion paper that I wrote with my colleagues Jeff Valentine and Emily Tanner-Smith about between-case standardized mean difference effect sizes for single-case designs. The paper provides a relatively non-technical introduction to BC-SMD effect sizes and a tutorial on how to use the scdhlm web-app for calculating estimates of the BC-SMD for user-provided data. If you have any questions or feedback about the app, please feel free to contact me!\n\n\n\n\n\nReuseCC BY 4.0CitationBibTeX citation:@online{pustejovsky2016,\n  author = {Pustejovsky, James E.},\n  title = {New Tutorial Paper on {BC-SMD} Effect Sizes},\n  date = {2016-12-19},\n  url = {https://jepusto.com/posts/scdhlm-tutorial},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nPustejovsky, James E. 2016. “New Tutorial Paper on BC-SMD Effect\nSizes.” December 19, 2016. https://jepusto.com/posts/scdhlm-tutorial."
  },
  {
    "objectID": "posts/Robust-meta-analysis-2/index.html",
    "href": "posts/Robust-meta-analysis-2/index.html",
    "title": "Another meta-sandwich",
    "section": "",
    "text": "In a previous post, I provided some code to do robust variance estimation with metafor and sandwich. Here’s another example, replicating some more of the calculations from Tanner-Smith & Tipton (2013). (See here for the complete code.)\nAs a starting point, here are the results produced by the robumeta package:\n\nlibrary(grid)\nlibrary(robumeta)\n\ndata(corrdat)\nrho &lt;- 0.8\n\nHTJ &lt;- robu(effectsize ~ males + college + binge,\n            data = corrdat, \n            modelweights = \"CORR\", rho = rho,\n            studynum = studyid,\n            var.eff.size = var, small = FALSE)\nHTJ\n\nRVE: Correlated Effects Model  \n\nModel: effectsize ~ males + college + binge \n\nNumber of studies = 39 \nNumber of outcomes = 172 (min = 1 , mean = 4.41 , median = 4 , max = 18 )\nRho = 0.8 \nI.sq = 75.08352 \nTau.sq = 0.1557714 \n\n               Estimate  StdErr t-value dfs P(|t|&gt;) 95% CI.L 95% CI.U Sig\n1 X.Intercept.  0.31936 0.27784   1.149  35   0.258  -0.2447  0.88340    \n2        males -0.00331 0.00376  -0.882  35   0.384  -0.0109  0.00431    \n3      college  0.41226 0.18685   2.206  35   0.034   0.0329  0.79159  **\n4        binge  0.13774 0.12586   1.094  35   0.281  -0.1178  0.39326    \n---\nSignif. codes: &lt; .01 *** &lt; .05 ** &lt; .10 *\n---\n\n\nTo exactly re-produce the results with metafor, I’ll need to use the weights proposed by HTJ. In their approach to the correlated effects case, effect size \\(i\\) from study \\(j\\) receives weight equal to \\(\\left[\\left(v_{\\cdot j} + \\hat\\tau^2\\right)(1 + (k_j - 1) \\rho)\\right]^{-1}\\), where \\(v_{\\cdot j}\\) is the average sampling variance of the effect sizes from study \\(j\\), \\(\\hat\\tau^2\\) is an estimate of the between-study variance, \\(k_j\\) is the number of correlated effects in study \\(j\\), and \\(\\rho\\) is a user-specified value of the intra-study correlation. However, it appears that robumeta actually uses a slightly different set weights, which are equivalent to taking \\(\\rho = 1\\). I calculate the latter weights, fit the model in metafor, and output the robust standard errors and \\(t\\)-tests:\n\ndevtools::source_gist(id = \"11144005\", filename = \"metafor-sandwich.R\")\n\ncorrdat &lt;- within(corrdat, {\n  var_mean &lt;- tapply(var, studyid, mean)[studyid]\n  k &lt;- table(studyid)[studyid]\n  var_HTJ &lt;- as.numeric(k * (var_mean + as.numeric(HTJ$mod_info$tau.sq)))\n})\n\nmeta1 &lt;- rma.mv(effectsize ~ males + college + binge, \n                V = var_HTJ, \n                data = corrdat, method = \"FE\")\nmeta1$cluster &lt;- corrdat$studyid\nRobustResults(meta1)\n\n\nt test of coefficients:\n\n          Estimate Std. Error t value Pr(&gt;|t|)  \nintrcpt  0.3193586  0.2778360  1.1494  0.25816  \nmales   -0.0033143  0.0037573 -0.8821  0.38374  \ncollege  0.4122631  0.1868489  2.2064  0.03401 *\nbinge    0.1377393  0.1258637  1.0944  0.28127  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nOne could specify a similar (though not exactly identical model) in metafor as follows. In the HTJ approach, \\(\\rho\\) represents the total correlation induced by both the within-study sampling error and intra-study correlation in true effects. In contrast, the metafor approach would take \\(\\rho\\) to be correlation due to within-study sampling error alone. I’ll first need to create a block-diagonal covariance matrix given a user-specified value of \\(\\rho\\).\n\nlibrary(Matrix)\nequicorr &lt;- function(x, rho) {\n  corr &lt;- rho + (1 - rho) * diag(nrow = length(x))\n  tcrossprod(x) * corr \n} \ncovMat &lt;- as.matrix(bdiag(with(corrdat, tapply(var_mean, studyid, equicorr, rho = 0.8, simplify = FALSE))))\n\nPassing this block-diagonal covariance matrix to rma.mv, I now estimate the model\n\\[T_{ij} = \\mathbf{X}_{ij} \\beta + \\nu_i + e_{ij},\\]\nwhere \\(Var(\\nu_i) = \\sigma^2\\), \\(Var(e_{ij}) = v_{ij}\\), and \\(Cor(e_{ij}, e_{ik}) = \\rho\\). Note that \\(\\sigma^2\\) is now estimated via REML.\n\nmeta2 &lt;- rma.mv(yi = effectsize ~ males + college + binge, \n                V = covMat, random = ~ 1 | studyid, \n                data = corrdat,\n                method = \"REML\")\nc(sigma.sq = meta2$sigma2)\n\n sigma.sq \n0.2477825 \n\n\nThe between-study heterogeneity estimate is considerably larger than the moment estimate from robumeta. Together with the difference in weighting, this leads to some changes in the coefficient estimates and their estimated precision:\n\nRobustResults(meta2)\n\n\nt test of coefficients:\n\n          Estimate Std. Error t value Pr(&gt;|t|)   \nintrcpt -0.8907096  0.4148219 -2.1472 0.038783 * \nmales    0.0163074  0.0055805  2.9222 0.006052 **\ncollege  0.3180139  0.2273396  1.3988 0.170658   \nbinge   -0.0984026  0.0897269 -1.0967 0.280265   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nIt is important to keep in mind that the estimate of between-study heterogeneity depends on the posited model for the covariance structure, including the assumed value of \\(\\rho\\). HTJ recommend conducting sensitivity analysis across a range of values for the within-study effect correlation. Re-calculating the value of \\(\\sigma^2\\) for \\(\\rho\\) between 0.0 and 0.9 yields the following:\n\nsigma2 &lt;- function(rho) {\n  covMat &lt;- as.matrix(bdiag(with(corrdat, tapply(var_mean, studyid, equicorr, rho = rho, simplify = FALSE))))\n  rma.mv(yi = effectsize ~ males + college + binge, \n                  V = covMat, random = ~ 1 | studyid, \n                  data = corrdat,\n                  method = \"REML\")$sigma2\n}\nrho_sens &lt;- seq(0,0.9,0.1)\nsigma2_sens &lt;- sapply(rho_sens, sigma2)\ncbind(rho = rho_sens, sigma2 = round(sigma2_sens, 4))\n\n      rho sigma2\n [1,] 0.0 0.2519\n [2,] 0.1 0.2513\n [3,] 0.2 0.2507\n [4,] 0.3 0.2502\n [5,] 0.4 0.2497\n [6,] 0.5 0.2492\n [7,] 0.6 0.2487\n [8,] 0.7 0.2482\n [9,] 0.8 0.2478\n[10,] 0.9 0.2474\n\n\nThe between-study heterogeneity is quite insensitive to the assumed value of \\(\\rho\\).\nThe difference between the results based on metafor versus on robumeta appears to be due to the subtle difference in the weighting approach: metafor uses block-diagonal weights that contain off-diagonal terms for effects drawn from a common study, whereas robumeta uses entirely diagonal weights.\n\n\n\n\n\nReuseCC BY 4.0CitationBibTeX citation:@online{pustejovsky2014,\n  author = {Pustejovsky, James E.},\n  title = {Another Meta-Sandwich},\n  date = {2014-04-23},\n  url = {https://jepusto.com/posts/Robust-meta-analysis-2},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nPustejovsky, James E. 2014. “Another Meta-Sandwich.” April\n23, 2014. https://jepusto.com/posts/Robust-meta-analysis-2."
  },
  {
    "objectID": "posts/rdd-interactions-again/index.html",
    "href": "posts/rdd-interactions-again/index.html",
    "title": "Estimating average effects in regression discontinuities with covariate interactions",
    "section": "",
    "text": "Regression discontinuity designs (RDDs) are now a widely used tool for program evaluation in economics and many other fields. RDDs occur in situations where some treatment/program of interest is assigned on the basis of a numerical score (called the running variable), all units scoring above a certain threshold receiving treatment and all units scoring at or below the threshold having treatment withheld (or vice versa, with treatment assigned to units scoring below the threshold). This mechanism provides a way to identify the marginal average treatment effect (MATE): the average effect of treatment assignment for units on the cusp of the threshold.\nRDDs are appealing for a couple of reasons. First and foremost, RDD-like mechanism occurs all over the place, since providing treatment on the basis of a numerical measure of need/eligibility is a natural way to allocate resources. Furthermore, analysis of the designs is straight-forward, as it involves nothing more complicated than a linear regression model, estimated using (weighted or un-weighted) least squares, and which can be represented graphically using a simple scatterplot. Things get a little bit more complicated if you are trying to account for imperfect compliance with treatment assignment—as in the “fuzzy” RDD—but for the moment let me focus on “sharp” RDDs.\nThe simplest approach to estimating the MATE is to use a local linear regression in the neighborhood of the threshold, with the outcome regressed on the running variable, treatment indicator, and their interaction. However, in practice it is quite common to also include additional covariates in the local linear regression. If the covariates are also interacted with the treatment indicator, there is no longer a single regression coefficient corresponding to the treatment effect. In my last post, I suggested a “centering trick” for estimating the MATE based on a model that included covariate-by-treatment interactions. In this post, I’ll explain the reasoning behind this proposal.\n\nG’day, MATE\nI think it’s helpful to start by thinking about the definition of the MATE in non-parametric terms. Let \\(R\\) be the running variable, assumed to be centered at the threshold; \\(T\\) be an indicator for treatment assignment, with \\(T = I(R &gt; 0)\\); and \\(X\\) be a covariate, which may be vector-valued. Denote the potential outcomes as \\(Y^0\\) (a unit’s outcome if not assigned to treatment) and \\(Y^1\\) (a unit’s outcome if assigned to treatment), so that the observed outcome is \\(Y = Y^0 (1 - T) + Y^1 T\\). Now consider the potential response surfaces\n\\[\\begin{aligned}\\mu_0(x, r) &= \\text{E}\\left(\\left.Y^0 \\right|X = x, R = r\\right) \\\\ \\mu_1(x, r) &= \\text{E}\\left(\\left.Y^1 \\right|X = x, R = r\\right).\\end{aligned}\\]\nIn an RDD, the average treatment effect at a given point \\((x, r)\\) on the response surface is not generally identified by conditioning because one of the potential outcomes will never be observed: if \\(r &lt; 0\\) then \\(\\text{Pr}( T = 0 \\vert X = x, R = r) = 1\\) and \\(\\text{Pr}( T = 1 \\vert X = x, R = r) = 0\\) (and vice versa for \\(r &gt; 0\\)). However, the treatment effect for the subpopulation where \\(R = 0\\) can be identified under the assumption that the potential response surfaces are continuous in a neighborhood of the threshold. Thus the MATE, which can be written as\n\\[\\begin{aligned}\n\\delta_M &= \\text{E}\\left(\\left. Y^1 - Y^0 \\right| R = 0\\right) \\\\\n&= \\text{E}\\left[\\mu_1(X, 0) - \\mu_0(X,0)\\right].\n\\end{aligned}\\]\n\n\nRegression estimation\nNow assume that we have a simple random sample \\(\\left(y_i,r_i,t_i, x_i\\right)_{i=1}^n\\) of units and that each unit has a weight \\(w_i\\) defined based on some measure of distance from the threshold. We can use these data to estimate the response surfaces (somehow…more on that in a minute) on each side of the cut-off, with \\(\\hat\\mu_0(x, r)\\) for \\(r &lt; 0\\) and \\(\\hat\\mu_1(x, r)\\) for \\(r &gt; 0\\). If we then use the sample distribution of \\(X\\) in the neighborhood of \\(R = 0\\) in place of the conditional density \\(d\\left(X = x \\vert R = 0\\right)\\), we can estimate the MATE as\n\\[\\hat\\delta_M = \\frac{1}{W} \\sum_{i=1}^n w_i \\left[\\hat\\mu_1(x_i, 0) - \\hat\\mu_0(x_i, 0)\\right],\\]\nwhere \\(W = \\sum_{i=1}^n w_i\\). This is a regression estimator for \\(\\delta_M\\). It could be non-, semi-, or fully parametric depending on the technique used to estimate the response surfaces. Note that this estimator is a little bit different than the regression estimator that would be used in the context of an observational study (see, e.g., Shafer & Kang, 2008). In that context, one would use \\(\\hat\\mu_j(x_i, r_i)\\) rather than \\(\\hat\\mu_j(x_i, 0)\\), but in an RDD doing so would involve extrapolating beyond the cutpoint (i.e., using \\(\\hat\\mu_1(x_i, r_i)\\) for \\(r_i &lt; 0\\)).\nNow suppose that we again use a linear regression in some neighborhood of the cut-point to estimate the response surfaces. For the (weighted) sample in the neighborhood of the cut-point, we assume that\n\\[\\mu_{t_i}(x_i, r_i) = \\beta_0 + \\beta_1 r_i + \\beta_2 t_i + \\beta_3 r_i t_i + \\beta_4 x_i + \\beta_5 x_i t_i.\\]\nSubstituting this into the formula for \\(\\hat\\delta_M\\) leads to\n\\[\\begin{aligned}\\hat\\delta_M &= \\frac{1}{W} \\sum_{i=1}^n w_i \\left[\\hat\\beta_2 + \\hat\\beta_5 x_i \\right] \\\\\n&= \\hat\\beta_2 + \\hat\\beta_5 \\sum_{i=1}^n \\frac{w_i x_i}{W}.\\end{aligned}\\]\nNow, the centering trick involves nothing more than re-centering the covariate so that \\(\\sum_{i=1}^n w_i x_i = 0\\) and \\(\\hat\\delta_M = \\hat\\beta_2\\). Of course, one could just use the non-parametric form of the regression estimator, but the centering trick is useful because it comes along with an easy-to-calculate standard error (since it is just a regression coefficient estimate).\n\n\nMultiple covariates\nAll of this works out in the exact same way if you have interactions between the treatment and multiple covariates. However, there are a few tricky cases that are worth noting. If you include interactions between the treatment indicator and a polynomial function of the treatment, each term of the polynomial has to be centered. For example, if you want to control for \\(x\\), \\(x^2\\), and their interactions with treatment, you will need to calculate\n\\[\\tilde{x}_{1i} = x_i - \\frac{1}{W} \\sum_{i=1}^n w_i x_i, \\qquad \\tilde{x}_{2i} = x_i^2 - \\frac{1}{W} \\sum_{i=1}^n w_i x_i^2\\]\nand then use these re-centered covariates in the regression\n\\[\\mu_{t_i}(x_i, r_i) = \\beta_0 + \\beta_1 r_i + \\beta_2 t_i + \\beta_3 r_i t_i + \\beta_4 \\tilde{x}_{1i} + \\beta_5 \\tilde{x}_{2i} + \\beta_6 \\tilde{x}_{1i} t_i + \\beta_7 \\tilde{x}_{2i} t_i.\\]\nThe same principle will also hold if you want to include higher-order interactions between covariates and the treatment: calculate the interaction term first, then re-center it. There’s one exception though. If you want to include an interaction between a covariate \\(x\\), the running variable, and the treatment indicator (who knows…you might aspire to do this some day…), then all you need to do is center \\(x\\). In particular, you should not calculate the interaction \\(x_i r_i\\) and then re-center it (doing so could pull the average away from the threshold of \\(R = 0\\)).\n\n\nR, MATEs!\nHere’s some R code that implements the centering trick for the simulated example from my last post:\n\nlibrary(sandwich)\nlibrary(lmtest)\nlibrary(rdd)\n\n# simulate an RDD\nset.seed(20160124)\nsimulate_RDD &lt;- function(n = 2000, R = rnorm(n, mean = qnorm(.2))) {\n  n &lt;- length(R)\n  T &lt;- as.integer(R &gt; 0)\n  X1 &lt;- 10 + 0.6 * (R - qnorm(.2)) + rnorm(n, sd = sqrt(1 - 0.6^2))\n  X2 &lt;- sample(LETTERS[1:4], n, replace = TRUE, prob = c(0.2, 0.3, 0.35, 0.15))\n  Y0 &lt;- 0.4 * R + 0.1 * (X1 - 10) + c(A = 0, B = 0.30, C = 0.40, D = 0.55)[X2] + rnorm(n, sd = 0.9)\n  Y1 &lt;- 0.35 + 0.3 * R + 0.18 * (X1 - 10) + c(A = -0.50, B = 0.30, C = 0.20, D = 0.60)[X2] + rnorm(n, sd = 0.9)\n  Y &lt;- (1 - T) * Y0 + T * Y1\n  data.frame(R, T, X1, X2, Y0, Y1, Y)\n}\nRD_data &lt;- simulate_RDD(n = 2000)\n\n# calculate kernel weights\nbw &lt;- with(RD_data, IKbandwidth(R, Y, cutpoint = 0))\nRD_data$w &lt;- kernelwts(RD_data$R, center = 0, bw = bw)\n\n# center the covariates\nX_mat &lt;- model.matrix(~ 0 + X2 + X1, data = RD_data)\nX_cent &lt;- as.data.frame(apply(X_mat, 2, function(x) x - weighted.mean(x, w = RD_data$w)))\nRD_data_aug &lt;- cbind(X_cent, subset(RD_data, select = c(-X1, -X2)))\ncov_names &lt;- paste(names(X_cent)[-1], collapse = \" + \")\n\n# calculate the MATE using RDestimate\nRD_form &lt;- paste(\"Y ~ R |\", cov_names)\nsummary(RDestimate(as.formula(RD_form), data = RD_data_aug))\n\n\nCall:\nRDestimate(formula = as.formula(RD_form), data = RD_data_aug)\n\nType:\nsharp \n\nEstimates:\n           Bandwidth  Observations  Estimate  Std. Error  z value  Pr(&gt;|z|) \nLATE       1.0894     1177          0.2981    0.10659     2.797    0.0051559\nHalf-BW    0.5447      611          0.2117    0.14846     1.426    0.1539482\nDouble-BW  2.1787     1832          0.2734    0.08305     3.292    0.0009949\n              \nLATE       ** \nHalf-BW       \nDouble-BW  ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nF-statistics:\n           F      Num. DoF  Denom. DoF  p\nLATE       23.30  11        1165        0\nHalf-BW    10.97  11         599        0\nDouble-BW  47.41  11        1820        0\n\n# or using lm\nlm_form &lt;- paste(\"Y ~ R + T + R:T + T*(\", cov_names,\")\")\nlm_fit &lt;- lm(as.formula(lm_form), weights = w, data = subset(RD_data_aug, w &gt; 0))\ncoeftest(lm_fit, vcov. = vcovHC(lm_fit, type = \"HC1\"))[\"T\",]\n\n   Estimate  Std. Error     t value    Pr(&gt;|t|) \n0.298142798 0.106588790 2.797130893 0.005240719 \n\n\n\n\nComments\nI’ve shown that the “centering trick” is just a way to express a certain regression estimator for the marginal average treatment effect in an RDD. Having suggested that this is a good idea, I should also note a few points that might bear further investigation.\n\nMy regression estimator uses the sample distribution of \\(X\\) in the neighborhood of the threshold as an estimate of \\(d(X = x \\vert R = 0)\\). This seems reasonable, but I wonder whether there might be a better approach to estimating this conditional density.\nAs far as I understand, the current best practice for defining the “neighborhood” of the threshold is to use weights based on a triangular kernel and an “optimal” bandwidth proposed by Imbens and Kalyanaraman (2012). The optimal bandwidth is derived for the simple RDD model with no covariates, though the authors comment that inclusion of additional covariates should not greatly affect the result unless the covariates are strongly correlated with the outcome, conditional on the running variable. However, what if interest centers on the covariate-by-treatment interaction itself, rather than just the MATE? It is not clear that the bandwidth is optimal for estimation/inference on the interaction term.\nSo far I’ve considered the MATE identified by a sharp RDD, in which we examine the effects of treatment assignment, regardless of whether units assigned to treatment actually received/participated in it. In fuzzy RDDs, the target parameter is the average effect of treatment receipt for those on the threshold of eligibility and who comply with the assignment rule. The effect is estimated using two-stage least squares, taking treatment assignment as an instrument for treatment receipt. I’m not entirely sure how the regression estimator approach would work in this instrumental variables setting.\n\n\n\n\n\n\n\nReuseCC BY 4.0CitationBibTeX citation:@online{pustejovsky2016,\n  author = {Pustejovsky, James E.},\n  title = {Estimating Average Effects in Regression Discontinuities with\n    Covariate Interactions},\n  date = {2016-01-27},\n  url = {https://jepusto.com/posts/rdd-interactions-again},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nPustejovsky, James E. 2016. “Estimating Average Effects in\nRegression Discontinuities with Covariate Interactions.” January\n27, 2016. https://jepusto.com/posts/rdd-interactions-again."
  },
  {
    "objectID": "posts/Pusto-Tipton-2018-Theorem-2-redux/index.html",
    "href": "posts/Pusto-Tipton-2018-Theorem-2-redux/index.html",
    "title": "Corrigendum to Pustejovsky and Tipton (2018), redux",
    "section": "",
    "text": "\\[\n\\def\\Pr{{\\text{Pr}}}\n\\def\\E{{\\text{E}}}\n\\def\\Var{{\\text{Var}}}\n\\def\\Cov{{\\text{Cov}}}\n\\def\\bm{\\mathbf}\n\\def\\bs{\\boldsymbol}\n\\] UPDATE, March 8, 2023: The correction to our paper has now been published at Journal of Business and Economic Statistics. It is available at https://doi.org/10.1080/07350015.2023.2174123.\nIn my 2018 paper with Beth Tipton, published in the Journal of Business and Economic Statistics, we considered how to do cluster-robust variance estimation in fixed effects models estimated by weighted (or unweighted) least squares. As explained in my previous post, we were recently alerted that Theorem 2 in the paper is incorrect as stated. It turns out, the conditions in the original version of the theorem are too general. A more limited version of the Theorem does actually hold, but only for models estimated using ordinary (unweighted) least squares, under a working model that assumes independent, homoskedastic errors. In this post, I’ll give the revised theorem, following the notation and setup of the previous post (so better read that first, or what follows won’t make much sense!).\n\nTheorem 2, revised\nConsider the model \\[\n\\bm{y}_i = \\bm{R}_i \\bs\\beta + \\bm{S}_i \\bs\\gamma + \\bm{T}_i \\bs\\mu + \\bs\\epsilon_i, (\\#eq:regression)\n\\] where \\(\\bm{y}_i\\) is an \\(n_i \\times 1\\) vector of responses for cluster \\(i\\), \\(\\bm{R}_i\\) is an \\(n_i \\times r\\) matrix of focal predictors, \\(\\bm{S}_i\\) is an \\(n_i \\times s\\) matrix of additional covariates that vary across multiple clusters, and \\(\\bm{T}_i\\) is an \\(n_i \\times t\\) matrix encoding cluster-specific fixed effects, all for \\(i = 1,...,m\\). Let \\(\\bm{U}_i = \\left[ \\bm{R}_i \\ \\bm{S}_i \\right]\\) be the set of predictors that vary across clusters and \\(\\bm{X}_i = \\left[ \\bm{R}_i \\ \\bm{S}_i \\ \\bm{T}_i \\right]\\) be the full set of predictors. Let \\(\\bm{\\ddot{U}}_i = \\left(\\bm{I} - \\bm{T}_i \\bm{M}_{\\bm{T}}\\bm{T}_i'\\right) \\bm{U}_i\\) be an absorbed version of the focal predictors and the covariates. The cluster-robust variance estimator for the coefficients of \\(\\bm{U}_i\\) is \\[\n\\bm{V}^{CR2} = \\bm{M}_{\\bm{\\ddot{U}}} \\left(\\sum_{i=1}^m \\bm{\\ddot{U}}_i' \\bm{W}_i \\bm{A}_i \\bm{e}_i \\bm{e}_i' \\bm{A}_i \\bm{W}_i \\bm{\\ddot{U}}_i \\right) \\bm{M}_{\\bm{\\ddot{U}}},\n(#eq:CRVE)\n\\] where \\(\\bm{A}_1,...,\\bm{A}_m\\) are the CR2 adjustment matrices.\nIf we assume a working model in which \\(\\bs\\Psi_i = \\sigma^2 \\bm{I}_i\\) for \\(i = 1,...,m\\) and estimate the model by ordinary least squares, then the CR2 adjustment matrices have a fairly simple form: \\[\n\\bm{A}_i = \\left(\\bm{I}_i - \\bm{X}_i \\bm{M_X} \\bm{X}_i'\\right)^{+1/2},\n(#eq:A-matrix)\n\\] where \\(B^{+1/2}\\) is the symmetric square root of the Moore-Penrose inverse of \\(\\bm{B}\\). However, this form is computationally expensive because it involves the full set of predictors, \\(\\bm{X}_i\\), including the cluster-specific fixed effects \\(\\bm{T}_i\\). If the model is estimated after absorbing the cluster-specific fixed effects, then it would be convenient to use the adjustment matrices based on the absorbed predictors only, \\[\n\\bm{\\tilde{A}}_i = \\left(\\bm{I}_i - \\bm{\\ddot{U}}_i \\bm{M_\\ddot{U}} \\bm{\\ddot{U}}_i'\\right)^{+1/2}.\n(#eq:A-tilde)\n\\] The original version of Theorem 2 asserted that \\(\\bm{A}_i = \\bm{\\tilde{A}}_i\\), which is not actually the case. However, for ordinary least squares with the independent, homoskedastic working model, we can show that \\(\\bm{A}_i \\bm{\\ddot{U}}_i = \\bm{\\tilde{A}}_i \\bm{\\ddot{U}}_i\\). Thus, it doesn’t matter whether we use \\(\\bm{A}_i\\) or \\(\\bm{\\tilde{A}}_i\\) to calculate the cluster-robust variance estimator. We’ll get the same result either way, but \\(\\bm{\\tilde{A}}_i\\) is bit easier to compute.\nHere’s a formal statement of Theorem 2:\n\nLet \\(\\bm{L}_i = \\left(\\bm{\\ddot{U}}'\\bm{\\ddot{U}} - \\bm{\\ddot{U}}_i'\\bm{\\ddot{U}}_i\\right)\\) and assume that \\(\\bm{L}_1,...,\\bm{L}_m\\) have full rank \\(r + s\\). If \\(\\bm{W}_i = \\bm{I}_i\\) and \\(\\bs\\Phi_i = \\bm{I}_i\\) for \\(i = 1,...,m\\), then \\(\\bm{A}_i \\bm{\\ddot{U}}_i = \\bm{\\tilde{A}}_i \\bm{\\ddot{U}}_i\\), where \\(\\bm{A}_i\\) and \\(\\tilde{\\bm{A}}_i\\) are as defined in @ref(eq:A-matrix) and @ref(eq:A-tilde), respectively.\n\n\n\nProof\nWe can prove this revised Theorem 2 by showing how \\(\\bm{A}_i\\) can be constructed in terms of \\(\\bm{\\tilde{A}}_i\\) and \\(\\bm{T}_i\\). First, because \\(\\bm{T}_i'\\bm{T}_k = \\bm{0}\\) for any \\(i \\neq k\\), it follows that \\(\\bm{T}_i \\bm{M_T} \\bm{T}_i'\\) is idempotent, i.e., \\[\n\\bm{T}_i \\bm{M_T} \\bm{T}_i' \\bm{T}_i \\bm{M_T} \\bm{T}_i' = \\bm{T}_i \\bm{M_T} \\bm{T}_i'.\n\\]\nNext, denote the thin QR decomposition of \\(\\bm{\\ddot{U}}_i\\) as \\(\\bm{Q}_i \\bm{R}_i\\), where \\(\\bm{Q}_i\\) is semi-orthogonal \\((\\bm{Q}_i'\\bm{Q}_i = \\bm{I})\\) and \\(\\bm{R}_i\\) has the same rank as \\(\\bm{\\ddot{U}}_i\\). Next, let \\(\\bm{\\tilde{B}}_i = \\bm{I}_i - \\bm{\\ddot{U}}_i \\bm{M_\\ddot{U}} \\bm{\\ddot{U}}_i'\\) and observe that this can be written as \\[\n\\tilde{\\bm{B}}_i = \\bm{I}_i - \\bm{Q}_i \\bm{Q}_i' + \\bm{Q}_i \\left(\\bm{I} - \\bm{R}_i \\bm{M}_{\\bm{\\ddot{U}}} \\bm{R}_i'\\right)\\bm{Q}_i'.\n\\] It can then be seen that \\[\n\\bm{\\tilde{A}}_i = \\tilde{\\bm{B}}_i^{+1/2} = \\bm{I}_i - \\bm{Q}_i \\bm{Q}_i' + \\bm{Q}_i \\left(\\bm{I} - \\bm{R}_i \\bm{M}_{\\bm{\\ddot{U}}} \\bm{R}_i'\\right)^{+1/2} \\bm{Q}_i'.\n\\] It follows that \\(\\bm{\\tilde{A}}_i \\bm{T}_i = \\bm{T}_i\\) because \\(\\bm{Q}_i'\\bm{T}_i = \\bm{0}\\). Further, \\(\\bm{\\tilde{B}}_i \\bm{T}_i = \\bm{T}_i\\) as well.\nNow, let \\(\\bm{B}_i = \\left(\\bm{I}_i - \\bm{X}_i \\bm{M_X} \\bm{X}_i'\\right)\\) and observe that this can be written as \\[\n\\bm{B}_i = \\bm{I}_i - \\bm{\\ddot{U}}_i \\bm{M_{\\ddot{U}}}\\bm{\\ddot{U}}_i' - \\bm{T}_i \\bm{M_T}\\bm{T}_i' = \\bm{\\tilde{B}}_i - \\bm{T}_i \\bm{M_T}\\bm{T}_i'\n\\] because \\(\\bm{\\ddot{U}}_i'\\bm{T}_i = \\bm{0}\\).\nWe then construct the full adjustment matrix \\(\\bm{A}_i\\) as \\[\n\\bm{A}_i = \\tilde{\\bm{A}}_i - \\bm{T}_i \\bm{M_T}\\bm{T}_i'.\n(#eq:A-constructed)\n\\] Showing that \\(\\bm{B}_i \\bm{A}_i \\bm{B}_i \\bm{A}_i = \\bm{B}_i\\) will suffice to verify that \\(\\bm{A}_i\\) is the symmetric square root of the Moore-Penrose inverse of \\(\\bm{B}_i\\). Because \\(\\bm{T}_i \\bm{M_T} \\bm{T}_i'\\) is idempotent, \\(\\bm{\\tilde{B}}_i \\bm{T}_i = \\bm{T}_i\\), and \\(\\bm{\\tilde{A}}_i \\bm{T}_i = \\bm{T}_i\\), we have \\[\n\\begin{aligned}\n\\bm{B}_i \\bm{A}_i \\bm{B}_i \\bm{A}_i &= \\left(\\tilde{\\bm{B}}_i - \\bm{T}_i \\bm{M_T}\\bm{T}_i'\\right) \\left(\\tilde{\\bm{A}}_i - \\bm{T}_i \\bm{M_T}\\bm{T}_i'\\right)\\left(\\tilde{\\bm{B}}_i - \\bm{T}_i \\bm{M_T}\\bm{T}_i'\\right) \\left(\\tilde{\\bm{A}}_i - \\bm{T}_i \\bm{M_T}\\bm{T}_i'\\right) \\\\\n&= \\left(\\tilde{\\bm{B}}_i\\tilde{\\bm{A}}_i - \\bm{T}_i \\bm{M_T}\\bm{T}_i'\\right)\\left(\\tilde{\\bm{B}}_i\\tilde{\\bm{A}}_i - \\bm{T}_i \\bm{M_T}\\bm{T}_i'\\right) \\\\\n&= \\left(\\tilde{\\bm{B}}_i\\tilde{\\bm{A}}_i\\tilde{\\bm{B}}_i\\tilde{\\bm{A}}_i - \\bm{T}_i \\bm{M_T}\\bm{T}_i'\\right) \\\\\n&= \\left(\\tilde{\\bm{B}}_i - \\bm{T}_i \\bm{M_T}\\bm{T}_i'\\right) \\\\\n&= \\bm{B}_i.\n\\end{aligned}\n\\]\nFrom the representation of \\(\\bm{A}_i\\) in @ref(eq:A-constructed), it is clear that \\(\\bm{A}_i \\bm{\\ddot{U}}_i = \\bm{\\tilde{A}}_i \\bm{\\ddot{U}}_i - \\bm{T}_i \\bm{M_T} \\bm{T}_i' \\bm{\\ddot{U}}_i = \\bm{\\tilde{A}}_i \\bm{\\ddot{U}}_i\\).\n\n\n\n\n\n\nReuseCC BY 4.0CitationBibTeX citation:@online{e. pustejovsky2022,\n  author = {E. Pustejovsky, James},\n  title = {Corrigendum to {Pustejovsky} and {Tipton} (2018), Redux},\n  date = {2022-11-07},\n  url = {https://jepusto.com/posts/Pusto-Tipton-2018-Theorem-2-redux},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nE. Pustejovsky, J. (2022, November 7). Corrigendum to Pustejovsky\nand Tipton (2018), redux. https://jepusto.com/posts/Pusto-Tipton-2018-Theorem-2-redux"
  },
  {
    "objectID": "posts/procedural-sensitivities-paper/index.html",
    "href": "posts/procedural-sensitivities-paper/index.html",
    "title": "New paper: procedural sensitivities of effect size measures for SCDs",
    "section": "",
    "text": "I’m very happy to share that my article “Procedural sensitivities of effect sizes for single-case designs with directly observed behavioral outcome measures” has been accepted at Psychological Methods. There’s no need to delay in reading it, since you can check out the pre-print and supporting materials. Here’s the abstract:\n\nA wide variety of effect size indices have been proposed for quantifying the magnitude of treatment effects in single-case designs. Commonly used measures include parametric indices such as the standardized mean difference, as well as non-overlap measures such as the percentage of non-overlapping data, improvement rate difference, and non-overlap of all pairs. Currently, little is known about the properties of these indices when applied to behavioral data collected by systematic direct observation, even though systematic direct observation is the most common method for outcome measurement in single-case research. This study uses Monte Carlo simulation to investigate the properties of several widely used single-case effect size measures when applied to systematic direct observation data. Results indicate that the magnitude of the non-overlap measures and of the standardized mean difference can be strongly influenced by procedural details of the study’s design, which is a significant limitation to using these indices as effect sizes for meta-analysis of single-case designs. A less widely used parametric index, the log-response ratio, has the advantage of being insensitive to sample size and observation session length, although its magnitude is influenced by the use of partial interval recording.\n\nThis paper was a long time coming. The core idea came out of a grant proposal I wrote during the summer of 2014, which I fleshed out for a poster presented at AERA in April of 2015. After finishing a draft of the paper, I tried to publish it in a special education journal, reasoning that the main audience for the paper is researchers interested in meta-analyzing single case research studies that are commonly used in some parts of special education. That turned out to be a non-starter. Four rejection letters later, I re-worked the paper a bit to give more technical details, then submitted it to a more methods-ish journal. This yielded an R&R, I revised the paper extensively, resubmitted it, and it was declined. Buying in fully to the sunk costs fallacy, I sent the paper to Psychological Methods. This time, I received very extensive and helpful feedback from several anonymous reviewers and an associate editor (thank you, anonymous peers!), which helped me to revise the paper yet again, and this time it was accepted. Sixth time is the charm, as they say.\nHere’s the complete time-line of submissions:\n\nAugust 5, 2015: submitted to journal #1 (special education)\nAugust 28, 2015: desk reject decision from journal #1\nSeptember 3, 2015: submitted to journal #2 (special education)\nNovember 6, 2015: reject decision (after peer review) from journal #2\nNovember 18, 2015: submitted to journal #3 (special education)\nNovember 22, 2015: desk reject decision from journal #3 as not appropriate for their audience. I was grateful to get a quick decision.\nNovember 23, 2015: submitted to journal #4 (special education)\nFebruary 17, 2016: reject decision (after peer review) from journal #4\nApril 19, 2016: submitted to journal #5 (methods)\nAugust 16, 2016: revise-and-resubmit decision from journal #5\nOctober 14, 2016: re-submitted to journal #5\nFebruary 2, 2017: reject decision from journal #5\nMay 10, 2017: submitted to Psychological Methods\nSeptember 1, 2017: revise-and-resubmit decision from Psychological Methods\nSeptember 26, 2017: re-submitted to Psychological Methods\nNovember 22, 2017: conditional acceptance\nDecember 6, 2017: re-submitted with minor revisions\nJanuary 10, 2018: accepted at Psychological Methods\n\n\n\n\n\n\nReuseCC BY 4.0CitationBibTeX citation:@online{pustejovsky2018,\n  author = {Pustejovsky, James E.},\n  title = {New Paper: Procedural Sensitivities of Effect Size Measures\n    for {SCDs}},\n  date = {2018-01-11},\n  url = {https://jepusto.com/posts/procedural-sensitivities-paper},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nPustejovsky, James E. 2018. “New Paper: Procedural Sensitivities\nof Effect Size Measures for SCDs.” January 11, 2018. https://jepusto.com/posts/procedural-sensitivities-paper."
  },
  {
    "objectID": "posts/PET-PEESE-performance/index.html",
    "href": "posts/PET-PEESE-performance/index.html",
    "title": "You wanna PEESE of d’s?",
    "section": "",
    "text": "Publication bias—or more generally, outcome reporting bias or dissemination bias—is recognized as a critical threat to the validity of findings from research syntheses. In the areas with which I am most familiar (education and psychology), it has become more or less a requirement for research synthesis projects to conduct analyses to detect the presence of systematic outcome reporting biases. Some analyses go further by trying correct for its distorting effects on average effect size estimates. Widely known analytic techniques for doing so include Begg and Mazumdar’s rank-correlation test, the Trim-and-Fill technique proposed by Duval and Tweedie, and Egger regression (in its many variants). Another class of methods involves selection models (or weight function models), as proposed by Hedges and Vevea, Vevea and Woods, and others. As far as I can tell, selection models are well known among methodologists but very seldom applied due to their complexity and lack of ready-to-use software (though an R package has recently become available). More recent proposals include the PET-PEESE technique introduced by Stanley and Doucouliagos; Simonsohn, Nelson, and Simmon’s p-curve technique; Van Assen, Van Aert, and Wichert’s p-uniform, and others. The list of techniques grows by the day.\nAmong these methods, Egger regression, PET, and PEESE are superficially quite appealing due to their simplicity. These methods each involve estimating a fairly simple meta-regression model, using as the covariate the sampling variance of the effect size or some transformation thereof. PET uses the standard error of the effect size as the regressor; PEESE uses the sampling variance (i.e., the squared standard error); PET-PEESE involves first testing whether the PET estimate is statistically significant, using PEESE if it is or PET otherwise. The intercept from one of these regressions is the average effect size estimate from a study with zero sampling variance; the estimated intercept is used as a “bias-corrected” estimator of the population average effect. These methods are also appealing due to their extensibility. Because they are just meta-regressions, it is comparatively easy to extend them to meta-regression models that control for further covariates, to use robust variance estimation to account for dependencies among effect size estimates, etc.\nIn a recent blog post, Uri Simonsohn reports some simulation evidence indicating that the PET-PEESE estimator can have large biases under certain conditions, even in the absence of publication bias. The simulations are based on standardized mean differences from two-group experiments and involve simulating collections of studies that include many with small sample sizes, as might be found in certain areas of psychology. On the basis of these performance assessments, he argues that this purported cure is actually worse than the disease—that PET-PEESE should not be used in meta-analyses of psychological research because it performs too poorly to be trusted. In a response to Uri’s post, Joe Hilgard suggests that some simple modifications to the method can improve its performance. Specifically, Joe suggests using a function of sample size as the covariate (in place of the standard error or sampling variance of \\(d\\)), and also using PET or PEESE as stand-alone estimators, rather than using them contingent on a significance test.\nIn this post, I follow up Joe’s suggestions while replicating and expanding upon Uri’s simulations, to try and provide a fuller picture of the relative performance of these estimators. In brief, the simulations show that:\n\nTests for small-sample bias that use PET or PEESE can have wildly incorrect type-I error rates in the absence of publication bias. Don’t use them.\nThe sample-size variants of PET and PEESE do maintain the correct type-I error rates in the absence of publication bias.\nThe sample-size variants of PET and PEESE are exactly unbiased in the absence of publication bias.\nHowever, these adjusted estimators still have a cost, being less precise than the conventional fixed-effect estimator.\nIn the presence of varying degrees of publication bias, none of the estimators consistently out-perform the others. If you really really need to use a regression-based correction, the sample-size variant of PEESE seems like it might be a reasonable default method, but it’s still really pretty rough.\n\n\nWhy use sample size?\nTo see why it makes sense to use a function of sample size as the covariate for PET-PEESE analyses, rather than using the standard error of the effect size estimate, let’s look at the formulas. Say that we have a standardized mean difference estimate from a two-group design (without covariates) with sample sizes \\(n_0\\) and \\(n_1\\):\n\\[\nd = \\frac{\\bar{y}_1 - \\bar{y}_0}{s_p},\n\\]\nwhere \\(\\bar{y}_0\\) and \\(\\bar{y}_1\\) are the sample means within each group and \\(s_p^2\\) is the pooled sample variance. Following convention, we’ll assume that the outcomes are normally distributed within each group, and the groups have common variance. The exact sampling variance of \\(d\\) is a rather complicated formula, but one which can be approximated reasonably well as\n\\[\n\\text{Var}(d) \\approx \\frac{n_0 + n_1}{n_0 n_1} + \\frac{\\delta^2}{2(n_0 + n_1)},\n\\]\nwhere \\(\\delta\\) is the true standardized mean difference parameter. This formula is a delta-method approximation. The first term captures the variance of the numerator of \\(d\\), so it gets at how precisely the unstandardized difference in means is estimated. The second term captures the variance of the denominator of \\(d\\), so it gets at how precisely the scale of the outcome is estimated. The second term also involves the unknown parameter \\(\\delta\\), which must be estimated in practice. The conventional formula for the estimated sampling variance of \\(d\\) substitutes \\(d\\) in place of \\(\\delta\\):\n\\[\nV = \\frac{n_0 + n_1}{n_0 n_1} + \\frac{d^2}{2(n_0 + n_1)}.\n\\]\nIn PET-PEESE analysis, \\(V\\) or its square root is used as a covariate in a regression of the effect sizes, as a means of adjusting for publication bias. There are two odd things about this. First, publication bias is about the statistical significance of the group differences, but statistical significance does not depend on the scale of the outcome. The test of the null hypothesis of no differences between groups is not based on \\(d / \\sqrt{V}\\). Instead, it is a function of the \\(t\\) statistic:\n\\[\nt = d / \\sqrt{\\frac{n_0 + n_1}{n_0 n_1}}.\n\\]\nConsequently, it makes sense to use only the first term of \\(V\\) as a covariate for purposes of detecting publication biases.\nThe second odd thing is that \\(V\\) is generally going to be correlated with \\(d\\) because we have to use \\(d\\) to calculate \\(V\\). As Joe explained in his response to Uri, this means that there will be a non-zero correlation between \\(d\\) and \\(V\\) (or between \\(d\\) and \\(\\sqrt{V}\\)) except in some very specific cases, even in the absence of any publication bias. Pretty funky.\nThis second problem with regression tests for publication bias has been recognized for a while in the literature (e.g., Macaskill, Walter, & Irwig, 2001; Peters et al., 2006; Moreno et al., 2009), but most of the work here has focused on other effect size measures, like odds ratios, that are relevant in clinical medicine. The behavior of these estimators might well differ for \\(d\\)’s because the dependence between the effect measure and its variance has a different structure.\nBelow I’ll investigate how this stuff works with standardized mean differences, which haven’t been studied as extensively as odds ratios. Actually, I know of only two simulation studies that examined the performance of PET-PEESE methods with standardized mean difference estimates: Inzlicht, Gervais, and Berkman (2015) and Stanley (2017). (Know of others? Leave a comment!) Neither considered using sample-size variants of PET-PEESE. The only source I know of that did consider this is this blog post from Will Gervais, which starts out optimistic about the sample-size variants but ends on a discouraged note. The simulations below build upon Will’s work, as well as Uri’s, by 1) considering a more extensive set of data-generating processes and 2) examining accuracy in addition to bias.\n\n\nSimulation model\nThe simulations are based on the following data-generating model, which closely follows the structure that Uri used:\n\nPer-cell sample size is generated as \\(n = 12 + B (n_{max} - 12)\\), where \\(B \\sim Beta(\\alpha, \\beta)\\) and \\(n_{max}\\) is the maximum observed sample size. I take \\(n_{max} = 50\\) or \\(120\\) and look at three sample size distributions (note that these distributions are pre-selection, so the observed sample size distributions will deviate from these if there is selective publication):\n\n\\(\\alpha = \\beta = 1\\) corresponds to a uniform distribution on \\([12,n_{max}]\\);\n\\(\\alpha = 1, \\beta = 3\\) is a distribution with more small studies; and\n\\(\\alpha = 3, \\beta = 1\\) is a distribution with more large studies.\n\nTrue effects are simulated as \\(\\delta \\sim N(\\mu, \\sigma^2)\\), for \\(\\mu = 0, 0.1, 0.2, ..., 1.0\\) and \\(\\sigma = 0.0, 0.1, 0.2, 0.4\\). Note that the values of \\(\\sigma\\) are standard deviations of the true effects, with \\(\\sigma = 0.0\\) corresponding to the constant effect model and \\(\\sigma = 0.4\\) corresponding to rather substantial effect heterogeneity.\nStandardized mean difference effect size estimates are generated as in a two-group between-subjects experiment with equal per-cell sample sizes. I do this by taking \\(t = D / \\sqrt{S / [2(n - 1)]}\\), where \\(D \\sim N(\\delta \\sqrt{n / 2}, 1)\\) and \\(S \\sim \\chi^2_{2(n - 1)}\\), then calculating\n\\[\n  d = \\left(1 - \\frac{3}{8 n - 9}\\right) \\times \\sqrt{\\frac{2}{n}} \\times t.\n  \\]\n(That first term is Hedges’ \\(g\\) correction, cuz that’s how I roll.)\nObserved effects are filtered based on statistical significance. Let \\(p\\) be the p-value corresponding to the observed \\(t\\) and the one-tailed hypothesis test of \\(\\delta \\leq 0\\). If \\(p &lt; .025\\), \\(d\\) is observed with probability 1. If \\(p \\geq .025\\), then \\(d\\) is observed with probability \\(\\pi\\). Noted that this mechanism corresponds to filtering based on two-sided hypothesis tests, where effects are filtered if they are statistically non-significant effects or statistically significant but in the wrong direction. I look at three scenarios:\n\n\\(\\pi = 1.0\\) corresponds to no selective publication (all simulated effects are observed);\n\\(\\pi = 0.2\\) corresponds to an intermediate degree of selective publication (some but not non-significant effects are observed); and\n\\(\\pi = 0.0\\) corresponds to very strong selective publication (only statistically significant effects are observed).\n\nEach meta-analysis includes a total of \\(k = 100\\) observed studies. Note that in scenarios with publication bias, more (sometimes many more) than 100 studies are generated in order to get 100 observed effects.\n\nFor each simulated meta-sample, I calculated the following:\n\nthe usual fixed-effect meta-analytic average (I skipped random effects for simplicity);\nthe PET estimator (including intercept and slope);\nthe PEESE estimator (including intercept and slope);\nPET-PEESE, which is equal to the PEESE intercept if \\(H_0: \\beta_0 \\leq 0\\) is rejected at the 10% level, and is otherwise equal to the PET intercept (this definition follows Stanley, 2017);\nthe modified PET estimator, which I’ll call “SPET” for “sample-size PET” (suggestions for better names welcome);\nthe modified PEESE estimator, which I’ll call “SPEESE”; and\nSPET-SPEESE, which follows the same conditional logic as PET-PEESE.\n\nSimulation results are summarized across 4000 replications. The R code for all this lives here. Complete numerical results live here. Code for creating the graphs below lives here.\n\n\nResults\n\nFalse-positive rates for publication bias detection\nFirst, let’s consider the performance of PET and PEESE as tests for detecting publication bias. Here, a statistically significant estimate for the coefficient on the SE (for PET) or on \\(V\\) (for PEESE) is taken as evidence of small-sample bias. For that logic to hold, the tests should maintain the nominal error rates in the absence of publication bias.\nThe figure below depicts the Type-I error rates of the PET and PEESE tests when \\(\\pi = 1\\) (so no publication bias at all), for a one-sided test of \\(H_0: \\beta_1 \\leq 0\\) at the nominal level of \\(\\alpha = .05\\). Rejection rates are plotted for varying true mean effects, levels of heterogeneity, and sample size distributions. Separate colors are used for maximum sample sizes of 50 or 120.\n\n\n\n\n\n\n\n\n\nBoth tests are horribly mis-calibrated, tending to reject the null hypothesis far more often than they should. This happens because there is a non-zero correlation between \\(d\\) and \\(V\\), even in the absence of publication bias. Thus, it does not follow that rejecting \\(H_0: \\beta_1 \\leq 0\\) implies rejection of the hypothesis that there is no publication bias. (Sorry, that’s at least a triple negative!)\nHere’s the same graph, but using the SPET and SPEESE estimators:\n\n\n\n\n\n\n\n\n\nYes, this may be the World’s Most Boring Figure, but it does make clear that both the SPET and SPEESE tests maintain the correct Type-I error rate. (Any variation in rejection rates is just Monte Carlo error.) Thus, it seems pretty clear that if we want to test for small-sample bias, SPET or SPEESE should be used rather than PET or PEESE.\n\n\nBias of bias-corrected estimators\nNow let’s consider the performance of these methods as estimators of the population mean effect. Uri’s analysis focused on the bias of the estimators, meaning the difference between the average value of the estimator (across repeated samples) and the true parameter. The plot below depicts the expected level of PET, PEESE, and PET-PEESE as a function of the true mean effect, using the uniform distribution of studies and a maximum sample size of \\(n = 50\\):\n\n\n\n\n\n\n\n\n\nAll three of these estimators are pretty bad in terms of bias. In the absence of publication bias, they consistently under-estimate the true mean effect. With intermediate or strong publication bias, PET and PET-PEESE have a consistent downward bias. As an unconditional estimator, PEESE tends to have a positive bias when the true effect is small, but this decreases and becomes negative as the true effect increases. For all three estimators, bias increases as the degree of heterogeneity increases.\nHere is how these estimators compare to the modified SPET, SPEESE, and SPET-SPEESE estimators, as well as to the usual fixed-effect average with no correction for publication bias:\n\n\n\n\n\n\n\n\n\nIn the left column, we see that SPET and SPEESE are exactly unbiased (and SPET-SPEESE is nearly so) in the absence of selective publication. So is regular old fixed effect meta-analysis, of course. In the middle and right columns, studies are selected based partially or fully on statistical significance, and things get messy. Overall, there’s no consistent winner between PEESE versus SPEESE. At small or moderate levels of between-study heterogeneity, and when the true mean effect is small, PEESE, SPEESE, and SPET-SPEESE have fairly similar biases, but PEESE appears to have a slight edge. This seems to me to be nothing but a fortuitous accident, in that the bias induced by the correlation between \\(d\\) and \\(V\\) just happens to work in the right direction. Then, as the true mean effect increases, SPEESE and SPET-SPEESE start to edge out PEESE. This makes sense because the bias induced by the correlation between \\(d\\) and \\(V\\) will be larger when the true effect sizes are larger.\nThese trends seem mostly to hold for the other sample size distributions I examined too, although the biases of PEESE and PET-PEESE aren’t as severe when the maximum sample size is larger. You can see for yourself here:\n\nUniform distribution of studies, maximum sample size of 120\nMore small studies, maximum sample size of 50\nMore small studies, maximum sample size of 120\nMore large studies, maximum sample size of 50\nMore large studies, maximum sample size of 120\n\n\n\nAccuracy of bias-corrected estimators\nBias isn’t everything, of course. Now let’s look at the overall accuracy of these estimators, as measured by root mean squared error (RMSE). RMSE is a function of both bias and sampling variance, and so is one way to weigh an estimator that is biased but fairly precise against an estimator that is perfectly unbiased but noisy. The following chart plots the RMSE of all of the estimators (following the same layout as above, just with a different vertical axis):\n\n\n\n\n\n\n\n\n\nStarting in the left column where there’s no selective publication, we can see that the normal fixed-effect average has the smallest RMSE (and so is most accurate). The next most accurate is SPEESE, which uniformly beats out PEESE, PET-PEESE, SPET, and SPET-SPEESE. It’s worth noting, though, that there is a fairly large penalty for using SPEESE when it is unnecessary: even with a quite large sample of 100 studies, SPEESE still has twice the RMSE of the FE estimator.\nThe middle column shows these estimators’ RMSE when there is an intermediate degree of selective publication. Because of the “fortuitous accident” of how the correlation between \\(d\\) and \\(V\\) affects the PEESE estimator, it is more accurate than SPEESE for small values of the true mean effect. Its advantage is larger when heterogeneity is larger, and heterogeneity also affects the point (i.e., what true mean effect) at which SPEESE catches up with PEESE. Then at larger true mean effects, the accuracy of SPEESE continues to improve while the accuracy of PEESE degrades. It is also interesting to note that at this intermediate degree of selective publication, none of the other bias-correction estimators (PET-PEESE, SPET, SPET-SPEESE) compete with PEESE and SPEESE.\nFinally, the right column plots RMSE when there’s strong selective publication, so only statistically significant effects appear. Just as in the middle column, PEESE edges out SPEESE for smaller values of the true mean effect. For very small true effects, both of these estimators are edged out by PET-PEESE and SPET-SPEESE. This only holds over a very small range for the true mean effect though, and for true effects above that range these conditional estimators perform poorly—consistently worse than just using PEESE or SPEESE.\nHere are charts for the other sample size distributions:\n\nUniform distribution of studies, maximum sample size of 120\nMore small studies, maximum sample size of 50\nMore small studies, maximum sample size of 120\nMore large studies, maximum sample size of 50\nMore large studies, maximum sample size of 120\n\nThe trends that I’ve noted mostly seem to hold for the other sample size distributions (but correct me if you disagree! I’m getting kind of bleary-eyed at the moment…). One difference worth noting is that when the sample size distribution skews towards having more large studies, the accuracy of the regular fixed-effect estimator improves a bit. At intermediate degrees of selective publication, the fixed-effect estimator is consistently more accurate than SPEESE, and mostly more accurate than PEESE too. With strong selective publication, though, the FE estimator blows up just as before.\n\n\n\nConclusions, caveats, further thoughts\nWhere does this leave us? The one thing that seems pretty clear is that if the meta-analyst’s goal is to test for potential small-sample bias, then SPET or SPEESE should be used rather than PET or PEESE. Beyond that, we’re in a bit of a morass. None of the estimators consistently out-performs the others across the conditions of the simulation. It’s only under certain conditions that any of the bias-correction methods are more accurate than using the regular FE estimator, and those conditions aren’t easy to identify in a real data analysis because they depend on the degree of publication bias.\n\nCaveats\nThese findings are also pretty tentative because of the limitations of the simulation conditions examined here. The distribution of sample sizes seems to affect the relative accuracy of the estimators to a certain degree, but I’ve only looked at a limited set of possibilities, and also limited consideration to rather large meta-samples of 100 studies.\nAnother caveat is that the simulations are based on \\(d\\) estimates from a two-group, between-subjects design with no covariates. In many applications, there is considerably more diversity in study designs. A given meta-analysis might include two-group, post-test only designs as well as between-subjects designs with a pre-test covariate or with repeated measures, as well as two-group designs with multiple (or multi-dimensional) outcomes. All of this introduces further layers of complexity into the relationship between sample size, effect magnitude, and selective publication.\nA further, quite important caveat is that selective publication is not the only possible explanation for a correlation between effect size and sample sizes. In another recent post, Uri sketches a scenario where investigators choose sample size to achieve adequate power (so following best practice!) for predicted effect sizes. If 1) true effects are heterogeneous and 2) investigators’ predictions are correlated with true effect sizes, then a meta-analysis will have effect size estimates that are correlated with sample size even in the absence of publication bias. A blog post by Richard Morey illustrates another possibility that leads to effect-sample size correlation, in which resource constraints induce negative correlation between sample size and the reliability of the outcome measure.\n\n\nHold me hostage\nIt seems to me that one lesson we can draw from this is that these regression-based corrections are pretty meager as analytic methods. We need to understand the mechanism of selective publication in order to be able to correct for its consequences, but the regression-based corrections don’t provide direct information here (even though their performance depends on it!). I think this speaks to the need for methods that directly model the mechanism, which means turning to selection models and studying the distribution of p-values. Also, without bringing in other pieces of information (like p-values), it seems more or less impossible to tease apart selective publication from other possible explanations for effect-sample size correlation.\nIf I had to pick one of the regression-based bias-correction method to use in an application—as in, if you handcuffed me to my laptop and threatened to not let me go until I analyzed your effect sizes—then on the basis of these simulation exercises, I think I would probably go with SPEESE as a default, and perhaps also report PEESE, but I wouldn’t bother with any of the others. Even though SPEESE is less accurate than PEESE and some other estimators under certain conditions, on a practical level it seems kind of silly to use different estimators when testing for publication bias versus trying to correct for it. And whatever advantage that regular PEESE has over SPEESE strikes me as kind of like cheating—it relies on an induced correlation between \\(d\\) and \\(V\\) to gain an accuracy advantage under certain conditions, but that correlation causes big problems under other conditions.\nEven if you chained me to the laptop, I would also definitely include a caution that these estimators should be interpreted more as sensitivity analyses than as bias-corrected estimates of the overall mean effect. This is roughly in line with the conclusions of Inzlicht, Gervais, and Berkman (2015). From their abstract:\n\nOur simulations revealed that not one of the bias-correction techniques revealed itself superior in all conditions, with corrections performing adequately in some situations but inadequately in others. Such a result implies that meta-analysts ought to present a range of possible effect sizes and to consider them all as being possible.\n\nTheir conclusion was in reference to PET, PEESE, and PET-PEESE. Unfortunately, the tweaks of SPET and SPEESE don’t clarify the situation.\n\n\nOutstanding questions\nThese exercises have left me wondering about a couple of things, which I’ll just mention briefly:\n\nI haven’t calculated confidence interval coverage levels for these simulations. I should probably add that but need to move on at the moment.\nThe ever-popular Trim-and-Fill procedure is based on the assumption that a funnel plot will be symmetric in the absence of publication bias. This assumption won’t hold if there’s correlation between \\(d\\) and \\(V\\), and so it would be interesting to see if using a function of sample size (i.e., just the first term of \\(V\\)) could improve the performance of Trim-and-Fill.\nUnder the model examined here, the bias in PET, PEESE, SPET, and SPEESE comes from the fact that the relevant regression relationships aren’t actually linear under selective publication. I do wonder whether using some more flexible sort of regression model (perhaps including a non-linear term) could reduce bias. The trick would be to find something that’s still constrained enough so that bias improvements aren’t swamped by increased variance.\nMany of the applications that I am familiar with involve syntheses where some studies contribute multiple effect size estimates, which might also be inter-correlated. Very little work has examined how regression corrections like PET-PEESE perform in such settings (the only study I know of is Reed, 2015, which involves a specialized and I think rather unusual data-generating model). For that matter, I don’t know of any work that looks at other publication bias correction methods either. Or what selective publication even means in this setting. Somebody should really work on that.\n\n\n\n\n\n\n\n\nReuseCC BY 4.0CitationBibTeX citation:@online{pustejovsky2017,\n  author = {Pustejovsky, James E.},\n  title = {You Wanna {PEESE} of d’s?},\n  date = {2017-04-27},\n  url = {https://jepusto.com/posts/PET-PEESE-performance},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nPustejovsky, James E. 2017. “You Wanna PEESE of d’s?” April\n27, 2017. https://jepusto.com/posts/PET-PEESE-performance."
  },
  {
    "objectID": "posts/parallel-R-on-TACC/index.html",
    "href": "posts/parallel-R-on-TACC/index.html",
    "title": "Running R in parallel on the TACC",
    "section": "",
    "text": "UPDATE (4/8/2014): I have learned from Mr. Yaakoub El Khamra that he and the good folks at TACC have made some modifications to TACC’s custom MPI implementation and R build in order to correct bugs in Rmpi and snow that were causing crashes. This post has been updated to reflect the modifications.\nI’ve started to use the Texas Advanced Computing Cluster to run statistical simulations in R. It takes a little bit of time to get up and running, but once you do it is an amazing tool. To get started, you’ll need\n\nAn account on the TACC and an allocation of computing time.\nAn ssh client like PUTTY.\nSome R code that can be adapted to run in parallel.\nA SLURM script that tells the server (called Stampede) how to run the R.\n\n\nThe R script\nI’ve been running my simulations using a combination of several packages that provide very high-level functionality for parallel computing, namely foreach, doSNOW, and the maply function in plyr. All of this runs on top of an Rmpi implementation developed by the folks at TACC (more details here).\nIn an earlier post, I shared code for running a very simple simulation of the Behrens-Fisher problem. Here’s adapted code for running the same simulation on Stampede. The main difference is that there are a few extra lines of code to set up a cluster, seed a random number generator, and pass necessary objects (saved in source_func) to the nodes of the cluster:\n\nlibrary(Rmpi)\nlibrary(snow)\nlibrary(foreach)\nlibrary(iterators)\nlibrary(doSNOW)\nlibrary(plyr)\n\n# set up parallel processing\ncluster &lt;- getMPIcluster()\nregisterDoSNOW(cluster)\n\n# export source functions\nclusterExport(cluster, source_func)\n\nOnce it is all set up, running the code is just a matter of turning on the parallel option in mdply:\n\nBFresults &lt;- mdply(parms, .fun = run_sim, .drop=FALSE, .parallel=TRUE)\n\nI fully admit that my method of passing source functions is rather kludgy. One alternative would be to save all of the source functions in a separate file (say, source_functions.R), then source the file at the beginning of the simulation script:\n\nrm(list=ls())\nsource(\"source_functions.R\")\nprint(source_func &lt;- ls())\n\nAnother, more elegant alternative would be to put all of your source functions in a little package (say, BehrensFisher), install the package, and then pass the package in the maply call:\n\nBFresults &lt;- mdply(parms, .fun = run_sim, .drop=FALSE, .parallel=TRUE, .paropts = list(.packages=\"BehrensFisher\"))\n\nOf course, developing a package involves a bit more work on the front end.\n\n\nThe SLURM script\nSuppose that you’ve got your R code saved in a file called Behrens_Fisher.R. Here’s an example of a SLURM script that runs the R script after configuring an Rmpi cluster:\n\n#!/bin/bash\n#SBATCH -J Behrens          # Job name\n#SBATCH -o Behrens.o%j      # Name of stdout output file (%j expands to jobId)\n#SBATCH -e Behrens.o%j      # Name of stderr output file(%j expands to jobId)\n#SBATCH -n 32               # Total number of mpi tasks requested\n#SBATCH -p normal           # Submit to the 'normal' or 'development' queue\n#SBATCH -t 0:20:00          # Run time (hh:mm:ss)\n#SBATCH -A A-yourproject    # Allocation name to charge job against\n#SBATCH --mail-user=you@email.address # specify email address for notifications\n#SBATCH --mail-type=begin   # email when job begins\n#SBATCH --mail-type=end     # email when job ends\n\n# load R module\nmodule load Rstats           \n\n# call R code from RMPISNOW\nibrun RMPISNOW &lt; Behrens_Fisher.R \n\nThe file should be saved in a plain text file called something like run_BF.slurm. The file has to use ANSI encoding and Unix-type end-of-line encoding; Notepad++ is a text editor that can create files in this format.\nNote that for full efficiency, the -n option should be a multiple of 16 because their are 16 cores per compute node. Further details about SBATCH options can be found here.\n\n\nRunning on Stampede\nFollow these directions to log in to the Stampede server. Here’s the User Guide for Stampede. The first thing you’ll need to do is ensure that you’ve got the proper version of MVAPICH loaded. To do that, type\n\nmodule swap intel intel/14.0.1.106\nmodule setdefault\n\nThe second line sets this as the default, so you won’t need to do this step again.\nSecond, you’ll need to install whatever R packages you’ll need to run your code. To do that, type the following at the login4$ prompt:\n\nlogin4$module load Rstats\nlogin4$R\n\nThis will start an interactive R session. From the R prompt, use install.packages to download and install, e.g.\n\ninstall.packages(\"plyr\",\"reshape\",\"doSNOW\",\"foreach\",\"iterators\")\n\nThe packages will be installed in a local library. Now type q() to quit R.\nNext, make a new directory for your project:\n\nlogin4$mkdir project_name\nlogin4$cd project_name\n\nUpload your files to the directory (using psftp, for instance). Check that your R script is properly configured by viewing it in Vim.\nFinally, submit your job by typing\n\nlogin4$sbatch run_BF.slurm\n\nor whatever your SLURM script is called. To check the status of the submitted job, type showq -u followed by your TACC user name (more details here).\n\n\nFurther thoughts\nTACC accounts come with a limited number of computing hours, so you should be careful to write efficient code. Before you even start worrying about running on TACC, you should profile your code and try to find ways to speed up the computations. (Some simple improvements in my Behrens-Fisher code would make it run MUCH faster.) Once you’ve done what you can in terms of efficiency, you should do some small test runs on Stampede. For example, you could try running only a few iterations for each combination of factors, and/or running only some of the combinations rather than the full factorial design. Based on the run-time for these jobs, you’ll then be able to estimate how long the full code would take. If it’s acceptable (and within your allocation), then go ahead and sbatch the full job. If it’s not, you might reconsider the number of factor levels in your design or the number of iterations you need. I might have more comments about those some other time.\nComments? Suggestions? Corrections? Drop a comment.\n\n\n\n\n\n\nReuseCC BY 4.0CitationBibTeX citation:@online{pustejovsky2013,\n  author = {Pustejovsky, James E.},\n  title = {Running {R} in Parallel on the {TACC}},\n  date = {2013-12-20},\n  url = {https://jepusto.com/posts/parallel-R-on-TACC},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nPustejovsky, James E. 2013. “Running R in Parallel on the\nTACC.” December 20, 2013. https://jepusto.com/posts/parallel-R-on-TACC."
  },
  {
    "objectID": "posts/number-of-significant-effects/index.html",
    "href": "posts/number-of-significant-effects/index.html",
    "title": "Finding the distribution of significant effect sizes",
    "section": "",
    "text": "In basic meta-analysis, where each study contributes just a single effect size estimate, there has been a lot of work devoted to developing models for selective reporting. Most of these models formulate the selection process as a function of the statistical significance of the effect size estimate; some also allow for the possibility that the precision of the study’s effect influences the probability of selection (i.e., bigger studies are more likely to be reported, regardless of statistical significance).\nA problem that I’ve been mulling recently is how to think about selective reporting in meta-analyses that include some studies with multiple effect size estimates. This setting is quite a bit more complicated than basic meta-analysis because there are several different ways that selective reporting could happen. It could be that each effect size estimate is selected (or censored) individually, on the basis of its statistical significance. However, it seems just as plausible that the pattern of statistical significance across the full set of results could influence whether any of the results get selected.\nIn pondering this stuff, I’m trying to find ways to simplify the space of possibilities or formulate stylized (or “toy”) problems that are more tractable. Here is one such problem. I’ll write it as a question such as you might find in a problem set from a course on statistical distribution theory."
  },
  {
    "objectID": "posts/number-of-significant-effects/index.html#compound-symmetry",
    "href": "posts/number-of-significant-effects/index.html#compound-symmetry",
    "title": "Finding the distribution of significant effect sizes",
    "section": "Compound symmetry",
    "text": "Compound symmetry\nIn general, the distribution of \\(N_A\\) will depend on the joint distribution of \\((T_1,...,T_m)\\), so we will need to make some further assumptions regarding that joint distribution in order to make progress here. One simplifying assumption that seems worth considering is that the effect size estimates follow a compound symmetric distribution. Specifically, assume that all of the effect size estimates have equal sampling variance, \\(V_1 = V_2 = \\cdots = V_m = V\\), and that there is a constant correlation between every pair of effect size estimates: \\[\\text{Cov}(T_h, T_i) = \\rho V\\] for some correlation \\(\\rho\\). Further, assume that the true effect sizes vary based on a compound symmetric distribution where \\[\n\\theta_i \\sim N(\\mu, \\omega^2).\n\\] All of this implies that the joint distribution of the effect size estimates is compound symmetric: \\[\n\\left(\\begin{array}{c} T_1 \\\\ T_2 \\\\ \\vdots \\\\ T_m \\end{array}\\right) \\sim N\\left[ \\mu \\mathbf{1}_m, \\ \\left(\\omega^2 + \\rho V\\right)\\mathbf{J}_m + (1 - \\rho) V \\mathbf{I}_m \\right],\n\\] where \\(\\mathbf{1}_m\\) is an \\(m \\times 1\\) vector of 1’s, \\(\\mathbf{J}_m\\) is an \\(m \\times m\\) matrix of 1’s, and \\(\\mathbf{I}_m\\) is an \\(m \\times m\\) identity matrix.\nGiven the above assumptions, What is the distribution of \\(N_A\\)?\nPlease write to me if you’d like to discuss the theory or implications of this problem."
  },
  {
    "objectID": "posts/Multivariate-delta-method/index.html",
    "href": "posts/Multivariate-delta-method/index.html",
    "title": "The multivariate delta method",
    "section": "",
    "text": "The delta method is surely one of the most useful techniques in classical statistical theory. It’s perhaps a bit odd to put it this way, but I would say that the delta method is something like the precursor to the bootstrap, in terms of its utility and broad range of applications—both are “first-line” tools for solving statistical problems. There are many good references on the delta-method, ranging from the Wikipedia page to a short introduction in The American Statistician (Oehlert, 1992). Many statistical theory textbooks also include a longer or shorter discussion of the method (e.g., Stuart & Ord, 1996; Casella & Berger, 2002).\nI use the delta method all the time in my work, especially to derive approximations to the sampling variance of some estimator (or covariance between two estimators). Here I’ll give one formulation of the multivariate delta method that I find particularly useful for this purpose. (This is nothing at all original. I’m only posting it on the off chance that others might find my crib notes helpful—and by “others” I mostly mean myself in six months…)\n\nMulti-variate delta method covariances\nSuppose that we have a \\(p\\)-dimensional vector of statistics \\(\\mathbf{T} = \\left(T_1,...,T_p \\right)\\) that converge in distribution to the parameter vector \\(\\boldsymbol\\theta = \\left(\\theta_1,...,\\theta_p\\right)\\) and have asymptotic covariance matrix \\(\\boldsymbol\\Sigma / n\\), i.e.,\n\\[\n\\sqrt{n} \\left(\\mathbf{T} - \\boldsymbol\\theta\\right) \\stackrel{D}{\\rightarrow} N\\left( \\mathbf{0}, \\boldsymbol\\Sigma \\right).\n\\]\nNow consider two functions \\(f\\) and \\(g\\), both of which take vectors as inputs, return scalar quantities, and don’t have funky (discontinuous) derivatives. The asymptotic covariance between \\(f(\\mathbf{T})\\) and \\(g(\\mathbf{T})\\) is then approximately\n\\[\n\\text{Cov} \\left(f(\\mathbf{T}), g(\\mathbf{T}) \\right) \\approx \\frac{1}{n} \\sum_{j=1}^p \\sum_{k=1}^p  \\frac{\\partial f}{ \\partial \\theta_j}\\frac{\\partial g}{ \\partial \\theta_k}\\sigma_{jk},\n\\]\nwhere \\(\\sigma_{jk}\\) is the entry in row \\(j\\) and column \\(k\\) of the matrix \\(\\boldsymbol\\Sigma\\). If the entries of \\(\\mathbf{T}\\) are asymptotically uncorrelated , then this simplifies to\n\\[\n\\text{Cov} \\left(f(\\mathbf{T}), g(\\mathbf{T}) \\right) \\approx \\frac{1}{n} \\sum_{j=1}^p \\frac{\\partial f}{ \\partial \\theta_j}\\frac{\\partial g}{ \\partial \\theta_j} \\sigma_{jj}.\n\\]\nIf we are interested in the variance of a single statistic, then the above formulas simplify further to\n\\[\n\\text{Var} \\left(f(\\mathbf{T})\\right) \\approx \\frac{1}{n} \\sum_{j=1}^p \\sum_{k=1}^p  \\frac{\\partial f}{ \\partial \\theta_j}\\frac{\\partial f}{ \\partial \\theta_k}\\sigma_{jk}\n\\]\nor\n\\[\n\\text{Var} \\left(f(\\mathbf{T}) \\right) \\approx \\frac{1}{n}\\sum_{j=1}^p \\left(\\frac{\\partial f}{ \\partial \\theta_j}\\right)^2 \\sigma_{jj}\n\\]\nin the case of uncorrelated \\(\\mathbf{T}\\).\nFinally, if we are dealing with a univariate transformation \\(f(\\theta)\\), then of course the above simplifies even further to\n\\[\n\\text{Var}\\left(f(T)\\right) = \\left(\\frac{\\partial f}{\\partial \\theta}\\right)^2 \\text{Var}(T)\n\\]\n\n\nPearson’s \\(r\\)\nThese formulas are useful for all sorts of things. For example, they can be used to derive the sampling variance of Pearson’s correlation coefficient. Suppose we have a simple random sample of \\(n\\) observations from a multivariate normal distribution with mean 0 and variance-covariance matrix \\(\\boldsymbol\\Phi = \\left[\\begin{array}{cc}\\phi_{xx} & \\phi_{xy} \\\\ \\phi_{xy} & \\phi_{yy} \\end{array}\\right]\\). Pearson’s correlation is calculated as\n\\[\nr = \\frac{s_{xy}}{\\sqrt{s_{xx} s_{yy}}},\n\\]\nwhere \\(s_{xx}\\) and \\(s_{yy}\\) are sample variances and \\(s_{xy}\\) is the sample covariance. These sample variances and covariances are unbiased estimates of \\(\\phi_{xx}\\), \\(\\phi_{yy}\\), and \\(\\phi_{xy}\\), respectively. So in terms of the above notation, we have \\(\\mathbf{T} = \\left(s_{xx}, s_{yy}, s_{xy}\\right)\\), \\(\\boldsymbol\\theta = \\left(\\phi_{xx}, \\phi_{yy}, \\phi_{xy}\\right)\\), and \\(\\rho = \\phi_{xy} / \\sqrt{\\phi_{xx} \\phi_{yy}}\\).\nFrom a previous post, we can work out the variance-covariance matrix of \\(\\mathbf{T}\\):\n\\[\n\\text{Var}\\left(\\sqrt{n - 1} \\left[\\begin{array}{c} s_{xx} \\\\ s_{yy} \\\\ s_{xy}\\end{array}\\right]\\right) = \\boldsymbol\\Sigma = \\left[\\begin{array}{ccc} 2 \\phi_{xx}^2 & & \\\\ 2 \\phi_{xy}^2 & 2 \\phi_{yy}^2 & \\\\ 2 \\phi_{xy} \\phi_{xx} & 2 \\phi_{xy} \\phi_{yy} & \\phi_{xy}^2 + \\phi_{xx} \\phi_{yy}\\end{array}\\right].\n\\]\nThe last piece is to find the derivatives of \\(r\\) with respect to \\(\\mathbf{T}\\):\n\\[\n\\begin{aligned}\n\\frac{\\partial r}{\\partial \\phi_{xy}} &= \\phi_{xx}^{-1/2} \\phi_{yy}^{-1/2} \\\\\n\\frac{\\partial r}{\\partial \\phi_{xx}} &= -\\frac{1}{2} \\phi_{xy} \\phi_{xx}^{-3/2} \\phi_{yy}^{-1/2} \\\\\n\\frac{\\partial r}{\\partial \\phi_{yy}} &= -\\frac{1}{2} \\phi_{xy} \\phi_{xx}^{-1/2} \\phi_{yy}^{-3/2}\n\\end{aligned}\n\\]\nPutting the pieces together, we have\n\\[\n\\begin{aligned}\n(n - 1) \\text{Var}(r) &\\approx \\sigma_{11} \\left(\\frac{\\partial r}{\\partial \\phi_{xy}}\\right)^2 + \\sigma_{22} \\left(\\frac{\\partial r}{ \\partial \\phi_{xx}}\\right)^2 + \\sigma_{33} \\left(\\frac{\\partial r}{ \\partial \\phi_{yy}}\\right)^2 \\\\\n& \\qquad \\qquad + 2 \\sigma_{12} \\frac{\\partial r}{\\partial \\phi_{xy}}\\frac{\\partial r}{\\partial \\phi_{xx}} + 2 \\sigma_{13} \\frac{\\partial r}{\\partial \\phi_{xy}}\\frac{\\partial r}{\\partial \\phi_{yy}}+ 2 \\sigma_{23} \\frac{\\partial r}{\\partial \\phi_{xx}}\\frac{\\partial r}{\\partial \\phi_{yy}} \\\\\n&= \\frac{\\phi_{xy}^2 + \\phi_{xx} \\phi_{yy}}{\\phi_{xx} \\phi_{yy}} + \\frac{\\phi_{xy}^2\\phi_{xx}^2}{2 \\phi_{xx}^3 \\phi_{yy}} + \\frac{\\phi_{xy}^2\\phi_{yy}^2}{2 \\phi_{xx} \\phi_{yy}^3} \\\\\n& \\qquad \\qquad - \\frac{2\\phi_{xy} \\phi_{xx}}{\\phi_{xx}^2 \\phi_{yy}} - \\frac{2\\phi_{xy} \\phi_{yy}}{\\phi_{xx} \\phi_{yy}^2} + \\frac{\\phi_{xy}^4}{\\phi_{xx}^2 \\phi_{yy}^2} \\\\\n&= 1 - 2\\frac{\\phi_{xy}^2}{\\phi_{xx} \\phi_{yy}} + \\frac{\\phi_{xy}^4}{\\phi_{xx}^2 \\phi_{yy}^2} \\\\\n&= \\left(1 - \\rho^2\\right)^2.\n\\end{aligned}\n\\]\n\n\nFisher’s \\(z\\)-transformation\nMeta-analysts will be very familiar with Fisher’s \\(z\\)-transformation of \\(r\\), given by \\(z(\\rho) = \\frac{1}{2} \\log\\left(\\frac{1 + \\rho}{1 - \\rho}\\right)\\). Fisher’s \\(z\\) is the variance-stabilizing (and also normalizing) transformation of \\(r\\), meaning that \\(\\text{Var}\\left(z(r)\\right)\\) is approximately a constant function of sample size, not depending on the degree of correlation \\(\\rho\\). We can see this using another application of the delta method:\n\\[\n\\frac{\\partial z}{\\partial \\rho} = \\frac{1}{1 - \\rho^2}.\n\\]\nThus,\n\\[\n\\text{Var}\\left(z(r)\\right) \\approx \\frac{1}{(1 - \\rho^2)^2} \\times \\text{Var}(r) = \\frac{1}{n - 1}.\n\\]\nThe variance of \\(z\\) is usually given as \\(1 / (n - 3)\\), which is even closer to exact. Here we’ve obtained the variance of \\(z\\) using two applications of the delta-method. Because of the chain rule, we’d have ended up with the same result if we’d gone straight from the sample variances and covariances, using the multivariate delta method and the derivatives of \\(z\\) with respect to \\(\\boldsymbol\\theta\\).\n\n\nCovariances between correlations\nThese same techniques can be used to work out expressions for the covariances between correlations estimated on the same sample. For instance, suppose you’ve measured four variables, \\(W\\), \\(X\\), \\(Y\\), and \\(Z\\), on a simple random sample of \\(n\\) observations. What is \\(\\text{Cov}(r_{xy}, r_{xz})\\)? What is \\(\\text{Cov}(r_{wx}, r_{yz})\\)? I’ll leave the derivations for you to work out. See Steiger (1980) for solutions.\n\n\n\n\n\n\nReuseCC BY 4.0CitationBibTeX citation:@online{pustejovsky2018,\n  author = {Pustejovsky, James E.},\n  title = {The Multivariate Delta Method},\n  date = {2018-04-11},\n  url = {https://jepusto.com/posts/Multivariate-delta-method},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nPustejovsky, James E. 2018. “The Multivariate Delta\nMethod.” April 11, 2018. https://jepusto.com/posts/Multivariate-delta-method."
  },
  {
    "objectID": "posts/measurement-comparable-effect-sizes/index.html",
    "href": "posts/measurement-comparable-effect-sizes/index.html",
    "title": "New article: Measurement-comparable effect sizes for single-case studies of free-operant behavior",
    "section": "",
    "text": "My article “Measurement-comparable effect sizes for single-case studies of free-operant behavior” has been accepted at Psychological Methods. Postprint and supporting materials are available. Here’s the abstract:\nSingle-case research comprises a set of designs and methods for evaluating the effects of interventions, practices, or programs on individual cases, through comparison of outcomes measured at different points in time. Although there has long been interest in meta-analytic technique for synthesizing single-case research, there has been little scrutiny of whether proposed effect sizes remain on a directly comparable metric when outcomes are measured using different operational procedures. Much of single-case research focuses on behavioral outcomes in free-operant contexts, which may be measured using a variety of different direct observation procedures. This article describes a suite of effect sizes for quantifying changes in free-operant behavior, motivated by an alternating renewal process model that allows measurement comparability to be established in precise terms. These effect size metrics have the advantage of comporting with how direct observation data are actually collected and summarized. Effect size estimators are proposed that are applicable when the behavior being measured remains stable within a given treatment condition. The methods are illustrated by two examples, including a re-analysis of a systematic review of the effects of choice-making opportunities on problem behavior.\n\n\n\n\n\nReuseCC BY 4.0CitationBibTeX citation:@online{pustejovsky2014,\n  author = {Pustejovsky, James E.},\n  title = {New Article: {Measurement-comparable} Effect Sizes for\n    Single-Case Studies of Free-Operant Behavior},\n  date = {2014-02-04},\n  url = {https://jepusto.com/posts/measurement-comparable-effect-sizes},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nPustejovsky, James E. 2014. “New Article: Measurement-Comparable\nEffect Sizes for Single-Case Studies of Free-Operant Behavior.”\nFebruary 4, 2014. https://jepusto.com/posts/measurement-comparable-effect-sizes."
  },
  {
    "objectID": "posts/inverting-partitioned-matrices/index.html",
    "href": "posts/inverting-partitioned-matrices/index.html",
    "title": "Inverting partitioned matrices",
    "section": "",
    "text": "There’s lots of linear algebra out there that’s quite useful for statistics, but that I never learned in school or never had cause to study in depth. In the same spirit as my previous post on the Woodbury identity, I thought I would share my notes on another helpful bit of math about matrices. At some point in high school or college, you might have learned how to invert a small matrix by hand. You might recall the formula for the inverse of a two-by-two matrix: \\[\n\\left[\\begin{array}{cc} a & b \\\\ c & d\\end{array}\\right]^{-1} = \\frac{1}{ad - bc}\\left[\\begin{array}{rr} d & -b \\\\ -c & a\\end{array}\\right].\n\\] It turns out that there’s a straight-forward generalization of this formula to matrices of arbitrary size, but that are partitioned into four pieces. The following is based on the presentation from some old notes by Dr. Thomas Minka, Old and New Matrix Algebra Useful for Statistics. The statement there is quite detailed and general. My version will be for a more specific, simple case, which I’ve found to be common and handy, and that can be presented in a fairly simple form.\nLet \\(\\mathbf{P}\\) be a matrix of arbitrary size that is composed of four sub-matrices: \\[\n\\mathbf{P} = \\left[\\begin{array}{cc} \\mathbf{A} & \\mathbf{B} \\\\ \\mathbf{C} & \\mathbf{D}\\end{array}\\right],\n\\] where \\(\\mathbf{A}\\) and \\(\\mathbf{D}\\) are \\(a \\times a\\) and \\(d \\times d\\) matrices, both of which are invertible, and where \\(\\mathbf{B}\\) and \\(\\mathbf{C}\\) are of conformable dimension.1 Let \\(\\mathbf{X} = \\left(\\mathbf{D} - \\mathbf{C}\\mathbf{A}^{-1} \\mathbf{B}\\right)^{-1}\\), a \\(d \\times d\\) matrix. Then \\[\n\\mathbf{P}^{-1} = \\left[\\begin{array}{cc} \\mathbf{A}^{-1} + \\mathbf{A}^{-1} \\mathbf{B} \\mathbf{X} \\mathbf{C} \\mathbf{A}^{-1} & - \\mathbf{A}^{-1} \\mathbf{B} \\mathbf{X} \\\\ - \\mathbf{X} \\mathbf{C} \\mathbf{A}^{-1} & \\mathbf{X}\\end{array}\\right].\n\\] This representation is particularly helpful if \\(d &lt; a\\), because in this case \\(\\mathbf{X}\\) is of lower dimension and so simpler (in a sense) than \\(\\mathbf{A}^{-1}\\).\nAnother equivalency is more helpful when \\(d &gt; a\\). Here, take \\(\\mathbf{W} = \\left(\\mathbf{A} - \\mathbf{B}\\mathbf{D}^{-1} \\mathbf{C}\\right)^{-1}\\), an \\(a \\times a\\) matrix (and so of lower dimension than \\(\\mathbf{D}\\)). Then \\[\n\\mathbf{P}^{-1} = \\left[\\begin{array}{cc} \\mathbf{W} & - \\mathbf{W} \\mathbf{B} \\mathbf{D}^{-1} \\\\ - \\mathbf{D}^{-1} \\mathbf{C} \\mathbf{W} & \\mathbf{D}^{-1} + \\mathbf{D}^{-1} \\mathbf{C} \\mathbf{W} \\mathbf{B} \\mathbf{D}^{-1}\\end{array}\\right].\n\\] Of course, this is just two ways of writing the same thing. You can see this by applying everyone’s favorite matrix identity to find that \\(\\mathbf{W} = \\mathbf{A}^{-1} + \\mathbf{A}^{-1} \\mathbf{B} \\mathbf{X} \\mathbf{C} \\mathbf{A}^{-1}\\) and \\(\\mathbf{X} = \\mathbf{D}^{-1} + \\mathbf{D}^{-1} \\mathbf{C} \\mathbf{W} \\mathbf{B} \\mathbf{D}^{-1}\\). It is an interesting little algebraic exercise to show that \\(\\mathbf{W} \\mathbf{B} \\mathbf{D}^{-1} = \\mathbf{A}^{-1} \\mathbf{B} \\mathbf{X}\\) and that \\(\\mathbf{D}^{-1} \\mathbf{C} \\mathbf{W} = \\mathbf{X} \\mathbf{C} \\mathbf{A}^{-1}\\).\nThese representations of \\(\\mathbf{P}^{-1}\\) are useful for a variety of statistical problems. To give just one example, they lead to a very direct proof of the Frisch-Waugh-Lovell theorem, including under more general conditions than are usually stated."
  },
  {
    "objectID": "posts/inverting-partitioned-matrices/index.html#footnotes",
    "href": "posts/inverting-partitioned-matrices/index.html#footnotes",
    "title": "Inverting partitioned matrices",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nMinka’s notes on partitioned matrices treat a more general case, in which \\(\\mathbf{A}\\) and \\(\\mathbf{D}\\) need not be square matrices, nor must they be invertible.↩︎"
  },
  {
    "objectID": "posts/IES-2016-PI-meeting/index.html",
    "href": "posts/IES-2016-PI-meeting/index.html",
    "title": "Presentation at IES 2016 PI meeting",
    "section": "",
    "text": "I am just back from the Institute of Education Sciences 2016 Principal Investigators meeting. Rob Horner had organized a session titled “Single-case methods: Current status and needed directions” as a tribute to our colleague Will Shadish, who passed away this past year. Rob invited me to give some brief remarks about Will as a mentor, and then to present some of my work with Will and Larry Hedges on effect sizes for single-case research. Here are the slides from my part of the presentation.\n\n\n\n\n\nReuseCC BY 4.0CitationBibTeX citation:@online{pustejovsky2016,\n  author = {Pustejovsky, James E.},\n  title = {Presentation at {IES} 2016 {PI} Meeting},\n  date = {2016-12-19},\n  url = {https://jepusto.com/posts/IES-2016-PI-meeting},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nPustejovsky, James E. 2016. “Presentation at IES 2016 PI\nMeeting.” December 19, 2016. https://jepusto.com/posts/IES-2016-PI-meeting."
  },
  {
    "objectID": "posts/gradual-effects-model-paper/index.html",
    "href": "posts/gradual-effects-model-paper/index.html",
    "title": "New paper: A gradual effects model for single-case designs",
    "section": "",
    "text": "I’m very happy to share a new paper, co-authored with my student Danny Swan, “A gradual effects model for single-case designs,” which is now available online at Multivariate Behavioral Research. You can access the published version at the journal website (click here for free access while supplies last) or the pre-print on PsyArxiv (always free!). Here’s the abstract and the supplementary materials. Danny wrote R functions for fitting the model, (available as part of the SingleCaseES package) as well as a slick web interface, if you prefer to point-and-click.\nThis paper grew out of Danny’s qualifying process (QP), which is the major exam that our doctoral students have to pass before they can begin their dissertation work. For the QP, students work with a faculty advisor to develop an extensive literature review and proposal for an original research project. They produce a written research proposal, then take written and oral exams on their work. For Danny’s QP, he picked up one of the many loose ends in my dissertation, studied up on generalized linear models to understand how to express and fit the model, and developed a simulation study evaluating the model. After he successfully passed his QP, we worked together to refine the estimation methods and the simulation design, and then draft a manuscript (much of it cribbed from his QP proposal). I’m very proud and pleased that Danny continued to develop the work and saw it through to a first-authored publication.\n\n\n\n\n\nReuseCC BY 4.0CitationBibTeX citation:@online{pustejovsky2018,\n  author = {Pustejovsky, James E.},\n  title = {New Paper: {A} Gradual Effects Model for Single-Case Designs},\n  date = {2018-05-14},\n  url = {https://jepusto.com/posts/gradual-effects-model-paper},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nPustejovsky, James E. 2018. “New Paper: A Gradual Effects Model\nfor Single-Case Designs.” May 14, 2018. https://jepusto.com/posts/gradual-effects-model-paper."
  },
  {
    "objectID": "posts/Getting-started-with-ARPobservation/index.html",
    "href": "posts/Getting-started-with-ARPobservation/index.html",
    "title": "Getting started with ARPobservation",
    "section": "",
    "text": "UPDATED 5/29/2014 after posting the package to CRAN\nHere are step-by-step instructions on how to download and install ARPobservation. For the time being, ARPobservation is available as a pre-compiled binary for Windows. For Mac/Linux, you’ll have to download the source from Github.\n\nDownload and install R. R is free, open-source software that is used by many data analysts and statisticians. ARPobservation is a contributed package that runs within R, so you’ll need to get the base software first.\n(Optional but recommended) Download and install RStudio, which is a very nice front-end interface to R.\nOpen R or RStudio and type the following sequence of commands in the console:\n\n::: {.cell}\ninstall.packages(\"ARPobservation\")\nlibrary(ARPobservation)\n:::\nYou’ll only need to do the above once. Once you’ve got the package installed, type the following in order to access the package within an R session: library(ARPobservation).\nTo open the package documentation, type package?ARPobservation. To access the documentation for an individual function in this package, just type ? followed by the name of the function. For instance, one of the main functions in the package is called r_behavior_stream; to access its documentation, type ?r_behavior_stream.\n\n\n\n\n\nReuseCC BY 4.0CitationBibTeX citation:@online{pustejovsky2013,\n  author = {Pustejovsky, James E.},\n  title = {Getting Started with {ARPobservation}},\n  date = {2013-10-24},\n  url = {https://jepusto.com/posts/Getting-started-with-ARPobservation},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nPustejovsky, James E. 2013. “Getting Started with\nARPobservation.” October 24, 2013. https://jepusto.com/posts/Getting-started-with-ARPobservation."
  },
  {
    "objectID": "posts/from-longhorn-to-badger/index.html",
    "href": "posts/from-longhorn-to-badger/index.html",
    "title": "From Longhorn to Badger",
    "section": "",
    "text": "It’s taken me a while to finally get around to updating my website with some personal news. I’ve moved from UT Austin to the UW Madison School of Education, where I am now an associate professor in the Educational Psychology Department’s Quantitative Methods program. We left Austin at the very end of July, arriving in Madison on August 1st. Our moving truck took a bit longer to arrive, but we’re now more or less installed in our new (or rather old–1950’s era) home. I grew up in Wisconsin (in the Milwaukee area), so this move brings us much closer to my family, who have already come to visit. We’ve also already been enjoying the fantastic bike paths and facilities that Madison has to offer.\nOn a professional level, I’m very much looking forward to the opportunities that the School of Education and Educational Psychology Department present, especially to opportunities for collaboration with new colleagues and students. I’m planning to offer a course on research synthesis and meta-analysis this coming Spring semester—something I’ve never had the opportunity to teach in a semester-long format, actually—and I’m looking forward to offering my own pedagogical perspective on material that I think about constantly in a research context. Gene Glass, who is credited as the originator of the term meta-analysis and who conducted some of the first meta-analyses within the social sciences, received his Ph.D. in Educational Psychology from UW Madison in 1965, so perhaps I’ll be able to channel a bit of his spirit in my course.\nEven as I’m excited to get started at Madison, I will also very much miss my colleagues at UT Austin, who were so supportive during my pre-tenure phase. Because of COVID, I didn’t really get to say a proper farewell before we skipped town. I am continuing to advise several doctoral students in the Quantitative Methods program though, so we will likely get to connect in video meetings, at least.\nMoving during the COVID pandemic has presented some challenges (logistical, emotional, and family-related) for me, though I know many others have had to deal with far worse. As we all weather this together, please feel free to leave a comment or drop me a line if you’d like to talk about stats, meta-analysis, R programming, or what-not.\n\n\n\n\n\nReuseCC BY 4.0CitationBibTeX citation:@online{pustejovsky2020,\n  author = {Pustejovsky, James E.},\n  title = {From {Longhorn} to {Badger}},\n  date = {2020-08-28},\n  url = {https://jepusto.com/posts/from-longhorn-to-badger},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nPustejovsky, James E. 2020. “From Longhorn to Badger.”\nAugust 28, 2020. https://jepusto.com/posts/from-longhorn-to-badger."
  },
  {
    "objectID": "posts/effective-sample-size-aggregation/index.html",
    "href": "posts/effective-sample-size-aggregation/index.html",
    "title": "Effective sample size aggregation",
    "section": "",
    "text": "In settings with independent observations, sample size is one way to quickly characterize the precision of an estimate. But what if your estimate is based on weighted data, where each observation doesn’t necessarily contribute to equally to the estimate? Here, one useful way to gauge the precision of an estimate is the effective sample size or ESS. Suppose that we have \\(N\\) independent observations \\(Y_1,...,Y_N\\) drawn from a population with standard deviation \\(\\sigma\\), and that observation \\(i\\) receives weight \\(w_i\\). We take the weighted sample mean \\[\n\\tilde{y} = \\frac{1}{W} \\sum_{i=1}^N w_i Y_i, \\qquad \\text{where} \\qquad W = \\sum_{i=1}^N w_i.\n\\] with sampling variance \\[\n\\text{Var}(\\tilde{y}) = \\frac{\\sigma^2}{W^2} \\sum_{i=1}^N w_i^2.\n\\]\nThe ESS is the number of observations from an equally weighted sample that would yield the same level of precision as the weighted sample mean. In an equally weighted sample of size \\(\\tilde{N}\\), the variance would be simply \\(\\sigma^2 / \\tilde{N}\\), and so ESS is the value of \\(\\tilde{N}\\) that solves \\[\n\\frac{\\sigma^2}{\\tilde{N}} = \\frac{\\sigma^2}{W^2} \\sum_{i=1}^N w_i^2.\n\\]\nRe-arranging, the ESS is thus defined as \\[\n\\tilde{N} = \\frac{W^2}{\\sum_{i=1}^N w_i^2}.\n\\]\nThe ESS is reported in several packages for propensity score weighting, including twang and optweight. In the propensity score context, ESS is a useful measure for comparing different sets of estimated propensity weights, in that weights (or propensity score models/matching methods) that have a larger ESS will yield a more precise estimate of a treatment effect. Given two sets of weights that achieve equivalent degrees of balance, the weights with larger ESS are thus preferable. Methods introduced by Zubizarreta (2015)—and implemented in the optweight package—take this logic a step further by using ESS as an objective function to be minimized, subject to specified balancing constraints.\n\nMulti-site effective sample size\nTwo of my recent projects have involved applying propensity score weighting methods in multi-site settings, where we are interested in estimating site-specific treatment effects as well as an overall aggregate effect. It is straight-forward to calculate an ESS for each site, but how then should we aggregate the ESS across sites to characterize the precision of the overall estimate? Several times now, I have found myself having to re-derive the aggregated ESS, and so I am going to work through it here now so as to save future-me (and perhaps you, dear reader) some time.\nSuppose that we have \\(J\\) sites, \\(n_j\\) observations from site \\(j\\) for \\(j = 1,...,J\\), and total sample size \\(N = \\sum_{j=1}^J n_j\\). Observation \\(i\\) from site \\(j\\) has outcome \\(Y_{ij}\\) and weight \\(w_{ij}\\). The site-specific weighted average at site \\(j\\) is then \\[\n\\tilde{y}_j = \\frac{1}{W_j} \\sum_{i=1}^{n_j} w_{ij} Y_{ij}, \\qquad \\text{where} \\qquad W_j = \\sum_{i=1}^{n_j} w_{ij}\n\\] and the overall average is \\[\n\\tilde{y} = \\frac{1}{N} \\sum_{j=1}^J n_j \\ \\tilde{y}_j = \\frac{1}{N} \\sum_{j=1}^J \\sum_{i=1}^{n_j} \\frac{n_j w_{ij}}{W_j} Y_{ij}.\n\\]\nFor calculating the overall average, observation \\(i\\) from unit \\(j\\) contributes weight \\(u_{ij} = n_j w_{ij} / W_j\\).\nUsing these unit-specific weights, the effective sample size for the overall average is \\[\nESS = \\frac{N^2}{\\sum_{j=1}^J \\sum_{i=1}^{n_j} u_{ij}^2}.\n\\] We can also define a site-specific ESS for site \\(j\\): \\[\nESS_j = \\frac{W_j^2}{\\sum_{i=1}^{n_j} w_{ij}^2}.\n\\]\nUsing the decomposition of the weights as \\(u_{ij} = n_j w_{ij} / W_j\\), the overall ESS can be written as \\[\nESS = \\frac{N^2}{\\sum_{j=1}^J n_j^2 \\left(\\sum_{i=1}^{n_j} w_{ij}^2 / W_j^2\\right)}.\n\\] Noting that the term in the parentheses of the denominator is equivalent to \\(1 / ESS_j\\), the overall ESS can therefore be written in terms of the site-specific ESSs and sample sizes: \\[\nESS = \\frac{N^2}{\\sum_{j=1}^J n_j^2 / ESS_j}.\n\\]\nThere you go. Future me will thank me for this!\n\n\n\n\n\n\nReuseCC BY 4.0CitationBibTeX citation:@online{pustejovsky2019,\n  author = {Pustejovsky, James E.},\n  title = {Effective Sample Size Aggregation},\n  date = {2019-01-22},\n  url = {https://jepusto.com/posts/effective-sample-size-aggregation},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nPustejovsky, James E. 2019. “Effective Sample Size\nAggregation.” January 22, 2019. https://jepusto.com/posts/effective-sample-size-aggregation."
  },
  {
    "objectID": "posts/double-poisson-in-Stan/index.html",
    "href": "posts/double-poisson-in-Stan/index.html",
    "title": "Implementing Efron’s double Poisson distribution in Stan",
    "section": "",
    "text": "\\[\n\\def\\Pr{{\\text{Pr}}}\n\\def\\E{{\\text{E}}}\n\\def\\Var{{\\text{Var}}}\n\\def\\Cov{{\\text{Cov}}}\n\\def\\bm{\\mathbf}\n\\def\\bs{\\boldsymbol}\n\\]\nFor a project I am working on, we are using Stan to fit generalized random effects location-scale models to a bunch of count data. We’re interested in using the double-Poisson distribution, as described by Efron (1986). This is an interesting distribution because it admits for both over- and under-dispersion relative to the Poisson distribution, whereas most of the conventional alternatives such as the negative binomial distribution or Poisson-normal mixture distribution allow only for over-dispersion. The double-Poisson distribution is not implemented in Stan, so we’ve had to write our own distribution function. That’s fine and not particularly difficult. What’s a bit more of a challenge is writing Stan functions to generate random samples from the double-Poisson, so that we can generate posterior predictive checks.1 In this post, I’ll walk through the implementation of the custom distribution functions needed to use the double-Poisson in Stan. The gamlss.dist package provides a full set of distributional functions for the double-Poisson distribution, including a sampler. Thus, I can validate my Stan functions against the functions from gamlss.dist.2\nCode\nlibrary(tidyverse)\nlibrary(patchwork)   # composing figures\nlibrary(gamlss.dist) # DPO distribution functions\nlibrary(rstan)       # Stan interface to R\nlibrary(brms)        # fitting generalized linear models\nlibrary(bayesplot)   # Examine fitted models\nlibrary(loo)         # Model fit measures"
  },
  {
    "objectID": "posts/double-poisson-in-Stan/index.html#the-double-poisson",
    "href": "posts/double-poisson-in-Stan/index.html#the-double-poisson",
    "title": "Implementing Efron’s double Poisson distribution in Stan",
    "section": "The double-Poisson",
    "text": "The double-Poisson\nThe double-Poisson distribution is a discrete distribution for non-negative counts, with support \\(\\mathcal{S}_X = \\{0, 1, 2, 3, ...\\}\\). The mean-variance relationship of the double-Poisson is approximately constant; for \\(X \\sim DPO(\\mu, \\phi)\\), \\(\\text{E}(X) \\approx \\mu\\) and \\(\\text{Var}(X) \\approx \\mu / \\phi\\), so that the double-Poisson distribution approximately satisfies the assumptions of a quasi-Poisson generalized linear model (although not quite exactly so).\nEfron (1986) gives the following expression for the density of the double-Poisson distribution with mean \\(\\mu\\) and inverse-disperson \\(\\phi\\): \\[\nf(x | \\mu, \\phi) = \\frac{\\phi^{1/2} e^{-\\phi \\mu}}{c(\\mu,\\phi)} \\left(\\frac{e^{-x} x^x}{x!}\\right) \\left(\\frac{e \\mu}{x}\\right)^{\\phi x},\n\\] where \\(c(\\mu,\\phi)\\) is a scaling constant to ensure that the density sums to one, which is closely approximated by \\[\nc(\\mu, \\phi) \\approx 1 + \\frac{1 - \\phi}{12 \\mu \\phi}\\left(1 + \\frac{1}{\\mu \\phi}\\right).\n\\] We then have \\[\n\\ln f(x | \\mu, \\phi) = \\frac{1}{2} \\ln \\phi - \\phi \\mu - \\ln c(\\mu, \\phi) + x (\\phi + \\phi \\ln \\mu - 1) + (1 - \\phi) x \\ln(x) - \\ln \\left(x!\\right),\n\\] where \\(0 \\times \\ln (0)\\) is evaluated as 0."
  },
  {
    "objectID": "posts/double-poisson-in-Stan/index.html#comparison-models",
    "href": "posts/double-poisson-in-Stan/index.html#comparison-models",
    "title": "Implementing Efron’s double Poisson distribution in Stan",
    "section": "Comparison models",
    "text": "Comparison models\nBefore using the custom distribution, I’ll fit a couple of out-of-the-box models that are useful points of comparison. Surely the simplest, quickest, and dirtiest way to estimate such a regression is with a generalized linear model, using the “quasi-Poisson” family to allow for non-unit dispersion. In R:\n\n\nCode\nquasi_fit &lt;- glm(Y ~ X, family = quasipoisson(link = \"log\"), data = dat)\nsummary(quasi_fit)\n\n\n\nCall:\nglm(formula = Y ~ X, family = quasipoisson(link = \"log\"), data = dat)\n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  1.98784    0.01219  163.03   &lt;2e-16 ***\nX            0.29276    0.01178   24.85   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for quasipoisson family taken to be 0.6324771)\n\n    Null deviance: 777.74  on 599  degrees of freedom\nResidual deviance: 384.90  on 598  degrees of freedom\nAIC: NA\n\nNumber of Fisher Scoring iterations: 4\n\n\nThis approach recovers the data-generating parameters quite well, with a dispersion estimate of 0.632 compared to the true dispersion parameter of 0.6.\nNow let me fit the same generalized linear model but assuming that the outcome follows a true Poisson distribution (with unit dispersion). I’ll fit the model in a Bayesian framework with the brms package.\n\n\nCode\nPoisson_fit &lt;- \n  brm(\n    Y ~ X, family = poisson(link = \"log\"),\n    data = dat, \n    warmup = 500, \n    iter = 1500, \n    chains = 4, \n    cores = 4,\n    seed = 20230913\n  )\n\nsummary(Poisson_fit)\n\n\n Family: poisson \n  Links: mu = log \nFormula: Y ~ X \n   Data: dat (Number of observations: 600) \n  Draws: 4 chains, each with iter = 1500; warmup = 500; thin = 1;\n         total post-warmup draws = 4000\n\nRegression Coefficients:\n          Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept     1.99      0.02     1.96     2.02 1.00     2865     2658\nX             0.29      0.01     0.26     0.32 1.00     2552     2381\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nThis specification recovers the intercept and slope parameters well too, but doesn’t provide any estimate of dispersion.\nAs an alternative, I’ll also fit the model using the negative binomial distribution, which is a generalization of the Poisson that allows for over-dispersion (but not under-dispersion):\n\n\nCode\nnegbin_fit &lt;- \n  brm(\n    Y ~ X, family = negbinomial(link = \"log\"),\n    data = dat, \n    warmup = 500, \n    iter = 1500, \n    chains = 4, \n    cores = 4,\n    seed = 20230913\n  )\n\nsummary(negbin_fit)\n\n\n Family: negbinomial \n  Links: mu = log; shape = identity \nFormula: Y ~ X \n   Data: dat (Number of observations: 600) \n  Draws: 4 chains, each with iter = 1500; warmup = 500; thin = 1;\n         total post-warmup draws = 4000\n\nRegression Coefficients:\n          Estimate Est.Error l-95% CI u-95% CI  Rhat Bulk_ESS Tail_ESS\nIntercept     1.98      0.01     1.96     1.99  8.46        4        4\nX             0.31      0.03     0.29     0.36 11.39        4       NA\n\nFurther Distributional Parameters:\n                                                                                                                      Estimate\nshape 288635341964439312984004668864260822628440088608000200668088264284802644482664682404200862480406040224646204882688664.00\n                                                                                                                     Est.Error\nshape 499993580246645223926226042864464406466686242028244246282864888460026284662046208044222402820440664060080422624068464.00\n               l-95% CI\nshape 12948941308882.21\n                                                                                                                       u-95% CI\nshape 1154541367857757248926006442246840288482660022402000800442022846826208466628446428606800248620604060886464806228422446.00\n      Rhat Bulk_ESS Tail_ESS\nshape  Inf        4       NA\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nThe brms package implements the negative binomial using the rate parameterization, so the shape parameter corresponds to the inverse dispersion. Thus, a large shape parameter (as in the above fit) implies dispersion that is very close to one (i.e., close to the Poisson)."
  },
  {
    "objectID": "posts/double-poisson-in-Stan/index.html#double-poisson-model",
    "href": "posts/double-poisson-in-Stan/index.html#double-poisson-model",
    "title": "Implementing Efron’s double Poisson distribution in Stan",
    "section": "Double-Poisson model",
    "text": "Double-Poisson model\nNow I’ll fit the same model as previously but using my custom-built double-Poisson distribution. Following Paul Buerkner’s vignette on using custom distributions in brms, I’ll first specify the custom family object for the double-Poisson:\n\n\nCode\ndouble_Poisson &lt;- custom_family(\n  \"dpo\", dpars = c(\"mu\",\"phi\"),\n  links = c(\"log\",\"log\"),\n  lb = c(0, 0), ub = c(NA, NA),\n  type = \"int\"\n)\n\n\nI set the defaults to use a log-link for the mean (just as with the Poisson and negative binomial families) and a log-link for the inverse-dispersion. Next, I’ll create an object to add the custom stan code from above into the code created by brm for fitting the model:\n\n\nCode\ndouble_Poisson_stanvars &lt;- stanvar(scode = stancode_qr, block = \"functions\")\n\n\nI’ll also need to specify a prior to use for the \\(\\phi\\) parameter of the double-Poisson distribution:\n\n\nCode\nphi_prior &lt;- prior(exponential(1), class = \"phi\")\n\n\nNow I’m ready to fit the model:\n\n\nCode\nDPO_fit &lt;- \n  brm(\n    Y ~ X, family = double_Poisson,\n    prior = phi_prior,\n    stanvars = double_Poisson_stanvars,\n    data = dat, \n    warmup = 500, \n    iter = 1500, \n    chains = 4, \n    cores = 4,\n    seed = 20230913\n  )\n\nsummary(DPO_fit)\n\n\n Family: dpo \n  Links: mu = log; phi = identity \nFormula: Y ~ X \n   Data: dat (Number of observations: 600) \n  Draws: 4 chains, each with iter = 1500; warmup = 500; thin = 1;\n         total post-warmup draws = 4000\n\nRegression Coefficients:\n          Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept     1.99      0.01     1.96     2.01 1.00     3592     2849\nX             0.29      0.01     0.27     0.32 1.00     3330     3103\n\nFurther Distributional Parameters:\n    Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nphi     1.55      0.09     1.38     1.72 1.00     3043     2560\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nThe regression coefficient estimates are basically identical to those from the Poisson and negative-binomial models, estimated with slightly better precision than with the Poisson or negative binomial families. However, we get a posterior for \\(\\phi\\) that corresponds to under-dispersion. Here’s the posterior for the dispersion (i.e., \\(1 / \\phi\\)):\n\n\nCode\nmcmc_areas(DPO_fit, pars = \"phi\", transformations = \\(x) 1 / x) + \n  theme_minimal()"
  },
  {
    "objectID": "posts/double-poisson-in-Stan/index.html#model-comparison",
    "href": "posts/double-poisson-in-Stan/index.html#model-comparison",
    "title": "Implementing Efron’s double Poisson distribution in Stan",
    "section": "Model comparison",
    "text": "Model comparison\nI’d like to get a sense of how much better the double-Poisson model does with capturing the real data-generating process compared to the simple Poisson model or the negative binomial model. There’s a wide range of diagnostics that can inform such comparisons. I’ll consider the leave-one-out information criteria (LOOIC) and also look at some posterior predictive checks.\nTo calculate LOOIC for the double-Poisson model, I first need to provide a log_lik function that brms can use4. Here’s code, using the Stan function from above:\n\n\nCode\nexpose_functions(DPO_fit, vectorize = TRUE)\nlog_lik_dpo &lt;- function(i, prep) {\n  mu &lt;- brms::get_dpar(prep, \"mu\", i = i)\n  phi &lt;- brms::get_dpar(prep, \"phi\", i = i)\n  y &lt;- prep$data$Y[i]\n  dpo_lpmf(y, mu, phi)\n}\n\n\nI can then compute LOOIC for all three models:\n\n\nCode\nloo(DPO_fit, Poisson_fit, negbin_fit)\n\n\nOutput of model 'DPO_fit':\n\nComputed from 4000 by 600 log-likelihood matrix.\n\n         Estimate   SE\nelpd_loo  -1305.7 16.9\np_loo         2.9  0.2\nlooic      2611.4 33.7\n------\nMCSE of elpd_loo is 0.0.\nMCSE and ESS estimates assume MCMC draws (r_eff in [0.7, 1.3]).\n\nAll Pareto k estimates are good (k &lt; 0.7).\nSee help('pareto-k-diagnostic') for details.\n\nOutput of model 'Poisson_fit':\n\nComputed from 4000 by 600 log-likelihood matrix.\n\n         Estimate   SE\nelpd_loo  -1330.0 11.3\np_loo         1.3  0.1\nlooic      2660.1 22.6\n------\nMCSE of elpd_loo is 0.0.\nMCSE and ESS estimates assume MCMC draws (r_eff in [0.5, 0.9]).\n\nAll Pareto k estimates are good (k &lt; 0.7).\nSee help('pareto-k-diagnostic') for details.\n\nOutput of model 'negbin_fit':\n\nComputed from 4000 by 600 log-likelihood matrix.\n\n         Estimate   SE\nelpd_loo  -1332.9 11.6\np_loo         2.9  0.3\nlooic      2665.8 23.2\n------\nMCSE of elpd_loo is NA.\nMCSE and ESS estimates assume MCMC draws (r_eff in [0.0, 0.0]).\n\nPareto k diagnostic values:\n                         Count Pct.    Min. ESS\n(-Inf, 0.7]   (good)     157   26.2%   2       \n   (0.7, 1]   (bad)        0    0.0%   &lt;NA&gt;    \n   (1, Inf)   (very bad) 443   73.8%   &lt;NA&gt;    \nSee help('pareto-k-diagnostic') for details.\n\nModel comparisons:\n            elpd_diff se_diff\nDPO_fit       0.0       0.0  \nPoisson_fit -24.3       6.0  \nnegbin_fit  -27.2       6.0  \n\n\nBy these measures, the double-Poisson model has substantially better fit than either of the other models.\nTo do posterior predictive checks, I need to provide a posterior_predict function that brms can use. I’ll again do an implementation that uses my custom dpo_rng() from Stan.5\n\n\nCode\nposterior_predict_dpo &lt;- function(i, prep, maxval = NULL, ...) {\n  mu &lt;- brms::get_dpar(prep, \"mu\", i = i)\n  phi &lt;- brms::get_dpar(prep, \"phi\", i = i)\n  if (is.null(maxval)) maxval &lt;- 20 * mu / min(phi, 1)\n  dpo_rng(mu, phi, maxval = maxval)\n}\n\n\nFunctions in hand, I can now compute posterior predictions for the double-Poisson model and make pretty pictures of them, along with corresponding plots for the Poisson and negative-binomial models.\n\n\nCode\nYrep_Poisson &lt;- posterior_predict(Poisson_fit, draws = 400) \ncolor_scheme_set(\"blue\")\nPoisson_root &lt;- ppc_rootogram(dat$Y, Yrep_Poisson, style = \"hanging\") + labs(title = \"Poisson\")\n\nYrep_negbin &lt;- posterior_predict(negbin_fit, draws = 400)\ncolor_scheme_set(\"green\")\nnegbin_root &lt;- ppc_rootogram(dat$Y, Yrep_negbin, style = \"hanging\") + labs(title = \"Negative-binomial\")\n\nYrep_dpo &lt;- posterior_predict(DPO_fit, draws = 400)\ncolor_scheme_set(\"purple\")\ndpo_root &lt;- ppc_rootogram(dat$Y, Yrep_dpo, style = \"hanging\") + labs(title = \"Double-Poisson\")\n\ndpo_root / Poisson_root / negbin_root &\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nThe differences in predicted frequencies are not that obvious from these plots. The main notable difference is that the Poisson and negative-binomial distributions predict more small counts (in the range of 0 to 3) than are observed, whereas the double-Poisson does better at matching the observed frequency in this range.\nI think the lack of glaring differences in the above plots happens because I’m just looking at the marginal distribution of the outcome, and the (explained) variation due to the predictor dampens the degree of under-dispersion. To see this, I’ll create some plots that are grouped by quintiles of \\(X\\):\n\n\nCode\ndat$g &lt;- cut(dat$X, breaks = quantile(dat$X, seq(0,1,0.2)), include.lowest = TRUE)\n\ncolor_scheme_set(\"blue\")\nPoisson_bars &lt;- ppc_bars_grouped(\n  dat$Y, Yrep_Poisson, dat$g, \n  prob = 0.5, \n  facet_args = list(ncol = 5)\n) + \n  labs(title = \"Poisson\")\n\ncolor_scheme_set(\"green\")\nnegbin_bars &lt;- ppc_bars_grouped(\n  dat$Y, Yrep_negbin, dat$g, \n  prob = 0.5, \n  facet_args = list(ncol = 5)\n) + \n  labs(title = \"Negative-binomial\")\n\ncolor_scheme_set(\"purple\")\ndpo_bars &lt;- ppc_bars_grouped(\n  dat$Y, Yrep_dpo, dat$g, \n  prob = 0.5, \n  facet_args = list(ncol = 5)\n) + \n  labs(title = \"Double-Poisson\")\n\ndpo_bars / Poisson_bars / negbin_bars &\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nStill kind of subtle, I suppose, but you can see more clearly that the double-Poisson does a better job than the other distributions at matching the modes (peaks) of the empirical distribution in each of these subgroups.\nOne last approach is to look directly at the degree of dispersion in the posterior predictive distributions relative to the actual data. I’ll calculate this dispersion by re-fitting the quick-and-dirty quasi-poisson model in each sample:\n\n\nCode\ndispersion_coef &lt;- function(y) {\n  quasi_fit &lt;- glm(y ~ dat$X, family = quasipoisson(link = \"log\"))\n  sum(residuals(quasi_fit, type = \"pearson\")^2) / quasi_fit$df.residual\n}\n\ncolor_scheme_set(\"blue\")\nPoisson_disp &lt;- ppc_stat(dat$Y, Yrep_Poisson, stat = dispersion_coef, binwidth = 0.02) + \n  labs(title = \"Poisson\")\n\ncolor_scheme_set(\"green\")\nnegbin_disp &lt;- ppc_stat(dat$Y, Yrep_negbin, stat = dispersion_coef, binwidth = 0.02) + \n  labs(title = \"Negative-binomial\")\n\ncolor_scheme_set(\"purple\")\ndpo_disp &lt;- ppc_stat(dat$Y, Yrep_dpo, stat = dispersion_coef, binwidth = 0.02) + \n  labs(title = \"Double-Poisson\")\n\ndpo_disp / Poisson_disp / negbin_disp &\n  theme_minimal() & \n  xlim(c(0.45, 1.3))\n\n\n\n\n\n\n\n\n\nFrom this, we can clearly see that the Poisson and negative binomial model generate data with approximately unit dispersion, which doesn’t match at all with the degree of dispersion in the observed data."
  },
  {
    "objectID": "posts/double-poisson-in-Stan/index.html#footnotes",
    "href": "posts/double-poisson-in-Stan/index.html#footnotes",
    "title": "Implementing Efron’s double Poisson distribution in Stan",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nTo be clear up front, what I present is more complicated than really necessary because of these existing R functions to simulate values from the double-Poisson—we can just use the functions from gamlss.dist for purposes of posterior predictive checks (about which more below). I’m trying to work in Stan to the maximum extent possible solely as an excuse to learn more about the language, which I haven’t used much up until today.↩︎\nI should also note that the bamlss package provides similar functionality and can be combined with gamlss.dist to accomplish basically the same thing as I’m going to do here.↩︎\nThe simpler version is what’s needed for generating posterior predictive checks, the fancy version is just to show off how clever I am.↩︎\nRather than exposing and calling the Stan function, one could just re-implement the log likelihood in R. (Probably the easier way in practice, but again I’m trying to learn me some Stan here…)↩︎\nOf course, I could have saved a bunch of trouble by just using gamlss.dist::rDPO() instead.↩︎"
  },
  {
    "objectID": "posts/design-comparable-effect-sizes-in-multiple-baseline-designs/index.html",
    "href": "posts/design-comparable-effect-sizes-in-multiple-baseline-designs/index.html",
    "title": "New article: Design-comparable effect sizes in multiple baseline designs: A general modeling framework",
    "section": "",
    "text": "My article with Larry Hedges and Will Shadish, titled “Design-comparable effect sizes in multiple baseline designs: A general modeling framework” has been accepted at Journal of Educational and Behavioral Statistics. The abstract is below. Here’s the article at the journal website. Postprint and supporting materials are available. An R package that implements the proposed methods is available here.\nIn single-case research, the multiple baseline design is a widely used approach for evaluating the effects of interventions on individuals. Multiple baseline designs involve repeated measurement of outcomes over time and the controlled introduction of a treatment at different times for different individuals. This article outlines a general approach for defining effect sizes in multiple baseline designs that are directly comparable to the standardized mean difference from a between-subjects randomized experiment. The target, design-comparable effect size parameter can be estimated using restricted maximum likelihood together with a small-sample correction analogous to Hedges’ g. The approach is demonstrated using hierarchical linear models that include baseline time trends and treatment-by-time interactions. A simulation compares the performance of the proposed estimator to that of an alternative, and an application illustrates the model-fitting process.\n\n\n\n\n\nReuseCC BY 4.0CitationBibTeX citation:@online{pustejovsky2014,\n  author = {Pustejovsky, James E.},\n  title = {New Article: {Design-comparable} Effect Sizes in Multiple\n    Baseline Designs: {A} General Modeling Framework},\n  date = {2014-07-20},\n  url = {https://jepusto.com/posts/design-comparable-effect-sizes-in-multiple-baseline-designs},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nPustejovsky, James E. 2014. “New Article: Design-Comparable Effect\nSizes in Multiple Baseline Designs: A General Modeling\nFramework.” July 20, 2014. https://jepusto.com/posts/design-comparable-effect-sizes-in-multiple-baseline-designs."
  },
  {
    "objectID": "posts/Current-Projects/index.html",
    "href": "posts/Current-Projects/index.html",
    "title": "Current projects",
    "section": "",
    "text": "Interested in working with me? See below for descriptions of several potential projects. If you have interest and abilities that line up with one of these, feel free to contact me.\n\nReview of methods for direct observation of behavior. Several different methods for recording direct observations of behavior are commonly used in single-case research and other areas of psychology; prominent methods include continuous duration recording, momentary time sampling, and partial interval recording. Textbook advice about appropriate use of different methods is conflicting and often ambiguous, and simulation studies evaluating the operating characteristics of different methods also yield mixed results. The goals of this project are to: find and organize the current guidance about direct observation procedures; understand the basis of that guidance (e.g., simulation studies, heuristic models); and relate the guidance to a unifying statistical framework, by translating claims and conclusions into the terms of a parametric model (known as an alternating renewal process). This project would be appropriate either for a quantitative methods student who is interested in learning about direct observation methods for measuring behavior or for a student from school psychology, counseling psychology, or special education who is familiar with direct observation methods and interested in learning about statistical models for the data they generate.\nApplications of meta-analysis for single-case studies of free-operant behavior. I have recently proposed a suite of new effect size metrics for quantifying treatment effects in single-case studies of free-operant behavior. The crux of this line of work is that it is important to use effect size metrics that are comparable across different methods of recording direct observation data. This project will involve: reviewing several published systematic reviews that incorporate evidence from single-case studies, in order to determine what measurement procedures were used to collect data, then re-analyzing the data from one or more of these studies, using the newly proposed effect size metrics and methods. This project would be appropriate for a special education student who is familiar with meta-analysis.\nApplications of design-comparable effect size measures for longitudinal studies. Co-authors and I have recently proposed a method of estimating effect sizes from single-case studies (or other types of longitudinal designs) that are in the same metric as Cohen’s d-type effect sizes from conventional between-subjects experiments. The goals of this project are to: develop exemplar code that implements effect size calculations in several major statistical packages (including SPSS, SAS, Stata, and R); review the algorithms available in major statistical packages for estimating the uncertainty of variance components (i.e., information matrices); develop further applications and extensions to the proposed effect sizes. This project would be appropriate for a quantitative methods student who is familiar with estimation of hierarchical linear models in SPSS, SAS, and other major statistical software platforms.\nProgramming information matrices for hierarchical linear modeling. The Fisher information plays a pivotal role in hierarchical linear models, both as an approximate estimates of parameter uncertainty and as a key component of small-sample hypothesis tests such as those of Kenward and Roger (1997,2009). The goals of this project are to: create an R package for constructing analytic information matrices for HLM models estimated with the well-known nlme package; also add functions for the revised Kenward & Roger hypothesis tests; and evaluate the performance of different information matrices (expected, observed, and average) for calculating degrees-of-freedom adjustments in the context of effect size estimation. This project could be appropriate for a quantitative methods student or a statistics student who has strong programming skills and wants to 1) learn more about the statistical guts of HLM estimation and 2) level-up on their R programming by designing a publishable package.\nA discrete-time Markov chain model for partial interval recording data. Partial interval recording is a commonly used method for recording direct observations of human behavior. Data generated by this method is problematic because, as typically analyzed, it yields upwardly biased measures of prevalence (the proportion of time that a behavior occurs). This shortcoming can be addressed by modeling the data using a discrete-time Markov chain and using maximum likelihood methods to estimate parameters corresponding directly to prevalence and incidence (the frequency with which new behaviors occur). The goals of this project are to create an R package implementing maximum likelihood estimation (and possibly other methods) for partial interval recording data and evaluate this estimation approach using asymptotic theory and simulation. This project could be appropriate for an advanced quantitative methods student or statistics student who is interested in learning about Markov chain models and who has strong programming skills.\n\n\n\n\n\n\nReuseCC BY 4.0CitationBibTeX citation:@online{pustejovsky2013,\n  author = {Pustejovsky, James E.},\n  title = {Current Projects},\n  date = {2013-08-20},\n  url = {https://jepusto.com/posts/Current-Projects},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nPustejovsky, James E. 2013. “Current Projects.” August 20,\n2013. https://jepusto.com/posts/Current-Projects."
  },
  {
    "objectID": "posts/correlations-between-SMDs/index.html",
    "href": "posts/correlations-between-SMDs/index.html",
    "title": "Correlations between standardized mean differences",
    "section": "",
    "text": "Several students and colleagues have asked me recently about an issue that comes up in multivariate meta-analysis when some of the studies include multiple treatment groups and multiple outcome measures. In this situation, one might want to include effect size estimates for each treatment group and each outcome measure. In order to do so in fully multivariate meta-analysis, estimates of the covariances among all of these efffect sizes are needed. The covariance among effect sizes arises for several reasons:\n\nFor a single outcome measure, effect sizes based on different treatment groups compared to a common control group will be correlated because the same control group data is used to calculate both effect sizes;\nEffect sizes based on a single treatment group and a single control group, but for different outcome measures, will be correlated because the outcomes are measured on the same set of units (in both the treatment group and the control group).\nEffect sizes based on different treatment groups and for different outcome measures will be correlated because the outcomes are measured on the same set of units in the control group (though not in the treatment group).\n\nFor standardized mean difference (SMD) measures of effect size, formulas for the covariance are readily available for the first two cases (see e.g., Gleser & Olkin, 2009), but not for the third case. Below I review the formulas for the covariance between SMDs in the first two cases and provide a formula for the third case.\n\nNotation and Model\nSuppose that the experiment has a control group that includes \\(n_0\\) units and \\(T\\) treatment groups that include \\(n_1,...,n_T\\) units, respectively. Also suppose that \\(J\\) outcome measures are made on each unit in each group. The formulas below assume that the data follow a one-way MANOVA model. Let \\(y_{ijt}\\) denote the score for unit \\(i\\) on outcome \\(j\\) in group \\(t\\). Then I assume that\n\\[y_{ijt} = \\mu_{jt} + \\epsilon_{ijt},\\]\nwhere the errors are multi-variate normally distributed with mean zero, variance that can differ across outcome but not across treatment group, and correlation that is constant across treatment groups, i.e. \\(\\text{Var}\\left(\\epsilon_{ijt}\\right) = \\sigma^2_j\\), \\(\\text{Cov}\\left(\\epsilon_{ijt}, \\epsilon_{ikt} \\right) = \\rho_{jk}\\).\nDenote the mean score on outcome \\(j\\) in group \\(t\\) as \\(\\bar{y}_{jt}\\) and the standard deviation of the scores on outcome \\(j\\) in group \\(t\\) as \\(s_{jt}\\), both for \\(j = 1,...,J\\) and \\(t = 0,...,T\\) (with \\(t = 0\\) corresponding to the control group). Also required are estimates of the correlations among outcome measures 1 through \\(J\\), after partialling out differences between treatment groups. Let \\(r_{jk}\\) denote the partial correlation between measure \\(j\\) and measure \\(k\\), for \\(j = 1,...,J - 1\\) and \\(k = j + 1,...,J\\).\nWith multiple treatment groups, one might wonder how best to compute the standard deviation for purposes of scaling the treatment effect estimates. In their discussion of SMDs from multiple treatment studies, Gleser and Olkin (2009) assume (though they don’t actually state outright) that the standard deviation will be pooled across all \\(T + 1\\) groups. The pooled standard deviation for outcome \\(m\\) is calculated as the square root of the pooled variance,\n\\[s_{jP}^2 = \\frac{1}{N - T - 1} \\sum_{t=0}^T (n_t - 1)s_{jt}^2,\\]\nwhere \\(N = \\sum_{t=0}^T n_t\\). The standardized mean difference for treatment \\(t\\) on outcome \\(j\\) is then estimated as\n\\[d_{jt} = \\frac{\\bar{y}_{jt} - \\bar{y}_{j0}}{s_{jP}}\\]\nfor \\(j = 1,...,J\\) and \\(t = 1,...,T\\). The conventional estimate of the large-sample variance of \\(d_{jt}\\) is\n\\[\\text{Var}(d_{jt}) \\approx \\frac{1}{n_0} + \\frac{1}{n_t} + \\frac{d_{jt}^2}{2 (N - T - 1)}.\\]\n\n\nCovariances\nFor SMDs based on a common outcome measure and a common control group, but different treatment groups, the large-sample covariance between the effect size estimates can be estimated as\n\\[\\text{Cov}(d_{jt},d_{ju}) \\approx \\frac{1}{n_0} + \\frac{d_{jt} d_{ju}}{2 (N - T - 1)}.\\]\nThe above differs slightly from Gleser and Olkin (2009, Formula 19.19) because it uses the degrees of freedom \\(N - T - 1\\) in the denominator of the second term, rather than the total sample size. If the total sample size is larger relative to the number of treatment groups, the discrepancy should be minor.\nSMDs based on a single treatment group but for different outcome measures follow a structure that is essentially equivalent to what Gleser and Olkin (2009) call a “multiple-endpoint” study. The large-sample covariance between the effect size estimates can be estimated as\n\\[\\text{Cov}(d_{jt},d_{kt}) \\approx r_{jk} \\left(\\frac{1}{n_0} + \\frac{1}{n_t}\\right) + \\frac{r_{jk}^2 d_{jt} d_{kt}}{2 (N - T - 1)}\\]\n(cf. Gleser & Olkin, 2009, Formula 19.19). Note that if the degrees of freedom are large relative to \\(d_{jt}\\) and \\(d_{kt}\\), then the correlation between the effect sizes will be approximately equal to \\(\\text{Cor}(d_{jt},d_{kt}) \\approx r_{jk}\\).\nFinally, the large-sample covariance between SMDs based on different treatment groups and different outcome measures can be estimated as\n\\[\\text{Cov}(d_{jt},d_{ku}) \\approx \\frac{r_{jk}}{n_0} + \\frac{r_{jk}^2 d_{jt} d_{ku}}{2 (N - T - 1)}.\\]\nThis is similar to the previous formula, but does not include the term corresponding to the covariance between different outcome measures in a common treatment group.\nIf \\(r_{jj} = 1\\) is used for the correlation of an outcome measure with itself, all of the above formulas (including the variance of \\(d_{jt}\\)) can be expressed compactly as\n\\[\\text{Cov}(d_{jt},d_{ku}) \\approx r_{jk} \\left(\\frac{1}{n_0} + \\frac{I(t = u)}{n_t}\\right) + \\frac{r_{jk}^2 d_{jt} d_{ku}}{2 (N - T - 1)},\\]\nwhere \\(I(A)\\) is equal to one if \\(A\\) is true and equal to zero otherwise.\n\n\nReferences\n\nGleser, L. J., & Olkin, I. (2009). Stochastically dependent effect sizes. In H. Cooper, L. V. Hedges, & J. C. Valentine (Eds.), The Handbook of Research Synthesis and Meta-Analysis (2nd ed., pp. 357-376). New York, NY: Russell Sage Foundation.\n\n\n\n\n\n\n\nReuseCC BY 4.0CitationBibTeX citation:@online{pustejovsky2015,\n  author = {Pustejovsky, James E.},\n  title = {Correlations Between Standardized Mean Differences},\n  date = {2015-09-17},\n  url = {https://jepusto.com/posts/correlations-between-SMDs},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nPustejovsky, James E. 2015. “Correlations Between Standardized\nMean Differences.” September 17, 2015. https://jepusto.com/posts/correlations-between-SMDs."
  },
  {
    "objectID": "posts/code-folding-with-blogdown-academic/index.html",
    "href": "posts/code-folding-with-blogdown-academic/index.html",
    "title": "Code folding with blogdown + Academic theme",
    "section": "",
    "text": "{{% alert warning %}} 2020-05-03 This post describes an implementation of code folding for an older version of the Academic Theme. It does not work with Academic 4.+. See my updated instructions to get it working with newer versions of Academic. {{% /alert %}}\nRmarkdown documents now have a very nifty code folding option, which allows the reader of a compiled html document to toggle whether to view or hide code chunks. However, the feature is not supported in blogdown, the popular Rmarkdown-based website/blog creation package. I recently ran across an implementation of codefolding for blogdown, developed by Sébastien Rochette. I have been putzing around, trying to get it to work with my blog, which uses the Hugo Academic theme—alas, to no avail. To my amazement and good fortune, Sébastien swooped in with a pull request that cleaned up my blundering attempts at implementation. Now all of my posts have working code folding!\nIn this post, I’ll lay out how to make Sébastien’s code folding feature work with the Academic theme. To be totally clear, all of the hard bits of this were solved by Sébastien. I don’t know javascript to save my life, and my only contribution is to write down the instructions in what I hope is a coherent fashion, so that you too can soon be doing the happy code folding dance if you so desire."
  },
  {
    "objectID": "posts/code-folding-with-blogdown-academic/index.html#code-folding-with-the-academic-theme",
    "href": "posts/code-folding-with-blogdown-academic/index.html#code-folding-with-the-academic-theme",
    "title": "Code folding with blogdown + Academic theme",
    "section": "1 Code folding with the Academic theme",
    "text": "1 Code folding with the Academic theme\n\nYou’ll first need to pull in some javascript assets. Create a folder called js under the \\static directory of your site. Add the files transition.js, collapse.js, and dropdown.js from bootstrap.\nAlso add Sébastien’s codefolding javascript, codefolding.js.\nCreate a folder called css under the \\static directory of your site. Add the file codefolding.css. This is the css for the buttons that will appear on your posts.\nAdd the file article_footer_js.html to the \\layouts\\partials directory of your site.\nAdd the file header_maincodefolding.html to the \\layouts\\partials directory of your site.\nIf you do not already have a file head_custom.html in the \\layouts\\partials directory, create it.. Add the following lines of code to the file:\n::: {.cell}\n{{ if not .Site.Params.disable_codefolding }}\n  &lt;script src=\"{{ \"js/collapse.js\" | relURL }}\"&gt;&lt;/script&gt;\n  &lt;script src=\"{{ \"js/dropdown.js\" | relURL }}\"&gt;&lt;/script&gt;\n  &lt;script src=\"{{ \"js/transition.js\" | relURL }}\"&gt;&lt;/script&gt;\n{{ end }}\n:::\nIf you do not already have a file footer.html in the \\layouts\\partials directory, copy it over from \\themes\\hugo-academic\\layouts\\partials. Add the following lines of code to it, somewhere towards the bottom (see my version for example):\n::: {.cell}\n&lt;!-- Init code folding --&gt;\n{{ partial \"article_footer_js.html\" . }}\n:::\nIf you do not already have the file single.html in the directory \\layouts\\_default, copy it over from \\themes\\hugo-academic\\layouts\\_default. Add the following line of code at an appropriate point so that your posts will include the “Show/hide code” button (I put it after the title, before the meta-data; see here):\n::: {.cell}\n {{ partial \"header_maincodefolding\" . }}\n:::\nModify your config.toml file (in the base directory of your site) to include the following lines:\n::: {.cell}\n# Set to true to disable code folding\ndisable_codefolding = false\n# Set to \"hide\" or \"show\" all codes by default\ncodefolding_show = \"show\"\n# Set to true to exclude the \"Show/hide all\" button\ncodefolding_nobutton = false\n:::\nAlso edit the custom_css parameter so that the codefolding.css file will get loaded:\n::: {.cell}\ncustom_css = [\"codefolding.css\"]\n:::"
  },
  {
    "objectID": "posts/code-folding-with-blogdown-academic/index.html#using-the-codefolding-parameters",
    "href": "posts/code-folding-with-blogdown-academic/index.html#using-the-codefolding-parameters",
    "title": "Code folding with blogdown + Academic theme",
    "section": "2 Using the codefolding parameters",
    "text": "2 Using the codefolding parameters\nThe config.toml file now has three parameters that control code folding:\n\ndisable_codefolding controls whether to load the code folding scripts on your site. Set it to true to disable code folding globally.\ncodefolding_show controls whether code blocks will be shown or hidden by default. If your previous posts have lots of code in them, set the default to show to minimize changes in the appearance of your site.\ncodefolding_nobutton controls whether the “Show/hide code” button will appear at the top of posts that include code blocks. Set it to true to disable the button but keep the other code folding functionality.\n\nThe above parameters are defaults for your entire site. To over-ride the defaults, you can also set the parameters in the YAML header of any post:\n\nSet disable_codefolding: true to turn off code folding for the post.\nSet codefolding_show: hide to hide the code blocks in the post (as in this post).\nSet codefolding_nobutton: true to turn off the “Show/hide code” button at the top of the post (as in the present post).\n\nI hope these instructions work for you. If not, questions, corrections, and clarifications are welcome. Thanks again to Sébastien Rochette for working out this solution and for graciously troubleshooting my attempt at implementation. Happy blogging, y’all!"
  },
  {
    "objectID": "posts/clubSandwich-for-RVE-meta-analysis/index.html",
    "href": "posts/clubSandwich-for-RVE-meta-analysis/index.html",
    "title": "The clubSandwich package for meta-analysis with RVE",
    "section": "",
    "text": "UPDATED April 09, 2024 to use current syntax for constraints argument in clubSandwich::Wald_test().\nI’ve recently been working on small-sample correction methods for hypothesis tests in linear regression models with cluster-robust variance estimation. My colleague (and grad-schoolmate) Beth Tipton has developed small-sample adjustments for t-tests (of single regression coefficients) in the context of meta-regression models with robust variance estimation, and together we have developed methods for multiple-contrast hypothesis tests. We have an R package (called clubSandwich) that implements all this stuff, not only for meta-regression models but also for other models and contexts where cluster-robust variance estimation is often used.\nThe alpha-version of the package is currently available on Github. See the Github README for instructions on how to install it in R. Below I demonstrate how to use the package to get robust variance estimates, t-tests, and F-tests, all with small-sample corrections. The example uses a dataset of effect sizes from a Campbell Collaboration systematic review of dropout prevention programs, conducted by Sandra Jo Wilson and her colleagues.\nThe original analysis included a meta-regression with covariates that capture methodological, participant, and program characteristics. I’ll use a regression specification that is similar to Model III from Wilson et al. (2011), but treat the evaluator_independence and implementation_quality variables as categorical rather than interval-level; the original analysis clustered at the level of the sample (some studies reported results from multiple samples), whereas I will cluster at the study level. I fit the model two ways, first using the robumeta package and then using metafor.\n\nrobumeta model\n\noptions(width=150)\nlibrary(robumeta)\nlibrary(clubSandwich)\n\nRegistered S3 method overwritten by 'clubSandwich':\n  method    from    \n  bread.mlm sandwich\n\ndata(dropoutPrevention)\n\nm3_robu &lt;- robu(LOR1 ~ study_design + attrition + group_equivalence + adjusted\n                + outcome + evaluator_independence\n                + male_pct + white_pct + average_age\n                + implementation_quality + program_site + duration + service_hrs, \n                data = dropoutPrevention, studynum = studyID, var.eff.size = varLOR, \n                modelweights = \"HIER\")\nprint(m3_robu)\n\nRVE: Hierarchical Effects Model with Small-Sample Corrections \n\nModel: LOR1 ~ study_design + attrition + group_equivalence + adjusted + outcome + evaluator_independence + male_pct + white_pct + average_age + implementation_quality + program_site + duration + service_hrs \n\nNumber of clusters = 152 \nNumber of outcomes = 385 (min = 1 , mean = 2.53 , median = 1 , max = 30 )\nOmega.sq = 0.24907 \nTau.sq = 0.1024663 \n\n                                                Estimate   StdErr t-value  dfs    P(|t|&gt;) 95% CI.L 95% CI.U Sig\n1                                 X.Intercept.  0.016899 0.615399  0.0275 16.9 0.97841541 -1.28228  1.31608    \n2          study_designNon.random..non.matched -0.002626 0.185142 -0.0142 40.5 0.98875129 -0.37667  0.37141    \n3                       study_designRandomized -0.086872 0.140044 -0.6203 38.6 0.53869676 -0.37024  0.19650    \n4                                    attrition  0.118889 0.247228  0.4809 15.5 0.63732597 -0.40666  0.64444    \n5                            group_equivalence  0.502463 0.195838  2.5657 28.7 0.01579282  0.10174  0.90318  **\n6                        adjustedadjusted.data -0.322480 0.125413 -2.5713 33.8 0.01470796 -0.57741 -0.06755  **\n7                              outcomeenrolled  0.097059 0.139842  0.6941 16.5 0.49727848 -0.19862  0.39274    \n8                            outcomegraduation  0.147643 0.134938  1.0942 30.2 0.28253825 -0.12786  0.42315    \n9                        outcomegraduation.ged  0.258034 0.169134  1.5256 16.3 0.14632629 -0.10006  0.61613    \n10 evaluator_independenceIndirect..influential -0.765085 0.399109 -1.9170  6.2 0.10212896 -1.73406  0.20389    \n11              evaluator_independencePlanning -0.920874 0.346536 -2.6574  5.6 0.04027061 -1.78381 -0.05794  **\n12              evaluator_independenceDelivery -0.916673 0.304303 -3.0124  4.7 0.03212299 -1.71432 -0.11903  **\n13                                    male_pct  0.167965 0.181538  0.9252 16.4 0.36824526 -0.21609  0.55202    \n14                                   white_pct  0.022915 0.149394  0.1534 21.8 0.87950385 -0.28704  0.33287    \n15                                 average_age  0.037102 0.027053  1.3715 21.2 0.18458247 -0.01913  0.09333    \n16     implementation_qualityPossible.problems  0.411779 0.128898  3.1946 26.7 0.00358205  0.14714  0.67642 ***\n17  implementation_qualityNo.apparent.problems  0.658570 0.123874  5.3164 34.6 0.00000635  0.40699  0.91015 ***\n18                           program_sitemixed  0.444384 0.172635  2.5741 28.6 0.01550504  0.09109  0.79768  **\n19                program_siteschool.classroom  0.426658 0.159773  2.6704 37.4 0.01115192  0.10303  0.75028  **\n20    program_siteschool..outside.of.classroom  0.262517 0.160519  1.6354 30.1 0.11236814 -0.06525  0.59028    \n21                                    duration  0.000427 0.000873  0.4895 36.7 0.62736846 -0.00134  0.00220    \n22                                 service_hrs -0.003434 0.005012 -0.6852 36.7 0.49752503 -0.01359  0.00672    \n---\nSignif. codes: &lt; .01 *** &lt; .05 ** &lt; .10 *\n---\nNote: If df &lt; 4, do not trust the results\n\n\nNote that robumeta produces small-sample corrected standard errors and t-tests, and so there is no need to repeat those calculations with clubSandwich. The evaluator_independence variable has four levels, and it might be of interest to test whether the average program effects differ by the degree of evaluator independence. The null hypothesis in this case is that the 10th, 11th, and 12th regression coefficients are all equal to zero. A small-sample adjusted F-test for this hypothesis can be obtained as follows. (The vcov = \"CR2\" option means that the standard errors will be corrected using the bias-reduced linearization method proposed by McCaffrey, Bell, and Botts, 2001.)\n\nWald_test(m3_robu, constraints = constrain_zero(10:12), vcov = \"CR2\")\n\n test Fstat df_num df_denom  p_val sig\n  HTZ  2.78      3     16.8 0.0732   .\n\n\nBy default, the Wald_test function provides an F-type test with degrees of freedom estimated using the approximate Hotelling’s \\(T^2_Z\\) method. The test has less than 17 degrees of freedom, even though there are 152 independent studies in the data, and has a p-value of .07, so not-quite-significant at conventional levels. The low degrees of freedom are a consequence of the fact that one of the levels of evaluator independence has only a few effect sizes in it:\n\ntable(dropoutPrevention$evaluator_independence)\n\n\n          Independent Indirect, influential              Planning              Delivery \n                    6                    33                    43                   303 \n\n\n\n\nmetafor model\nOur package also works with models fit using the metafor package. Here I re-fit the same regression specification, but use REML to estimate the variance components (robumeta uses a method-of-moments estimator) and use a somewhat different weighting scheme than that used in robumeta.\n\nlibrary(metafor)\nm3_metafor &lt;- rma.mv(LOR1 ~ study_design + attrition + group_equivalence + adjusted\n                      + outcome + evaluator_independence\n                      + male_pct + white_pct + average_age\n                      + implementation_quality + program_site + duration + service_hrs, \n                      V = varLOR, random = list(~ 1 | studyID, ~ 1 | studySample),\n                     data = dropoutPrevention)\nsummary(m3_metafor)\n\n\nMultivariate Meta-Analysis Model (k = 385; method: REML)\n\n   logLik   Deviance        AIC        BIC       AICc   \n-489.0357   978.0714  1026.0714  1119.5371  1029.6217   \n\nVariance Components:\n\n            estim    sqrt  nlvls  fixed       factor \nsigma^2.1  0.2274  0.4769    152     no      studyID \nsigma^2.2  0.1145  0.3384    317     no  studySample \n\nTest for Residual Heterogeneity:\nQE(df = 363) = 1588.4397, p-val &lt; .0001\n\nTest of Moderators (coefficients 2:22):\nQM(df = 21) = 293.8694, p-val &lt; .0001\n\nModel Results:\n\n                                             estimate      se     zval    pval    ci.lb    ci.ub      \nintrcpt                                        0.5296  0.7250   0.7304  0.4651  -0.8915   1.9506      \nstudy_designNon-random, non-matched           -0.0494  0.1722  -0.2871  0.7741  -0.3870   0.2881      \nstudy_designRandomized                         0.0653  0.1628   0.4010  0.6884  -0.2538   0.3843      \nattrition                                     -0.1366  0.2429  -0.5623  0.5739  -0.6126   0.3395      \ngroup_equivalence                              0.4071  0.1573   2.5877  0.0097   0.0988   0.7155   ** \nadjustedadjusted data                         -0.3581  0.1532  -2.3371  0.0194  -0.6585  -0.0578    * \noutcomeenrolled                               -0.2831  0.0771  -3.6709  0.0002  -0.4343  -0.1320  *** \noutcomegraduation                             -0.0913  0.0657  -1.3896  0.1646  -0.2201   0.0375      \noutcomegraduation/ged                          0.6983  0.0805   8.6750  &lt;.0001   0.5406   0.8561  *** \nevaluator_independenceIndirect, influential   -0.7530  0.4949  -1.5214  0.1282  -1.7230   0.2171      \nevaluator_independencePlanning                -0.7700  0.4869  -1.5814  0.1138  -1.7242   0.1843      \nevaluator_independenceDelivery                -1.0016  0.4600  -2.1774  0.0294  -1.9033  -0.1000    * \nmale_pct                                       0.1021  0.1715   0.5951  0.5518  -0.2341   0.4382      \nwhite_pct                                      0.1223  0.1804   0.6777  0.4979  -0.2313   0.4758      \naverage_age                                    0.0061  0.0291   0.2091  0.8344  -0.0509   0.0631      \nimplementation_qualityPossible problems        0.4738  0.1609   2.9445  0.0032   0.1584   0.7892   ** \nimplementation_qualityNo apparent problems     0.6318  0.1471   4.2965  &lt;.0001   0.3436   0.9201  *** \nprogram_sitemixed                              0.3289  0.2413   1.3631  0.1729  -0.1440   0.8019      \nprogram_siteschool classroom                   0.2920  0.1736   1.6821  0.0926  -0.0482   0.6321    . \nprogram_siteschool, outside of classroom       0.1616  0.1898   0.8515  0.3945  -0.2104   0.5337      \nduration                                       0.0013  0.0009   1.3423  0.1795  -0.0006   0.0031      \nservice_hrs                                   -0.0003  0.0047  -0.0654  0.9478  -0.0096   0.0090      \n\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nmetafor produces model-based standard errors, t-tests, and confidence intervals. The coef_test function from clubSandwich will calculate robust standard errors and robust t-tests for each of the coefficients:\n\ncoef_test(m3_metafor, vcov = \"CR2\")\n\n                                       Coef.  Estimate       SE  t-stat d.f. (Satt) p-val (Satt) Sig.\n                                     intrcpt  0.529569 0.724851  0.7306       20.08      0.47347     \n         study_designNon-random, non-matched -0.049434 0.204152 -0.2421       58.42      0.80952     \n                      study_designRandomized  0.065272 0.149146  0.4376       53.17      0.66342     \n                                   attrition -0.136575 0.306429 -0.4457       10.52      0.66485     \n                           group_equivalence  0.407108 0.210917  1.9302       23.10      0.06595    .\n                       adjustedadjusted data -0.358124 0.136132 -2.6307       43.20      0.01176    *\n                             outcomeenrolled -0.283124 0.237199 -1.1936        7.08      0.27108     \n                           outcomegraduation -0.091295 0.091465 -0.9981        9.95      0.34188     \n                       outcomegraduation/ged  0.698328 0.364882  1.9138        8.02      0.09188    .\n evaluator_independenceIndirect, influential -0.752994 0.447670 -1.6820        6.56      0.13929     \n              evaluator_independencePlanning -0.769968 0.403898 -1.9063        6.10      0.10446     \n              evaluator_independenceDelivery -1.001648 0.355989 -2.8137        4.89      0.03834    *\n                                    male_pct  0.102055 0.148410  0.6877        9.68      0.50782     \n                                   white_pct  0.122255 0.141470  0.8642       16.88      0.39961     \n                                 average_age  0.006084 0.033387  0.1822       15.79      0.85772     \n     implementation_qualityPossible problems  0.473789 0.148660  3.1871       22.44      0.00419   **\n  implementation_qualityNo apparent problems  0.631842 0.138073  4.5761       28.68      &lt; 0.001  ***\n                           program_sitemixed  0.328941 0.196848  1.6710       27.47      0.10607     \n                program_siteschool classroom  0.291952 0.146014  1.9995       42.70      0.05195    .\n    program_siteschool, outside of classroom  0.161640 0.171700  0.9414       29.27      0.35420     \n                                    duration  0.001270 0.000978  1.2988       31.96      0.20332     \n                                 service_hrs -0.000309 0.004828 -0.0641       49.63      0.94915     \n\n\nNote that coef_test assumed that it should cluster based on studyID, which is the outer-most random effect in the metafor model. This can also be specified explicitly by including the option cluster = dropoutPrevention$studyID in the call.\nThe F-test for degree of evaluator independence uses the same syntax as before:\n\nWald_test(m3_metafor, constraints = constrain_zero(10:12), vcov = \"CR2\")\n\n test Fstat df_num df_denom  p_val sig\n  HTZ  2.71      3     18.3 0.0753   .\n\n\nDespite some differences in weighting schemes, the p-value is very close to the result obtained using robumeta.\n\n\n\n\n\n\nReuseCC BY 4.0CitationBibTeX citation:@online{pustejovsky2015,\n  author = {Pustejovsky, James E.},\n  title = {The {clubSandwich} Package for Meta-Analysis with {RVE}},\n  date = {2015-07-10},\n  url = {https://jepusto.com/posts/clubSandwich-for-RVE-meta-analysis},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nPustejovsky, James E. 2015. “The clubSandwich Package for\nMeta-Analysis with RVE.” July 10, 2015. https://jepusto.com/posts/clubSandwich-for-RVE-meta-analysis."
  },
  {
    "objectID": "posts/clubSandwich-at-RUG/index.html",
    "href": "posts/clubSandwich-at-RUG/index.html",
    "title": "clubSandwich at the Austin R User Group Meetup",
    "section": "",
    "text": "Last night I attended a joint meetup between the Austin R User Group and R Ladies Austin, which was great fun. The evening featured several lightning talks on a range of topics, from breaking into data science to network visualization to starting your own blog. I gave a talk about sandwich standard errors and my clubSandwich R package. Here are links to some of the talks:\n\nCaitlin Hudon: Getting Plugged into Data Science\nClaire McWhite: A quick intro to networks\nNathaniel Woodward: Blogdown Demo! (link includes his slides and a demo screencast)\nme: Robust, easy standard errors with the clubSandwich package.\n\n\n\n\n\n\nReuseCC BY 4.0CitationBibTeX citation:@online{pustejovsky2018,\n  author = {Pustejovsky, James E.},\n  title = {clubSandwich at the {Austin} {R} {User} {Group} {Meetup}},\n  date = {2018-04-26},\n  url = {https://jepusto.com/posts/clubSandwich-at-RUG},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nPustejovsky, James E. 2018. “clubSandwich at the Austin R User\nGroup Meetup.” April 26, 2018. https://jepusto.com/posts/clubSandwich-at-RUG."
  },
  {
    "objectID": "posts/Bug-in-nlme-with-fixed-sigma/index.html",
    "href": "posts/Bug-in-nlme-with-fixed-sigma/index.html",
    "title": "Bug in nlme::lme with fixed sigma and REML estimation",
    "section": "",
    "text": "About one year ago, the nlme package introduced a feature that allowed the user to specify a fixed value for the residual variance in linear mixed effect models fitted with lme(). This feature is interesting to me because, when used with the varFixed() specification for the residual weights, it allows for estimation of a wide variety of meta-analysis models, including basic random effects models, bivariate models for estimating effects by trial arm, and other sorts of multivariate/multi-level random effects models. However, in kicking the tires on this feature, I noticed that the results that it produces are not quite consistent with the results produced by metafor, which is the main package I use for fitting meta-analytic models.\nIn this post, I document several examples of discrepant estimates between lme() and rma.mv(), using standard datasets included in the metafor package. The main take-aways are:\n\nThe discrepancies arise only with REML estimation (not with ML estimation).\nThe discrepancies are present whether or not the varFixed specification is used.\nThe discrepancies are mostly small (with minimal impact on the standard errors of the fixed effect estimates), but are larger than I would expect from computational/convergence differences alone.\n\nAnother example, based on a different dataset, is documented in this bug report. Wolfgang Viechtbauer, author of the metafor package, identified this problem with lme a few months ago already (see his responses in this thread on the R mixed models mailing list) and noted that the issue was localized to REML estimation. My thanks to Wolfgang for providing feedback on this post.\n\nBasic random effects model\nThis example fits a basic random effects model to the BCG vaccine data, available within metafor:\n\nlibrary(metafor)\nlibrary(nlme)\n\nbcg_example &lt;- function(method = \"REML\", constant_var = FALSE) {\n  \n  data(dat.bcg)\n  dat &lt;- escalc(measure=\"OR\", ai=tpos, bi=tneg, ci=cpos, di=cneg, data=dat.bcg)\n  \n  v_bar &lt;- mean(dat$vi)\n  if (constant_var) dat$vi &lt;- v_bar\n  \n  # random-effects model using rma.uni()\n  LOR_uni_fit &lt;- rma(yi, vi, data=dat, method = method)\n  LOR_uni &lt;- with(LOR_uni_fit, \n                  data.frame(f = \"rma.uni\", \n                             logLik = logLik(LOR_uni_fit),\n                             est = as.numeric(b), \n                             se = se, \n                             tau = sqrt(tau2)))\n  \n  # random-effects model using rma.mv()\n  LOR_mv_fit &lt;- rma.mv(yi, vi, random = ~ 1 | trial, data=dat, method = method)\n  LOR_mv &lt;- with(LOR_mv_fit, \n                 data.frame(f = \"rma.mv\", \n                            logLik = logLik(LOR_mv_fit),\n                            est = as.numeric(b), \n                            se = se, \n                            tau = sqrt(sigma2)))\n  \n  # random-effects model using lme()\n  if (constant_var) {\n    LOR_lme_fit &lt;- lme(yi ~ 1, data = dat, method = method, \n                       random = ~ 1 | trial,\n                       control = lmeControl(sigma = sqrt(v_bar)))\n    tau &lt;- sqrt(as.numeric(coef(LOR_lme_fit$modelStruct$reStruct, unconstrained = FALSE)) * v_bar) \n  } else {\n    LOR_lme_fit &lt;- lme(yi ~ 1, data = dat, method = method, \n                       random = ~ 1 | trial,\n                       weights = varFixed(~ vi),\n                       control = lmeControl(sigma = 1))\n    tau &lt;- sqrt(as.numeric(coef(LOR_lme_fit$modelStruct$reStruct, unconstrained = FALSE)))\n  }\n  LOR_lme &lt;- data.frame(f = \"lme\", \n                        logLik = logLik(LOR_lme_fit),\n                        est = as.numeric(fixef(LOR_lme_fit)), \n                        se = as.numeric(sqrt(vcov(LOR_lme_fit))), \n                        tau = tau)\n  \n  rbind(LOR_uni, LOR_mv, LOR_lme)\n  \n}\n\nbcg_example(\"REML\", constant_var = FALSE)\n\n        f    logLik        est        se       tau\n1 rma.uni -12.57566 -0.7451778 0.1860279 0.5811816\n2  rma.mv -12.57566 -0.7451778 0.1860280 0.5811818\n3     lme -13.34043 -0.7471979 0.1916902 0.6030524\n\nbcg_example(\"REML\", constant_var = TRUE)\n\n        f    logLik        est        se       tau\n1 rma.uni -12.96495 -0.7716272 0.1977007 0.5911451\n2  rma.mv -12.96495 -0.7716272 0.1977007 0.5911452\n3     lme -15.62846 -0.7716272 0.1899448 0.5571060\n\nbcg_example(\"ML\", constant_var = FALSE)\n\n        f    logLik        est        se       tau\n1 rma.uni -13.07276 -0.7419668 0.1779534 0.5499605\n2  rma.mv -13.07276 -0.7419669 0.1779534 0.5499608\n3     lme -13.07276 -0.7419668 0.1779534 0.5499605\n\nbcg_example(\"ML\", constant_var = TRUE)\n\n        f     logLik        est        se       tau\n1 rma.uni -13.525084 -0.7716272 0.1899447 0.5571059\n2  rma.mv -13.525084 -0.7716272 0.1899447 0.5571059\n3     lme  -2.479133 -0.7716272 0.1899447 0.5571060\n\n\n\n\nBi-variate random effects model\nThis example fits a bi-variate random effects model, also to the BCG vaccine data:\n\nbcg_bivariate &lt;- function(method = \"REML\", constant_var = FALSE) {\n  data(dat.bcg)\n  dat_long &lt;- to.long(measure=\"OR\", ai=tpos, bi=tneg, ci=cpos, di=cneg, data=dat.bcg)\n  levels(dat_long$group) &lt;- c(\"exp\", \"con\")\n  dat_long$group &lt;- relevel(dat_long$group, ref=\"con\")\n  dat_long &lt;- escalc(measure=\"PLO\", xi=out1, mi=out2, data=dat_long)\n\n  v_bar &lt;- mean(dat_long$vi)\n  \n  if (constant_var) dat_long$vi &lt;- v_bar\n  \n  # bivariate random-effects model using rma.mv()\n  \n  bv_rma_fit &lt;- rma.mv(yi, vi, mods = ~ group, \n                       random = ~ group | study, \n                       struct = \"UN\", method = method,\n                       data=dat_long)\n  bv_rma &lt;- with(bv_rma_fit, data.frame(f = \"rma.mv\",\n                                        logLik = logLik(bv_rma_fit),\n                                        tau1 = sqrt(tau2[1]),\n                                        tau2 = sqrt(tau2[2])))\n  \n  # bivariate random-effects model using lme()\n  if (constant_var) {\n    bv_lme_fit &lt;- lme(yi ~ group, data = dat_long, method = method, \n                      random = ~ group | study,\n                      control = lmeControl(sigma = sqrt(v_bar)))\n    tau_sq &lt;- colSums(coef(bv_lme_fit$modelStruct$reStruct, unconstrained = FALSE) * matrix(c(1,0,0, 1,2,1), 3, 2)) * v_bar\n    \n  } else {\n    bv_lme_fit &lt;- lme(yi ~ group, data = dat_long, method = method, \n                      random = ~ group | study,\n                      weights = varFixed(~ vi),\n                      control = lmeControl(sigma = 1))\n    \n    tau_sq &lt;- colSums(coef(bv_lme_fit$modelStruct$reStruct, unconstrained = FALSE) * matrix(c(1,0,0, 1,2,1), 3, 2))\n    \n  }\n  \n  bv_lme &lt;- data.frame(f = \"lme\",\n                       logLik = logLik(bv_lme_fit),\n                       tau1 = sqrt(tau_sq[1]),\n                       tau2 = sqrt(tau_sq[2]))\n  \n  rbind(bv_rma, bv_lme)\n  \n}\n\nbcg_bivariate(\"REML\", constant_var = FALSE)\n\n       f    logLik     tau1     tau2\n1 rma.mv -31.50167 1.244429 1.617808\n2    lme -32.32612 1.254436 1.631619\n\nbcg_bivariate(\"REML\", constant_var = TRUE)\n\n       f    logLik     tau1     tau2\n1 rma.mv -31.09623 1.191679 1.644897\n2    lme -37.06035 1.142260 1.578434\n\nbcg_bivariate(\"ML\", constant_var = FALSE)\n\n       f    logLik     tau1     tau2\n1 rma.mv -33.08793 1.196399 1.551558\n2    lme -33.08793 1.196399 1.551558\n\nbcg_bivariate(\"ML\", constant_var = TRUE)\n\n       f     logLik    tau1     tau2\n1 rma.mv -32.647023 1.14226 1.578434\n2    lme  -2.237355 1.14226 1.578435\n\n\n\n\nThree-level random-effects model\nThis example fits a three-level random-effects model to the data from Konstantopoulos (2011):\n\nKonstantopoulos &lt;- function(method = \"REML\", constant_var = FALSE) {\n  \n  dat &lt;- get(data(dat.konstantopoulos2011))\n  v_bar &lt;- mean(dat$vi)\n  if (constant_var) dat$vi &lt;- v_bar\n  \n  # multilevel random-effects model using rma.mv()\n  ml_rma_fit &lt;- rma.mv(yi, vi, random = ~ 1 | district/school, data=dat, method = method)\n  \n  ml_rma &lt;- with(ml_rma_fit, \n                 data.frame(f = \"rma.mv\", \n                            logLik = logLik(ml_rma_fit),\n                            est = as.numeric(b), \n                            se = se, \n                            tau1 = sqrt(sigma2[1]), \n                            tau2 = sqrt(sigma2[2])))\n  \n  # multilevel random-effects model using lme()\n  if (constant_var) {\n    ml_lme_fit &lt;- lme(yi ~ 1, data = dat, method = method, \n                      random = ~ 1 | district / school,\n                      control = lmeControl(sigma = sqrt(v_bar)))\n    tau &lt;- sqrt(as.numeric(coef(ml_lme_fit$modelStruct$reStruct, unconstrained = FALSE)) * v_bar)\n    \n  } else {\n    ml_lme_fit &lt;- lme(yi ~ 1, data = dat, method = method, \n                      random = ~ 1 | district / school,\n                      weights = varFixed(~ vi),\n                      control = lmeControl(sigma = 1))\n    tau &lt;- sqrt(as.numeric(coef(ml_lme_fit$modelStruct$reStruct, unconstrained = FALSE)))\n    \n  }  \n  ml_lme &lt;- data.frame(f = \"lme\",\n                       logLik = logLik(ml_lme_fit),\n                       est = as.numeric(fixef(ml_lme_fit)),\n                       se = as.numeric(sqrt(diag(vcov(ml_lme_fit)))),\n                       tau1 = tau[2],\n                       tau2 = tau[1])\n  \n  rbind(ml_rma, ml_lme)\n  \n}\n\nKonstantopoulos(\"REML\", constant_var = FALSE)\n\n       f     logLik       est         se      tau1      tau2\n1 rma.mv  -7.958724 0.1847132 0.08455592 0.2550724 0.1809324\n2    lme -10.716781 0.1841827 0.08641374 0.2605790 0.1884588\n\nKonstantopoulos(\"REML\", constant_var = TRUE)\n\n       f     logLik       est         se      tau1      tau2\n1 rma.mv  -9.724839 0.1724309 0.08052701 0.2401816 0.1878155\n2    lme -16.119274 0.1724309 0.07980479 0.2380275 0.1848778\n\nKonstantopoulos(\"ML\", constant_var = FALSE)\n\n       f    logLik       est         se      tau1      tau2\n1 rma.mv -8.394936 0.1844554 0.08048168 0.2402881 0.1812865\n2    lme -8.394936 0.1844554 0.08048168 0.2402881 0.1812865\n\nKonstantopoulos(\"ML\", constant_var = TRUE)\n\n       f    logLik       est         se      tau1      tau2\n1 rma.mv -10.11095 0.1712365 0.07645094 0.2250687 0.1881229\n2    lme  90.21692 0.1712365 0.07645093 0.2250687 0.1881228\n\n\n\n\n\n\n\n\nReuseCC BY 4.0CitationBibTeX citation:@online{pustejovsky2016,\n  author = {Pustejovsky, James E.},\n  title = {Bug in Nlme::lme with Fixed Sigma and {REML} Estimation},\n  date = {2016-11-07},\n  url = {https://jepusto.com/posts/Bug-in-nlme-with-fixed-sigma},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nPustejovsky, James E. 2016. “Bug in Nlme::lme with Fixed Sigma and\nREML Estimation.” November 7, 2016. https://jepusto.com/posts/Bug-in-nlme-with-fixed-sigma."
  },
  {
    "objectID": "posts/Back-from-IES-PI-meeting/index.html",
    "href": "posts/Back-from-IES-PI-meeting/index.html",
    "title": "Back from the IES PI meeting",
    "section": "",
    "text": "I’m just back from the Institute of Education Sciences’ Principle Investigators conference in Washington D.C. It was an envigorating trip for me, and not only because of the opportunity to catch up with colleagues and friends from across the country. A running theme across several of the keynote addresses was the importance of increasing the transparency and replicability of education research, and it was exciting to hear about promising reforms underway and to talk about how to change the norms of our discipline(s).\nI contributed to the conference in two ways. First, I gave a presentation on incorporating randomization and randomization inference into single-case designs, as part of a session on innovations in single-case research methods organized by Dr. Wendy Machalicek. You can a static version of my slides here; unfortunately the animations don’t work in pdf.\nSecond, I brought a poster presenting some work from my IES-funded methods grant. Thanks very much to the folks who stopped by to talk during the poster session! Y’all gave me some very helpful feedback about technical aspects of the work and about how to better contextualize it for single case researchers.\nIf you didn’t make it: this project was joint work with Danny Swan, a doctoral student in our Quantitative Methods program. It involved developing a model for estimating effect sizes from single-case designs where the effects of the intervention take time to reach full potency. Rather than assuming that the intervention produces immediate shifts in the level of the outcome, we model the effects using an impulse response function (cribbed from an old paper by Box and Tiao) that leads to non-linear trends in response to the introduction of the intervention. Using an impulse response function also makes it possible to model more complex design patterns, like treatment reversal designs with returns-to-baseline and treatment re-introduction phases, in a very parsimonious way. Check out the full paper, the accompanying R package, and Danny’s interactive web-app.\n\n\n\n\n\nReuseCC BY 4.0CitationBibTeX citation:@online{pustejovsky2018,\n  author = {Pustejovsky, James E.},\n  title = {Back from the {IES} {PI} Meeting},\n  date = {2018-01-10},\n  url = {https://jepusto.com/posts/Back-from-IES-PI-meeting},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nPustejovsky, James E. 2018. “Back from the IES PI Meeting.”\nJanuary 10, 2018. https://jepusto.com/posts/Back-from-IES-PI-meeting."
  },
  {
    "objectID": "posts/ARPobservation-now-on-CRAN/index.html",
    "href": "posts/ARPobservation-now-on-CRAN/index.html",
    "title": "ARPobservation now on CRAN",
    "section": "",
    "text": "Version 1.0 of the ARPobservation package is now available on the Comprehensive R Archive Network. This makes it even easier to install. Here’s the package description:\nARPobservation: Tools for simulating different methods of observing behavior based on alternating renewal processes\nARPobservation provides a set of tools for simulating data based on direct observation of behavior. It works by first simulating a behavior stream based on an alternating renewal process, given specified distributions of event durations and interim times. Different procedures for recording data can then be applied to the simulated behavior stream. Currently, functions are provided for the following recording methods: continuous duration recording, event counting, momentary time sampling, partial interval recording, and whole interval recording.\n\n\n\n\n\nReuseCC BY 4.0CitationBibTeX citation:@online{pustejovsky2014,\n  author = {Pustejovsky, James E.},\n  title = {ARPobservation Now on {CRAN}},\n  date = {2014-05-31},\n  url = {https://jepusto.com/posts/ARPobservation-now-on-CRAN},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nPustejovsky, James E. 2014. “ARPobservation Now on CRAN.”\nMay 31, 2014. https://jepusto.com/posts/ARPobservation-now-on-CRAN."
  },
  {
    "objectID": "posts/ancova-puzzler/index.html",
    "href": "posts/ancova-puzzler/index.html",
    "title": "An ANCOVA puzzler",
    "section": "",
    "text": "Doing effect size calculations for meta-analysis is a good way to lose your faith in humanity—or at least your faith in researchers’ abilities to do anything like sensible statistical inference. Try it, and you’re surely encounter head-scratchingly weird ways that authors have reported even simple analyses, like basic group comparisons. When you encounter this sort of thing, you have two paths: you can despair, curse, and/or throw things, or you can view the studies as curious little puzzles—brain-teasers, if you will—to keep you awake and prevent you from losing track of those notes you took during your stats courses, back when. Here’s one of those curious little puzzles, which I recently encountered in helping a colleague with a meta-analysis project.\nA researcher conducts a randomized experiment, assigning participants to each of \\(G\\) groups. Each participant is assessed on a variable \\(Y\\) at pre-test and at post-test (we can assume there’s no attrition). In their study write-up, the researcher reports sample sizes for each group, means and standard deviations for each group at pre-test and at post-test, and adjusted means at post-test, where the adjustment is done using a basic analysis of covariance, controlling for pre-test scores only. The data layout looks like this:\n\n\n\n\n\n\n\n\n\n\n\n\nGroup\n\\(N\\)\nPre-test \\(M\\)\nPre-test \\(SD\\)\nPost-test \\(M\\)\nPost-test \\(SD\\)\nAdjusted post-test \\(M\\)\n\n\n\n\nGroup A\n\\(n_A\\)\n\\(\\bar{x}_{A}\\)\n\\(s_{A0}\\)\n\\(\\bar{y}_{A}\\)\n\\(s_{A1}\\)\n\\(\\tilde{y}_A\\)\n\n\nGroup B\n\\(n_B\\)\n\\(\\bar{x}_{B}\\)\n\\(s_{B0}\\)\n\\(\\bar{y}_{B}\\)\n\\(s_{B1}\\)\n\\(\\tilde{y}_B\\)\n\n\n\\(\\vdots\\)\n\\(\\vdots\\)\n\\(\\vdots\\)\n\\(\\vdots\\)\n\\(\\vdots\\)\n\\(\\vdots\\)\n\\(\\vdots\\)\n\n\n\nNote that the write-up does not provide an estimate of the correlation between the pre-test and the post-test, nor does it report a standard deviation or standard error for the mean change-score between pre-test and post-test within each group. All we have are the summary statistics, plus the adjusted post-test scores. We can assume that the adjustment was done according to the basic ANCOVA model, assuming a common slope across groups as well as homoskedasticity and so on. The model is then \\[\ny_{ig} = \\alpha_g + \\beta x_{ig} + e_{ig},\n\\] for \\(i = 1,...,n_g\\) and \\(g = 1,...,G\\), where \\(e_{ig}\\) is an independent error term that is assumed to have constant variance across groups.\n\nFor realz?\nHere’s an example with real data, drawn from Table 2 of Murawski (2006):\n\n\n\n\n\n\n\n\n\n\n\n\nGroup\n\\(N\\)\nPre-test \\(M\\)\nPre-test \\(SD\\)\nPost-test \\(M\\)\nPost-test \\(SD\\)\nAdjusted post-test \\(M\\)\n\n\n\n\nGroup A\n25\n37.48\n4.64\n37.96\n4.35\n37.84\n\n\nGroup B\n26\n36.85\n5.18\n36.46\n3.86\n36.66\n\n\nGroup C\n16\n37.88\n3.88\n37.38\n4.76\n36.98\n\n\n\nThat study reported this information for each of several outcomes, with separate analyses for each of two sub-groups (LD and NLD). The text also reports that they used a two-level hierarchical linear model for the ANCOVA adjustment. For simplicity, let’s just ignore the hierarchical linear model aspect and assume that it’s a straight, one-level ANCOVA.\n\n\nThe puzzler\nCalculate an estimate of the standardized mean difference between group \\(B\\) and group \\(A\\), along with the sampling variance of the SMD estimate, that adjusts for pre-test differences between groups. Candidates for numerator of the SMD include the adjusted mean difference, \\(\\tilde{y}_B - \\tilde{y}_A\\) or the difference-in-differences, \\(\\left(\\bar{y}_B - \\bar{x}_B\\right) - \\left(\\bar{y}_A - \\bar{x}_A\\right)\\). In either case, the tricky bit is finding the sampling variance of this quantity, which involves the pre-post correlation. For the denominator of the SMD, you use the post-test SD, either pooled across just groups \\(A\\) and \\(B\\) or pooled across all \\(G\\) groups, assuming a common population variance.\nHave an idea for how to solve this? Post it in the comments or email it to me. Need the solution because you have a study like this in your meta-analysis? Contact me and I’ll share it with you directly. I’m being coy because I’m teaching meta-analysis next semester, and I feel like this would make a good extra credit problem…\n\n\n\n\n\n\nReuseCC BY 4.0CitationBibTeX citation:@online{pustejovsky2020,\n  author = {Pustejovsky, James E.},\n  title = {An {ANCOVA} Puzzler},\n  date = {2020-11-24},\n  url = {https://jepusto.com/posts/ancova-puzzler},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nPustejovsky, James E. 2020. “An ANCOVA Puzzler.” November\n24, 2020. https://jepusto.com/posts/ancova-puzzler."
  },
  {
    "objectID": "posts/alternative-formulas-for-the-SMD/index.html",
    "href": "posts/alternative-formulas-for-the-SMD/index.html",
    "title": "Alternative formulas for the standardized mean difference",
    "section": "",
    "text": "The standardized mean difference (SMD) is surely one of the best known and most widely used effect size metrics used in meta-analysis. In generic terms, the SMD parameter is defined as the difference in population means between two groups (often this difference represents the effect of some intervention), scaled by the population standard deviation of the outcome metric. Estimates of the SMD can be obtained from a wide variety of experimental designs, ranging from simple, completely randomized designs, to repeated measures designs, to cluster-randomized trials.\nThere’s some nuance involved in figuring out how to calculate estimates of the SMD from each design, mostly to do with exactly what sort of standard deviation to use in the denominator of the effect size. I’ll leave that discussion for another day. Here, I’d like to look at the question of how to estimate the sampling variance of the SMD. An estimate of the sampling variance is needed in order to meta-analyze a collection of effect sizes, and so getting the variance calculations right is an important (and sometimes time consuming) part of any meta-analysis project. However, the standard textbook treatments of effect size calculations cover this question only for a limited number of simple cases. I’d like to suggest a different, more general way of thinking about it, which provides a way to estimate the SMD and its variance in some non-standard cases (and also leads to slight differences from conventional formulas for the standard ones). All of this will be old hat for seasoned synthesists, but I hope it might be useful for students and researchers just getting started with meta-analysis.\nTo start, let me review (regurgitate?) the standard presentation.\n\nSMD from a simple, independent groups design\nTextbook presentations of the SMD estimator almost always start by introducing the estimator in the context of a simple, independent groups design. Call the groups T and C, the sample sizes \\(n_T\\) and \\(n_C\\), the sample means \\(\\bar{y}_T\\) and \\(\\bar{y}_C\\), and the sample variances \\(s_T^2\\) and \\(s_C^2\\). A basic moment estimator of the SMD is then\n\\[\nd = \\frac{\\bar{y}_T - \\bar{y}_C}{s_p}\n\\]\nwhere \\(s_p^2 = \\frac{\\left(n_T - 1\\right)s_T^2 + \\left(n_C - 1\\right) s_C^2}{n_T + n_C - 2}\\) is a pooled estimator of the population variance. The standard estimator for the sampling variance of \\(d\\) is\n\\[\nV_d = \\frac{n_T + n_C}{n_T n_C} + \\frac{d^2}{2\\left(n_T + n_C - 2\\right)},\n\\]\nor some slight variant thereof. This estimator is based on a delta-method approximation for the asymptotic variance of \\(d\\).\nIt is well known that \\(d\\) has a small sample bias that depends on sample sizes. Letting\n\\[\nJ(x) = 1 - \\frac{3}{4x - 1},\n\\]\nthe bias-corrected estimator is\n\\[\ng = J\\left(n_T + n_C - 2\\right) \\times d,\n\\]\nand is often referred to as Hedges’ \\(g\\) because it was proposed in Hedges (1981). Some meta-analysts use \\(V_d\\), but with \\(d^2\\) replaced by \\(g^2\\), as an estimator of the large-sample variance of \\(g\\); others use\n\\[\nV_g = J^2\\left(n_T + n_C - 2\\right) \\left(\\frac{n_T + n_C}{n_T n_C} + \\frac{g^2}{2\\left(n_T + n_C - 2\\right)}\\right).\n\\]\nViechtbauer (2007) provides further details on variance estimation and confidence intervals for the SMD in this case.\n\n\nA general formula for \\(g\\) and its sampling variance\nThe above formulas are certainly useful, but in practice meta-analyses often include studies that use other, more complex designs. Good textbook presentations also cover computation of \\(g\\) and its variance for some other cases (e.g., Borenstein, 2009, also covers one-group pre/post designs and analysis of covariance). Less careful presentations only cover the simple, independent groups design and thus may inadvertently leave the impression that the variance estimator \\(V_d\\) given above applies in general. With other types of studies, \\(V_d\\) can be a wildly biased estimator of the actual sampling variance of \\(d\\), because it is derived under the assumption that the numerator of \\(d\\) is estimated as the difference in means of two simple random samples. In some designs (e.g., ANCOVA designs, randomized block designs, repeated measures designs), the treatment effect estimate will be much more precise than this; in other designs (e.g., cluster-randomized trials), it will be less precise.\nHere’s what I think is a more useful way to think about the sampling variance of \\(d\\). Let’s suppose that we have an unbiased estimator for the difference in means that goes into the numerator of the SMD. Call this estimator \\(b\\), its sampling variance \\(\\text{Var}(b)\\), and its standard error \\(se_{b}\\). Also suppose that we have an unbiased (or reasonably close-to-unbiased) estimator of the population variance of the outcome, the square root of which goes into the denominator of the SMD. Call this estimator \\(S^2\\), with expectation \\(\\text{E}\\left(S^2\\right) = \\sigma^2\\) and sampling variance \\(\\text{Var}(S^2)\\). Finally, suppose that \\(b\\) and \\(S^2\\) are independent (which will often be a pretty reasonable assumption). A delta-method approximation for the sampling variance of \\(d = b / S\\) is then\n\\[\n\\text{Var}\\left(d\\right) \\approx \\frac{\\text{Var}(b)}{\\sigma^2} + \\frac{\\delta^2}{2 \\nu},\n\\]\nwhere \\(\\nu = 2 \\left[\\text{E}\\left(S^2\\right)\\right]^2 / \\text{Var}\\left(S^2\\right)\\). Plugging in sample estimates of the relevant parameters provides a reasonable estimator for the sampling variance of \\(d\\):\n\\[\nV_d = \\left(\\frac{se_b}{S}\\right)^2 + \\frac{d^2}{2 \\nu}.\n\\]\nThis estimator has two parts. The first part involves \\(se_b / S\\), which is just the standard error of \\(b\\), but re-scaled into standard deviation units; this part captures the variability in \\(d\\) from its numerator. This scaled standard error can be calculated directly if an article reports \\(se_b\\).\nThe second part of \\(V_d\\) is \\(d^2 / (2 \\nu)\\), which captures the variability in \\(d\\) due to its denominator. More precise estimates of \\(\\sigma\\) will have larger degrees of freedom, so that the second part will be smaller. For some designs, the degrees of freedom \\(\\nu\\) depend only on sample sizes, and thus can be calculated exactly. For some other designs, \\(\\nu\\) must be estimated.\nThe same degrees of freedom can also be used in the small-sample correction for the bias of \\(d\\), as given by\n\\[\ng = J(\\nu) \\times d.\n\\]\nThis small-sample correction is based on a Satterthwaite-type approximation to the distribution of \\(d\\).\nHere’s another way to express the variance estimator for \\(d\\):\n\\[\nV_d = d^2 \\left(\\frac{1}{t^2} + \\frac{1}{2 \\nu}\\right),\n\\]\nwhere \\(t\\) is the test statistic corresponding to the hypothesis test for no difference between groups. I’ve never seen that formula in print before, but it could be convenient if an article reports the \\(t\\) statistic (or \\(F = t^2\\) statistic).\n\n\nNon-standard estimators of \\(d\\)\nThe advantage of this formulation of \\(d\\), \\(g\\), and \\(V_d\\) is that it can be applied in quite a wide variety of circumstances, including cases that aren’t usually covered in textbook treatments. Rather than having to use separate formulas for every combination of design and analytic approach under the sun, the same formulas apply throughout. What changes are the components of the formulas: the scaled standard error \\(se_b / S\\) and the degrees of freedom \\(\\nu\\). The general formulation also makes it easier to swap in different estimates of \\(b\\) or \\(S\\)—i.e., if you estimate the numerator a different way but keep the denominator the same, you’ll need a new scaled standard error but can still use the same degrees of freedom. A bunch of examples:\n\nIndependent groups with different variances\nSuppose that we’re looking at two independent groups but do not want to assume that their variances are the same. In this case, it would make sense to standardize the difference in means by the control group standard deviation (without pooling), so that \\(d = \\left(\\bar{y}_T - \\bar{y}_C\\right) / s_C\\). Since \\(s_C^2\\) has \\(\\nu = n_C - 1\\) degrees of freedom, the small-sample bias correction will then need to be \\(J(n_C - 1)\\). The scaled standard error will be\n\\[\n\\frac{se_b}{s_C} = \\sqrt{\\frac{s_T^2}{s_C^2 n_T} + \\frac{1}{n_C}}.\n\\]\nThis is then everything that we need to calculate \\(V_d\\), \\(g\\), \\(V_g\\), etc.\n\n\nMultiple independent groups\nSuppose that the study involves \\(K - 1\\) treatment groups, 1 control group, and \\(N\\) total participants. If the meta-analysis will include SMDs comparing each treatment group to the control group, it would make sense to pool the sample variance across all \\(K\\) groups rather than just the pair of groups, so that a common estimate of scale is used across all the effect sizes. The pooled standard deviation is then calculated as\n\\[\ns_p^2 = \\frac{1}{N - K} \\sum_{k=0}^K (n_k - 1) s_k^2.\n\\]\nFor a comparison between treatment group \\(k\\) and the control group, we would then use\n\\[\nd = \\frac{\\bar{y}_k - \\bar{y}_C}{s_p}, \\qquad \\nu = N - K, \\qquad \\frac{se_b}{s_p} = \\sqrt{\\frac{1}{n_C} + \\frac{1}{n_k}},\n\\]\nwhere \\(n_k\\) is the sample size for treatment group \\(k\\) (cf. Gleser & Olkin, 2009).\n\n\nSingle group, pre-test post-test design\nSuppose that a study involves taking pre-test and post-test measurements on a single group of \\(n\\) participants. Borenstein (2009) recommends calculating the standardized mean difference for this study as the difference in means between the post-test and pre-test, scaled by the pooled (across pre- and post-test measurements) standard deviation. With obvious notation:\n\\[\nd = \\frac{\\bar{y}_{post} - \\bar{y}_{pre}}{s_p}, \\qquad \\text{where} \\qquad s_p^2 = \\frac{1}{2}\\left(s_{pre}^2 + s_{post}^2\\right).\n\\]\nIn this design,\n\\[\n\\frac{se_b}{s_p} = \\sqrt{\\frac{2(1 - r)}{n}},\n\\]\nwhere \\(r\\) is the sample correlation between the pre- and post-tests. The remaining question is what to use for \\(\\nu\\). Borenstein (2009) uses \\(\\nu = n - 1\\). My previous post on the sampling covariance of sample variances gave the result that \\(\\text{Var}(s_p^2) = \\sigma^4 (1 + \\rho^2) / (n - 1)\\), which would instead suggest using\n\\[\n\\nu = \\frac{2 (n - 1)}{1 + r^2}.\n\\]\nThis formula will tend to give slightly larger degrees of freedom, but probably won’t be that discrepant from Borenstein’s approach except in quite small samples. It would be interesting to investigate which approach is better in small samples (i.e., leading to less biased estimates of the SMD and more accurate estimates of sampling variance, and by how much), although its possible than neither is all that good because the variance estimator itself is based on a large-sample approximation.\n\n\nTwo group, pre-test post-test design: ANCOVA estimation\nSuppose that a study involves taking pre-test and post-test measurements on two groups of participants, with sample sizes \\(n_T\\) and \\(n_C\\) respectively. One way to analyze this design is via ANCOVA using the pre-test measure as the covariate, so that the treatment effect estimate is the difference in adjusted post-test means. In this design, the scaled standard error will be approximately\n\\[\n\\frac{se_b}{S} = \\sqrt{ \\frac{(n_C + n_T)(1 - r^2)}{n_C n_T} },\n\\]\nwhere \\(r\\) is the pooled, within-group sample correlation between the pre-test and the post-test measures (this approximation assumes that the pre-test SMD between groups is relatively small). Alternately, if \\(se_b\\) is provided then the scaled standard error could be calculated directly.\nBorenstein (2009) suggests calculating \\(d\\) as the difference in adjusted means, scaled by the pooled sample variances on the post-test measures. The post-test pooled sample variance will have the same degrees of freedom as in the two-sample t-test case: \\(\\nu = n_C + n_T - 2\\). (Borenstein instead uses \\(\\nu = n_C + n_T - 2 - q\\), where \\(q\\) is the number of covariates in the analysis, but this won’t usually make much difference unless the total sample size is quite small.)\nScaling by the pooled post-test sample variance isn’t the only reasonable way to estimate the SMD though. If the covariate is a true pre-test, then why not scale by the pooled pre-test sample variance instead? To do so, you would need to calculate \\(se_b / S\\) directly and use \\(\\nu = n_C + n_T - 2\\). If it is reasonable to assume that the pre- and post-test population variances are equal, then another alternative would be to pool across the pre-test and post-test sample variances in each group. Using this approach, you would again need to calculate \\(se_b / S\\) directly and then use \\(\\nu = 2(n_C + n_T - 2) / (1 + r^2)\\).\n\n\nTwo group, pre-test post-test design: repeated measures estimation\nAnother way to analyze the data from the same type of study design is to use repeated measures ANOVA. I’ve recently encountered a number of studies that use this approach (here’s a recent example from a highly publicized study in PLOS ONE—see Table 2). The studies I’ve seen typically report the sample means and variances in each group and at each time point, from which the difference in change scores can be calculated. Let \\(\\bar{y}_{gt}\\) and \\(s_{gt}^2\\) denote the sample mean and sample variance in group \\(g = T, C\\) at time \\(t = 0, 1\\). The numerator of \\(d\\) would then be calculated as\n\\[\nb = \\left(\\bar{y}_{T1} - \\bar{y}_{T0}\\right) - \\left(\\bar{y}_{C1} - \\bar{y}_{C0}\\right),\n\\]\nwhich has sampling variance \\(\\text{Var}(b) = 2(1 - \\rho)\\sigma^2\\left(n_C + n_T \\right) / (n_C n_T)\\), where \\(\\rho\\) is the correlation between the pre-test and the post-test measures. Thus, the scaled standard error is\n\\[\n\\frac{se_b}{S} = \\sqrt{\\frac{2(1 - r)(n_C + n_T)}{n_C n_T}}.\n\\]\nAs with ANCOVA, there are several potential options for calculating the denominator of \\(d\\):\n\nUsing the pooled sample variances on the post-test measures, with \\(\\nu = n_C + n_T - 2\\);\nUsing the pooled sample variances on the pre-test measures, with \\(\\nu = n_C + n_T - 2\\); or\nUsing the pooled sample variances at both time points and in both groups, i.e.,\n\\[\n  S^2 = \\frac{(n_C - 1)(s_{C0}^2 + s_{C1}^2) + (n_T - 1)(s_{T0}^2 + s_{T1}^2)}{2(n_C + n_T - 2)},\n  \\]\nwith \\(\\nu = 2(n_C + n_T - 2) / (1 + r^2)\\).\n\nThe range of approaches to scaling is the same as for ANCOVA. This makes sense because both analyses are based on data from the same study design, so the parameter of interest should be the same (i.e., the target parameter should not change based on the analytic method). Note that all of these approaches are a bit different than the effect size estimator proposed by Morris and DeShon (2002) for the two-group, pre-post design; their approach does not fit into my framework because it involves taking a difference between standardized effect sizes (and therefore involves two separate estimates of scale, rather than just one).\n\n\nRandomized trial with longitudinal follow-up\nMany independent-groups designs—especially randomized trials in field settings—involve repeated, longitudinal follow-up assessments. An increasingly common approach to analysis of such data is through hierarchical linear models, which can be used to account for the dependence structure among measurements taken on the same individual. In this setting, Feingold (2009) proposes that the SMD be calculated as the model-based estimate of the treatment effect at the final follow-up time, scaled by the within-groups variance of the outcome at that time point. Let \\(\\hat\\beta_1\\) denote the estimated difference in slopes (change per unit time) between groups in a linear growth model, \\(F\\) denote the duration of the study, and \\(s_{pF}^2\\) denote the pooled sample variance of the outcome at the final time point. For this model, Feingold (2009) proposes to calculate the standardized mean difference as\n\\[\nd = \\frac{F \\hat\\beta_1}{s_{pF}}.\n\\]\nIn a later paper, Feingold (2015) proposes that the sampling variance of \\(d\\) be estimated as \\(F \\times se_{\\hat\\beta_1} / s_{pF}\\), where \\(se_{\\hat\\beta_1}\\) is the standard error of the estimated slope. My framework suggests that a better estimate of the sampling variance, which accounts for the uncertainty of the scale estimate, would be to use\n\\[\nV_d = \\left(\\frac{F \\times se_{\\hat\\beta_1}}{s_{pF}}\\right)^2 + \\frac{d^2}{2 \\nu},\n\\]\nwith \\(\\nu = n_T + n_C - 2\\). The same \\(\\nu\\) could be used to bias-correct the effect size estimate.\nIf estimates of the variance components of the HLM are reported, one could use them to construct a model-based estimate of the scale parameter in the denominator of \\(d\\). I explored this approach in a paper that uses HLM to model single-case designs, which are a certain type of longitudinal experiment that typically involve a very small number of participants (Pustejovsky, Hedges, & Shadish, 2014). Estimates of the scale parameter can usually be written as\n\\[\nS_{model}^2 = \\mathbf{r}'\\boldsymbol\\omega,\n\\]\nwhere \\(\\boldsymbol\\omega\\) is a vector of all the variance components in the model and \\(\\mathbf{r}\\) is a vector of weights that depend on the model specification and length of follow-up. This estimate of scale will usually be more precise than \\(s_{pF}^2\\) because it makes use of all of the data (and modeling assumptions). However, it can be challenging to determine appropriate degrees of freedom for \\(S_{model}^2\\). For single-case designs, I used estimates of \\(\\text{Var}(\\boldsymbol\\omega)\\) based on the inverse of the expected information matrix—call the estimate \\(\\mathbf{V}_{\\boldsymbol\\omega}\\)—in which case\n\\[\n\\nu = \\frac{2 S_{model}^4}{\\mathbf{r}' \\mathbf{V}_{\\boldsymbol\\omega} \\mathbf{r}}.\n\\]\nHowever, most published articles will not provide estimates of the sampling variances of the variance components—in fact, a lot of software for estimating HLMs does not even provide these. It would be useful to work out some reasonable approximations for the degrees of freedom in these models—approximations that can be calculated based on the information that’s typically available—and to investigate the extent to which there’s any practical benefit to using \\(S_{model}^2\\) over \\(s_{pF}^2\\).\n\n\nCluster-randomized trials\nHedges (2007) addresses estimation of standardized mean differences for cluster-randomized trials, in which the units of measurement are nested within higher-level clusters that comprise the units of randomization. Such designs involve two variance components (within- and between-cluster variance), and thus there are three potential approaches to scaling the treatment effect: standardize by the total variance (i.e., the sum of the within- and between-cluster components), standardize by the within-cluster variance, or standardize by the between-cluster variance. Furthermore, some of the effect sizes can be estimated in several different ways, each with a different sampling variance. Hedges (2007) gives sampling variance estimates for each estimator of each effect size, but they all follow the same general formula as given above. (The appendix of the article actually gives the same formula as above, but using a more abstract formulation.)\nFor example, suppose the target SMD parameter uses the total variance and that we have data from a two-level, two-arm cluster randomized trial with \\(M\\) clusters, \\(n\\) observations per cluster, and total sample sizes in each arm of \\(N_T\\) and \\(N_C\\), respectively. Let \\(\\tau^2\\) be the between-cluster variance, \\(\\sigma^2\\) be the within-cluster variance, and \\(\\rho = \\tau^2 / (\\tau^2 + \\sigma^2)\\). The target parameter is \\(\\delta = \\left(\\mu_T - \\mu_C\\right) / \\left(\\tau^2 + \\sigma^2\\right)\\). The article assumes that the treatment effect will be estimated by the difference in grand means, \\(\\bar{\\bar{y}}_T - \\bar{\\bar{y}}_C\\). Letting \\(S_B^2\\) be the pooled sample variance of the cluster means within each arm and \\(S_W^2\\) be the pooled within-cluster sample variance, the total variance is estimated as\n\\[\nS_{total}^2 = S_B^2 + \\frac{n - 1}{n} S_W^2.\n\\]\nAn estimate of the SMD is then\n\\[\nd = \\left(\\bar{\\bar{y}}_T - \\bar{\\bar{y}}_C \\right) / \\sqrt{S_{total}^2}.\n\\]\nThe scaled standard error of \\(\\bar{\\bar{y}}_T - \\bar{\\bar{y}}_C\\) is\n\\[\nse_b = \\sqrt{\\left(\\frac{N_C + N_T}{N_C N_T}\\right)\\left[1 + (n - 1)\\rho\\right]}.\n\\]\nThe appendix of the article demonstrates that \\(\\text{E}\\left(S_{total}^2\\right) = \\tau^2 + \\sigma^2\\) and\n\\[\n\\text{Var}\\left( S_{total}^2 \\right) = \\frac{2}{n^2}\\left(\\frac{(n \\tau^2 + \\sigma^2)^2}{M - 2} + \\frac{(n - 1)^2 \\sigma^4}{N_C + N_T - M}\\right),\n\\]\nby which it follows that\n\\[\n\\nu = \\frac{n^2 M (M - 2)}{M[(n - 1)\\rho + 1]^2 + (M - 2)(n - 1)(1 - \\rho)^2}.\n\\]\nSubstituting \\(se_b / S_{total}\\) and \\(\\nu\\) into the formula for \\(V_d\\) gives the same as Expression (14) in the article.\nA limitation of Hedges (2007) is that it only covers the case where the treatment effect is estimated by the difference in grand means (although it does cover the case of unequal cluster sizes, which gets quite messy). In practice, every cluster-randomized trial I’ve ever seen uses baseline covariates to adjust the mean difference (often based on a hierarchical linear model) and improve the precision of the treatment effect estimate. The SMD estimate should also be based on this covariate-adjustment estimate, scaled by the total variance without adjusting for the covariate. An advantage of the general formulation given above is that its clear how to estimate the sampling variance of \\(d\\). I would guess that it will often be possible to calculate the scaled standard error directly, given the standard error of the covariate-adjusted treatment effect estimate. And since \\(S_{total}\\) would be estimated just as before, its degrees of freedom remain the same.\nHedges (2011) discusses estimation of SMDs in three-level cluster-randomized trials—an even more complicated case. However, the general approach is the same; all that’s needed are the scaled standard error and the degrees of freedom \\(\\nu\\) of whatever combination of variance components go into the denominator of the effect size. In both the two-level and three-level cases, the degrees of freedom get quite complicated in unbalanced samples and are probably not calculable from the information that is usually provided in an article. Hedges (2007, 2011) comments on a couple of cases where more tractable approximations can be used, although it seems like there might be room for further investigation here.\n\n\n\nClosing thoughts\nI think this framework is useful in that it unifies a large number of cases that have been treated separately, and can also be applied (more-or-less immediately) to \\(d\\) estimators that haven’t been widely considered before, such as the \\(d\\) that involves scaling by the pooled pre-and-post, treatment-and-control sample variance. I hope it also illustrates that, while the point estimator \\(d\\) can be applied across a large number of study designs, the sampling variance of \\(d\\) depends on the details of the design and estimation methods. The same is true for other families of effect sizes as well. For example, in other work I’ve demonstrated that the sampling variance of the correlation coefficient depends on the design from which the correlations are estimated (Pustejovsky, 2014).\nIf you have read this far, I’d love to get your feedback about whether you think this is a useful way to organize the calculations of \\(d\\) estimators. Is this helpful? Or nothing you didn’t already know? Or still more complicated than it should be? Leave a comment!\n\n\nReferences\nBorenstein, M. (2009). Effect sizes for continuous data. In H. M. Cooper, L. V Hedges, & J. C. Valentine (Eds.), The Handbook of Research Synthesis and Meta-Analysis (pp. 221–236). New York, NY: Russell Sage Foundation.\nFeingold, A. (2009). Effect sizes for growth-modeling analysis for controlled clinical trials in the same metric as for classical analysis. Psychological Methods, 14(1), 43–53. doi:10.1037/a0014699\nFeingold, A. (2015). Confidence interval estimation for standardized effect sizes in multilevel and latent growth modeling. Journal of Consulting and Clinical Psychology, 83(1), 157–168. doi:10.1037/a0037721\nGleser, L. J., & Olkin, I. (2009). Stochastically dependent effect sizes. In H. Cooper, L. V. Hedges, & J. C. Valentine (Eds.), The Handbook of Research Synthesis and Meta-Analysis (2nd ed., pp. 357–376). New York, NY: Russell Sage Foundation.\nHedges, L. V. (2007). Effect sizes in cluster-randomized designs. Journal of Educational and Behavioral Statistics, 32(4), 341–370. doi:10.3102/1076998606298043\nHedges, L. V. (2011). Effect sizes in three-level cluster-randomized experiments. Journal of Educational and Behavioral Statistics, 36(3), 346–380. doi:10.3102/1076998610376617\nMorris, S. B., & DeShon, R. P. (2002). Combining effect size estimates in meta-analysis with repeated measures and independent-groups designs. Psychological Methods, 7(1), 105–125. doi:10.1037//1082-989X.7.1.105\nPustejovsky, J. E. (2014). Converting from d to r to z when the design uses extreme groups, dichotomization, or experimental control. Psychological Methods, 19(1), 92–112. doi:10.1037/a0033788\nPustejovsky, J. E., Hedges, L. V, & Shadish, W. R. (2014). Design-comparable effect sizes in multiple baseline designs: A general modeling framework. Journal of Educational and Behavioral Statistics, 39(5), 368–393. doi:10.3102/1076998614547577\nViechtbauer, W. (2007). Approximate confidence intervals for standardized effect sizes in the two-independent and two-dependent samples design. Journal of Educational and Behavioral Statistics, 32(1), 39–60. doi:10.3102/1076998606298034\n\n\n\n\n\n\nReuseCC BY 4.0CitationBibTeX citation:@online{pustejovsky2016,\n  author = {Pustejovsky, James E.},\n  title = {Alternative Formulas for the Standardized Mean Difference},\n  date = {2016-06-03},\n  url = {https://jepusto.com/posts/alternative-formulas-for-the-SMD},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nPustejovsky, James E. 2016. “Alternative Formulas for the\nStandardized Mean Difference.” June 3, 2016. https://jepusto.com/posts/alternative-formulas-for-the-SMD."
  },
  {
    "objectID": "posts/Another-project-idea/index.html",
    "href": "posts/Another-project-idea/index.html",
    "title": "Another project idea: Meta-analytic methods for correlational data",
    "section": "",
    "text": "Several different approaches have been proposed for meta-analysis of correlation coefficients. One of the major differences between approaches is the choice of scale: whether effect sizes should be analyzed on the Pearson-r scale or first transformed to the Fisher-z scale. This project will study methods for modeling correlation coefficients on the r scale in the presence of between-study effect heterogeneity. Specific topics include:\n\nrefined methods for variance estimation;\nhierarchical modeling to capture differences between distinct operationalizations of the same construct; and\napplication to a large correlational meta-analysis.\n\nThis project would be appropriate for a Quantitative Methods graduate student with interests in meta-analysis and hierarchical models.\n\n\n\n\n\nReuseCC BY 4.0CitationBibTeX citation:@online{pustejovsky2013,\n  author = {Pustejovsky, James E.},\n  title = {Another Project Idea: {Meta-analytic} Methods for\n    Correlational Data},\n  date = {2013-09-13},\n  url = {https://jepusto.com/posts/Another-project-idea},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nPustejovsky, James E. 2013. “Another Project Idea: Meta-Analytic\nMethods for Correlational Data.” September 13, 2013. https://jepusto.com/posts/Another-project-idea."
  },
  {
    "objectID": "posts/ARPobservation-basic-use/index.html",
    "href": "posts/ARPobservation-basic-use/index.html",
    "title": "ARPobservation: Basic use",
    "section": "",
    "text": "The ARPobservation package provides a set of tools for simulating data generated by different procedures for direct observation of behavior. This is accomplished in two steps. The first step is to simulate a “behavior stream” itself, which is assumed to follow some type of alternating renewal process. The second step is to apply a procedure or “filter,” which turns the simulated behavior stream into the data recorded by a given observation procedure. Each of these steps is illustrated below."
  },
  {
    "objectID": "posts/ARPobservation-basic-use/index.html#simulating-behavior-streams",
    "href": "posts/ARPobservation-basic-use/index.html#simulating-behavior-streams",
    "title": "ARPobservation: Basic use",
    "section": "Simulating behavior streams",
    "text": "Simulating behavior streams\nBehavior streams are simulated according to an equilibrium alternating renewal process, which involves the following assumptions.\n\nEach instance of a behavior, termed an event, lasts a random amount of time, drawn from a specified distribution F_mu with mean mu.\nThe length of time in between instances of behavior, termed the interim time, also lasts a random amount of time, drawn from a specified distribution F_lambda with mean lambda.\nAll events and interim times are mutually independent.\nThe entire process is in equilibrium.\n\nThe function r_behavior_stream generates random behavior streams. As an initial example, suppose that both the events and the interim times are exponentially distributed, that events last on average 10 seconds, and that the average interim time is 30 seconds. Also suppose that the behavior stream is observed for 300 seconds. The following code will simulate a behavior stream with these parameters:\n\nlibrary(ARPobservation)\nset.seed(8)              # for reproducibility\n\nr_behavior_stream(n = 1, mu = 10, lambda = 30, F_event = F_exp(), F_interim = F_exp(), stream_length = 300)\n\n$stream_length\n[1] 300\n\n$b_streams\n$b_streams[[1]]\n$b_streams[[1]]$start_state\n[1] 0\n\n$b_streams[[1]]$b_stream\n [1]  61.46643  67.45959 117.53097 120.56840 175.94950 185.74134 265.04376\n [8] 269.42231 276.13827 284.70467 286.36179 290.82906\n\n\n\nattr(,\"class\")\n[1] \"behavior_stream\"\n\n\nThe function returns an object of class behavior_stream, which isn’t terribly nice to look at. The first characteristic of the object is stream_length, which just reports back how long the behavior stream is. The second characteristic is b_streams, a list containing one or more simulated behavior streams. Each behavior stream is also a list. The first element indicate the initial state of the stream, so start_state = ``{r} BS$b_streams[[1]]$start_state means that the behavior was not occuring when observation began. The second element is a vector of transition times. The first entry in the vector indicates that the first event began at time {r} round(BS$b_streams[[1]]$b_stream[1],2); the following entry indicates that the first event ended (and the next interim time began) at time {r} round(BS$b_streams[[1]]$b_stream[2],2). Similarly, the second event began at time {r} round(BS$b_streams[[1]]$b_stream[3],2) and ended at time {r} round(BS$b_streams[[1]]$b_stream[4],2).\nThe argument n controls the number of simulated behavior streams returned:\n\nr_behavior_stream(n = 3, mu = 10, lambda = 30, F_event = F_exp(), F_interim = F_exp(), stream_length = 300)\n\n$stream_length\n[1] 300\n\n$b_streams\n$b_streams[[1]]\n$b_streams[[1]]$start_state\n[1] 1\n\n$b_streams[[1]]$b_stream\n [1]   8.480116  34.311542  43.069956  49.912461  50.087867  85.046893\n [7] 103.030351 116.377965 117.101992 140.227289 161.762642 180.640609\n[13] 196.060432 201.493182 212.232970 236.486373 238.432946 276.824019\n\n\n$b_streams[[2]]\n$b_streams[[2]]$start_state\n[1] 0\n\n$b_streams[[2]]$b_stream\n [1]   6.702804  23.820354  26.087981  33.461543  62.786605  74.705604\n [7] 163.806646 164.761520 271.270557 283.207882 286.136103 297.587748\n\n\n$b_streams[[3]]\n$b_streams[[3]]$start_state\n[1] 0\n\n$b_streams[[3]]$b_stream\n [1] 196.4605 203.7452 237.9514 245.2451 246.2089 254.6313 256.6439 258.5644\n [9] 262.1140 265.3249 283.9702 298.7830\n\n\n\nattr(,\"class\")\n[1] \"behavior_stream\"\n\n\nNote that now b_streams is a list with three entries, each of which contains a start_state and a b_stream.\nMost of the time, you won’t need to look at the simulated behavior streams directly. Instead, you’ll just simulate a bunch of streams and store them for later analysis. Let’s store 10 simulated behavior streams in an object called BS10:\n\nBS10 &lt;- r_behavior_stream(n = 10, mu = 10, lambda = 30, F_event = F_exp(), F_interim = F_exp(), stream_length = 300)"
  },
  {
    "objectID": "posts/ARPobservation-basic-use/index.html#applying-observation-procedures",
    "href": "posts/ARPobservation-basic-use/index.html#applying-observation-procedures",
    "title": "ARPobservation: Basic use",
    "section": "Applying observation procedures",
    "text": "Applying observation procedures\nSeveral different functions are available to turn the behavior_stream object into familiar types of behavioral observation data. For example, the continuous recording procedure (CDR) involves summarizing the behavior stream by the overall proportion of observation time during which events occur. This can be accomplished by feeding BS into the function continuous_duration_recording:\n\ncontinuous_duration_recording(BS10)\n\n [1] 0.1680877 0.4426930 0.1290537 0.3506492 0.2372437 0.3568621 0.2897521\n [8] 0.2570101 0.1704727 0.2968024\n\n\nThe function returns a vector containing one number per simulated behavior stream. As expected all of the numbers are proportions between 0 and 1.\nMore interesting is to simulate many more behavior streams, apply CDR, and calculate the mean and variance of the results or plot them in a histogram:\n\nBS_lots &lt;- r_behavior_stream(n = 10000, mu = 10, lambda = 30, F_event = F_exp(), F_interim = F_exp(), stream_length = 300)\nCDR &lt;- continuous_duration_recording(BS_lots)\nc(mean = mean(CDR), var = var(CDR))\n\n       mean         var \n0.250140703 0.009567949 \n\n\n\nhist(CDR)\n\n\n\n\n\n\n\n\nAnother well-known recording procedure is partial interval recording (PIR), which involves dividing the observation session into short intervals, then scoring each interval according to whether or not the behavior occurs at any point during the interval. The function interval_recording applies partial interval recording (or the closely related procedure of whole interval recording) to a set of simulated behavior streams. Suppose that the observer uses 20 s intervals, back-to-back for 300 s, for a total of 15 intervals. This procedure can be applied to the simulated behavior streams using\n\ninterval_recording(BS10, interval_length = 20, summarize = FALSE)\n\n      [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10]\n [1,]    1    0    1    0    1    1    0    0    0     1\n [2,]    0    0    0    0    1    1    1    1    1     1\n [3,]    1    1    1    1    1    1    1    1    1     1\n [4,]    0    1    1    0    1    1    0    0    1     1\n [5,]    0    1    0    1    0    1    1    0    1     0\n [6,]    0    1    0    1    0    1    1    1    0     0\n [7,]    0    1    0    1    0    1    1    1    0     0\n [8,]    1    1    0    0    1    1    1    1    0     0\n [9,]    0    1    0    1    0    1    0    1    0     0\n[10,]    0    0    1    1    0    1    1    1    1     0\n[11,]    1    0    0    1    0    1    1    1    0     1\n[12,]    1    1    1    1    0    1    1    1    1     1\n[13,]    1    1    0    1    0    1    1    1    0     1\n[14,]    1    1    1    1    1    1    1    0    1     0\n[15,]    1    1    1    1    1    1    0    1    0     1\n\n\nSince summarize is set to false, the function returns a 15 by 10 matrix, with one column for each behavior stream. Each column contains one entry for each interval, equal to one if any behavior occured during that interval (and zero otherwise). Typically, PIR data is summarized by calculating the proportion of intervals across the entire observation session. The summary proportion can be calculated automatically by setting the option summarize = TRUE.\n\ninterval_recording(BS10, interval_length = 20, summarize = TRUE)\n\n [1] 0.5333333 0.7333333 0.4666667 0.7333333 0.4666667 1.0000000 0.7333333\n [8] 0.7333333 0.4666667 0.5333333\n\ncolMeans(interval_recording(BS10, interval_length = 20, summarize = FALSE)) # compare to summarized results\n\n [1] 0.5333333 0.7333333 0.4666667 0.7333333 0.4666667 1.0000000 0.7333333\n [8] 0.7333333 0.4666667 0.5333333\n\n\nSometimes, the PIR procedure is used with a short amount of time in between each interval, which allows the observer to record data or notes. Typical use might involve 15 s intervals of active observation, each followed by 5 s of rest time. This procedure can be applied using the rest_proportion option. Since 5 s is 25% of the full interval length, the rest proportion is 0.25.\n\ninterval_recording(BS10, interval_length = 20, rest_length = 5, summarize = TRUE)\n\n [1] 0.4000000 0.7333333 0.4000000 0.6000000 0.4666667 0.8666667 0.5333333\n [8] 0.6666667 0.4000000 0.5333333\n\n\nThe whole interval recording procedure is implemented using interval_recording with partial = FALSE. Two other observation procedures are also available: momentary time recording (a.k.a. momentary time sampling), using the function momentary_time_recording, and event counting, using event_counting. See the documentation for these functions for usage and examples.\nFinally, a convenience function is available to apply multiple observation procedures to the same set of simulated behavior streams. Suppose that you want to compare the data generated by CDR with the data generated by PIR with 15 s active intervals and 5 s rest times. This can be accomplished using\n\nreported_observations(BS10, data_types = c(\"C\", \"P\"), interval_length = 20, rest_length = 5)\n\n           C         P\n1  0.1680877 0.4000000\n2  0.4426930 0.7333333\n3  0.1290537 0.4000000\n4  0.3506492 0.6000000\n5  0.2372437 0.4666667\n6  0.3568621 0.8666667\n7  0.2897521 0.5333333\n8  0.2570101 0.6666667\n9  0.1704727 0.4000000\n10 0.2968024 0.5333333\n\n\nThis function returns a data frame with one column for each procedure and one row for each simulated behavior stream. Say that you also want to include data based on momentary time recording, with 20 s in between each moment. Just add an \"M\" to the list of data types to include:\n\nreported_observations(BS10, data_types = c(\"C\", \"M\", \"P\"), interval_length = 20, rest_length = 5)\n\n           C          M         P\n1  0.1680877 0.20000000 0.4000000\n2  0.4426930 0.46666667 0.7333333\n3  0.1290537 0.06666667 0.4000000\n4  0.3506492 0.40000000 0.6000000\n5  0.2372437 0.26666667 0.4666667\n6  0.3568621 0.40000000 0.8666667\n7  0.2897521 0.26666667 0.5333333\n8  0.2570101 0.20000000 0.6666667\n9  0.1704727 0.06666667 0.4000000\n10 0.2968024 0.20000000 0.5333333"
  },
  {
    "objectID": "posts/assigning-after-dplyr/index.html",
    "href": "posts/assigning-after-dplyr/index.html",
    "title": "Assigning after dplyr",
    "section": "",
    "text": "Hadley Wickham’s dplyr and tidyr packages completely changed the way I do data manipulation/munging in R. These packages make it possible to write shorter, faster, more legible, easier-to-intepret code to accomplish the sorts of manipulations that you have to do with practically any real-world data analysis. The legibility and interpretability benefits come from\n\nusing functions that are simple verbs that do exactly what they say (e.g., filter, summarize, group_by) and\nchaining multiple operations together, through the pipe operator %&gt;% from the magrittr package.\n\nChaining is particularly nice because it makes the code read like a story. For example, here’s the code to calculate sample means for the baseline covariates in a little experimental dataset I’ve been working with recently:\n\nlibrary(dplyr)\ndat &lt;- read.csv(\"http://jepusto.com/data/Mineo_2009_data.csv\")\n\ndat %&gt;%\n  group_by(Condition) %&gt;%\n  select(Age, starts_with(\"Baseline\")) %&gt;%\n  summarise_each(funs(mean)) -&gt;\n  baseline_means\n\nWarning: `summarise_each()` was deprecated in dplyr 0.7.0.\nℹ Please use `across()` instead.\n\n\nWarning: `funs()` was deprecated in dplyr 0.8.0.\nℹ Please use a list of either functions or lambdas:\n\n# Simple named list: list(mean = mean, median = median)\n\n# Auto named with `tibble::lst()`: tibble::lst(mean, median)\n\n# Using lambdas list(~ mean(., trim = .2), ~ median(., na.rm = TRUE))\n\n\nEach line of the code is a different action: first group the data by Condition, then select the relevant variables, then summarise each of the variables with its sample mean in each group. The results are stored in a dataset called baseline_means.\nAs I’ve gotten familiar with dplyr, I’ve adopted the style of using the backwards assignment operator (-&gt;) to store the results of a chain of manipulations. This is perhaps a little bit odd—in all the rest of my code I stick with the forward assignment operator (&lt;-) with the object name on the left—but the alternative is to break the “flow” of the story, effectively putting the punchline before the end of the joke. Consider:\n\nbaseline_means &lt;- dat %&gt;%\n  group_by(Condition) %&gt;%\n  select(Age, starts_with(\"Baseline\")) %&gt;%\n  summarise_each(funs(mean))\n\nWarning: `summarise_each()` was deprecated in dplyr 0.7.0.\nℹ Please use `across()` instead.\n\n\nAdding missing grouping variables: `Condition`\n\n\nWarning: `funs()` was deprecated in dplyr 0.8.0.\nℹ Please use a list of either functions or lambdas:\n\n# Simple named list: list(mean = mean, median = median)\n\n# Auto named with `tibble::lst()`: tibble::lst(mean, median)\n\n# Using lambdas list(~ mean(., trim = .2), ~ median(., na.rm = TRUE))\n\n\nThat’s just confusing to me. So backward assignment operator it is.\n\nAssigning as a verb\nMy only problem with this convention is that, with complicated chains of manipulations, I often find that I need to tweak the order of the verbs in the chain. For example, I might want to summarize all of the variables, and only then select which ones to store:\n\ndat %&gt;%\n  group_by(Condition) %&gt;%\n  summarise_each(funs(mean)) %&gt;%\n  select(Age, starts_with(\"Baseline\")) -&gt;\n  baseline_means\n\nWarning: `summarise_each()` was deprecated in dplyr 0.7.0.\nℹ Please use `across()` instead.\n\n\nWarning: `funs()` was deprecated in dplyr 0.8.0.\nℹ Please use a list of either functions or lambdas:\n\n# Simple named list: list(mean = mean, median = median)\n\n# Auto named with `tibble::lst()`: tibble::lst(mean, median)\n\n# Using lambdas list(~ mean(., trim = .2), ~ median(., na.rm = TRUE))\n\n\nWarning: There were 3 warnings in `summarise()`.\nThe first warning was:\nℹ In argument: `Expressive.Language = mean(Expressive.Language)`.\nℹ In group 1: `Condition = \"OtherVR\"`.\nCaused by warning in `mean.default()`:\n! argument is not numeric or logical: returning NA\nℹ Run `dplyr::last_dplyr_warnings()` to see the 2 remaining warnings.\n\n\nIn revising the code, it’s necessary to change the symbols at the end of the second and third steps, which is a minor hassle. It’s possible to do it by very carefully cutting-and-pasting the end of the second step through everything but the -&gt; after the third step, but that’s a delicate operation, prone to error if you’re programming after hours or after beer. Wouldn’t it be nice if every step in the chain ended with %&gt;% so that you could move around whole lines of code without worrying about the bit at the end?\nHere’s one crude way to end each link in the chain with a pipe:\n\ndat %&gt;%\n  group_by(Condition) %&gt;%\n  select(Age, starts_with(\"Baseline\")) %&gt;%\n  summarise_each(funs(mean)) %&gt;%\n  identity() -&gt; baseline_means\n\nWarning: `summarise_each()` was deprecated in dplyr 0.7.0.\nℹ Please use `across()` instead.\n\n\nAdding missing grouping variables: `Condition`\n\n\nWarning: `funs()` was deprecated in dplyr 0.8.0.\nℹ Please use a list of either functions or lambdas:\n\n# Simple named list: list(mean = mean, median = median)\n\n# Auto named with `tibble::lst()`: tibble::lst(mean, median)\n\n# Using lambdas list(~ mean(., trim = .2), ~ median(., na.rm = TRUE))\n\n\nBut this is still pretty ugly—it’s got an extra function call that’s not a verb, and the name of the resulting object is tucked away in the middle of a line. What I need is a verb to take the results of a chain of operations and assign to an object. Base R has a suitable candidate here: the assign function. How about the following?\n\ndat %&gt;%\n  group_by(Condition) %&gt;%\n  select(Age, starts_with(\"Baseline\")) %&gt;%\n  summarise_each(funs(mean)) %&gt;%\n  assign(\"baseline_means_new\", .)\n\nWarning: `summarise_each()` was deprecated in dplyr 0.7.0.\nℹ Please use `across()` instead.\n\n\nAdding missing grouping variables: `Condition`\n\n\nWarning: `funs()` was deprecated in dplyr 0.8.0.\nℹ Please use a list of either functions or lambdas:\n\n# Simple named list: list(mean = mean, median = median)\n\n# Auto named with `tibble::lst()`: tibble::lst(mean, median)\n\n# Using lambdas list(~ mean(., trim = .2), ~ median(., na.rm = TRUE))\n\nexists(\"baseline_means_new\")\n\n[1] FALSE\n\n\nThis doesn’t work because of some subtlety with the environment into which baseline_means_new is assigned. A brute-force fix would be to specify that the assign should be into the global environment. This will probably work 90%+ of the time, but it’s still not terribly elegant.\nHere’s a function that searches the call stack to find the most recent invocation of itself that does not involve non-standard evaluation, then assigns to its parent environment:\n\nput &lt;- function(x, name, where = NULL) {\n  if (is.null(where)) {\n    sys_calls &lt;- sys.calls()\n    put_calls &lt;- grepl(\"\\\\&lt;put\\\\(\", sys_calls) & !grepl(\"\\\\&lt;put\\\\(\\\\.\",sys_calls)\n    where &lt;- sys.frame(max(which(put_calls)) - 1)\n  }\n  assign(name, value = x, pos = where)\n}\n\nHere are my quick tests that this function is assigning to the right environment:\n\nput(dat, \"dat1\")\ndat %&gt;% put(\"dat2\")\n\nf &lt;- function(dat, name) {\n  put(dat, \"dat3\")\n  dat %&gt;% put(\"dat4\")\n  put(dat, name)\n  c(exists(\"dat3\"), exists(\"dat4\"), exists(name))\n}\n\nf(dat,\"dat5\")\n\n[1] TRUE TRUE TRUE\n\ngrep(\"dat\",ls(), value = TRUE)\n\n[1] \"dat\"  \"dat1\" \"dat2\"\n\n\nThis appears to work even if you’ve got multiple nested calls to put:\n\nput(f(dat, \"dat6\"), \"dat7\")\ngrep(\"dat\",ls(), value = TRUE)\n\n[1] \"dat\"  \"dat1\" \"dat2\" \"dat7\"\n\ndat7\n\n[1] TRUE TRUE TRUE\n\nf(dat, \"dat8\") %&gt;% put(\"dat9\")\ngrep(\"dat\",ls(), value = TRUE)\n\n[1] \"dat\"  \"dat1\" \"dat2\" \"dat7\" \"dat9\"\n\ndat9\n\n[1] TRUE TRUE TRUE\n\n\n\n\nIt works! (I think…)\nTo be consistent with the style of dplyr, let me also tweak the function to allow name to be the unquoted object name:\n\nput &lt;- function(x, name, where = NULL) {\n  name_string &lt;- deparse(substitute(name))\n  if (is.null(where)) {\n    sys_calls &lt;- sys.calls()\n    put_calls &lt;- grepl(\"\\\\&lt;put\\\\(\", sys_calls) & !grepl(\"\\\\&lt;put\\\\(\\\\.\",sys_calls)\n    where &lt;- sys.frame(max(which(put_calls)) - 1)\n  }\n  assign(name_string, value = x, pos = where)\n}\n\nReturning to my original chain of manipulations, here’s how it looks with the new function:\n\ndat %&gt;%\n  group_by(Condition) %&gt;%\n  select(Age, starts_with(\"Baseline\")) %&gt;%\n  summarise_each(funs(mean)) %&gt;%\n  put(baseline_means_new)\n\nWarning: `summarise_each()` was deprecated in dplyr 0.7.0.\nℹ Please use `across()` instead.\n\n\nAdding missing grouping variables: `Condition`\n\n\nWarning: `funs()` was deprecated in dplyr 0.8.0.\nℹ Please use a list of either functions or lambdas:\n\n# Simple named list: list(mean = mean, median = median)\n\n# Auto named with `tibble::lst()`: tibble::lst(mean, median)\n\n# Using lambdas list(~ mean(., trim = .2), ~ median(., na.rm = TRUE))\n\nprint(baseline_means_new)\n\n# A tibble: 3 × 4\n  Condition   Age Baseline.Gaze Baseline.Vocalizations\n  &lt;chr&gt;     &lt;dbl&gt;         &lt;dbl&gt;                  &lt;dbl&gt;\n1 OtherVR    122.          91.9                   2.86\n2 SelfVR     139.          95.5                   1.43\n3 SelfVid    121.         102.                    1.86\n\n\nIf you’ve been following along, let me know what you think of this. Is it a good idea, or is it dangerous? Are there cases where this will break? Can you think of a better name?\n\n\n\n\n\n\nReuseCC BY 4.0CitationBibTeX citation:@online{pustejovsky2016,\n  author = {Pustejovsky, James E.},\n  title = {Assigning After Dplyr},\n  date = {2016-05-13},\n  url = {https://jepusto.com/posts/assigning-after-dplyr},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nPustejovsky, James E. 2016. “Assigning After Dplyr.” May\n13, 2016. https://jepusto.com/posts/assigning-after-dplyr."
  },
  {
    "objectID": "posts/bug-in-nlme-getVarCov/index.html",
    "href": "posts/bug-in-nlme-getVarCov/index.html",
    "title": "Bug in nlme::getVarCov",
    "section": "",
    "text": "I have recently been working to ensure that my clubSandwich package works correctly on fitted lme and gls models from the nlme package, which is one of the main R packages for fitting hierarchical linear models. In the course of digging around in the guts of nlme, I noticed a bug in the getVarCov function. The purpose of the function is to extract the estimated variance-covariance matrix of the errors from a fitted lme or gls model.\nIt seems that this function is sensitive to the order in which the input data are sorted. This bug report noted the problem, but unfortunately their proposed fix doesn’t seem to solve the problem. In this post I’ll demonstrate the bug and a solution. (I’m posting this here because the R project’s bug reporting system is currently closed to people who were not registered as of early July, evidently due to some sort of spamming problem.)\n\nThe issue\nHere’s a simple demonstration of the problem. I’ll first fit a gls model with a heteroskedastic variance function and an AR(1) auto-correlation structure (no need to worry about the substance of the specification—we’re just worried about computation here) and then extract the variances for each of the units.\n\n# Demonstrate the problem with gls model\n\nlibrary(nlme)\ndata(Ovary)\n\ngls_raw &lt;- gls(follicles ~ sin(2*pi*Time) + cos(2*pi*Time), data = Ovary,\n               correlation = corAR1(form = ~ 1 | Mare),\n               weights = varPower())\n\nMares &lt;- levels(gls_raw$groups)\nV_raw &lt;- lapply(Mares, function(g) getVarCov(gls_raw, individual = g))\n\nNow I’ll repeat the process using the same data, but sorted in a different order\n\nOvary_sorted &lt;- Ovary[with(Ovary, order(Mare, Time)),]\ngls_sorted &lt;- update(gls_raw, data = Ovary_sorted)\n\nV_sorted &lt;- lapply(Mares, function(g) getVarCov(gls_sorted, individual = g))\n\nThe variance component estimates are essentially equal:\n\nall.equal(gls_raw$modelStruct, gls_sorted$modelStruct)\n\n[1] TRUE\n\n\nHowever, the extracted variance-covariance matrices are not:\n\nall.equal(V_raw, V_sorted)\n\n[1] TRUE\n\n\nHere’s the code of the relevant function:\n\nnlme:::getVarCov.gls\n\nfunction (obj, individual = 1, ...) \n{\n    if (is.null(csT &lt;- obj$modelStruct$corStruct)) \n        stop(\"not implemented for uncorrelated errors\")\n    if (is.null(grp &lt;- getGroups(csT))) \n        stop(\"not implemented for correlation structures without a grouping factor\")\n    S &lt;- corMatrix(csT)[[individual]]\n    if (!is.null(obj$modelStruct$varStruct)) {\n        ind &lt;- if (is.numeric(individual)) {\n            as.integer(grp) == individual\n        }\n        else grp == individual\n        vw &lt;- 1/varWeights(obj$modelStruct$varStruct)[ind]\n    }\n    else vw &lt;- rep(1, nrow(S))\n    vars &lt;- (obj$sigma * vw)^2\n    result &lt;- t(S * sqrt(vars)) * sqrt(vars)\n    class(result) &lt;- c(\"marginal\", \"VarCov\")\n    attr(result, \"group.levels\") &lt;- names(obj$groups)\n    result\n}\n&lt;bytecode: 0x0000021868e0fd48&gt;\n&lt;environment: namespace:nlme&gt;\n\n\nThe issue is in the 4th line of the body. getVarCov.gls assumes that varWeights(obj$modelStruct$varStruct) is sorted in the same order as obj$groups, which is not necessarily true. Instead, varWeights seem to return the weights sorted according to the grouping variable. For this example, that means that the varWeights will not depend on the order in which the groups are sorted.\n\nidentical(gls_raw$groups, gls_sorted$groups)\n\n[1] FALSE\n\nidentical(varWeights(gls_raw$modelStruct$varStruct), \n          varWeights(gls_sorted$modelStruct$varStruct))\n\n[1] TRUE\n\n\n\n\nFix for nlme:::getVarCov.gls\nI think this can be solved by either\n\nputting the varWeights back into the same order as the raw data or\nsorting obj$groups before identifying the rows corresponding to the specified individual.\n\nHere’s a revised function that takes the second approach:\n\n# proposed patch for getVarCov.gls\n\ngetVarCov_revised_gls &lt;- function (obj, individual = 1, ...) {\n    S &lt;- corMatrix(obj$modelStruct$corStruct)[[individual]]\n    if (!is.null(obj$modelStruct$varStruct)) {\n        ind &lt;- sort(obj$groups) == individual\n        vw &lt;- 1 / varWeights(obj$modelStruct$varStruct)[ind]\n    }\n    else vw &lt;- rep(1, nrow(S))\n    vars &lt;- (obj$sigma * vw)^2\n    result &lt;- t(S * sqrt(vars)) * sqrt(vars)\n    class(result) &lt;- c(\"marginal\", \"VarCov\")\n    attr(result, \"group.levels\") &lt;- names(obj$groups)\n    result\n}\n\nTesting that it works correctly:\n\nV_raw &lt;- lapply(Mares, function(g) getVarCov_revised_gls(gls_raw, individual = g))\nV_sorted &lt;- lapply(Mares, function(g) getVarCov_revised_gls(gls_sorted, individual = g))\nall.equal(V_raw, V_sorted)\n\n[1] TRUE\n\n\n\n\nFix for nlme:::getVarCov.lme\nThe same issue comes up in getVarCov.lme. Here’s the fix and verification:\n\n# proposed patch for getVarCov.lme\n\ngetVarCov_revised_lme &lt;- function (obj, individuals, type = c(\"random.effects\", \"conditional\", \"marginal\"), ...) {\n    type &lt;- match.arg(type)\n    if (any(\"nlme\" == class(obj))) \n        stop(\"not implemented for \\\"nlme\\\" objects\")\n    if (length(obj$group) &gt; 1) \n        stop(\"not implemented for multiple levels of nesting\")\n    sigma &lt;- obj$sigma\n    D &lt;- as.matrix(obj$modelStruct$reStruct[[1]]) * sigma^2\n    if (type == \"random.effects\") {\n        result &lt;- D\n    }\n    else {\n        result &lt;- list()\n        groups &lt;- sort(obj$groups[[1]])\n        ugroups &lt;- unique(groups)\n        if (missing(individuals)) \n            individuals &lt;- as.matrix(ugroups)[1, ]\n        if (is.numeric(individuals)) \n            individuals &lt;- ugroups[individuals]\n        for (individ in individuals) {\n            indx &lt;- which(individ == ugroups)\n            if (!length(indx)) \n                stop(gettextf(\"individual %s was not used in the fit\", \n                  sQuote(individ)), domain = NA)\n            if (is.na(indx)) \n                stop(gettextf(\"individual %s was not used in the fit\", \n                  sQuote(individ)), domain = NA)\n            ind &lt;- groups == individ\n            if (!is.null(obj$modelStruct$corStruct)) {\n                V &lt;- corMatrix(obj$modelStruct$corStruct)[[as.character(individ)]]\n            }\n            else V &lt;- diag(sum(ind))\n            if (!is.null(obj$modelStruct$varStruct)) \n                sds &lt;- 1/varWeights(obj$modelStruct$varStruct)[ind]\n            else sds &lt;- rep(1, sum(ind))\n            sds &lt;- obj$sigma * sds\n            cond.var &lt;- t(V * sds) * sds\n            dimnames(cond.var) &lt;- list(1:nrow(cond.var), 1:ncol(cond.var))\n            if (type == \"conditional\") \n                result[[as.character(individ)]] &lt;- cond.var\n            else {\n                Z &lt;- model.matrix(obj$modelStruct$reStruc, getData(obj))[ind, \n                  , drop = FALSE]\n                result[[as.character(individ)]] &lt;- cond.var + \n                  Z %*% D %*% t(Z)\n            }\n        }\n    }\n    class(result) &lt;- c(type, \"VarCov\")\n    attr(result, \"group.levels\") &lt;- names(obj$groups)\n    result\n}\n\nlme_raw &lt;- lme(follicles ~ sin(2*pi*Time) + cos(2*pi*Time), \n               random = ~ 1 | Mare,\n               correlation = corExp(form = ~ Time),\n               weights = varPower(),\n               data=Ovary)\n\nlme_sorted &lt;- update(lme_raw, data = Ovary_sorted)\n\nall.equal(lme_raw$modelStruct, lme_sorted$modelStruct)\n\n[1] TRUE\n\n# current getVarCov\nV_raw &lt;- lapply(Mares, function(g) getVarCov(lme_raw, individual = g, type = \"marginal\"))\nV_sorted &lt;- lapply(Mares, function(g) getVarCov(lme_sorted, individual = g, type = \"marginal\"))\nall.equal(V_raw, V_sorted)\n\n[1] TRUE\n\n# revised getVarCov \nV_raw &lt;- lapply(Mares, function(g) getVarCov_revised_lme(lme_raw, individual = g, type = \"marginal\"))\nV_sorted &lt;- lapply(Mares, function(g) getVarCov_revised_lme(lme_sorted, individual = g, type = \"marginal\"))\nall.equal(V_raw, V_sorted)\n\n[1] TRUE\n\n\n\n\nSession info\n\nsessionInfo()\n\nR version 4.3.1 (2023-06-16 ucrt)\nPlatform: x86_64-w64-mingw32/x64 (64-bit)\nRunning under: Windows 10 x64 (build 19045)\n\nMatrix products: default\n\n\nlocale:\n[1] LC_COLLATE=English_United States.utf8 \n[2] LC_CTYPE=English_United States.utf8   \n[3] LC_MONETARY=English_United States.utf8\n[4] LC_NUMERIC=C                          \n[5] LC_TIME=English_United States.utf8    \n\ntime zone: America/Chicago\ntzcode source: internal\n\nattached base packages:\n[1] stats     graphics  grDevices datasets  utils     methods   base     \n\nother attached packages:\n[1] nlme_3.1-162\n\nloaded via a namespace (and not attached):\n [1] digest_0.6.35     fastmap_1.1.1     xfun_0.42         lattice_0.21-8   \n [5] knitr_1.45        htmltools_0.5.7   rmarkdown_2.26    cli_3.6.2        \n [9] grid_4.3.1        renv_1.0.5        compiler_4.3.1    rstudioapi_0.16.0\n[13] tools_4.3.1       evaluate_0.23     yaml_2.3.8        rlang_1.1.3      \n[17] jsonlite_1.8.8    htmlwidgets_1.6.4\n\n\n\n\n\n\n\n\nReuseCC BY 4.0CitationBibTeX citation:@online{pustejovsky2016,\n  author = {Pustejovsky, James E.},\n  title = {Bug in {nlme::getVarCov}},\n  date = {2016-08-10},\n  url = {https://jepusto.com/posts/bug-in-nlme-getVarCov},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nPustejovsky, James E. 2016. “Bug in Nlme::getVarCov.”\nAugust 10, 2016. https://jepusto.com/posts/bug-in-nlme-getVarCov."
  },
  {
    "objectID": "posts/clubSandwich-for-CRVE-FE/index.html",
    "href": "posts/clubSandwich-for-CRVE-FE/index.html",
    "title": "Clustered standard errors and hypothesis tests in fixed effects models",
    "section": "",
    "text": "UPDATED April 09, 2024 to use current syntax for constraints argument in clubSandwich::Wald_test().\nI’ve recently been working with my colleague Beth Tipton on methods for cluster-robust variance estimation in the context of some common econometric models, focusing in particular on fixed effects models for panel data—or what statisticians would call “longitudinal data” or “repeated measures.” We have a new working paper, which you can find here.\nThe importance of using CRVE (i.e., “clustered standard errors”) in panel models is now widely recognized. Less widely recognized, perhaps, is the fact that standard methods for constructing hypothesis tests and confidence intervals based on CRVE can perform quite poorly in when you have only a limited number of independent clusters. What’s worse, it can be hard to determine what counts as a large-enough sample to trust standard CRVE methods, because the finite-sample behavior of the variance estimators and test statistics depends on the configuration of the covariates, not just the total sample size. For example, suppose you have state-level panel data from 50 states across 15 years and are trying to estimate the effect of some policy using difference-in-differences. If only 5 or 6 states have variation in the policy variable over time, then you’re almost certainly in small-sample territory. And the sample size issues can be subtler than this, too, as I’ll show below.\nOne solution to this problem is to use bias-reduced linearization (BRL), which was proposed by Bell and McCaffrey (2002) and has recently begun to receive attention from econometricians (e.g., Cameron & Miller, 2015; Imbens & Kolesar, 2015). The idea of BRL is to correct the bias of standard CRVE based on a working model, and then to use a degrees-of-freedom correction for Wald tests based on the bias-reduced CRVE. That may seem silly (after all, the whole point of CRVE is to avoid making distributional assumptions about the errors in your model), but it turns out that the correction can help quite a bit, even when the working model is wrong. The degrees-of-freedom correction is based on a standard Satterthwaite-type approximation, and also relies on the working model. There’s now quite a bit of evidence (which we review in the working paper) that BRL performs well even in samples with a small number of clusters.\nIn the working paper, we make two contributions to all this:\nThe paper explains all this in greater detail, and also reports a fairly extensive simulation study that we designed to emuluate the types of covariates and study designs encountered in micro-economic applications. We’ve also got an R package that implements our methods (plus some other variants of CRVE, which I’ll explain some other time) in a fairly streamlined way. Here’s an example of how to use the package to do inference for a fixed effects panel data model."
  },
  {
    "objectID": "posts/clubSandwich-for-CRVE-FE/index.html#effects-of-changing-the-minimum-legal-drinking-age",
    "href": "posts/clubSandwich-for-CRVE-FE/index.html#effects-of-changing-the-minimum-legal-drinking-age",
    "title": "Clustered standard errors and hypothesis tests in fixed effects models",
    "section": "Effects of changing the minimum legal drinking age",
    "text": "Effects of changing the minimum legal drinking age\nCarpenter and Dobkin (2011) analyzed the effects of changes in the minimum legal drinking age on rates of motor vehicle fatalies among 18-20 year olds, using state-level panel data from the National Highway Traffic Administration’s Fatal Accident Reporting System. In their new textbook, Angrist and Pischke (2014) developed a stylized example based on Carpenter and Dobkin’s work. I’ll use Angrist and Pischke’s data and follow their analysis, just because their data are easily available.\nThe outcome is the incidence of deaths in motor vehicle crashes among 18-20 year-olds (per 100,000 residents), for each state plus the District of Columbia, over the period 1970 to 1983. Tthere were several changes in the minimum legal drinking age during this time period, with variability in the timing of changes across states. Angrist and Pischke (following Carpenter and Dobkin) use a difference-in-differences strategy to estimate the effects of lowering the minimum legal drinking age from 21 to 18. A basic specification is\n\\[y_{it} = \\alpha_i + \\beta_t + \\gamma r_{it} + \\epsilon_{it},\\]\nfor \\(i\\) = 1,…,51 and \\(t\\) = 1970,…,1983. In this model, \\(\\alpha_i\\) is a state-specific fixed effect, \\(\\beta_t\\) is a year-specific fixed effect, \\(r_{it}\\) is the proportion of 18-20 year-olds in state \\(i\\) in year \\(t\\) who are legally allowed to drink, and \\(\\gamma\\) captures the effect of shifting the minimum legal drinking age from 21 to 18. Following Angrist and Pischke’s analysis, I’ll estimate this model both by (unweighted) OLs and by weighted least squares with weights corresponding to population size in a given state and year.\n\nUnweighted OLS\nThe following code does some simple data-munging and the estimates the model by OLS:\n\n# get data from Angrist & Pischke's website\nlibrary(foreign)\ndeaths &lt;- read.dta(\"http://masteringmetrics.com/wp-content/uploads/2015/01/deaths.dta\", convert.factors=FALSE)\n\n# subset for 18-20 year-olds, deaths in motor vehicle accidents\nMVA_deaths &lt;- subset(deaths, agegr==2 & dtype==2 & year &lt;= 1983, select = c(-dtype, -agegr))\n\n# fit by OLS\nlm_unweighted &lt;- lm(mrate ~ 0 + legal + factor(state) + factor(year), data = MVA_deaths)\n\nThe coef_test function from clubSandwich can then be used to test the hypothesis that changing the minimum legal drinking age has no effect on motor vehicle deaths in this cohort (i.e., \\(H_0: \\gamma = 0\\)). The usual way to test this is to cluster the standard errors by state, calculate the robust Wald statistic, and compare that to a standard normal reference distribution. The code and results are as follows:\n\n# devtools::install_github(\"jepusto/clubSandwich\") # install the clubSandwich package\nlibrary(clubSandwich)\n\nRegistered S3 method overwritten by 'clubSandwich':\n  method    from    \n  bread.mlm sandwich\n\ncoef_test(lm_unweighted, vcov = \"CR1\", cluster = MVA_deaths$state, test = \"z\")[\"legal\",]\n\n Coef. Estimate   SE t-stat d.f. (z) p-val (z) Sig.\n legal     7.59 2.38   3.19      Inf   0.00143   **\n\n\nOur work argues shows that a better approach would be to use the bias-reduced linearization CRVE, together with Satterthwaite degrees of freedom. In the package, the BRL adjustment is called “CR2” because it is directly analogous to the HC2 correction used in heteroskedasticity-robust variance estimation. When applied to an OLS model estimated by lm, the default working model is an identity matrix, which amounts to the “working” assumption that the errors are all uncorrelated and homoskedastic. Here’s how to apply this approach in the example:\n\ncoef_test(lm_unweighted, vcov = \"CR2\", cluster = MVA_deaths$state, test = \"Satterthwaite\")[\"legal\",]\n\n Coef. Estimate   SE t-stat d.f. (Satt) p-val (Satt) Sig.\n legal     7.59 2.43   3.12        25.7      0.00442   **\n\n\nThe Satterthwaite degrees of freedom will be different for each coefficient in the model, and so the coef_test function reports them right alongside the standard error. In this case, the degrees of freedom are about half of what you might expect, given that there are 51 clusters. The p-value for the CR2+Satterthwaite test is about twice as large as the p-value based on the standard Wald test. But of course, the coefficient is still statistically significant at conventional levels, and so the inference doesn’t change.\n\n\nUnweighted “within” estimation\nThe plm package in R provides another way to estimate the same model. It is convenient because it absorbs the state and year fixed effects before estimating the effect of legal. The clubSandwich package works with fitted plm models too:\n\nlibrary(plm)\nplm_unweighted &lt;- plm(mrate ~ legal, data = MVA_deaths, \n                      effect = \"twoways\", index = c(\"state\",\"year\"))\ncoef_test(plm_unweighted, vcov = \"CR1S\", cluster = \"individual\", test = \"z\")\n\n Coef. Estimate   SE t-stat d.f. (z) p-val (z) Sig.\n legal     7.59 2.38   3.19      Inf   0.00143   **\n\ncoef_test(plm_unweighted, vcov = \"CR2\", cluster = \"individual\", test = \"Satterthwaite\")\n\n Coef. Estimate   SE t-stat d.f. (Satt) p-val (Satt) Sig.\n legal     7.59 2.43   3.12        25.7      0.00442   **\n\n\nFor the standard approach, I’ve used the variant of the correction factor implemented in Stata (called CR1S in the clubSandwich package), but this makes very little difference in the standard error or the p-value. For the test based on CR2, the degrees of freedom are slightly different than the results based on the fitted lm model, but the p-values agree to four decimals. The differences in degrees of freedom are due to numerical imprecision in the calculations.\n\n\nPopulation-weighted estimation\nThe difference between the standard method and the new method are not terribly exciting in the above example. However, things change quite a bit if the model is estimated using population weights. As far as I know, plm does not handle weighted least squares, and so I go back to fitting in lm with dummies for all the fixed effects.\n\nlm_weighted &lt;- lm(mrate ~ 0 + legal + factor(state) + factor(year), \n                  weights = pop, data = MVA_deaths)\ncoef_test(lm_weighted, vcov = \"CR1\", cluster = MVA_deaths$state, test = \"z\")[\"legal\",]\n\n Coef. Estimate   SE t-stat d.f. (z) p-val (z) Sig.\n legal      7.5 2.16   3.47      Inf    &lt;0.001  ***\n\ncoef_test(lm_weighted, vcov = \"CR2\", cluster = MVA_deaths$state, test = \"Satterthwaite\")[\"legal\",]\n\n Coef. Estimate  SE t-stat d.f. (Satt) p-val (Satt) Sig.\n legal      7.5 2.3   3.27        8.65       0.0103    *\n\n\nUsing population weights slightly reduces the point estimate of the effect, while also slightly increasing its precision. If you were following the standard approach, you would probably be happy with the weighted estimates and wouldn’t think about it any further. However, our recommended approach—using the CR2 variance estimator and Satterthwaite correction—produces a p-value that is an order of magnitude larger (though still significant at the conventional 5% level). The degrees of freedom are just {r} round(coef_test(lm_weighted, vcov = \"CR2\", cluster = MVA_deaths$state, test = \"Satterthwaite\")[\"legal\",\"df\"], 1)—drastically smaller than would be expected based on the number of clusters.\nEven with weights, the coef_test function uses an “independent, homoskedastic” working model as a default for lm objects. In the present example, the outcome is a standardized rate and so a better assumption might be that the error variances are inversely proportional to population size. The following code uses this alternate working model:\n\ncoef_test(lm_weighted, vcov = \"CR2\", \n          cluster = MVA_deaths$state, target = 1 / MVA_deaths$pop, \n          test = \"Satterthwaite\")[\"legal\",]\n\n Coef. Estimate  SE t-stat d.f. (Satt) p-val (Satt) Sig.\n legal      7.5 2.2   3.41          13      0.00467   **\n\n\nThe new working model leads to slightly smaller standard errors and a couple of additional degrees of freedom, though we remain in small-sample territory.\n\n\nRobust Hausman test\nCRVE is also used in specification tests, as in the Hausman-type test for endogeneity of unobserved effects. Suppose that the model includes an additional control for the beer taxation rate in state \\(i\\) at time \\(t\\), denoted \\(s_{it}\\). The (unweighted) fixed effects model is then\n\\[y_{it} = \\alpha_i + \\beta_t + \\gamma_1 r_{it} + \\gamma_2 s_{it} + \\epsilon_{it},\\]\nand the estimated effects are as follows:\n\nlm_FE &lt;- lm(mrate ~ 0 + legal + beertaxa + factor(state) + factor(year), data = MVA_deaths)\ncoef_test(lm_FE, vcov = \"CR2\", cluster = MVA_deaths$state, test = \"Satterthwaite\")[c(\"legal\",\"beertaxa\"),]\n\n    Coef. Estimate   SE t-stat d.f. (Satt) p-val (Satt) Sig.\n    legal     7.59 2.51  3.019       24.58      0.00583   **\n beertaxa     3.82 5.27  0.725        5.77      0.49663     \n\n\nIf the unobserved effects \\(\\alpha_1,...,\\alpha_{51}\\) are uncorrelated with the regressors, then a more efficient way to estimate \\(\\gamma_1,\\gamma_2\\) is by weighted least squares, with weights based on a random effects model. However, if the unobserved effects covary with \\(\\mathbf{r}_i, \\mathbf{s}_i\\), then the random-effects estimates will be biased.\nWe can test for whether endogeneity is a problem by including group-centered covariates as additional regressors. Let \\(\\tilde{r}_{it} = r_{it} - \\frac{1}{T}\\sum_t r_{it}\\), with \\(\\tilde{s}_{it}\\) defined analogously. Now estimate the regression\n\\[y_{it} = \\beta_t + \\gamma_1 r_{it} + \\gamma_2 s_{it} + \\delta_1 \\tilde{r}_{it} + \\delta_2 \\tilde{s}_{it} + \\epsilon_{it},\\]\nwhich does not include state fixed effects. The parameters \\(\\delta_1,\\delta_2\\) represent the differences between the random effects and fixed effects estimands of \\(\\gamma_1, \\gamma_2\\). If these are both zero, then the random effects estimator is unbiased. Thus, the joint test for \\(H_0: \\delta_1 = \\delta_2 = 0\\) amounts to a test for non-endogeneity of the unobserved effects.\nFor efficiency, we should estimate this using weighted least squares, but OLS will work too:\n\nMVA_deaths &lt;- within(MVA_deaths, {\n  legal_cent &lt;- legal - tapply(legal, state, mean)[factor(state)]\n  beer_cent &lt;- beertaxa - tapply(beertaxa, state, mean)[factor(state)]\n})\n\nlm_Hausman &lt;- lm(mrate ~ 0 + legal + beertaxa + legal_cent + beer_cent + factor(year), data = MVA_deaths)\ncoef_test(lm_Hausman, vcov = \"CR2\", cluster = MVA_deaths$state, test = \"Satterthwaite\")[1:4,]\n\n      Coef. Estimate   SE  t-stat d.f. (Satt) p-val (Satt) Sig.\n      legal   -9.180 7.62 -1.2042       24.94       0.2398     \n   beertaxa    3.395 9.40  0.3613        6.44       0.7295     \n legal_cent   16.768 8.53  1.9665       33.98       0.0575    .\n  beer_cent    0.424 9.25  0.0458        5.86       0.9650     \n\n\nTo conduct a joint test on the centered covariates, we can use the Wald_test function. The usual way to test this hypothesis would be to use the CR1 variance estimator to calculate the robust Wald statistic, then use a \\(\\chi^2_2\\) reference distribution (or equivalently, compare a re-scaled Wald statistic to an \\(F(2,\\infty)\\) distribution). The Wald_test function reports the latter version:\n\nWald_test(\n  lm_Hausman, \n  constraints = constrain_zero(c(\"legal_cent\",\"beer_cent\")), \n  vcov = \"CR1\", \n  cluster = MVA_deaths$state, \n  test = \"chi-sq\"\n)\n\n   test Fstat df_num df_denom  p_val sig\n chi-sq  2.93      2      Inf 0.0534   .\n\n\nThe test is just shy of significance at the 5% level. If we instead use the CR2 variance estimator and our newly proposed approximate F-test (which is the default in Wald_test), then we get:\n\nWald_test(\n  lm_Hausman, \n  constraints = constrain_zero(c(\"legal_cent\",\"beer_cent\")), \n  vcov = \"CR2\", \n  cluster = MVA_deaths$state\n)\n\n test Fstat df_num df_denom p_val sig\n  HTZ  2.57      2     12.4 0.117    \n\n\nThe low degrees of freedom of the test indicate that we’re definitely in small-sample territory and should not trust the asymptotic \\(\\chi^2\\) approximation."
  },
  {
    "objectID": "posts/clubSandwich-for-CRVE-FE/index.html#references",
    "href": "posts/clubSandwich-for-CRVE-FE/index.html#references",
    "title": "Clustered standard errors and hypothesis tests in fixed effects models",
    "section": "References",
    "text": "References\n\nAngrist, J. D., & Pischke, J.-S. (2009). Mostly harmless econometrics: An empiricist’s companion. Princeton, NJ: Princeton University Press.\nAngrist, J. D. and Pischke, J.-S. (2014). Mastering ’metrics: The Path from Cause to Effect. Princeton, NJ: Princeton University Press.\nBell, R. M., & McCaffrey, D. F. (2002). Bias reduction in standard errors for linear regression with multi-stage samples. Survey Methodology, 28(2), 169-181.\nCameron, A. C., & Miller, D. L. (2015). A practitioner’s guide to cluster-robust inference. URL: http://cameron.econ.ucdavis.edu/research/Cameron_Miller_JHR_2015_February.pdf\nCarpenter, C., & Dobkin, C. (2011). The minimum legal drinking age and public health. Journal of Economic Perspectives, 25(2), 133-156. doi:10.1257/jep.25.2.133\nImbens, G. W., & Kolesar, M. (2015). Robust standard errors in small samples: Some practical advice. URL: https://www.princeton.edu/~mkolesar/papers/small-robust.pdf"
  },
  {
    "objectID": "posts/code-folding-update/index.html",
    "href": "posts/code-folding-update/index.html",
    "title": "An update on code folding with blogdown + Academic theme",
    "section": "",
    "text": "UPDATED November 21, 2020. Thanks to Allen O’Brien for pointing out a bug in the codefolding code, which led to the last code chunk defaulting to hidden rather than open. Allen sent along a simple fix to the codefolding.js file.\nAbout a year ago I added a code-folding feature to my site, following an approach developed by Sébastien Rochette. I recently updated my site to work with the latest version of the Academic theme for Hugo, and it turns out that this broke my code-folding implementation. It took a bit of putzing and some help from a freelance web developer to fix it, but it’s now working again, and I’m again doing my happy robot dance:\nIn this post, I’ll provide instructions on how to reproduce the approach with the current version of the Academic theme, which is 4.8 (March 2020). Credit where credit is due:"
  },
  {
    "objectID": "posts/code-folding-update/index.html#code-folding-with-the-academic-theme",
    "href": "posts/code-folding-update/index.html#code-folding-with-the-academic-theme",
    "title": "An update on code folding with blogdown + Academic theme",
    "section": "1 Code folding with the Academic theme",
    "text": "1 Code folding with the Academic theme\n\nYou’ll first need to add the codefolding javascript assets. Create a folder called js under the /static directory of your site. Add the file codefolding.js.\nCreate a folder called css under the /static directory of your site. Add the file codefolding.css. This is the css for the buttons that will appear on your posts.\nAdd the file article_footer_js.html to the /layouts/partials directory of your site.\nAdd the file header_maincodefolding.html to the /layouts/partials directory of your site.\nIf you do not already have a file head_custom.html in the /layouts/partials directory, create it. Add the following lines of code to the file:\n::: {.cell}\n{{ if not site.Params.disable_codefolding }}\n  &lt;script src=\"https://cdn.jsdelivr.net/npm/popper.js@1.16.0/dist/umd/popper.min.js\"&gt;&lt;/script&gt;\n  &lt;link rel=\"stylesheet\" href=\"{{ \"css/codefolding.css\" | relURL }}\" /&gt;\n{{ end }}\n:::\nIf you do not already have a file site_footer.html in the /layouts/partials directory, copy it over from /themes/hugo-academic/layouts/partials. Add the following lines of code to it, somewhere towards the bottom (see my version for example):\n::: {.cell}\n&lt;!-- Init code folding --&gt;\n{{ partial \"article_footer_js.html\" . }}\n:::\nIf you do not already have the file page_header.html in the /layouts/partials directory, copy it over from /themes/hugo-academic/layouts/partials. Add the following line of code at appropriate points so that your posts will include the “Show/hide code” button:\n::: {.cell}\n {{ partial \"header_maincodefolding\" . }}\n:::\nNote that you’ll likely need to add it twice due do conditionals in page_header.html. For example, my version of the file includes the partial at lines 62 and 91.\nModify your params.toml file (in the directory /config/_default) to include the following lines:\n::: {.cell}\n############################\n## Code folding\n############################\n\n# Set to true to disable code folding\ndisable_codefolding = false\n# Set to \"hide\" or \"show\" all codes by default\ncodefolding_show = \"show\"\n# Set to true to exclude the \"Show/hide all\" button\ncodefolding_nobutton = false\n:::"
  },
  {
    "objectID": "posts/code-folding-update/index.html#using-the-codefolding-parameters",
    "href": "posts/code-folding-update/index.html#using-the-codefolding-parameters",
    "title": "An update on code folding with blogdown + Academic theme",
    "section": "2 Using the codefolding parameters",
    "text": "2 Using the codefolding parameters\nThe params.toml file now has three parameters that control code folding:\n\ndisable_codefolding controls whether to load the code folding scripts on your site. Set it to true to disable code folding globally.\ncodefolding_show controls whether code blocks will be shown or hidden by default. If your previous posts have lots of code in them, set the default to show to minimize changes in the appearance of your site.\ncodefolding_nobutton controls whether the “Show/hide code” button will appear at the top of posts that include code blocks. Set it to true to disable the button but keep the other code folding functionality.\n\nThe above parameters are defaults for your entire site. To over-ride the defaults, you can also set the parameters in the YAML header of any post:\n\nSet disable_codefolding: true to turn off code folding for the post.\nSet codefolding_show: hide to hide the code blocks in the post (as in this post).\nSet codefolding_nobutton: true to turn off the “Show/hide code” button at the top of the post (as in the present post).\n\nI hope these instructions work for you. If not, questions, corrections, and clarifications are welcome. Happy blogging, y’all!"
  },
  {
    "objectID": "posts/Crashes-in-Austin-and-Travis-Co/index.html",
    "href": "posts/Crashes-in-Austin-and-Travis-Co/index.html",
    "title": "Fatal crashes in Austin/Travis County",
    "section": "",
    "text": "I have been hearing quite a bit lately about how there have been an unusually large number of fatal automobile crashes in Austin this year, resulting in a total of 69 fatalities (as of August 19th). Terrence Henry (of KUT) recently did a story on this problem, and the City of Austin has convened the Vision Zero Task Force to figure out what policies to implement in order to prevent these deaths. KUT published an interactive map showing the locations of the fatal crashes and the Vision Zero Task Force put together a heat map showing the locations of crashes over the past five years.\nI was curious to understand more about how fatalities have changed over time, but the only data I could find on the time trends was this graphic on the Vision Zero website. After a bit of digging, I found that I could get annual data for Austin (2006-2014) and for Travis County (2003-2014) from the Texas Motor Vehicle Crash Statistics reports provided by TXDOT (though the data are trapped in pdfs). It’s also possible to get disaggregated data for the time period of 2010 through the present from the TXDOT CRIS database Public File Extract, which gets updated with new information as it comes in, and so will presumably be more current than the annual reports.\nThe chart below plots the annual number of fatal crashes, fatalities, crashes in which incapacitating injuries occurred, incapacitating injuries, and total crashes, for both Austin and Travis County as a whole. For the current year data, I plotted both the actual numbers (through July 31, 2015) and very simple projections. (Details on how I put the figures together are at the end of this post.)\n\n\n\n\n\n\n\n\n\nThe first thing you can see from these graphs is that the projected number of fatal crashes and total number of fatalities is substantially higher than in previous years. In contrast, the projected number of incapacitating crashes, number of incapacitating injuries, and total number of crashes all appear to be (very roughly) consistent with the linear trends from previous years. Taken together, these trends suggest that the proportion of crashes that are fatal is higher than would be expected. Here’s a graph of the fatality rate over time:\n\n\n\n\n\n\n\n\n\n2015 is clearly an outlier, though not as high as the proportion of fatal crashes in Travis County during 2003 and 2004 (unfortunately the data for Austin don’t go back that far). These years have higher proportions because there were fewer crashes overall in these initial years. Also note that Travis County as a whole has a higher fatality rate than the city of Austin, probably because the non-Austin roads in Travis county are larger and have higher speed limits.\nI think this second graph provides good justification for one of the principles of the Vision Zero task force, which is to focus on infrastructure improvements to improve the safety of the transformation system for all of the people who interact with it, including pedestrians—in short, to make our streets and roads safe for humans. The graphs suggests that there’s more to the increase in fatal crashes than just population growth, more than just increases in vehicle miles travelled.\nThere’s a big limitation to using annual data for this sort of simple, “eyeballing” sort of analysis. If there are seasonal patterns in automobile crashes overall (such as more crashes during the colder months) or, more specifically, in fatal crashes, then my simple back-of-the-envelope projections could be somewhat misleading. To develop more nuanced projections, I would need to get finer-grained data on when crashes occur. unfortunately, for some reasons the public version of the underlying data does not include dates of individual crashes. (This is rather perplexing, considering that the interface will let you query a date range, even down to a single day.) More to come if I can figure out how to get access to data with dates on it.\n\nMethods\nHere’s how I constructed these figures:\n\nThe data for 2003 through 2009 are drawn from the Crash Statistics reports, and the data for 2010 through 2015 are drawn from the CRIS Public File Extract.\nThere are some discrepancies between the annual reports and the CRIS database for the latter period, so I am assuming that the latter is more accurate. Curiously, the number’s don’t quite match the Vision Zero graphic either.\nThe incapacitating crashes and injuries numbers are only available starting in 2010, as prior to that time a different set of classifications was used that does not appear to be directly comparable.\nThe dashed lines in each graph represent estimated linear trends, fit by ordinary least squares.\nThe actual figures are plotted with circles. The projections for 2015 are plotted with triangles.\nFor 2015, the projections were calculated by multiplying the actual number by 12 / 7 = 1.71 because the actuals are based on 7 out of 12 months. (Using 211 out of 365 days leads to a very similar multiplier of 1.73.)\nThe underlying data (drawing from both the annual reports and the CRIS database) are available here.\nThe code to re-create the figures is available in this Gist.\n\n\n\n\n\n\n\nReuseCC BY 4.0CitationBibTeX citation:@online{pustejovsky2015,\n  author = {Pustejovsky, James E.},\n  title = {Fatal Crashes in {Austin/Travis} {County}},\n  date = {2015-08-20},\n  url = {https://jepusto.com/posts/Crashes-in-Austin-and-Travis-Co},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nPustejovsky, James E. 2015. “Fatal Crashes in Austin/Travis\nCounty.” August 20, 2015. https://jepusto.com/posts/Crashes-in-Austin-and-Travis-Co."
  },
  {
    "objectID": "posts/delta-method-and-2SLS-SEs/index.html",
    "href": "posts/delta-method-and-2SLS-SEs/index.html",
    "title": "2SLS standard errors and the delta-method",
    "section": "",
    "text": "I just covered instrumental variables in my course on causal inference, and so I have two-stage least squares (2SLS) estimation on the brain. In this post I’ll share something I realized in the course of prepping for class: that standard errors from 2SLS estimation are equivalent to delta method standard errors based on the Wald IV estimator. (I’m no econometrician, so this had never occurred to me before. Perhaps it will be interesting to other non-econometrician readers. And perhaps the econometricians can point me to the relevant page in Wooldridge or Angrist and Pischke or whomever that explains this better than I have.)\nLet’s consider a system with an outcome \\(y_i\\), a focal treatment \\(t_i\\) identified by a single instrument \\(z_i\\), along with a row-vector of exogenous covariates \\(\\mathbf{x}_i\\), all for \\(i = 1,...,n\\). The usual estimating equations are:\n\\[\n\\begin{aligned}\ny_i &= \\mathbf{x}_i \\delta_0 + t_i \\delta_1 + e_i \\\\\nt_i &= \\mathbf{x}_i \\alpha_0 + z_i \\alpha_1 + u_i.\n\\end{aligned}\n\\]\nWith a single-instrument, the 2SLS estimator of \\(\\delta_1\\) is exactly equivalent to the Wald estimator\n\\[\n\\hat\\delta_1 = \\frac{\\hat\\beta_1}{\\hat\\alpha_1},\n\\]\nwhere \\(\\hat\\alpha_1\\) is the OLS estimator from the first-stage regression of \\(t_i\\) on \\(\\mathbf{x}_i\\) and \\(z_i\\) and \\(\\hat\\beta_1\\) is the OLS estimator from the regression\n\\[\ny_i = \\mathbf{x}_i \\beta_0 + z_i \\beta_1 + v_i.\n\\]\nThe delta-method approximation for \\(\\text{Var}(\\hat\\delta_1)\\) is\n\\[\n\\text{Var}\\left(\\hat\\delta_1\\right) \\approx \\frac{1}{\\alpha_1^2}\\left[ \\text{Var}\\left(\\hat\\beta_1\\right) + \\delta_1^2 \\text{Var}\\left(\\hat\\alpha_1\\right) - 2 \\delta_1 \\text{Cov}\\left(\\hat\\beta_1, \\hat\\alpha_1\\right) \\right].\n\\]\nSubstituting the estimators in place of parameters, and using heteroskedasticity-consistent (HC0, to be precise) estimators for \\(\\text{Var}\\left(\\hat\\beta_1\\right)\\), \\(\\text{Var}\\left(\\hat\\alpha_1\\right)\\), and \\(\\text{Cov}\\left(\\hat\\beta_1, \\hat\\alpha_1\\right)\\), it turns out the feasible delta-method variance estimator is exactly equivalent to the HC0 variance estimator from 2SLS.\n\nConnecting delta-method and 2SLS\nTo demonstrate this claim, let’s first partial out the covariates, taking \\(\\mathbf{\\ddot{y}} = \\left[\\mathbf{I} - \\mathbf{X}\\left(\\mathbf{X}'\\mathbf{X}\\right)^{-1}\\mathbf{X}'\\right]\\mathbf{y}\\), \\(\\mathbf{\\ddot{t}} = \\left[\\mathbf{I} - \\mathbf{X}\\left(\\mathbf{X}'\\mathbf{X}\\right)^{-1}\\mathbf{X}'\\right]\\mathbf{t}\\), and \\(\\mathbf{\\ddot{z}} = \\left[\\mathbf{I} - \\mathbf{X}\\left(\\mathbf{X}'\\mathbf{X}\\right)^{-1}\\mathbf{X}'\\right]\\mathbf{z}\\). The OLS estimators of \\(\\beta_1\\) and \\(\\alpha_1\\) are then\n\\[\n\\hat\\beta_1 = \\left(\\mathbf{\\ddot{z}}'\\mathbf{\\ddot{z}}\\right)^{-1}\\mathbf{\\ddot{z}}'\\mathbf{\\ddot{y}}, \\qquad \\text{and} \\qquad \\hat\\alpha_1 = \\left(\\mathbf{\\ddot{z}}'\\mathbf{\\ddot{z}}\\right)^{-1}\\mathbf{\\ddot{z}}'\\mathbf{\\ddot{t}}.\n\\]\nThe HC0 variance and covariance estimators for these coefficients have the usual sandwich form:\n\\[\n\\begin{aligned}\nV^{\\beta_1} &= \\left(\\mathbf{\\ddot{z}}'\\mathbf{\\ddot{z}}\\right)^{-1}\\left(\\sum_{i=1}^n \\ddot{z}_i^2 \\ddot{v}_i^2\\right) \\left(\\mathbf{\\ddot{z}}'\\mathbf{\\ddot{z}}\\right)^{-1} \\\\\nV^{\\alpha_1} &= \\left(\\mathbf{\\ddot{z}}'\\mathbf{\\ddot{z}}\\right)^{-1}\\left(\\sum_{i=1}^n \\ddot{z}_i^2 \\ddot{u}_i^2\\right) \\left(\\mathbf{\\ddot{z}}'\\mathbf{\\ddot{z}}\\right)^{-1} \\\\\nV^{\\alpha_1\\beta_1} &= \\left(\\mathbf{\\ddot{z}}'\\mathbf{\\ddot{z}}\\right)^{-1}\\left(\\sum_{i=1}^n \\ddot{z}_i^2 \\ddot{u}_i \\ddot{v}_i\\right) \\left(\\mathbf{\\ddot{z}}'\\mathbf{\\ddot{z}}\\right)^{-1},\n\\end{aligned}\n\\]\nwhere \\(\\ddot{v}_i\\) and \\(\\ddot{u}_i\\) are the residuals from the regressions of \\(\\mathbf{\\ddot{y}}\\) on \\(\\mathbf{\\ddot{z}}\\) and \\(\\mathbf{\\ddot{t}}\\) on \\(\\mathbf{\\ddot{z}}\\), respectively. Combining all these terms, the delta-method variance estimator is then\n\\[\nV^{\\delta_1} = \\frac{1}{\\hat\\alpha_1^2}\\left(\\mathbf{\\ddot{z}}'\\mathbf{\\ddot{z}}\\right)^{-1}\\left[\\sum_{i=1}^n \\ddot{z}_i^2\\left(\\ddot{v}_i^2 + \\hat\\delta_1^2 \\ddot{u}_i^2 - 2 \\hat\\delta_1\\ddot{u}_i \\ddot{v}_i\\right)\\right] \\left(\\mathbf{\\ddot{z}}'\\mathbf{\\ddot{z}}\\right)^{-1}.\n\\]\nRemember this formula because we’ll return to it shortly.\nNow consider the 2SLS estimator. To calculate this, we begin by taking the fitted values from the regression of \\(\\mathbf{\\ddot{t}}\\) on \\(\\mathbf{\\ddot{z}}\\):\n\\[\n\\mathbf{\\tilde{t}} = \\mathbf{\\ddot{z}}\\left(\\mathbf{\\ddot{z}}'\\mathbf{\\ddot{z}}\\right)^{-1}\\mathbf{\\ddot{z}}'\\mathbf{\\ddot{t}} = \\mathbf{\\ddot{z}} \\hat\\alpha_1.\n\\]\nWe then regress \\(\\mathbf{\\ddot{y}}\\) on \\(\\mathbf{\\tilde{t}}\\):\n\\[\n\\hat\\delta_1 = \\left(\\mathbf{\\tilde{t}}'\\mathbf{\\tilde{t}}\\right)^{-1} \\mathbf{\\tilde{t}}' \\mathbf{\\ddot{y}}.\n\\]\nThe HC0 variance estimator corresponding to the 2SLS estimator is\n\\[\nV^{2SLS} = \\left(\\mathbf{\\tilde{t}}'\\mathbf{\\tilde{t}}\\right)^{-1} \\left(\\sum_{i=1}^n \\tilde{t}_i^2 \\tilde{e}_i^2 \\right) \\left(\\mathbf{\\tilde{t}}'\\mathbf{\\tilde{t}}\\right)^{-1},\n\\]\nwhere \\(\\tilde{e}_i = \\ddot{y}_i - \\ddot{t}_i \\hat\\delta_1\\). Note that these residuals are calculated based on \\(\\ddot{t}_i\\), the full treatment variable, not the fitted values \\(\\tilde{t}_i\\). The full treatment variable can be expressed as \\(\\ddot{t}_i = \\tilde{t}_i + \\ddot{u}_i\\), by which it follows that\n\\[\n\\tilde{e}_i = \\ddot{y}_i - \\tilde{t}_i \\hat\\delta_1 - \\ddot{u}_i \\hat\\delta_1.\n\\]\nBut \\(\\tilde{t}_i \\hat\\delta_1 = \\ddot{z}_i \\hat\\alpha_1 \\hat\\delta_1 = \\ddot{z}_i \\hat\\beta_1\\), and so\n\\[\n\\tilde{e}_i = \\ddot{y}_i - \\ddot{z}_i \\hat\\beta_1 - \\ddot{u}_i \\hat\\delta_1 = \\ddot{v}_i - \\ddot{u}_i \\hat\\delta_1.\n\\]\nThe 2SLS variance estimator is therefore\n\\[\n\\begin{aligned}\nV^{2SLS} &= \\left(\\mathbf{\\tilde{t}}'\\mathbf{\\tilde{t}}\\right)^{-1} \\left(\\sum_{i=1}^n \\tilde{t}_i^2 \\tilde{e}_i^2 \\right) \\left(\\mathbf{\\tilde{t}}'\\mathbf{\\tilde{t}}\\right)^{-1} \\\\\n&= \\left(\\hat\\alpha_1^2 \\mathbf{\\ddot{z}}'\\mathbf{\\ddot{z}}\\right)^{-1} \\left(\\sum_{i=1}^n \\hat\\alpha_1^2 \\ddot{z}_i^2 \\tilde{e}_i^2 \\right) \\left(\\hat\\alpha_1^2 \\mathbf{\\ddot{z}}'\\mathbf{\\ddot{z}}\\right)^{-1} \\\\\n&= \\frac{1}{\\hat\\alpha_1^2}\\left(\\mathbf{\\ddot{z}}'\\mathbf{\\ddot{z}}\\right)^{-1} \\left(\\sum_{i=1}^n \\ddot{z}_i^2 \\tilde{e}_i^2 \\right) \\left(\\mathbf{\\ddot{z}}'\\mathbf{\\ddot{z}}\\right)^{-1} \\\\\n&= \\frac{1}{\\hat\\alpha_1^2}\\left(\\mathbf{\\ddot{z}}'\\mathbf{\\ddot{z}}\\right)^{-1} \\left[\\sum_{i=1}^n \\ddot{z}_i^2 \\left(\\ddot{v}_i - \\ddot{u}_i \\hat\\delta_1\\right)^2 \\right] \\left(\\mathbf{\\ddot{z}}'\\mathbf{\\ddot{z}}\\right)^{-1},\n\\end{aligned}\n\\]\nwhich agrees with \\(V^{\\delta_1}\\) as given above.\n\n\nSo what?\nIf you’ve continued reading this far…I’m slightly amazed…but if you have, you may be wondering why it’s worth knowing about this relationship. The equivalence between the 2SLS variance estimator and the delta method interests me for a couple of reasons.\n\nFirst is that I had always taken the 2SLS variance estimator as being conditional on \\(\\mathbf{t}\\)–that is, not accounting for random variation in the treatment assignment. The delta-method form of the variance makes it crystal clear that this isn’t the case—the variance does include terms for \\(\\text{Var}(\\hat\\alpha_1)\\) and \\(\\text{Cov}(\\hat\\beta_1, \\hat\\alpha_1)\\).\nOn the other hand, there’s perhaps a sense that equivalence with the 2SLS variance estimator (the more familiar form) validates the delta method variance estimator—that is, we wouldn’t be doing something fundamentally different by using the delta method variance with a Wald estimator. For instance, we might want to estimate \\(\\alpha_1\\) and/or \\(\\beta_1\\) by some other means (e.g., by estimating \\(\\alpha_1\\) as a marginal effect from a logistic regression or estimating \\(\\beta_1\\) with a multi-level model). It would make good sense in this instance to use the Wald estimator \\(\\hat\\beta_1 / \\hat\\alpha_1\\) and to estimate its variance using the delta method form.\nOne last reason I’m interested in this is that writing out the variance estimators will likely help in understanding how to approach small-sample corrections to \\(V^{2SLS}\\). But I’ll save that for another day.\n\n\n\n\n\n\n\nReuseCC BY 4.0CitationBibTeX citation:@online{pustejovsky2017,\n  author = {Pustejovsky, James E.},\n  title = {2SLS Standard Errors and the Delta-Method},\n  date = {2017-10-07},\n  url = {https://jepusto.com/posts/delta-method-and-2SLS-SEs},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nPustejovsky, James E. 2017. “2SLS Standard Errors and the\nDelta-Method.” October 7, 2017. https://jepusto.com/posts/delta-method-and-2SLS-SEs."
  },
  {
    "objectID": "posts/distribution-of-sample-variances/index.html",
    "href": "posts/distribution-of-sample-variances/index.html",
    "title": "The sampling distribution of sample variances",
    "section": "",
    "text": "A colleague and her students asked me the other day whether I knew of a citation that gives the covariance between the sample variances of two outcomes from a common sample. This sort of question comes up in meta-analysis problems occasionally. I didn’t know of a convenient reference that directly answers the question, but I was able to suggest some references that would help (listed below). While the students work on deriving it, I’ll provide the answer here so that they can check their work.\nSuppose that we have a sample of \\(n\\) observations \\(\\mathbf{y}_1,...,\\mathbf{y}_n\\) from a \\(p\\)-dimensional multivariate normal distribution with mean \\(\\boldsymbol\\mu\\) and covariance \\(\\boldsymbol\\Sigma = \\left[\\sigma_{jk}\\right]_{j,k=1,...,p}\\). Let \\(\\mathbf{\\bar{y}}\\) denote the (multivariate) sample mean, with entries \\(\\bar{y}_1,...,\\bar{y}_p\\). Let \\(\\mathbf{S}\\) denote the sample covariance matrix, with entries \\(\\left[s_{jk}\\right]_{j,k=1,...,p}\\) where\n\\[\ns_{jk} = \\frac{1}{n - 1}\\sum_{i=1}^n (y_{ij} - \\bar{y}_j)(y_{ik} - \\bar{y}_k).\n\\]\nThen \\((n - 1)\\mathbf{S}\\) follows a Wishart distribution with \\(n - 1\\) degrees of freedom and scale matrix \\(\\boldsymbol\\Sigma\\) (Searle, 2006, p. 352; Muirhead, 1982, p. 86; or any textbook on multivariate analysis).\nThe sampling covariance between two sample covariances, say \\(s_{jk}\\) and \\(s_{lm}\\), can then be derived from the properties of the Wishart distribution. Expressions for this are available in Searle (2006) or Muirhead (1982). The former is a bit hard to parse because it uses the \\(\\text{vec}\\) and Kronecker product operators; Muirhead (1982, p. 90) gives the following simple expression:\n\\[\n\\text{Cov}\\left(s_{jk}, s_{lm}\\right) = \\frac{\\sigma_{jl}\\sigma_{km} + \\sigma_{jm}\\sigma_{kl}}{n - 1}.\n\\]\nFor sample variances, this reduces to\n\\[\n\\text{Cov}\\left(s_j^2, s_l^2\\right) = \\frac{2\\sigma_{jl}^2}{n - 1}.\n\\]\nThe formula also reduces to the well-known result that the sampling variance of the sample variance is\n\\[\n\\text{Var}\\left(s_j^2\\right) = \\frac{2 \\sigma_{jj}^2}{n - 1}.\n\\]\nOne application of this bit of distribution theory is to find the sampling variance of an average of sample variances. Suppose that we have a bivariate normal distribution where both measures have the same variance \\(\\sigma_{11} = \\sigma_{22} = \\sigma^2\\) and correlation \\(\\rho\\). One estimate of this common variance is to take the simple average of the sample variances, \\(s_{\\bullet}^2 = \\left(s_1^2 + s_2^2\\right) / 2\\). Then using the above:\n\\[\\begin{aligned}\n\\text{Var}\\left(s_{\\bullet}^2\\right) &= \\frac{1}{4}\\left[\\text{Var}\\left(s_1^2\\right) + \\text{Var}\\left(s_2^2\\right) + 2\\text{Cov}\\left(s_1^2, s_2^2\\right) \\right] \\\\\n&= \\frac{\\sigma^4 \\left(1 + \\rho^2\\right)}{n - 1}.\n\\end{aligned}\\]\nTo see that this is correct, consider the extreme cases. If the two measures are perfectly correlated, then averaging the sample variances has no benefit because \\(\\text{Var}\\left(s_{\\bullet}^2\\right) = \\text{Var}\\left(s_1^2\\right) = \\text{Var}\\left(s_2^2\\right)\\). If they are exactly uncorrelated, then averaging the sample variances is equivalent to pooling the sample variance from two independent samples.\n\nReferences\nMuirhead, R. J. (1982). Aspects of Multivariate Statistical Theory. New York, NY: John Wiley & Sons.\nSearle, S. R. (2006). Matrix Algebra Useful for Statistics. Hoboken, NJ: John Wiley & Sons.\n\n\n\n\n\n\nReuseCC BY 4.0CitationBibTeX citation:@online{pustejovsky2016,\n  author = {Pustejovsky, James E.},\n  title = {The Sampling Distribution of Sample Variances},\n  date = {2016-04-25},\n  url = {https://jepusto.com/posts/distribution-of-sample-variances},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nPustejovsky, James E. 2016. “The Sampling Distribution of Sample\nVariances.” April 25, 2016. https://jepusto.com/posts/distribution-of-sample-variances."
  },
  {
    "objectID": "posts/dizzy-for-d-z/index.html",
    "href": "posts/dizzy-for-d-z/index.html",
    "title": "Cohen’s \\(d_z\\) makes me dizzy when considering measurement error",
    "section": "",
    "text": "\\[\n\\def\\Pr{{\\text{Pr}}}\n\\def\\E{{\\text{E}}}\n\\def\\Var{{\\text{Var}}}\n\\def\\Cov{{\\text{Cov}}}\n\\def\\cor{{\\text{cor}}}\n\\def\\bm{\\mathbf}\n\\def\\bs{\\boldsymbol}\n\\]\nMeta-analyses in education, psychology, and related fields rely heavily of Cohen’s \\(d\\), or the standardized mean difference effect size, for quantitatively describing the magnitude and direction of intervention effects. In these fields, Cohen’s \\(d\\) is so pervasive that its use is nearly automatic, and analysts rarely question its utility or consider alternatives (response ratios, anyone? POMP?). Despite this state of affairs, working with Cohen’s \\(d\\) is theoretically challenging because the standardized mean difference metric does not have a singular definition. Rather, its definition depends on the choice of the standardizing variance used in the denominator.1\nIn this post, I’m going to mull over how measurement error and design decisions influence the metric definition of Cohen’s \\(d\\) in basic within-group experimental designs. The distorting effects of measurement error has long been a source of concern within psychometric meta-analysis, a perspective associated with the work of Frank Schmidt and Jack Hunter, and measurement-error corrections are well developed and often applied in meta-analyses of correlations. Straight-forward measurement-error corrections have also been described for Cohen’s \\(d\\) from between-group designs (see recent work by Brenton Wiernik and Jeff Dahlke). However, I have literally never seen a meta-analytic application that applied these corrections and I have thus far been unable to locate work on such corrections specifically for effect sizes in within-group designs.2 So, time to muck about…"
  },
  {
    "objectID": "posts/dizzy-for-d-z/index.html#effect-size-definitions-in-within-group-designs",
    "href": "posts/dizzy-for-d-z/index.html#effect-size-definitions-in-within-group-designs",
    "title": "Cohen’s \\(d_z\\) makes me dizzy when considering measurement error",
    "section": "Effect size definitions in within-group designs",
    "text": "Effect size definitions in within-group designs\nIn basic between-group designs, the only variances in the model are the within-group variances, so the choice of standardizing variance is limited to a) the singular population variance, assuming it is homogeneous across groups, b) the variance of one group, or c) the average of the variances in each group. In most applications, homogeneity is assumed (often without much reflection), version (a) of Cohen’s \\(d\\) is estimated, and the meta-analyst can go along their merry way. For sake of succinctness, I’ll call this effect size \\(d_{b}\\), where the \\(b\\) indicates the usual version for basic between-group designs.\nFor within-group or repeated measures designs, the set of choices is more involved and includes a) standardizing by the across-participant variance in one condition (or both conditions, assuming homogeneity) or b) standardizing by the variance of the difference scores. The former approach is sometimes called \\(d_{av}\\)3, the latter is called \\(d_z\\)4. The \\(d_{av}\\) metric uses the same standardizing variance as the \\(d\\) from a basic between-group design, and so results from both types of designs are, in principle, on the same scale.\nIn the context of meta-analysis, the comparability of \\(d_b\\) and \\(d_{av}\\) is useful when working with a set of studies that include both types of designs. On the other hand, in meta-analyses that consist solely of within-group or repeated measures designs, comparability with \\(d_b\\) may be less of a priority and one could consider using \\(d_z\\) for synthesis. Purely on a pragmatic level, using \\(d_z\\) might be attractive because the only pieces of information needed to calculate it are the total sample size and the \\(t\\) statistic (or \\(p\\)-value) from the comparison between conditions. In contrast, calculating \\(d_{av}\\) also requires the between-participant standard deviations from one or both groups, which primary studies might not always report.\nGoing in to this exercise, I had the notion that measurement error would affect \\(d_z\\) to a greater degree than \\(d_{av}\\) because \\(d_z\\) involves difference scores and difference scores get hit by measurement error twice. Does this intuition hold up? Let me try to formalize things a bit."
  },
  {
    "objectID": "posts/dizzy-for-d-z/index.html#a-within-group-design-with-measurement-error",
    "href": "posts/dizzy-for-d-z/index.html#a-within-group-design-with-measurement-error",
    "title": "Cohen’s \\(d_z\\) makes me dizzy when considering measurement error",
    "section": "A within-group design with measurement error",
    "text": "A within-group design with measurement error\nSuppose we have a within-group design involving two conditions, where participants are assessed on \\(K\\) trials under each condition. Let \\(Y_{ijk}\\) denote the outcome from trial \\(k\\) for participant \\(j\\) under condition \\(i\\), for \\(i = 1,2\\), \\(j = 1,...,N\\), and \\(k = 1,...,K\\). A basic model for this set-up is \\[\nY_{ijk} = \\mu_i + u_{ij} + e_{ijk}\n\\] where \\(u_{1j}\\) and \\(u_{2j}\\) are participant-specific errors in the true scores under each condition and the \\(e_{ijk}\\)’s are measurement errors. For simplicity, I will assume that:\n\nthe true-score variance is equal across conditions, with \\(\\Var(u_{ij}) = \\sigma^2\\) for \\(i = 1,2\\),\nthe true scores are correlated across conditions, \\(\\cor(u_{1j}, u_{2j}) = \\rho\\), and\nmeasurement errors are uncorrelated and have homogeneous variance across conditions, with \\(\\Var(e_{ijk}) = \\psi^2\\).\n\nLet \\(\\phi = \\sigma^2 / (\\sigma^2 + \\psi^2)\\) denote the reliability (intra-class correlation) of a single observed score. Note that we can write \\(\\psi^2\\) in terms of the reliability and true-score variance as \\(\\psi^2 = \\sigma^2 \\times \\frac{1 - \\phi}{\\phi}\\).\nUnder this model, there are several different standardized mean difference metrics that we could consider. Since measurement reliability might vary from study to study, it would make sense to define the metric in terms of true score variances alone, as \\[\n\\delta_{av} = \\frac{\\mu_2 - \\mu_1}{\\sigma}\n\\] or in terms of the variance of the difference in true scores, as \\[\n\\delta_z = \\frac{\\mu_2 - \\mu_1}{\\sigma \\sqrt{2(1 - \\rho)}}.\n\\] However, we don’t directly observe the true scores, and we can’t estimate their variance unless we have information about score reliabilities. Thus, meta-analysts will usually need to calculate effect sizes in terms of observed scores that include measurement error.\nSuppose that the analysis is conducted by taking the average of the \\(K\\) trials for each participant under each condition, \\(\\bar{Y}_{ij} = \\frac{1}{K} \\sum_{k=1}^K Y_{ijk}\\), and conducting the analysis using these mean scores. The variance of the mean scores is \\[\n\\Var(\\bar{Y}_{ij}) = \\sigma^2 \\left(1 + \\frac{1 - \\phi}{\\phi K}\\right),\n\\] so we can define the observed-score standardized mean difference using raw score standardization as \\[\n\\tilde\\delta_{av} = \\frac{\\mu_2 - \\mu_1}{\\sigma \\sqrt{1 + \\frac{1 - \\phi}{\\phi K}}} = \\frac{1}{\\sqrt{1 + \\frac{1 - \\phi}{\\phi K}}} \\times \\delta_{av}.\n\\] From this expression, we can see that measurement error attenuates the true-score effect size because the multiplier term is always going to be less than one.\nSimilarly, the variance of the observed difference scores is \\[\n\\Var(\\bar{Y}_{2j} - \\bar{Y}_{1j}) =2 \\sigma^2 \\left(1 - \\rho + \\frac{1 - \\phi}{\\phi K}\\right),\n\\] so we can define the observed-score standardized mean difference using change score standardization as \\[\n\\tilde\\delta_z = \\frac{\\mu_2 - \\mu_1}{\\sigma \\sqrt{2 \\left(1 - \\rho + \\frac{1 - \\phi}{\\phi K}\\right)}} = \\sqrt{\\frac{1 - \\rho}{1 - \\rho + \\frac{1 - \\phi}{\\phi K}}} \\times \\delta_z.\n\\] Again, we can see that measurement error attenuates the true-score effect size because the multiplier term is always going to be less than one. However, unlike with \\(d_{av}\\), the attenuation factor here depends on \\(\\rho\\) in addition to \\(\\phi\\) and \\(K\\). This additional term in the correction factor is one indication that \\(d_z\\) might be less desirable for meta-analysis. Correcting \\(d_z\\) for the distortion from measurement error would require estimates of both the true-score correlation and the reliability of the scores, whereas correcting \\(d_{av}\\) would require only the latter."
  },
  {
    "objectID": "posts/dizzy-for-d-z/index.html#meta-analysis",
    "href": "posts/dizzy-for-d-z/index.html#meta-analysis",
    "title": "Cohen’s \\(d_z\\) makes me dizzy when considering measurement error",
    "section": "Meta-analysis",
    "text": "Meta-analysis\nThe relationships between the true-score effect sizes and the analogous observed score effect sizes starts to be a problem when we consider a meta-analysis of multiple primary studies. Primary studies will often use different instruments and procedures for measuring outcomes (necessitating the use of some standardized effect size), and those differences in instruments and procedures might come along with differences in score reliability as well as variation in the number of trials collected per condition (and plenty of other things, such as sample size, participant characteristics, etc.). Procedural heterogeneity like this creates two potential challenges for meta-analysis: bias in average effect sizes and extra heterogeneity in the distribution of effect sizes. Both could make findings from a meta-analysis more difficult to interpret, although I will argue that extra heterogeneity is more concerning than bias.\nTo illustrate, let’s now imagine that the parameters of the within-group study design, \\(\\delta_{av}\\) or \\(\\delta_z\\), \\(\\rho\\), \\(\\phi\\), and \\(K\\) are random variables, drawn from the distribution of parameters across a population of hypothetical studies.\n\nA model for \\(\\delta_{av}\\)\nLet’s first consider \\(\\delta_{av}\\) and assume that it follows a random effects model, with \\[\n\\delta_{av} \\sim N\\left(\\mu, \\tau^2\\right).\n\\] Let’s also assume that the remaining parameters \\(\\rho\\), \\(\\phi\\), and \\(K\\) are independent of \\(\\delta_{av}\\). These parameters determine the attenuation factor \\(A_{av} = \\left(1 + \\frac{1 - \\phi}{\\phi K}\\right)^{-1/2}\\), which relates the observed-score effect size parameter to the true score effect size parameter.\nThe bias of \\(\\tilde\\delta_{av}\\) is therefore \\[\n\\E\\left(\\tilde\\delta_{av}\\right) = \\E\\left(A_{av}\\delta_{av}\\right) = \\E(A_{av}) \\times \\mu.\n\\] Thus, under my very simplistic assumptions, a meta-analysis of observed score Cohen’s \\(d_{av}\\) estimates will be biased (downward) for the overall average effect in the true-score distribution.\nYou might find that the downward bias in \\(\\tilde\\delta_{av}\\) is undesirable. On the other hand, bias might not be as a big a problem as it first seems. If all of the observed-score effect sizes are biased to a degree that is unrelated to the true effects, then bias just stretches or compresses the scale of measurement, but doesn’t necessarily lead to interpretive problems. Imagine you have a ruler that is half an inch too short, and you’re trying to compare the heights of different objects. As long as you use the same ruler, then you will still be able to determine which objects are bigger and which are smaller, and by how much, even if the measurements are off in an absolute sense.\nApart from bias, however, variability in \\(A_{av}\\) will also induce additional heterogeneity in the distribution of observed score effect sizes. This is a clear problem because it creates additional uncertainty, making it harder to draw inferences about the distribution of effects, predict new effect sizes, or identify substantively interesting moderators. To measure this additional heterogeneity and keep its consequences separate from the consequences for bias, I will look at the coefficient of variation in \\(\\tilde\\delta_{av}\\). Under the assumption that \\(A_{av}\\) is independent of \\(\\delta_{av}\\), \\[\n\\frac{\\sqrt{\\Var(\\tilde\\delta_{av})}}{\\E(\\tilde\\delta_{av})} = \\frac{\\sqrt{(\\mu^2 + 2 \\tau^2) \\Var(A_{av}) + \\tau^2 \\left[\\E(A_{av})\\right]^2}}{\\E\\left(A_{av} \\right) \\times \\mu } = \\sqrt{\\left[\\left(1 + \\frac{2 \\tau^2}{\\mu^2}\\right) \\frac{\\Var(A_{av})}{\\left[\\E\\left(A_{av} \\right)\\right]^2} + \\frac{\\tau^2}{\\mu^2}\\right]}.\n\\] Thus, the coefficient of variation for \\(\\tilde\\delta_{av}\\) is amplified by a factor that depends on the squared coefficient of variation of \\(A_{av}\\). Under the same model, \\(\\tilde\\delta_z = A_z \\times \\delta_{av}\\), where \\(A_z = \\left(1 - \\rho + \\frac{1 - \\phi}{\\phi K}\\right)^{-1/2}\\), and so \\(\\E(\\tilde\\delta_z) = \\E(A_z) \\times \\mu\\) and \\[\n\\frac{\\sqrt{\\Var(\\tilde\\delta_z)}}{\\E(\\tilde\\delta_z)} = \\sqrt{\\left[\\left(1 + \\frac{2 \\tau^2}{\\mu^2}\\right) \\frac{\\Var(A_z)}{\\left[\\E\\left(A_z \\right)\\right]^2} + \\frac{\\tau^2}{\\mu^2}\\right]}.\n\\]\nTo see what’s going on here, let’s consider some specific distributions for these measurement factors. First, let’s assume:\n\n\\(\\rho \\sim B(14, 6)\\), so that \\(\\E(\\rho) = .7\\) and \\(\\Var(\\rho) = 0.1^2\\);\n\\(\\phi \\sim B(3, 5)\\), so that \\(\\E(\\rho) = .0.375\\) and \\(\\Var(\\rho) = 0.161^2\\);\n\\(K \\sim 1 + Pois(9)\\), so \\(\\E(K) = 10\\) and \\(\\Var(K) = 3^2\\); and\n\\(\\rho\\), \\(\\phi\\), and \\(K\\) are mutually independent and independent of \\(\\delta_{av}\\).\n\nBelow I simulate 50000 samples from these distributions and calculate \\(A_{av}\\) and \\(A_z\\).\n\n\nCode\nR &lt;- 50000\nrho &lt;- rbeta(R, 14, 6)\nphi &lt;- rbeta(R, 3, 5)\nK &lt;- 1 + rpois(R, 9)\nA_av &lt;- 1 / sqrt(1 + (1 - phi) / (phi * K))\nA_z &lt;- 1 / sqrt(2 * (1 - rho + (1 - phi) / (phi * K)))\n\nlibrary(ggplot2)\nlibrary(patchwork)\n\ndensity_plot &lt;- function(x, lab, col, limits) {\n  ggplot(data.frame(x), aes(x)) + \n    xlim(limits) +\n    geom_density(alpha = 0.4, fill = col) + \n    scale_y_continuous(labels = NULL) +\n    theme_minimal() + \n    labs(x = lab, y = NULL)\n}\n\np_A_av &lt;- density_plot(A_av, expression(A[av]),\"blue\", c(0.2,1))\np_A_z &lt;- density_plot(A_z, expression(A[z]), \"purple\", c(0, 3))\np_A_av + p_A_z\n\n\n\n\n\n\n\n\n\nThe distribution of \\(A_{av}\\) is mostly concentrated around the mean of \\(E(A_{av}) = 0.898\\), with a coefficient of variation of \\(CV(A_{av}) = 0.085\\). In contrast, the distribution of \\(A_{av}\\) has a mean very close to one, \\(E(A_z) = 1.006\\) but a coefficient of variation of \\(CV(A_z) = 0.21\\), about 2.5 times larger. Thus, variation in measurement procedure induces more extra heterogeneity into the distribution of \\(\\tilde\\delta_z\\) than into \\(\\tilde\\delta_{av}\\).\nNow, one potential objection to this hypothetical scenario is that researchers do not choose the number of trials at random, without consideration for the other parameters of the study design. A more realistic assumption might be that researchers choose \\(K\\) to ensure they achieve at least some threshold level of reliability for the observed scores. The reliability of the observed scores is \\(A_{av}^2\\), so ensuring some threshold of reliability is equivalent to ensuring the square root of the threshold for \\(A_{av}\\). Let’s suppose that researchers always ensure \\(A_{av} \\geq 0.8\\) so that reliability is always at least \\(0.64\\). This leads to the following distributions for \\(A_{av}\\) and \\(A_z\\):\n\n\nCode\nA_av_trunc &lt;- A_av[A_av &gt;= 0.8]\nA_z_trunc &lt;- A_z[A_av &gt;= 0.8]\np_A_av_trunc &lt;- density_plot(A_av_trunc, expression(A[av]),\"blue\", c(0.2, 1))\np_A_z_trunc &lt;- density_plot(A_z_trunc, expression(A[z]), \"purple\", c(0, 3))\np_A_av_trunc + p_A_z_trunc\n\n\n\n\n\n\n\n\n\nThe distribution of \\(A_{av}\\) loses its left tail, so that its mean is \\(E(A_{av}|A_{av} \\geq 0.8) = 0.917\\) and its coefficient of variation is reduced to \\(CV(A_{av} | A_{av} \\geq 0.8) = 0.049\\). The distribution of \\(A_{av}\\) now has a mean of \\(E(A_z | A_{av} \\geq 0.8) = 1.045\\) and a coefficient of variation of \\(CV(A_z | A_{av} \\geq 0.8) = 0.173\\), about 3.5 times larger than the squared coefficient of variation of \\(A_{av}\\).\nUnder both of these scenarios, the observed-score \\(\\tilde\\delta_z\\) is substantially more sensitive to procedural heterogeneity than is \\(\\tilde\\delta_{av}\\). Based on this model and hypothetical example, it seems clear \\(\\tilde\\delta_{av}\\) should be preferred over \\(\\tilde\\delta_z\\) as a metric for meta-analysis. However, these relationships are predicated on a certain model for the study-specific parameters. One might object to this model because there’s a sense that we have assumed that \\(\\delta_{av}\\) is the right answer. After all, the underlying effect size model is specified in terms of \\(\\delta_{av}\\), and the design parameters—including \\(\\rho\\) in particular—are treated as noise, uncorrelated with \\(\\delta_{av}\\). What happens to the observed-score metrics \\(\\tilde\\delta_z\\) and \\(\\tilde\\delta_{av}\\) if we instead start with a model specified in terms of \\(\\delta_z\\)?\n\n\nA model for \\(\\delta_z\\)\nLet’s now see how this works if we treat \\(\\delta_z\\) as the correct metric and assume that the design parameters are independent of \\(\\delta_z\\). Assume that \\[\n\\delta_z \\sim N(\\alpha, \\omega^2)\n\\] and that the remaining parameters \\(\\rho\\), \\(\\phi\\), and \\(K\\) are independent of \\(\\delta_{av}\\). Then the observed-score standardized mean difference using change score standardization can be written as \\(\\tilde\\delta_z = B_z \\times \\delta_z\\), where \\[\nB_z = \\sqrt{\\frac{1 - \\rho}{1 - \\rho + \\frac{1 - \\phi}{\\phi K}}}\n\\] and the observed-score standardized mean difference using raw score standardization can be written as \\(\\tilde\\delta_{av} = B_{av} \\times \\delta_z\\), where \\[\nB_{av} = \\frac{\\sqrt{2}(1 - \\rho)}{\\sqrt{1 - \\rho + \\frac{1 - \\phi}{\\phi K}}}.\n\\] The plots below show the distribution of \\(B_z\\) and \\(B_{av}\\) under the same scenarios considered above. First, the scenario where observed-score reliability is not controlled:\n\n\nCode\nB_z &lt;- sqrt((1 - rho) / (1 - rho + (1 - phi) / (phi * K)))\nB_av &lt;- sqrt(2) * (1 - rho) / sqrt(1 - rho + (1 - phi) / (phi * K))\n\np_B_av &lt;- density_plot(B_av, expression(B[av]),\"green\", c(0,1.5))\np_B_z &lt;- density_plot(B_z, expression(B[z]), \"yellow\", c(0, 1))\np_B_av + p_B_z\n\n\n\n\n\n\n\n\n\nSecond, the scenario where observed-score reliability is at least 0.64:\n\n\nCode\nB_z_trunc &lt;- B_z[A_av &gt;= 0.8]\nB_av_trunc &lt;- B_av[A_av &gt;= 0.8]\n\np_B_av_trunc &lt;- density_plot(B_av, expression(B[av]),\"green\", c(0,1.5))\np_B_z_trunc &lt;- density_plot(B_z, expression(B[z]), \"yellow\", c(0, 1))\np_B_av_trunc + p_B_z_trunc\n\n\n\n\n\n\n\n\n\nThe table below reports the coefficients of variation for each of the multiplicative factors I have considered.\n\n\nCode\nlibrary(dplyr)\ndat &lt;- tibble(A_av, A_z, B_av, B_z)\n\nrandom_rel &lt;- \n  dat |&gt;\n  summarise(\n    across(everything(), ~ sd(.) / mean(.)),\n    reliability = \"random\"\n  )\n\ncontrolled_rel &lt;-\n  dat |&gt;\n  filter(A_av &gt;= 0.8) |&gt;\n  summarise(\n    across(everything(), ~ sd(.) / mean(.)),\n    reliability = \"at least 0.64\"\n  )\n\nCVs &lt;- \n  bind_rows(random_rel, controlled_rel) |&gt;\n  mutate(\n    A_ratio = A_z / A_av,\n    B_ratio = B_z / B_av\n  ) |&gt;\n  select(reliability, A_av, A_z, A_ratio, B_av, B_z, B_ratio)\n\nknitr::kable(\n  CVs, \n  digits = c(0,3,3,1,3,3,2),\n  caption = \"Coefficients of variation\",\n  col.names = c(\"Reliability\",\"$A_{av}$\",\"$A_z$\",\"$A_z /A_{av}$\", \"$B_{av}$\",\"$B_z$\", \"$B_{z} / B_{av}$\"),\n  escape = TRUE\n)\n\n\n\nCoefficients of variation\n\n\n\n\n\n\n\n\n\n\n\nReliability\n\\(A_{av}\\)\n\\(A_z\\)\n\\(A_z /A_{av}\\)\n\\(B_{av}\\)\n\\(B_z\\)\n\\(B_{z} / B_{av}\\)\n\n\n\n\nrandom\n0.085\n0.210\n2.5\n0.285\n0.182\n0.64\n\n\nat least 0.64\n0.049\n0.173\n3.5\n0.257\n0.137\n0.53\n\n\n\n\n\nThe tables are now more or less turned. Under both reliability scenarios, \\(B_z\\) has a lower coefficient of variation than \\(B_{av}\\), indicating that \\(\\tilde\\delta_z\\) is less affected by procedural heterogeneity than is \\(\\tilde\\delta_{av}\\). However, \\(\\tilde\\delta_z\\) is still affected in absolute terms, considering that the coefficient of variation for \\(B_z\\) is about 79% of the coefficient of variation for \\(A_z\\). Of course, \\(\\tilde\\delta_{av}\\) is quite strongly affected under this model, with a coefficient of variation of 0.257.\n\n\nConsequences for heterogeneity\nTo make these results a bit more concrete, it’s useful to consider think in terms of heterogeneity of the observed score effect sizes. The figure below plots the CVs of observed-score effect size parameters as a function of the CVs of the true effect size distribution.\n\n\nCode\nlibrary(tidyr)\nlibrary(stringr)\n\nCV_obs &lt;- \n  CVs |&gt;\n  select(-A_ratio, -B_ratio) |&gt;\n  pivot_longer(-reliability, names_to = \"metric\", values_to = \"het\") |&gt;\n  expand_grid(tau_mu = seq(0,1,0.02)) |&gt;\n  mutate(\n    reliability = recode(reliability, 'at least 0.64' = \"Reliability of at least 0.64\", 'random' = \"Random reliability\"),\n    metric = paste0(str_replace(metric, \"\\\\_\",\"\\\\[\"),\"]\"),\n    CV = sqrt((1 + 2 * tau_mu^2) * het^2 + tau_mu^2)\n  )\n\nCV_ex &lt;- CV_obs |&gt;\n  filter(tau_mu == 0.5, reliability == \"Reliability of at least 0.64\") |&gt;\n  select(metric, CV) |&gt;\n  mutate(\n    CV = round(CV, 3),\n    metric = str_replace(str_sub(metric, 1, -2), \"\\\\[\", \"_\")\n  ) |&gt;\n  pivot_wider(names_from = metric, values_from = CV)\n\nCV_obs_labs &lt;-\n  CV_obs %&gt;%\n  filter(tau_mu == 0)\n\nggplot(CV_obs, aes(tau_mu, CV, color = metric)) + \n  facet_wrap(vars(reliability)) +\n  scale_x_continuous(expand = expansion(c(0.1,0),0), breaks = seq(0.2, 1, 0.2)) + \n  scale_y_continuous(expand = expansion(0,0), breaks = seq(0, 1, 0.2)) + \n  geom_vline(xintercept = 0) + \n  geom_hline(yintercept = 0) + \n  geom_abline(slope = 1, linetype = \"dashed\") + \n  geom_text(\n    data = CV_obs_labs, \n    aes(x = tau_mu, y = CV, color = metric, label = metric),\n    nudge_x = -0.05,\n    parse = TRUE\n  ) + \n  geom_line() + \n  theme_minimal() + \n  labs(x = expression(tau / mu), y = \"Coefficient of variation for observed score ES\") + \n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\n\n\nConsider, for instance, a scenario where observed-score reliability is always at least 0.64 and \\(\\tau / \\mu = 0.5\\), which would be the case if effect sizes are normally distributed and about 97% of effect sizes are positive. Under the effect size model based on \\(\\delta_{av}\\), the observed-score \\(\\tilde\\delta_{av}\\) is hardly affected by measurement heterogeneity at all, with a CV of 0.504 but the CV of the observed-score \\(\\tilde\\delta_z\\) is 0.543. Under the effect size model based on \\(\\delta_{av}\\), the observed-score \\(\\tilde\\delta_{av}\\) is strongly affected by measurement heterogeneity, with a CV of 0.591; in comparison, the CV of the observed-score \\(\\tilde\\delta_z\\) of 0.528. Under both models, these increases in CV are effectively constant for larger values of \\(\\tau / \\mu\\)."
  },
  {
    "objectID": "posts/dizzy-for-d-z/index.html#so-whats-your-point",
    "href": "posts/dizzy-for-d-z/index.html#so-whats-your-point",
    "title": "Cohen’s \\(d_z\\) makes me dizzy when considering measurement error",
    "section": "So what’s your point?",
    "text": "So what’s your point?\nUnfortunately, this particular trip down a rabbit hole doesn’t seem to yield many clear take-aways. For the scenario that I looked at here, the preferred choice of effect size metric is apparently driven by what assumptions we find more plausible. If we think the more plausible model is the one in which \\(\\rho\\) is independent of \\(\\delta_{av}\\), then \\(\\tilde\\delta_{av}\\) is less strongly affected by measurement variation and therefore preferred. Further, the attenuation in \\(\\tilde\\delta_{av}\\) depends only on \\(\\phi\\), which might mean that a correction for attenuation is more feasible. However, if we think the more plausible model is the one in which \\(\\rho\\) is independent of \\(\\delta_z\\), then \\(\\tilde\\delta_z\\) is less strongly affected by measurement variation and therefore preferred.5 In the latter model, one caveat is that the measurement error attenuation in \\(\\tilde\\delta_z\\) is still a complicated mess, depending on both the true score correlation \\(\\rho\\) and the reliability \\(\\phi\\). This would make it pretty hard to implement some sort of correction for attenuation.\nSo, how could one decide which meta-analytic model is more plausible in a given application? On a conceptual level, I would argue that the model for \\(\\delta_{av}\\) would tend to be more plausible in meta-analyses where there is more operational variation in the interventions examined. I would venture that syntheses that include many different versions of an intervention would tend to have a wider range of correlations between true scores (i.e., more heterogeneous correlations between potential outcomes), even holding the outcome measurement procedures constant. This doesn’t necessarily justify the assumption that \\(\\rho\\) is independent of \\(\\delta_{av}\\), but it does make it seem rather implausible that \\(\\delta_z\\) would be independent of \\(\\rho\\).\nOn a more practical level, it seems like there are a few empirical things that a meta-analyst could do to inform a choice between a model for \\(\\delta_{av}\\) and one for \\(\\delta_z\\). Pragmatically, one could calculate both effect size metrics and just see which one exhibits more heterogeneity. All else equal, it seems reasonable to prefer the metric that has less heterogeneity. One could also try to gather data on the correlation between observed scores, on the reliability of the observed scores, and on the number of trials used in each study. With this information, one could construct measurement-related predictors and use them in a meta-regression to explain variation in the observed effect size estimates. Alternately, one could use the formulas given above to implement attenuation corrections for the effect sizes and see if this leads to reduced heterogeneity. How well would any of these approaches actually work? Answering that question would take some further, more careful and systematic investigation."
  },
  {
    "objectID": "posts/dizzy-for-d-z/index.html#footnotes",
    "href": "posts/dizzy-for-d-z/index.html#footnotes",
    "title": "Cohen’s \\(d_z\\) makes me dizzy when considering measurement error",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nPerhaps because use of Cohen’s \\(d\\) is so under-scrutinized in practice, methodologists have spent many an afternoon blogging about this problem. For general discussions about issues with how to define Cohen’s \\(d\\), see excellent posts from Jan Vanhove (with a sequel), Jake Westfall, Uri Simonsohn, and Pierre Dragicevic; a more formal discussion by Thom Baguley; and some very interesting work on alternative conceptualizations by Tony Ades and colleagues. I can promise, dear reader, that the present blog post will not be nearly as cogent as these contributions—this is more about getting my own thoughts straight than making any recommendations—and so caveat lector applies.↩︎\nI would love to be corrected on both of these points. Please drop a comment or email me with suggested reading.↩︎\nWhen the standardizing variance is calculated using measurements from only one condition (i.e., the pre-test in a repeated measures design), this version of \\(d\\) corresponds to measure = \"SMCR\", the “standardized mean change using raw score standardization” in metafor::escalc.↩︎\nThis version of \\(d\\) corresponds to measure = \"SMCC\", the “standardized mean change using change score standardization” in metafor::escalc.↩︎\nAnd of course, these two models are not the only alternatives—one could look at intermediate scenarios where \\(\\rho\\) and \\(\\phi\\) are more or less strongly correlated with the true score effect sizes.↩︎"
  },
  {
    "objectID": "posts/Easily-simulate-thousands-of-single-case-designs/index.html",
    "href": "posts/Easily-simulate-thousands-of-single-case-designs/index.html",
    "title": "Easily simulate thousands of single-case designs",
    "section": "",
    "text": "Earlier this month, I taught at the Summer Research Training Institute on Single-Case Intervention Design and Analysis workshop, sponsored by the Institute of Education Sciences’ National Center for Special Education Research. While I was there, I shared a web-app for simulating data from a single-case design. This is a tool that I put together a couple of years ago as part of my ARPobservation R package, but haven’t ever really publicized or done anything formal with. It provides an easy way to simulate “mock” data from a single-case design where the dependent variable is measured using systematic direct observation of behavior. The simulated data can be viewed in the form of a graph or downloaded as a csv file. And it’s quite fast—simulating 1000’s of mock single-case designs takes only a few seconds. The tool also provides a visualization of the distribution of effect size estimates that you could anticipate observing in a single-case design, given a set of assumptions about how the dependent variable is measured and how it changes in response to treatment.\n\nDemo\nHere’s an example of the sort of data that the tool generates and the assumptions it asks you to make. Say that you’re interested in evaluating the effect of a Social Stories intervention on the behavior of a child with autism spectrum disorder, and that you plan to use a treatment reversal design. Your primary dependent variable is inappropriate play behavior, measured using frequency counts over ten minute observation sessions.\nThe initial baseline and treatment phases will be 7 sessions long. At baseline, the child engages in inappropriate play at a rate of about 0.8 per minute. You anticipate that the intervention could reduce inappropriate play by as much as 90% from baseline. Enter all of these details and assumptions into the simulator, and it will generate a graph like this:\n\nHit the “Simulate!” button again and you might get something like this:\n\nOr one of these:\n  \n\n\nAll of the above graphs were generated from the same hypothetical model—the variation in the clarity and strength of the functional relation is due to random error alone. The simulator can also produce graphs that show multiple realizations of the data-generating process. Here’s one with five replications:\n\nAnd here’s the same figure, but with trend lines added:\n\nThe trend lines represent the overall average level of the dependent variable during each session, across infinitely replications of the study. The variability around the trend line provides a sense of the extent of random error in the measurements of the dependent variable.\nI think it’s a rather interesting exercise to try and draw inferences based on visual inspection of randomly generated graphs like this—particularly because it forces you to grapple with random measurement error in a way that using only real data (or only hand-drawn mock data) doesn’t allow. It seems like it could really help a visual analyst to calibrate their interpretations of single-case graphs with visually apparent time trends, outliers, etc.\n\n\nUse cases\nSo far, this tool is really only a toy—something that I’ve puttered with off and on for a while, but never developed or applied for any substantive purpose. However, it occurs to me that it (or something similar to it) might have a number of purposes related to planning single-case studies, studying the process of visual inspection, or training single-case researchers.\nWhen I originally put the tool together, the leading case I imagined was to use the tool to help researchers make principled decisions about how to measure dependent variables in single-case designs. By using the tool to simulate hypothetical single-case studies, a researcher would be able to experiment with different measurement strategies—such as using partial interval recording instead of continuous duration recording, using shorter or longer observation sessions, or using short or longer baseline phases—before collecting data on real-life behavior in the field. I’m not sure if this is something that well-trained single-case researchers would actually find helpful, but it seems like it might help a novice (like me!) to temper one’s expectations or to move towards a more reliable measurement system.\nThere’s been quite a bit of research examining the reliability and error rates of inferences based on visual inspection (see Chapter 4 of Kratochwill & Levin, 2014 for a review of some of this literature). Some of this work has compared the inferences drawn by novices versus experts or by un-aided visual inspection versus visual inspection supplemented with graphical guides (like trend lines). But there are many other factors that could be investigated too, such as phase lengths (this could help to better justify the WWC single-case design standards around minimum phase lengths), use of different measurement systems, or use of different design elements on single-case graphs (can we get some color on these graphs, folks?!? And stop plotting 14 different dependent variables on the same graph?!?). The simulator would be an easy way to generate the stimuli one would need to do this sort of work.\nA closely related use-case is to generate stimuli for training researchers to do systematic visual inspection. Some of the SCD Institute instructors (including Tom Kratochwill, Rob Horner, Joel Levin, along with some of their other colleagues) have developed the website www.singlecase.org with a bunch of exercises meant to help researchers develop and test their visual analysis skills. It looks to me like the site uses simulated data (though I’m not entirely sure). The ARPsimulator tool could be used to do something similar, but based on a data-generating process that captures many of the features of systematic direct observation data. This might let researchers test their skills under more challenging and ambiguous, yet plausible, conditions, similar to what they will encounter when collecting real data in the field.\n\n\nFuture directions\nA number of future directions for this project have crossed my mind:\n\nCurrently, the outcome data are simulated as independent across observation sessions (given the true time trend). It wouldn’t be too hard to add a further option to generate auto-correlated data, although this would further increase the complexity of the model. Perhaps there would be a way to add this as an “advanced” option that would be concealed unless the user asks for it (i.e., “Are you Really Sure you want to go down this rabbit hole?”). So far, I have avoided adding these features because I’m not sure what reasonable defaults would be.\nJoel Levin, John Ferron, and some of the other SCD Institute instructors are big proponents of incorporating randomization procedures into the design of single-case studies, at least when circumstances allow. Currently, the ARPsimulator generates data based on a fixed, pre-specified design, such as an ABAB design with 6 sessions per phase or a multiple baseline design with 25 sessions total and intervention start-times of 8, 14, and 20. It wouldn’t be too hard to incorporate randomized phase-changes into the simulator. This might make a nice, contained project for a student who wants to learn more about randomization designs.\nAlong similar lines, John Ferron has developed and evaluated masked visual analysis procedures, which blend randomization and traditional response-guided approaches to designing single-case studies. It would take a bit more work, but it would be pretty nifty to incorporate these designs into ARPsimulator too.\nCurrently, the model behind ARPsimulator asks the user to specify a fixed baseline level of behavior, and this level of behavior is used for every simulated case—even in designs involving multiple cases. A more realistic (albeit more complicated) data-generating model would allow for between-case variation in the baseline level of behavior.\nPerhaps the most important outstanding question about the premise of this work is just how well the alternating renewal process model captures the features of real single-case data. Validating the model against empirical data from single-case studies would allow use to assess whether it is really a realistic approach to simulation, at least for certain classes of behavior. Another product of such an investigation would be to develop realistic default assumptions for the model’s parameters.\n\nAt the moment I have no plans to implement any of these unless there’s a reasonably focused need (sadly, I don’t have time to putter and putz to the same extent that I used to). If you, dear reader, would be interested in helping to pursue any of these directions, or if you have other, better ideas for how to make use of this tool, I would love to hear from you.\n\n\n\n\n\n\nReuseCC BY 4.0CitationBibTeX citation:@online{pustejovsky2018,\n  author = {Pustejovsky, James E.},\n  title = {Easily Simulate Thousands of Single-Case Designs},\n  date = {2018-06-21},\n  url = {https://jepusto.com/posts/Easily-simulate-thousands-of-single-case-designs},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nPustejovsky, James E. 2018. “Easily Simulate Thousands of\nSingle-Case Designs.” June 21, 2018. https://jepusto.com/posts/Easily-simulate-thousands-of-single-case-designs."
  },
  {
    "objectID": "posts/Four-methods-for-analyzing-PIR-data/index.html",
    "href": "posts/Four-methods-for-analyzing-PIR-data/index.html",
    "title": "New article: Four methods for analyzing PIR data",
    "section": "",
    "text": "My article with Daniel Swan, “Four methods for analyzing partial interval recording data, with application to single-case research” has been accepted for publication in Multivariate Behavioral Research. In an extension of my earlier paper on measurement-comparable effect sizes for single-case studies, this article provides some approaches to estimating effect sizes from single-case studies that use partial interval or whole interval recording to measure behavioral outcomes. The full abstract is below. Preprint and supporting materials are available. R functions that implement the proposed methods are available in the package ARPobservation.\nPartial interval recording is a procedure for collecting measurements during direct observation of behavior. It is used in several areas of educational and psychological research, particularly in connection with single-case research. Measurements collected using partial interval recording suffer from construct invalidity because they are not readily interpretable in terms of the underlying characteristics of the behavior. Using an alternating renewal process model for the behavior under observation, we demonstrate that ignoring the construct invalidity of PIR data can produce misleading inferences, such as inferring that an intervention reduces the prevalence of an undesirable behavior when in fact it has the opposite effect. We then propose four different methods for analyzing PIR summary measurements, each of which can be used to draw inferences about interpretable behavioral parameters. We demonstrate the methods by applying them to data from two single-case studies of problem behavior.\n\n\n\n\n\nReuseCC BY 4.0CitationBibTeX citation:@online{pustejovsky2015,\n  author = {Pustejovsky, James E.},\n  title = {New Article: {Four} Methods for Analyzing {PIR} Data},\n  date = {2015-02-11},\n  url = {https://jepusto.com/posts/Four-methods-for-analyzing-PIR-data},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nPustejovsky, James E. 2015. “New Article: Four Methods for\nAnalyzing PIR Data.” February 11, 2015. https://jepusto.com/posts/Four-methods-for-analyzing-PIR-data."
  },
  {
    "objectID": "posts/generalized-poisson-in-Stan/index.html",
    "href": "posts/generalized-poisson-in-Stan/index.html",
    "title": "Implementing Consul’s generalized Poisson distribution in Stan",
    "section": "",
    "text": "\\[\n\\def\\Pr{{\\text{Pr}}}\n\\def\\E{{\\text{E}}}\n\\def\\Var{{\\text{Var}}}\n\\def\\Cov{{\\text{Cov}}}\n\\def\\bm{\\mathbf}\n\\def\\bs{\\boldsymbol}\n\\]\nFor a project I am working on, we are using Stan to fit generalized random effects location-scale models to a bunch of count data. In a previous post, I walked through our implementation of Efron’s (1986) double-Poisson distribution, which we are interested in using because it allows for both over- and under-dispersion relative to the Poisson distribution. Another distribution with these properties is the generalized Poisson distribution described by Consul and Jain (1973).\nIn this post, I’ll walk through my implementation of the GPO in Stan. The gamlss.dist package provides a full set of distributional functions for the generalized Poisson distribution, including a sampler, but the functions are configured to only allow for over-dispersion. Since I’m interested in allowing for under-dispersion as well, I’ll need to write my own functions. As in my previous post, I can validate my Stan functions against the functions from gamlss.dist (although only for over-dispersion scenarios).\nCode\nlibrary(tidyverse)\nlibrary(patchwork)   # composing figures\nlibrary(gamlss.dist) # DPO distribution functions\nlibrary(rstan)       # Stan interface to R\nlibrary(brms)        # fitting generalized linear models\nlibrary(bayesplot)   # Examine fitted models\nlibrary(loo)         # Model fit measures"
  },
  {
    "objectID": "posts/generalized-poisson-in-Stan/index.html#the-generalized-poisson",
    "href": "posts/generalized-poisson-in-Stan/index.html#the-generalized-poisson",
    "title": "Implementing Consul’s generalized Poisson distribution in Stan",
    "section": "The generalized Poisson",
    "text": "The generalized Poisson\nConsul and Jain’s generalized Poisson distribution is a discrete distribution for non-negative counts, with support \\(\\mathcal{S}_X = \\{0, 1, 2, 3, ...\\}\\). The mean-variance relationship of the generalized Poisson is constant; for \\(X \\sim GPO(\\mu, \\phi)\\), \\(\\text{E}(X) = \\mu\\) and \\(\\text{Var}(X) = \\mu / \\phi\\) for \\(0 &lt; \\phi &lt; 1\\); the expectation and variance are not exact but are close approximations when there is underdispersion, so \\(\\phi &gt; 1\\). Thus, like the double-Poisson distribution, the generalized Poisson satisfies the assumptions of a quasi-Poisson generalized linear model (at least approximately).\nThe density of the generalized Poisson distribution with mean \\(\\mu\\) and inverse-disperson \\(\\phi\\) is: \\[\nf(x | \\mu, \\phi) = \\mu \\sqrt{\\phi} \\left( x + \\sqrt{\\phi}(\\mu - x) \\right)^{x-1} \\frac{\\exp \\left[-\\left( x + \\sqrt{\\phi}(\\mu - x)\\right)\\right]}{x!}.\n\\] We then have \\[\n\\ln f(x | \\mu, \\phi) = \\frac{1}{2} \\ln \\phi + \\ln \\mu + (x - 1) \\ln \\left( x + \\sqrt{\\phi}(\\mu - x) \\right) - \\left( x + \\sqrt{\\phi}(\\mu - x) \\right) - \\ln \\left(x!\\right).\n\\] Using the GPO with under-dispersed data is a little bit more controversial (by statistical standards) than using the DPO. This is because, for parameter values corresponding to under-dispersion, its probability mass function becomes negative for large counts. In particular, note that for values \\(x &gt; \\frac{\\mu\\sqrt\\phi}{\\sqrt\\phi - 1}\\), the quantity \\(x + \\sqrt{\\phi}(\\mu - x)\\) becomes negative, and so \\(f(x| \\mu, \\phi)\\) is no longer a proper probability. Consul suggested handling this situation by truncating the distribution at \\(m = \\left\\lfloor \\frac{\\mu\\sqrt\\phi}{\\sqrt\\phi - 1}\\right\\rfloor\\). However, doing so makes the distribution only an approximation, such that \\(\\mu\\) is no longer exactly the mean and \\(\\phi\\) is no longer exactly the inverse dispersion. For modest under-dispersion of no less than 60% of the mean, \\(1 &lt; \\phi &lt; 5 / 3\\) and the truncation point is fairly extreme, with \\(m \\approx 4.4 \\mu\\), so I’m not too worried about this issue. We’ll see how it plays out in application, of course."
  },
  {
    "objectID": "posts/generalized-poisson-in-Stan/index.html#candidate-models",
    "href": "posts/generalized-poisson-in-Stan/index.html#candidate-models",
    "title": "Implementing Consul’s generalized Poisson distribution in Stan",
    "section": "Candidate models",
    "text": "Candidate models\nNow let me fit the same generalized linear model but assuming that the outcome follow a couple of different distributions, including a true Poisson (with unit dispersion), a negative binomial, the double-Poisson distribution from the previous post, and the generalized Poisson distribution. Here goes!\n\n\nCode\nPoisson_fit &lt;- \n  brm(\n    Y ~ X, family = poisson(link = \"log\"),\n    data = dat, \n    warmup = 500, \n    iter = 2500, \n    chains = 4, \n    cores = 4,\n    seed = 20231204\n  )\n\n\n\n\nCode\nnegbin_fit &lt;- \n  brm(\n    Y ~ X, family = negbinomial(link = \"log\"),\n    data = dat, \n    warmup = 500, \n    iter = 2500, \n    chains = 4, \n    cores = 4,\n    seed = 20231204\n  )\n\n\n\n\nCode\nstancode_dpo &lt;- \"\nreal dpo_lpmf(int X, real mu, real phi) {\n  real ans;\n  real A = inv(2) * log(phi) - phi * mu;\n  if (X == 0)\n    ans = A;\n  else\n    ans = A + X * (phi * (1 + log(mu)) - 1) - lgamma(X + 1) + (1 - phi) * X * log(X);\n  return ans;\n}\nvector dpo_cdf_vec(real mu, real phi, int maxval) {\n  real d = exp(phi * (1 + log(mu)) - 1);\n  real prob;\n  int n = maxval + 1;\n  vector[n] cdf;\n  cdf[1] = sqrt(phi) * exp(-mu * phi);\n  prob = cdf[1] * d;\n  cdf[2] = cdf[1] + prob;\n  for (i in 2:maxval) {\n    prob = prob * d * exp((1 - phi) * (i - 1) * (log(i) - log(i - 1))) / (i^phi);\n    cdf[i + 1] = cdf[i] + prob;\n    if (prob / cdf[i + 1] &lt; 1e-8) {\n      n = i + 1;\n      break;\n    }\n  }\n  return cdf / cdf[n];\n}\nint dpo_quantile(real p, real mu, real phi, int maxval) {\n  vector[maxval + 1] cdf_vec = dpo_cdf_vec(mu, phi, maxval);\n  int q = 0;\n  while (cdf_vec[q + 1] &lt; p) {\n      q += 1;\n    }\n  return q;\n}\nint dpo_rng(real mu, real phi, int maxval) {\n  real p = uniform_rng(0,1);\n  int x = dpo_quantile(p, mu, phi, maxval);\n  return x;\n}\n\"\ndouble_Poisson &lt;- custom_family(\n  \"dpo\", dpars = c(\"mu\",\"phi\"),\n  links = c(\"log\",\"log\"),\n  lb = c(0, 0), ub = c(NA, NA),\n  type = \"int\"\n)\n\ndouble_Poisson_stanvars &lt;- stanvar(scode = stancode_dpo, block = \"functions\")\n\nphi_prior &lt;- prior(exponential(1), class = \"phi\")\n\nDPO_fit &lt;- \n  brm(\n    Y ~ X, family = double_Poisson,\n    prior = phi_prior,\n    stanvars = double_Poisson_stanvars,\n    data = dat, \n    warmup = 500, \n    iter = 2500, \n    chains = 4, \n    cores = 4,\n    seed = 20231204\n  )\n\nexpose_functions(DPO_fit, vectorize = TRUE)\n\nlog_lik_dpo &lt;- function(i, prep) {\n  mu &lt;- brms::get_dpar(prep, \"mu\", i = i)\n  phi &lt;- brms::get_dpar(prep, \"phi\", i = i)\n  y &lt;- prep$data$Y[i]\n  dpo_lpmf(y, mu, phi)\n}\n\nposterior_predict_dpo &lt;- function(i, prep, maxval = NULL, ...) {\n  mu &lt;- brms::get_dpar(prep, \"mu\", i = i)\n  phi &lt;- brms::get_dpar(prep, \"phi\", i = i)\n  if (is.null(maxval)) maxval &lt;- 20 * mu / min(phi, 1)\n  dpo_rng(mu, phi, maxval = maxval)\n}\n\n\n\n\nCode\nstancode_gpo &lt;- \"\nreal gpo_lpmf(int X, real mu, real phi) {\n  real ans;\n  real m = mu / (1 - inv(sqrt(phi)));\n  real z = X + sqrt(phi) * (mu - X);\n  if (phi &gt; 1 && X &gt; m)\n    ans = negative_infinity();\n  else \n    ans = log(mu) + inv(2) * log(phi) + (X - 1) * log(z) - z - lgamma(X + 1);\n  return ans;\n}\nint gpo_quantile(real p, real mu, real phi) {\n  int q = 0;\n  real phi_sqrt = sqrt(phi);\n  real mu_phi_sqrt = mu * phi_sqrt;\n  real m;\n  if (phi &gt; 1) \n    m = mu / (1 - inv(phi_sqrt));\n  else \n    m = positive_infinity();\n  real lpmf = - mu_phi_sqrt;\n  real cdf = exp(lpmf);\n  real ln_inc;\n  while (cdf &lt; p && q &lt; m) {\n    q += 1;\n    ln_inc = (q - 1) * log(mu_phi_sqrt + q * (1 - phi_sqrt)) - (q - 2) * log(mu_phi_sqrt + (q - 1) * (1 - phi_sqrt)) + (phi_sqrt - 1) - log(q);\n    lpmf += ln_inc;    \n    cdf += exp(lpmf);\n  }\n  return q;\n}\nint gpo_rng(real mu, real phi) {\n  real p = uniform_rng(0,1);\n  int x = gpo_quantile(p, mu, phi);\n  return x;\n}\n\"\ngeneralized_Poisson &lt;- custom_family(\n  \"gpo\", dpars = c(\"mu\",\"phi\"),\n  links = c(\"log\",\"log\"),\n  lb = c(0, 0), ub = c(NA, NA),\n  type = \"int\"\n)\n\ngeneralized_Poisson_stanvars &lt;- stanvar(scode = stancode_gpo, block = \"functions\")\n\nphi_prior &lt;- prior(exponential(1), class = \"phi\")\n\nGPO_fit &lt;- \n  brm(\n    Y ~ X, family = generalized_Poisson,\n    prior = phi_prior,\n    stanvars = generalized_Poisson_stanvars,\n    data = dat, \n    warmup = 500, \n    iter = 2500, \n    chains = 4, \n    cores = 4,\n    seed = 20231204\n  )\n\nexpose_functions(GPO_fit, vectorize = TRUE)\n\nlog_lik_gpo &lt;- function(i, prep) {\n  mu &lt;- brms::get_dpar(prep, \"mu\", i = i)\n  phi &lt;- brms::get_dpar(prep, \"phi\", i = i)\n  y &lt;- prep$data$Y[i]\n  gpo_lpmf(y, mu, phi)\n}\n\nposterior_predict_gpo &lt;- function(i, prep, maxval = NULL, ...) {\n  mu &lt;- brms::get_dpar(prep, \"mu\", i = i)\n  phi &lt;- brms::get_dpar(prep, \"phi\", i = i)\n  gpo_rng(mu, phi)\n}"
  },
  {
    "objectID": "posts/generalized-poisson-in-Stan/index.html#model-comparison",
    "href": "posts/generalized-poisson-in-Stan/index.html#model-comparison",
    "title": "Implementing Consul’s generalized Poisson distribution in Stan",
    "section": "Model comparison",
    "text": "Model comparison\nHere is a comparison of LOOIC for all of the models:\n\n\nCode\nloo_comparison &lt;- loo(Poisson_fit, negbin_fit, DPO_fit, GPO_fit)\nloo_comparison$diffs\n\n\n            elpd_diff se_diff\nDPO_fit        0.0       0.0 \nGPO_fit       -2.9       2.8 \nnegbin_fit    -8.9       4.4 \nPoisson_fit -120.8      20.1 \n\n\nThe model based on the double-Poisson distribution fits equally well to the true data-generating process here, suggesting that there’s really just not enough information to distriguish between the two models. The negative binomial distribution fit is substantially worse, and the Poisson distribution fit is awful.\nHere’s the posterior for the dispersion (i.e., \\(1 / \\phi\\)) based on the GPO and DPO models:\n\n\nCode\ncolor_scheme_set(\"green\")\nGPO_dispersion &lt;- \n  mcmc_areas(GPO_fit, pars = \"phi\", transformations = \\(x) 1 / x) + \n  theme_minimal() + \n  ggtitle(\"Generalized Poisson\")\n\ncolor_scheme_set(\"brightblue\")\nDPO_dispersion &lt;- \n  mcmc_areas(DPO_fit, pars = \"phi\", transformations = \\(x) 1 / x) + \n  theme_minimal() + \n  ggtitle(\"Double Poisson\")\n\nDPO_dispersion / GPO_dispersion & \n  xlim(1.5, 2.0)\n\n\n\n\n\n\n\n\n\nRight on par. To get a better sense of model fit, I’ll run some posterior predictive checks, using the quasi-likelihood dispersion as a summary statistic:\n\n\nCode\nYrep_Poisson &lt;- posterior_predict(Poisson_fit, ndraws = 500) \nYrep_negbin &lt;- posterior_predict(negbin_fit, ndraws = 500)\nYrep_dpo &lt;- posterior_predict(DPO_fit, ndraws = 500)\nYrep_gpo &lt;- posterior_predict(GPO_fit, ndraws = 500)\n\ndispersion_coef &lt;- function(y) {\n  quasi_fit &lt;- glm(y ~ dat$X, family = quasipoisson(link = \"log\"))\n  sum(residuals(quasi_fit, type = \"pearson\")^2) / quasi_fit$df.residual\n}\n\ncolor_scheme_set(\"blue\")\nPoisson_disp &lt;- ppc_stat(dat$Y, Yrep_Poisson, stat = dispersion_coef, binwidth = 0.02) + \n  labs(title = \"Poisson\")\n\ncolor_scheme_set(\"purple\")\nnegbin_disp &lt;- ppc_stat(dat$Y, Yrep_negbin, stat = dispersion_coef, binwidth = 0.02) + \n  labs(title = \"Negative-binomial\")\n\ncolor_scheme_set(\"brightblue\")\ndpo_disp &lt;- ppc_stat(dat$Y, Yrep_dpo, stat = dispersion_coef, binwidth = 0.02) + \n  labs(title = \"Double Poisson\")\n\ncolor_scheme_set(\"green\")\ngpo_disp &lt;- ppc_stat(dat$Y, Yrep_gpo, stat = dispersion_coef, binwidth = 0.02) + \n  labs(title = \"Generalized Poisson\")\n\nPoisson_disp / negbin_disp / dpo_disp / gpo_disp &\n  theme_minimal() & \n  xlim(c(0.8, 2.1))\n\n\n\n\n\n\n\n\n\nBoth the double Poisson and the generalized Poisson models generate data with levels of dispersion similar to the observed data. The negative binomial distribution is not noticeably worse."
  },
  {
    "objectID": "posts/generalized-poisson-in-Stan/index.html#marginal-posterior-predictive-densities",
    "href": "posts/generalized-poisson-in-Stan/index.html#marginal-posterior-predictive-densities",
    "title": "Implementing Consul’s generalized Poisson distribution in Stan",
    "section": "Marginal posterior predictive densities",
    "text": "Marginal posterior predictive densities\nHere’s some rootograms for the posterior predictive density of the raw outcomes:\n\n\nCode\ncolor_scheme_set(\"blue\")\nPoisson_root &lt;- ppc_rootogram(dat$Y, Yrep_Poisson, style = \"hanging\") + labs(title = \"Poisson\")\ncolor_scheme_set(\"purple\")\nnegbin_root &lt;- ppc_rootogram(dat$Y, Yrep_negbin, style = \"hanging\") + labs(title = \"Negative-binomial\")\ncolor_scheme_set(\"brightblue\")\ndpo_root &lt;- ppc_rootogram(dat$Y, Yrep_dpo, style = \"hanging\") + labs(title = \"Double Poisson\")\ncolor_scheme_set(\"green\")\ngpo_root &lt;- ppc_rootogram(dat$Y, Yrep_gpo, style = \"hanging\") + labs(title = \"Generalized Poisson\")\n\nPoisson_root / negbin_root / dpo_root / gpo_root &\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nYou can see from these that the Poisson model maybe expects slightly fewer low counts and slightly fewer high counts than are present in the observed data. However, the figure doesn’t really capture the degree of mis-fit that is apparent with the dispersion summary statistics. I think this is because the distribution of \\(Y\\) changes so much depending on the value of the predictor \\(X\\)."
  },
  {
    "objectID": "posts/generalized-poisson-in-Stan/index.html#posterior-predictive-residual-densities",
    "href": "posts/generalized-poisson-in-Stan/index.html#posterior-predictive-residual-densities",
    "title": "Implementing Consul’s generalized Poisson distribution in Stan",
    "section": "Posterior predictive residual densities",
    "text": "Posterior predictive residual densities\nOne way to focus in on the distributional assumption is to examine the distribution of residuals rather than raw outcomes. I’ll do that here by looking the deviance residuals from the quasi-Poisson GLM model, treating the calculation of the residuals as merely a transformation of the raw data. Here are some posterior predictive density plots of these deviance residuals:\n\n\nCode\n# quasi-Poisson deviance residuals\ndat$resid &lt;- residuals(quasi_fit)\n\n# function to calculate quasi-Poisson deviance residuals\nquasi_residuals &lt;- function(y) as.numeric(residuals(glm(y ~ dat$X, family = quasipoisson(link = \"log\"))))\n\n# transform posterior predictive data into residuals\nR &lt;- 50\nresid_Poisson &lt;- apply(Yrep_Poisson[1:R,], 1, quasi_residuals) |&gt; t()\nresid_negbin &lt;- apply(Yrep_negbin[1:R,], 1, quasi_residuals) |&gt; t()\nresid_dpo &lt;- apply(Yrep_dpo[1:R,], 1, quasi_residuals) |&gt; t()\nresid_gpo &lt;- apply(Yrep_gpo[1:R,], 1, quasi_residuals) |&gt; t()\n\n# make density plots\ncolor_scheme_set(\"blue\")\nPoisson_resid_density &lt;- ppc_dens_overlay(dat$resid, resid_Poisson) + labs(title = \"Poisson\")\n\ncolor_scheme_set(\"purple\")\nnegbin_resid_density &lt;- ppc_dens_overlay(dat$resid, resid_negbin) + labs(title = \"Negative-binomial\")\n\ncolor_scheme_set(\"brightblue\")\ndpo_resid_density &lt;- ppc_dens_overlay(dat$resid, resid_dpo) + labs(title = \"Double Poisson\")\n\ncolor_scheme_set(\"green\")\ngpo_resid_density &lt;- ppc_dens_overlay(dat$resid, resid_gpo) + labs(title = \"Generalized Poisson\")\n\nPoisson_resid_density / negbin_resid_density / dpo_resid_density / gpo_resid_density &\n  theme_minimal() & \n  xlim(c(-3.5, 3.5))\n\n\n\n\n\n\n\n\n\nIt’s quite a bit clearer from these plots that the DPO and GPO models are closer to replicating the distribution of the data than the Poisson model. The negative binomial model is not obviously mis-specified either.\nA notable difference between the negative binomial versus the DPO and GPO distributions is in the form of the mean-variance relationship. For the negative binomial, the variance increases with the square of the mean, whereas for the DPO and GPO, the variance increases in constant proportion to the mean. The residual posterior density plots above don’t really capture these mean-variance relationships in an obvious way. I took one more crack at a posterior predictive check to get at this. Below is a figure showing the loess smooth of the squared residuals versus \\(X\\) based on the posterior predictive distributions versus the real data. I couldn’t find an easy way to do this with the bayesplot functions I’ve used above, so I had to bang it out in regular ggplot.\n\n\nCode\nX_grid &lt;- seq(min(dat$X), max(dat$X), length.out = 200)\n\nsmooth_square_resid &lt;- function(r, x_dat, X_pred = X_grid) {\n  loess_fit &lt;- loess(I(r^2) ~ x_dat)\n  predict(loess_fit, newdata = data.frame(x_dat = X_pred))\n}\n\ndat$sm &lt;- smooth_square_resid(r = dat$resid, x_dat = dat$X, X_pred = dat$X)\n\nsmooth_square_resid_ppcs &lt;- function(R, x_dat, X_pred = X_grid) {\n  smooth_list &lt;- apply(R, 1, smooth_square_resid, x_dat = dat$X, X_pred = X_pred, simplify = FALSE)\n  tibble(\n    group = 1:length(smooth_list),\n    X = rep(list(X_pred), length(smooth_list)),\n    sm = smooth_list\n  )\n}\n\nsmooth_MV &lt;- \n  list(\n    Poisson = resid_Poisson, \n    `Negative binomial` = resid_negbin,\n    `Double Poisson` = resid_dpo,\n    `Generalized Poisson` = resid_gpo\n  ) %&gt;%\n  map_dfr(smooth_square_resid_ppcs, x_dat = dat$X, .id = \"distribution\") %&gt;%\n  unnest(X, sm) %&gt;%\n  mutate(\n    distribution = factor(distribution, levels = c(\"Poisson\", \"Negative binomial\",\"Double Poisson\", \"Generalized Poisson\"))\n  )\n\nggplot(smooth_MV, aes(X, sm, group = group, color = distribution)) + \n  geom_line(alpha = 0.4) + \n  geom_line(data = dat, aes(X, sm, group = NULL), color = \"black\", linewidth = 1.25) + \n  scale_color_manual(values = c(\"grey\",\"purple\",\"lightblue\",\"lightgreen\")) + \n  scale_y_continuous(limits = c(0, 9), breaks = seq(0,8,2), expand = expansion(0,0)) + \n  facet_wrap(~ distribution, ncol = 1) + \n  theme_minimal() + \n  theme(legend.position = \"none\") + \n  labs(y = \"Loess smooth of squared residuals\")\n\n\n\n\n\n\n\n\n\nAha! Here we can clearly see that the negative binomial model generates residuals that have more curvature to the mean-variance relationship, and so don’t really fit with the observed data. The double Poisson and generalized Poisson both generate residuals that match the observed mean-variance relationship decently well."
  },
  {
    "objectID": "posts/getting-started-with-scdhlm/index.html",
    "href": "posts/getting-started-with-scdhlm/index.html",
    "title": "Getting started with scdhlm",
    "section": "",
    "text": "UPDATED 10/2/2016 after posting the package to CRAN\nHere are step-by-step instructions on how to download and install the scdhlm package for R. You’ll need to have a copy of R installed. There are two ways to do the installation: through the Comprehensive R Archive Network (CRAN) or from the source code on Github. I describe each approach in turn.\n\nOption 1: Via CRAN\nGo via CRAN to install the most recent stable version of the package. Type the following commands at the R prompt:\n\ninstall.packages(\"scdhlm\")\nlibrary(scdhlm)\n\n\n\nOption 2: Via Github\nGo via Github to get the latest development version of the package. For this option, you will first need to install the devtools package:\n\ninstall.packages(\"devtools\")\n\nOnce you have successfully installed this package, type the following:\n\nlibrary(devtools)\ninstall_github(\"jepusto/scdhlm\")\nlibrary(scdhlm)\n\n\n\nFurther instructions\nYou’ll only need to do the installation once. Once you’ve got the package installed, type the following in order to access the package within an R session: library(scdhlm).\nTo open the package documentation, type package?scdhlm. To access the documentation for an individual function in this package, just type ? followed by the name of the function. For instance, one of the main functions in the package is called g_REML; to access its documentation, type ?g_REML.\n\n\nweb-interface for calculating effect sizes\nThe package includes an interactive app (written with shiny) for calculating design-comparable standardized mean differences. To run this app on your computer, you will first need to install RStudio (if you don’t already have it). Then ensure that you have the shiny, markdown, and ggplot2 packages installed by running the following:\n\ninstall.packages(\"shiny\")\ninstall.packages(\"markdown\")\ninstall.packages(\"ggplot2\")\n\nFinally, open the app by typing the following at the prompt within RStudio:\n\nlibrary(scdhlm)\nshine_scd()\n\nThe app should now open in your web browser.\n\n\n\n\n\n\nReuseCC BY 4.0CitationBibTeX citation:@online{pustejovsky2014,\n  author = {Pustejovsky, James E.},\n  title = {Getting Started with Scdhlm},\n  date = {2014-10-19},\n  url = {https://jepusto.com/posts/getting-started-with-scdhlm},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nPustejovsky, James E. 2014. “Getting Started with Scdhlm.”\nOctober 19, 2014. https://jepusto.com/posts/getting-started-with-scdhlm."
  },
  {
    "objectID": "posts/handmade-clubSandwich/index.html",
    "href": "posts/handmade-clubSandwich/index.html",
    "title": "A handmade clubSandwich for multi-site trials",
    "section": "",
    "text": "I’m just back from the Society for Research on Educational Effectiveness meetings, where I presented work on small-sample corrections for cluster-robust variance estimators in two-stage least squares models, which I’ve implemented in the clubSandwich R package. Here’s my presentation. So I had “clubSandwich” estimators on the brain when a colleague asked me about whether the methods were implemented in SAS.\nThe short answer is “no.”\nThe moderately longer answer is “not unless we can find funding to pay someone who knows how to program properly in SAS.” However, for the specific model that my colleague was interested in, it turns out that the small-sample corrections implemented in clubSandwich can be expressed in closed form, and they’re simple enough that they could easily be hand-calculated. I’ll sketch out the calculations in the remainder of this post."
  },
  {
    "objectID": "posts/handmade-clubSandwich/index.html#a-multi-site-trial",
    "href": "posts/handmade-clubSandwich/index.html#a-multi-site-trial",
    "title": "A handmade clubSandwich for multi-site trials",
    "section": "A multi-site trial",
    "text": "A multi-site trial\nConsider a multi-site trial conducted across \\(J\\) sites, which we take as a sample from a larger super-population of sites. Each site consists of \\(n_j\\) units, of which \\(p_j n_j\\) are randomized to treatment and the remainder \\((1 - p_j) n_j\\) are randomized to control. For each unit \\(i\\) in each site \\(j\\), we have an outcome \\(y_{ij}\\) and a treatment indicator \\(t_{ij}\\).\nA conventional approach to estimating the overall average impact in this setting is to use a model with a treatment indicator and fixed effects for each site: \\[\ny_{ij} = \\beta_j + \\delta t_{ij} + e_{ij}\n\\] and then to cluster the standard errors by site. Clustering by site makes sense here if (and only if) we’re interested in generalizing to the super-population of sites.\nLet \\(\\hat\\delta_j\\) denote the impact estimate from site \\(j\\), calculated as the difference in means between treated and untreated units at site \\(j\\): \\[\n\\hat\\delta_j = \\frac{1}{n_j p_j} \\left(\\sum_{i=1}^{n_j} t_{ij} y_{ij}\\right) - \\frac{1}{n_j (1 - p_j)} \\left(\\sum_{i=1}^{n_j} (1 - t_{ij}) y_{ij}\\right).\n\\] for \\(j = 1,..,J\\). The overall impact estimate here is a precision-weighted average of the site-specific impacts: \\[\n\\hat\\delta = \\frac{1}{W} \\sum_{j=1}^J w_j \\hat\\delta_j,\n\\] where \\(w_j = n_j p_j (1 - p_j)\\) and \\(W = \\sum_j w_j\\)."
  },
  {
    "objectID": "posts/handmade-clubSandwich/index.html#sandwich-estimators",
    "href": "posts/handmade-clubSandwich/index.html#sandwich-estimators",
    "title": "A handmade clubSandwich for multi-site trials",
    "section": "Sandwich estimators",
    "text": "Sandwich estimators\nThe conventional clustered variance estimator (or sandwich estimator) for \\(\\hat\\delta\\) is a simple function of the (weighted) sample variance of the site-specific effects. It can be calculated directly as: \\[\nV^{CR0} = \\frac{1}{W^2} \\sum_{j=1}^J w_j^2 \\left(\\hat\\delta_j - \\hat\\delta\\right)^2.\n\\] Under a conventional random effects model for the \\(\\delta_j\\)s, this estimator has a downward bias in finite samples.\nThe clubSandwich variance estimator here uses an estimator for the sample variance of site-specific effects that is unbiased under a certain working model. It is only slightly more complicated to calculate: \\[\nV^{CR2} = \\frac{1}{W^2} \\sum_{j=1}^J \\frac{w_j^2 \\left(\\hat\\delta_j - \\hat\\delta\\right)^2}{1 - w_j / W}.\n\\]\nThe other difference between conventional methods and the clubSandwich approach is in the reference distribution used to calculate hypothesis tests and confidence intervals. The conventional approach uses a standard normal reference distribution (i.e., a z-test) that is asymptotically justified. The clubSandwich approach uses a \\(t\\) reference distribution, with degrees of freedom estimated using a Satterthwaite approximation. In the present context, the degrees of freedom are a little bit ugly but still not hard to calculate: \\[\ndf = \\left[\\sum_{j=1}^J \\frac{w_j^2}{(W - w_j)^2} - \\frac{2}{W}\\sum_{j=1}^J \\frac{w_j^3}{(W - w_j)^2} + \\frac{1}{W^2} \\left(\\sum_{j=1}^J \\frac{w_j^2}{W - w_j} \\right)^2 \\right]^{-1}.\n\\]\nIn the special case that all sites are of the same size and use a constant treatment allocation, the weights become equal. The clubSandwich variance estimator then reduces to \\[\nV^{CR2} = \\frac{S_\\delta^2}{J} \\qquad \\text{where} \\qquad S_\\delta^2 = \\frac{1}{J - 1}\\sum_{j=1}^J \\left(\\hat\\delta_j - \\hat\\delta\\right)^2,\n\\] and the degrees of freedom reduce to simply \\(df = J - 1\\)."
  },
  {
    "objectID": "posts/handmade-clubSandwich/index.html#tennessee-star",
    "href": "posts/handmade-clubSandwich/index.html#tennessee-star",
    "title": "A handmade clubSandwich for multi-site trials",
    "section": "Tennessee STAR",
    "text": "Tennessee STAR\nHere is a worked example of the calculations (using R of course, because my SAS programming skills atrophied years ago). I’ll use data from the famous Tennessee STAR class size experiment, which was a multi-site trial in which students were randomized to small or regular-sized kindergarten classes within each of several dozen schools. To make the small-sample issues more pronounced, I’ll limit the sample to urban schools and look at impacts of small class-size on reading and math scores at the end of kindergarten. STAR was actually a three-arm trial—the third arm being a regular-sized class but with an additional teacher aide. For simplicity (and following convention), I’ll collapse the teacher-aide condition and the regular-sized class condition into a single arm and also limit the sample to students with complete outcome data on both tests.\n\n\nCode\nlibrary(tidyverse)\ndata(STAR, package = \"AER\")\n\nSTAR_urban &lt;-\n  STAR %&gt;%\n  filter(\n    # limit to urban/inner city schools\n    schoolk %in% c(\"urban\",\"inner-city\"),\n    # limit to complete outcome data\n    !is.na(readk), !is.na(mathk)\n  ) %&gt;%\n  droplevels() %&gt;%\n  # collapse control conditions\n  mutate(stark = fct_collapse(stark, regular = c(\"regular\",\"regular+aide\"))) %&gt;%\n  select(schoolidk, stark, readk, mathk)\n\nSTAR_summary &lt;- \n  STAR_urban %&gt;%\n  count(schoolidk)\n\n\nAfter these exclusions, the data include a total of 1810 students from 23 schools, ranging in size from 34 to 134 students.\nFor starters, let’s get the average impacts using a seeming unrelated regression specification, with both conventional and clubSandwich standard errors.\n\n\nCode\nlibrary(clubSandwich)\n\n\nRegistered S3 method overwritten by 'clubSandwich':\n  method    from    \n  bread.mlm sandwich\n\n\nCode\nSTAR_fit &lt;- lm(cbind(readk, mathk) ~ 0 + schoolidk + stark, data = STAR_urban)\n\n# conventional SEs\nCR0 &lt;- \n  coef_test(STAR_fit, vcov = \"CR0\", \n            cluster = STAR_urban$schoolidk, \n            test = \"z\",\n            coefs = c(\"readk:starksmall\",\"mathk:starksmall\"))\n\nCR0\n\n\n            Coef. Estimate   SE t-stat d.f. (z) p-val (z) Sig.\n readk:starksmall     6.16 2.73   2.25      Inf    0.0241    *\n mathk:starksmall    12.13 4.79   2.53      Inf    0.0113    *\n\n\nCode\n# clubSandwich SEs\nCR2 &lt;- \n  coef_test(STAR_fit, vcov = \"CR2\", \n            cluster = STAR_urban$schoolidk, \n            coefs = c(\"readk:starksmall\",\"mathk:starksmall\"))\n\nCR2\n\n\n            Coef. Estimate   SE t-stat d.f. (Satt) p-val (Satt) Sig.\n readk:starksmall     6.16 2.81   2.19          19       0.0409    *\n mathk:starksmall    12.13 4.92   2.47          19       0.0234    *\n\n\nNow I’ll do it “by hand”—or rather, with a bit of dplyr:\n\n\nCode\n# summary statistics by site\n\nschool_summaries &lt;- \n  STAR_urban %&gt;%\n  group_by(schoolidk, stark) %&gt;%\n  summarise(\n    # means by arm and site\n    readk = mean(readk),\n    mathk = mean(mathk),\n    n_arm = n()\n  ) %&gt;%\n  summarise(\n    # impact estimates by site\n    readk = diff(readk),\n    mathk = diff(mathk),\n    n = sum(n_arm),\n    p = n_arm[stark==\"small\"] / n\n  ) %&gt;%\n  mutate(w = n * p * (1 - p))\n\n\n`summarise()` has grouped output by 'schoolidk'. You can override using the\n`.groups` argument.\n\n\nCode\n# overall impacts\n\nschool_summaries %&gt;%\n  gather(\"subject\",\"impact_j\", readk, mathk) %&gt;%\n  group_by(subject) %&gt;%\n  summarise(\n    impact = weighted.mean(impact_j, w = w),\n    SE_CR0 = sqrt(sum(w^2 * (impact_j - impact)^2) / sum(w)^2),\n    SE_CR2 = sqrt(sum(w^2 * (impact_j - impact)^2 / (1 - w / sum(w))) / sum(w)^2),\n    df_CR2 = 1 / (sum(w^2 / (sum(w) - w)^2) - \n                    2 * sum(w^3 / (sum(w) - w)^2) / sum(w) + \n                    sum(w^2 / (sum(w) - w))^2 / sum(w)^2)\n  ) %&gt;%\n  knitr::kable(digits = 2)\n\n\n\n\n\nsubject\nimpact\nSE_CR0\nSE_CR2\ndf_CR2\n\n\n\n\nmathk\n12.13\n4.79\n4.92\n18.99\n\n\nreadk\n6.16\n2.73\n2.81\n18.99\n\n\n\n\n\nThe CR0 and CR2 standard errors match the results from coef_test, as do the Satterthwaite degrees of freedom. Note that the degrees of freedom are equal to 19 in this case, a bit less than \\(J - 1 = 22\\) due to variation in the weight assigned to each school."
  },
  {
    "objectID": "posts/handmade-clubSandwich/index.html#other-weights",
    "href": "posts/handmade-clubSandwich/index.html#other-weights",
    "title": "A handmade clubSandwich for multi-site trials",
    "section": "Other weights",
    "text": "Other weights\nSome analysts might not like the approach of using precision-weighted average of the site-specific impacts, as I’ve examined here. Instead, one might choose to weight the site-specific effects by the site-specific sample sizes, or to use some sort of random effects weighting that allows for random heterogeneity across sites. The formulas given above for conventional and clubSandwich clustered variance estimators apply directly to other weighting schemes too. Just substitute your favorite weights in place of \\(w_j\\). When doing so, the clubSandwich estimator will be exactly unbiased under the assumption that your preferred weighting scheme corresponds to inverse-variance weighting, and the Satterthwaite degrees of freedom approximation will be derived under the same model."
  },
  {
    "objectID": "posts/imputing-covariance-matrices-for-multi-variate-meta-analysis/index.html",
    "href": "posts/imputing-covariance-matrices-for-multi-variate-meta-analysis/index.html",
    "title": "Imputing covariance matrices for meta-analysis of correlated effects",
    "section": "",
    "text": "In many systematic reviews, it is common for eligible studies to contribute effect size estimates from not just one, but multiple relevant outcome measures, for a common sample of participants. If those outcomes are correlated, then so too will be the effect size estimates. To estimate the degree of correlation, you would need the sample correlation among the outcomes—information that is woefully uncommon for primary studies to report (and best of luck to you if you try to follow up with author queries). Thus, the meta-analyst is often left in a situation where the sampling variances of the effect size estimates can be reasonably well approximated, but the sampling covariances are unknown for some or all studies.\nSeveral solutions to this conundrum have been proposed in the meta-analysis methodology literature. One possible strategy is to just impute a correlation based on subject-matter knowledge (or at least feigned expertise), and assume that this correlation is constant across studies. This analysis could be supplemented with sensitivity analyses to examine the extent to which the parameter estimates and inferences are sensitive to alternative assumptions about the inter-correlation of effects within studies. A related strategy, described by Wei and Higgins (2013), is to meta-analyze any available correlation estimates and then use the results to impute correlations for any studies with missing correlations.\nBoth of these approaches require the meta-analyst to calculate block-diagonal sampling covariance matrices for the effect size estimates, which can be a bit unwieldy. I often use the impute-the-correlation strategy in my meta-analysis work and have written a helper function to compute covariance matrices, given known sampling variances and imputed correlations for each study. In the interest of not repeating myself, I’ve added the function to the latest version of my clubSandwich package. In this post, I’ll explain the function and demonstrate how to use it for conducting meta-analysis of correlated effect size estimates."
  },
  {
    "objectID": "posts/imputing-covariance-matrices-for-multi-variate-meta-analysis/index.html#an-r-function-for-block-diagonal-covariance-matrices",
    "href": "posts/imputing-covariance-matrices-for-multi-variate-meta-analysis/index.html#an-r-function-for-block-diagonal-covariance-matrices",
    "title": "Imputing covariance matrices for meta-analysis of correlated effects",
    "section": "An R function for block-diagonal covariance matrices",
    "text": "An R function for block-diagonal covariance matrices\nHere is the function:\n\nlibrary(clubSandwich)\n\nRegistered S3 method overwritten by 'clubSandwich':\n  method    from    \n  bread.mlm sandwich\n\nimpute_covariance_matrix\n\nfunction (vi, cluster, r, ti, ar1, smooth_vi = FALSE, subgroup = NULL, \n    return_list = identical(as.factor(cluster), sort(as.factor(cluster))), \n    check_PD = TRUE) \n{\n    cluster &lt;- droplevels(as.factor(cluster))\n    vi_list &lt;- split(vi, cluster)\n    if (smooth_vi) \n        vi_list &lt;- lapply(vi_list, function(x) rep(mean(x, na.rm = TRUE), \n            length(x)))\n    if (missing(r) & missing(ar1)) \n        stop(\"You must specify a value for r or for ar1.\")\n    if (!missing(r)) {\n        r_list &lt;- rep_len(r, length(vi_list))\n        if (missing(ar1)) {\n            vcov_list &lt;- Map(function(V, rho) (rho + diag(1 - \n                rho, nrow = length(V))) * tcrossprod(sqrt(V)), \n                V = vi_list, rho = r_list)\n        }\n    }\n    if (!missing(ar1)) {\n        if (missing(ti)) \n            stop(\"If you specify a value for ar1, you must provide a vector for ti.\")\n        ti_list &lt;- split(ti, cluster)\n        ar_list &lt;- rep_len(ar1, length(vi_list))\n        if (missing(r)) {\n            vcov_list &lt;- Map(function(V, time, phi) (phi^as.matrix(stats::dist(time))) * \n                tcrossprod(sqrt(V)), V = vi_list, time = ti_list, \n                phi = ar_list)\n        }\n        else {\n            vcov_list &lt;- Map(function(V, rho, time, phi) (rho + \n                (1 - rho) * phi^as.matrix(stats::dist(time))) * \n                tcrossprod(sqrt(V)), V = vi_list, rho = r_list, \n                time = ti_list, phi = ar_list)\n        }\n        vcov_list &lt;- lapply(vcov_list, function(x) {\n            attr(x, \"dimnames\") &lt;- NULL\n            x\n        })\n    }\n    if (!is.null(subgroup)) {\n        si_list &lt;- split(subgroup, cluster)\n        subgroup_list &lt;- lapply(si_list, function(x) sapply(x, \n            function(y) y == x))\n        vcov_list &lt;- Map(function(V, S) V * S, V = vcov_list, \n            S = subgroup_list)\n    }\n    if (check_PD) \n        check_PD(vcov_list)\n    if (return_list) {\n        return(vcov_list)\n    }\n    else {\n        vcov_mat &lt;- unblock(vcov_list)\n        cluster_index &lt;- order(order(cluster))\n        return(vcov_mat[cluster_index, cluster_index])\n    }\n}\n&lt;bytecode: 0x000001c046dcccd8&gt;\n&lt;environment: namespace:clubSandwich&gt;\n\n\nThe function takes three required arguments:\n\nvi is a vector of sampling variances.\ncluster is a vector identifying the study from which effect size estimates are drawn. Effects with the same value of cluster will be treated as correlated.\nr is the assumed value(s) of the correlation between effect size estimates from each study. Note that r can also be a vector with separate values for each study.\n\nHere is a simple example to demonstrate how the function works. Say that there are just three studies, contributing 2, 3, and 4 effects, respectively. I’ll just make up some values for the effect sizes and variances:\n\ndat &lt;- data.frame(study = rep(LETTERS[1:3], 2:4), \n                  yi = rnorm(9), \n                  vi = 4:12)\ndat\n\n  study         yi vi\n1     A -1.3029448  4\n2     A -0.3623436  5\n3     B -1.4290918  6\n4     B -0.7592721  7\n5     B  0.8992482  8\n6     C  0.8224456  9\n7     C  1.0812547 10\n8     C  0.5207703 11\n9     C  0.7078873 12\n\n\nI’ll assume that effect size estimates from a given study are correlated at 0.7:\n\nV_list &lt;- impute_covariance_matrix(vi = dat$vi, cluster = dat$study, r = 0.7)\nV_list\n\n$A\n         [,1]     [,2]\n[1,] 4.000000 3.130495\n[2,] 3.130495 5.000000\n\n$B\n         [,1]     [,2]     [,3]\n[1,] 6.000000 4.536518 4.849742\n[2,] 4.536518 7.000000 5.238320\n[3,] 4.849742 5.238320 8.000000\n\n$C\n         [,1]      [,2]      [,3]      [,4]\n[1,] 9.000000  6.640783  6.964912  7.274613\n[2,] 6.640783 10.000000  7.341662  7.668116\n[3,] 6.964912  7.341662 11.000000  8.042388\n[4,] 7.274613  7.668116  8.042388 12.000000\n\n\nThe result is a list of matrices, where each entry corresponds to the variance-covariance matrix of effects from a given study. To see that the results are correct, let’s examine the correlation matrix implied by these correlation matrices:\n\ncov2cor(V_list$A)\n\n     [,1] [,2]\n[1,]  1.0  0.7\n[2,]  0.7  1.0\n\ncov2cor(V_list$B)\n\n     [,1] [,2] [,3]\n[1,]  1.0  0.7  0.7\n[2,]  0.7  1.0  0.7\n[3,]  0.7  0.7  1.0\n\ncov2cor(V_list$C)\n\n     [,1] [,2] [,3] [,4]\n[1,]  1.0  0.7  0.7  0.7\n[2,]  0.7  1.0  0.7  0.7\n[3,]  0.7  0.7  1.0  0.7\n[4,]  0.7  0.7  0.7  1.0\n\n\nAs requested, effects are assumed to be equi-correlated with r = 0.7.\nIf the data are sorted in order of the cluster IDs, then the list of matrices returned by impute_covariance_matrix() can be fed directly into the rma.mv function in metafor (as I demonstrate below). However, if the data are not sorted by cluster, then feeding in the list of matrices will not work correctly. Instead, the full \\(N \\times N\\) variance-covariance matrix (where \\(N\\) is the total number of effect size estimates) will need to be calculated so that the rows and columns appear in the correct order. To address this possibility, the function includes an optional argument, return_list, which determines whether to output a list of matrices (one matrix per study/cluster) or a single matrix corresponding to the full variance-covariance matrix across all studies. By default, return_list tests for whether the cluster argument is sorted and returns the appropriate form. The argument can also be set directly by the user.\nHere’s what happens if we feed in the data in a different order:\n\ndat_scramble &lt;- dat[sample(nrow(dat)),]\ndat_scramble\n\n  study         yi vi\n8     C  0.5207703 11\n3     B -1.4290918  6\n4     B -0.7592721  7\n5     B  0.8992482  8\n1     A -1.3029448  4\n2     A -0.3623436  5\n6     C  0.8224456  9\n7     C  1.0812547 10\n9     C  0.7078873 12\n\nV_mat &lt;- round(impute_covariance_matrix(vi = dat_scramble$vi, cluster = dat_scramble$study, r = 0.7), 3)\nV_mat\n\n        [,1]  [,2]  [,3]  [,4] [,5] [,6]  [,7]   [,8]   [,9]\n [1,] 11.000 0.000 0.000 0.000 0.00 0.00 6.965  7.342  8.042\n [2,]  0.000 6.000 4.537 4.850 0.00 0.00 0.000  0.000  0.000\n [3,]  0.000 4.537 7.000 5.238 0.00 0.00 0.000  0.000  0.000\n [4,]  0.000 4.850 5.238 8.000 0.00 0.00 0.000  0.000  0.000\n [5,]  0.000 0.000 0.000 0.000 4.00 3.13 0.000  0.000  0.000\n [6,]  0.000 0.000 0.000 0.000 3.13 5.00 0.000  0.000  0.000\n [7,]  6.965 0.000 0.000 0.000 0.00 0.00 9.000  6.641  7.275\n [8,]  7.342 0.000 0.000 0.000 0.00 0.00 6.641 10.000  7.668\n [9,]  8.042 0.000 0.000 0.000 0.00 0.00 7.275  7.668 12.000\n\n\nTo see that this is correct, check that the diagonal entries of V_mat are the same as vi:\n\nall.equal(dat_scramble$vi, diag(V_mat))\n\n[1] TRUE"
  },
  {
    "objectID": "posts/imputing-covariance-matrices-for-multi-variate-meta-analysis/index.html#an-example-with-real-data",
    "href": "posts/imputing-covariance-matrices-for-multi-variate-meta-analysis/index.html#an-example-with-real-data",
    "title": "Imputing covariance matrices for meta-analysis of correlated effects",
    "section": "An example with real data",
    "text": "An example with real data\nKalaian and Raudenbush (1996) introduced a multi-variate random effects model, which can be used to perform a joint meta-analysis of studies that contribute effect sizes on distinct, related outcome constructs. They demonstrate the model using data from a synthesis on the effects of SAT coaching, where many studies reported effects on both the math and verbal portions of the SAT. The data are available in the clubSandwich package:\n\nlibrary(dplyr, warn.conflicts=FALSE)\ndata(SATcoaching)\n\n# calculate the mean of log of coaching hours\nmean_hrs_ln &lt;- \n  SATcoaching %&gt;% \n  group_by(study) %&gt;%\n  summarise(hrs_ln = mean(log(hrs))) %&gt;%\n  summarise(hrs_ln = mean(hrs_ln, na.rm = TRUE))\n\n# clean variables, sort by study ID\nSATcoaching &lt;- \n  SATcoaching %&gt;%\n  mutate(\n    study = as.factor(study),\n    hrs_ln = log(hrs) - mean_hrs_ln$hrs_ln\n  ) %&gt;%\n  arrange(study, test)\n\nSATcoaching %&gt;%\n  select(study, year, test, d, V, hrs_ln) %&gt;%\n  head(n = 20)\n\n                   study year   test     d      V      hrs_ln\n1  Alderman & Powers (A) 1980 Verbal  0.22 0.0817 -0.54918009\n2  Alderman & Powers (B) 1980 Verbal  0.09 0.0507 -0.19250515\n3  Alderman & Powers (C) 1980 Verbal  0.14 0.1045 -0.14371499\n4  Alderman & Powers (D) 1980 Verbal  0.14 0.0442 -0.19250515\n5  Alderman & Powers (E) 1980 Verbal -0.01 0.0535 -0.70333077\n6  Alderman & Powers (F) 1980 Verbal  0.14 0.0557 -0.88565233\n7  Alderman & Powers (G) 1980 Verbal  0.18 0.0561 -0.09719497\n8  Alderman & Powers (H) 1980 Verbal  0.01 0.1151  1.31157225\n9              Burke (A) 1986 Verbal  0.50 0.0825  1.41693276\n10             Burke (B) 1986 Verbal  0.74 0.0855  1.41693276\n11                Coffin 1987   Math  0.33 0.2534  0.39528152\n12                Coffin 1987 Verbal -0.23 0.2517  0.39528152\n13            Curran (A) 1988   Math -0.08 0.1065 -0.70333077\n14            Curran (A) 1988 Verbal -0.10 0.1066 -0.70333077\n15            Curran (B) 1988   Math -0.29 0.1015 -0.70333077\n16            Curran (B) 1988 Verbal -0.14 0.1007 -0.70333077\n17            Curran (C) 1988   Math -0.34 0.1104 -0.70333077\n18            Curran (C) 1988 Verbal -0.16 0.1092 -0.70333077\n19            Curran (D) 1988   Math -0.06 0.1089 -0.70333077\n20            Curran (D) 1988 Verbal -0.07 0.1089 -0.70333077\n\n\nThe correlation betwen math and verbal test scores are not available, but it seems reasonable to use a correlation of r = 0.66, as reported in the SAT technical information. To synthesize these effects, I’ll first compute the required variance-covariances:\n\nV_list &lt;- impute_covariance_matrix(vi = SATcoaching$V, \n                                   cluster = SATcoaching$study, \n                                   r = 0.66)\n\nThis can then be fed into metafor to estimate a fixed effect or random effects meta-analysis or meta-regression models:\n\nlibrary(metafor, quietly = TRUE)\n\n\nLoading the 'metafor' package (version 4.6-0). For an\nintroduction to the package please type: help(metafor)\n\n# bivariate fixed effect meta-analysis\nMVFE_null &lt;- rma.mv(d ~ 0 + test, V = V_list, data = SATcoaching)\nMVFE_null\n\n\nMultivariate Meta-Analysis Model (k = 67; method: REML)\n\nVariance Components: none\n\nTest for Residual Heterogeneity:\nQE(df = 65) = 72.1630, p-val = 0.2532\n\nTest of Moderators (coefficients 1:2):\nQM(df = 2) = 19.8687, p-val &lt; .0001\n\nModel Results:\n\n            estimate      se    zval    pval   ci.lb   ci.ub      \ntestMath      0.1316  0.0331  3.9783  &lt;.0001  0.0668  0.1965  *** \ntestVerbal    0.1215  0.0313  3.8783  0.0001  0.0601  0.1829  *** \n\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n# bivariate fixed effect meta-regression\nMVFE_hrs &lt;- rma.mv(d ~ 0 + test + test:hrs_ln, V = V_list, \n                   data = SATcoaching)\n\nWarning: 2 rows with NAs omitted from model fitting.\n\nMVFE_hrs\n\n\nMultivariate Meta-Analysis Model (k = 65; method: REML)\n\nVariance Components: none\n\nTest for Residual Heterogeneity:\nQE(df = 61) = 67.9575, p-val = 0.2523\n\nTest of Moderators (coefficients 1:4):\nQM(df = 4) = 23.7181, p-val &lt; .0001\n\nModel Results:\n\n                   estimate      se    zval    pval    ci.lb   ci.ub     \ntestMath             0.0946  0.0402  2.3547  0.0185   0.0159  0.1734   * \ntestVerbal           0.1119  0.0341  3.2762  0.0011   0.0449  0.1788  ** \ntestMath:hrs_ln      0.1034  0.0546  1.8946  0.0581  -0.0036  0.2103   . \ntestVerbal:hrs_ln    0.0601  0.0442  1.3592  0.1741  -0.0266  0.1467     \n\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n# bivariate random effects meta-analysis\nMVRE_null &lt;- rma.mv(d ~ 0 + test, V = V_list, data = SATcoaching, \n                 random = ~ test | study, struct = \"UN\")\nMVRE_null\n\n\nMultivariate Meta-Analysis Model (k = 67; method: REML)\n\nVariance Components:\n\nouter factor: study (nlvls = 47)\ninner factor: test  (nlvls = 2)\n\n            estim    sqrt  k.lvl  fixed   level \ntau^2.1    0.0122  0.1102     29     no    Math \ntau^2.2    0.0026  0.0507     38     no  Verbal \n\n        rho.Math  rho.Vrbl    Math  Vrbl \nMath           1                 -    20 \nVerbal   -1.0000         1      no     - \n\nTest for Residual Heterogeneity:\nQE(df = 65) = 72.1630, p-val = 0.2532\n\nTest of Moderators (coefficients 1:2):\nQM(df = 2) = 18.1285, p-val = 0.0001\n\nModel Results:\n\n            estimate      se    zval    pval   ci.lb   ci.ub      \ntestMath      0.1379  0.0434  3.1783  0.0015  0.0528  0.2229   ** \ntestVerbal    0.1168  0.0337  3.4603  0.0005  0.0506  0.1829  *** \n\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n# bivariate random effects meta-regression\nMVRE_hrs &lt;- rma.mv(d ~ 0 + test + test:hrs_ln, V = V_list, \n                   data = SATcoaching,\n                   random = ~ test | study, struct = \"UN\")\n\nWarning: 2 rows with NAs omitted from model fitting.\n\nMVRE_hrs\n\n\nMultivariate Meta-Analysis Model (k = 65; method: REML)\n\nVariance Components:\n\nouter factor: study (nlvls = 46)\ninner factor: test  (nlvls = 2)\n\n            estim    sqrt  k.lvl  fixed   level \ntau^2.1    0.0152  0.1234     28     no    Math \ntau^2.2    0.0014  0.0373     37     no  Verbal \n\n        rho.Math  rho.Vrbl    Math  Vrbl \nMath           1                 -    19 \nVerbal   -1.0000         1      no     - \n\nTest for Residual Heterogeneity:\nQE(df = 61) = 67.9575, p-val = 0.2523\n\nTest of Moderators (coefficients 1:4):\nQM(df = 4) = 23.6459, p-val &lt; .0001\n\nModel Results:\n\n                   estimate      se    zval    pval    ci.lb   ci.ub     \ntestMath             0.0893  0.0507  1.7631  0.0779  -0.0100  0.1887   . \ntestVerbal           0.1062  0.0357  2.9738  0.0029   0.0362  0.1762  ** \ntestMath:hrs_ln      0.1694  0.0725  2.3354  0.0195   0.0272  0.3116   * \ntestVerbal:hrs_ln    0.0490  0.0459  1.0681  0.2855  -0.0409  0.1389     \n\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nThe results of fitting this model using restricted maximum likelihood with metafor are actually a bit different from the estimates reported in the original paper, potentially because Kalaian and Raudenbush use a Cholesky decomposition of the sampling covariances, which alters the interpretation of the random effects variance components. The metafor fit is also a bit goofy because the correlation between the random effects for math and verbal scores is very close to -1, although evidently it is not uncommon to obtain such degenerate estimates of the random effects structure."
  },
  {
    "objectID": "posts/imputing-covariance-matrices-for-multi-variate-meta-analysis/index.html#robust-variance-estimation.",
    "href": "posts/imputing-covariance-matrices-for-multi-variate-meta-analysis/index.html#robust-variance-estimation.",
    "title": "Imputing covariance matrices for meta-analysis of correlated effects",
    "section": "Robust variance estimation.",
    "text": "Robust variance estimation.\nExperienced meta-analysts will no doubt point out that a further, alternative analytic strategy to the one described above would be to use robust variance estimation methods (RVE; Hedges, Tipton, & Johnson). However, RVE is not so much an alternative strategy as it is a complementary technique, which can be used in combination with any of the models estimated above. Robust standard errors and hypothesis tests can readily be obtained with the clubSandwich package. Here’s how to do it for the random effects meta-regression model:\n\nlibrary(clubSandwich)\ncoef_test(MVRE_hrs, vcov = \"CR2\")\n\n             Coef. Estimate     SE t-stat d.f. (Satt) p-val (Satt) Sig.\n          testMath   0.0893 0.0360   2.48       20.75       0.0218    *\n        testVerbal   0.1062 0.0215   4.94       16.45       &lt;0.001  ***\n   testMath:hrs_ln   0.1694 0.1010   1.68        7.90       0.1325     \n testVerbal:hrs_ln   0.0490 0.0414   1.18        7.57       0.2725     \n\n\nRVE is also available in the robumeta R package, but there are several differences between the implementation there and the method I’ve demonstrated here. From the user’s perspective, an advantage of robumeta is that it does all of the covariance imputation calculations “under the hood,” whereas with metafor the calculations need to be done prior to fitting the model. Beyond this, differences include:\n\nrobumeta uses a specific random effects structure that can’t be controlled by the user, whereas metafor can be used to estimate a variety of different random effects structures;\nrobumeta uses a moment estimator for the between-study variance, whereas metafor provides FML or REML estimation;\nrobumeta uses semi-efficient, diagonal weights when fitting the meta-regression, whereas metafor uses weights that are fully efficient (exactly inverse-variance) under the working model.\n\nThe advantages and disadvantages of these two approaches involve some subtleties that I’ll get into in a future post."
  },
  {
    "objectID": "posts/mean-variance-relationships-and-SMDs/index.html",
    "href": "posts/mean-variance-relationships-and-SMDs/index.html",
    "title": "Implications of mean-variance relationships for standardized mean differences",
    "section": "",
    "text": "I spend more time than I probably should discussing meta-analysis problems on the R-SIG-meta-analysis listserv. The questions that folks pose there are often quite interesting—especially when they’re motivated by issues that they’re wrestling with while trying to complete meta-analysis projects in their diverse fields. For those interested in meta-analytic methodology, I think perusing the mailing list is a good way to get a bit of ground sense about problems that come up in practice and places where there is a need for new methodological work, or at least further methodological guidance.\nRecently, a question came up on the listserv about whether it was reasonable to use the standardized mean difference metric for synthesizing studies where the outcomes are measured as proportions. Luke Martinez wrote:\nI think this is an interesting question because, while the SMD could work perfectly fine as an effect size metric for proportions, there are also other alternatives that could be considered, such as odds ratios or response ratios or raw differences in proportions. Further, there are some situations where the SMD has disadvantages for synthesizing contrasts between proportions. Thus, it’s a situation where one has to make a choice about the effect size metric, and where the most common metric (the SMD) might not be the right answer. As I wrote in reply:\nIn a follow-up, I elaborated on some potential problems with using the SMD:\nAnd I suggested a possible work-flow for examining the choice of effect size metric:\n(When I referred to “inveterate statistical curmudgeons”, I mostly had myself in mind.)\nIn this post, I want to provide a bit more detail regarding why I think mean-variance relationships in raw data can signal that the standardized mean differences might be less useful as an effect size metric compared to alternatives. The concern is actually broader than meta-analyses of outcomes measured as proportions, so I’ll start with a different case and then return to a situation similar to the one described in the original question."
  },
  {
    "objectID": "posts/mean-variance-relationships-and-SMDs/index.html#mean-variance-relationships-can-induce-heterogeneity",
    "href": "posts/mean-variance-relationships-and-SMDs/index.html#mean-variance-relationships-can-induce-heterogeneity",
    "title": "Implications of mean-variance relationships for standardized mean differences",
    "section": "Mean-variance relationships can induce heterogeneity",
    "text": "Mean-variance relationships can induce heterogeneity\nThe standardized mean difference parameter for a given study can be defined as: \\[\n\\delta_i = \\frac{\\mu_{Bi} - \\mu_{Ai}}{\\sigma_{Ai}},\n\\] where \\(\\mu_{Ai}\\) and \\(\\mu_{Bi}\\) are the (population) mean outcomes in group \\(A\\) and group \\(B\\) of study \\(i\\) and \\(\\sigma_{Ai}\\) is the (population) standard deviation in group \\(A\\) of study \\(i\\).1 The ideal case for using the SMD metric is when the outcomes in different studies are linearly equatable, so that the outcome scale in one study can be directly translated into the outcome scale of another study. However, if outcomes exhibit mean-variance relationships, linearly equatability seems rather implausible, and we might expect that SMDs will display heterogeneity across studies as a result.\nLet me lay out an example of a situation where the outcomes exhibit mean-variance relationships and where, as a consequence, the SMD metric becomes heterogeneous. Suppose that we have \\(k\\) studies, each involving a two-group comparison, with groups of equal size. In study \\(i\\), the outcomes in group \\(A\\) follow a poisson distribution with mean \\(\\mu_{Ai}\\), so that the variance of the outcomes in group \\(A\\) is also \\(\\mu_{Ai}\\), for \\(i = 1,...,k\\). The outcomes in group \\(B\\) follow a poisson distribution with mean \\(\\mu_{Bi}\\), so the variance is also \\(\\mu_{Bi}\\). Now, suppose that there is a fixed, proportional relationship between \\(\\mu_{Bi}\\) and \\(\\mu_{Ai}\\), so that \\(\\mu_{Bi} = \\lambda \\mu_{Ai}\\) for some \\(\\lambda &gt; 0\\). In other words, the treatment contrast is constant on the scale of the response ratio. However, the means in group \\(A\\) vary from study to study. To make things concrete, let’s assume that the means in group \\(A\\) follow a gamma distribution with shape parameter \\(\\alpha\\) and rate parameter \\(\\beta\\): \\[\n\\mu_{Ai} \\sim \\Gamma(\\alpha, \\beta).\n\\] What does this model imply about the distribution of standardized mean differences across this set of studies?\nUnder this model, the SMD parameter for study \\(i\\) is: \\[\n\\delta_i = \\frac{\\mu_{Bi} - \\mu_{Ai}}{\\sqrt{\\mu_{Ai}}} = (\\lambda - 1) \\times \\sqrt{\\mu_{Ai}}.\n\\] The first term in the above expression is a constant that only depend on the size of the response ratio, but the second term is random because we have assumed that the group \\(A\\) means vary from study to study. It will therefore create heterogeneity in the SMD parameters—the greater the variance of the \\(\\mu_{Ai}\\)’s, the greater the heterogeneity in \\(\\delta_i\\). Specifically, under the above assumptions, the effect size parameters follow a Nakagami distribution: \\[\n\\delta_i \\sim \\text{Nakagami}\\left(m = \\alpha, \\Omega = \\frac{(\\lambda - 1)^2 \\alpha}{\\beta}\\right)\n\\] Thus, even though we have a model where there is an underlying fixed relationship between \\(\\mu_{Ai}\\) and \\(\\mu_{Bi}\\), using the SMD metric for synthesis will lead to a situation with heterogeneous effects (even if all of the studies had large sample sizes and so effect sizes in individual studies are precisely estimated)."
  },
  {
    "objectID": "posts/mean-variance-relationships-and-SMDs/index.html#an-example-with-proportions",
    "href": "posts/mean-variance-relationships-and-SMDs/index.html#an-example-with-proportions",
    "title": "Implications of mean-variance relationships for standardized mean differences",
    "section": "An example with proportions",
    "text": "An example with proportions\nThis sort of behavior is not restricted to the poisson-gamma model I sketched above. The key features of that example are a) the assumption that the outcomes have a strong mean-variance relationship and b) the assumption that the \\(\\mu_{Ai}\\)’s are heterogeneous across studies. If both of these hold, then the resulting SMDs will also be heterogeneous. I’ll now describe a similar model, but where the outcomes within each study are proportions.\nAs before, suppose that we have \\(k\\) studies, each involving a two-group comparison, with groups of equal size. In study \\(i\\), the outcomes in group \\(A\\) follow a binomial distribution with mean proportion \\(\\pi_{Ai}\\) and \\(T_i\\) trials, so that the variance of the outcomes in group \\(A\\) is \\(\\pi_{Ai}\\left(1 - \\pi_{Ai}\\right) T_i\\), for \\(i = 1,...,k\\). The outcomes in group \\(B\\) also follow a binomial distribution, this one with mean proportion \\(\\pi_{Bi}\\) and \\(T_i\\) trials, so the variance is \\(\\pi_{Bi}\\left(1 - \\pi_{Bi}\\right) T_i\\). Next, to induce variation in the group-\\(A\\) means, let’s assume that the mean proportions follow a beta distribution: \\[\n\\pi_{Ai} \\sim \\text{Beta}(\\alpha, \\beta).\n\\]\nFinally, suppose that \\(\\pi_{Bi} = \\lambda_i \\pi_{Ai}\\) for some \\(\\lambda_i &gt; 0\\).\nUnder these assumptions, the SMD parameter for study \\(i\\) is: \\[\n\\delta_i = \\frac{\\pi_{Bi}T_i - \\pi_{Ai} T_i}{\\sqrt{\\pi_{Ai} (1 - \\pi_{Ai}) T_i}} = (\\lambda_i - 1) \\times \\sqrt{T_i} \\times \\sqrt{\\frac{\\pi_{Ai}}{1 - \\pi_{Ai}}}.\n\\] From the above expression, it can be seen that there are three potential sources of variation in \\(\\delta_i\\): variation in the study-specific response ratio \\(\\lambda_i\\), variation in the group-\\(A\\) proportions \\(\\pi_{Ai}\\), and variation in the number of trials \\(T_i\\). The total heterogeneity in \\(\\delta_i\\) will depend on all three, as well as on the co-variation between \\(\\lambda_i\\), \\(\\pi_{Ai}\\), and \\(T_i\\).\nTo make this concrete, let me simulate some meta-analytic data that follows the above model. To do so, I’ll need to make some additional distributional assumptions2:\n\nthat \\(\\lambda_i\\) is log-normally distributed such that \\(\\ln \\lambda_i \\sim N(\\ln \\Lambda, \\tau^2)\\);\nthat the number of trials is uniformly distributed on the integers between \\(t_{min}\\) and \\(t_{max}\\);\nthat \\(N_i\\), the number of observations per group in study \\(i\\), is uniformly distributed on the integers between \\(n_{min}\\) and \\(n_{max}\\); and\nthat \\(\\pi_{Ai}\\), \\(\\lambda_i\\), \\(T_i\\), and \\(N_i\\) are mutually independent.\n\nHere’s a function that generates study-specific parameter values and sample proportions:\n\nsim_binom_summary &lt;- function(pi_i, T_i, n_i) {\n  y &lt;- rbinom(n_i, size = T_i, prob = pi_i) / T_i\n  data.frame(M = mean(y), SD = sd(y))\n}\n\nsim_props &lt;- function(\n  k, # number of studies\n  alpha, beta, # parameters of pi_Ai distribution,\n  Lambda, tau, # parameters of lambda_i distribution\n  t_min, t_max, # parameters of T_i distribution\n  n_min, n_max # parameters of the sample size distribution\n) {\n  \n  # simulate parameters\n  pi_Ai &lt;- rbeta(k, shape1 = alpha, shape2 = beta)\n  lambda_i &lt;- exp(rnorm(k, mean = log(Lambda), sd = tau))\n  pi_Bi &lt;- lambda_i * pi_Ai\n  T_i &lt;- sample(t_min:t_max, size = k, replace = TRUE)\n  delta_i &lt;- (pi_Bi - pi_Ai) * T_i / sqrt(pi_Ai * (1 - pi_Ai) * T_i)\n  n_i &lt;- sample(n_min:n_max, size = k, replace = TRUE)\n  \n  # simulate data\n  stats_A &lt;- purrr::pmap_dfr(list(pi_i = pi_Ai, T_i = T_i, n_i = n_i),\n                             sim_binom_summary) \n                             \n  stats_B &lt;- purrr::pmap_dfr(list(pi_i = pi_Bi, T_i = T_i, n_i = n_i),\n                             sim_binom_summary)\n  \n  # compile\n  res &lt;- data.frame(\n    pi_Ai = pi_Ai, pi_Bi = pi_Bi, \n    lambda_i = lambda_i, T_i = T_i, \n    delta_i = delta_i, n_i = n_i,\n    mA = stats_A$M, sdA = stats_A$SD,\n    mB = stats_B$M, sdB = stats_B$SD\n  )\n\n  # effect size calculations\n  res &lt;- metafor::escalc(\n    data = res, measure = \"ROM\", var.names = c(\"lRR\", \"V_lRR\"),\n    m1i = mB, m2i = mA, \n    sd1i = sdB, sd2i = sdA,\n    n1i = n_i, n2i = n_i\n  )\n  res &lt;- metafor::escalc(\n    data = res, measure = \"SMD\", var.names = c(\"d\", \"V_d\"),\n    m1i = mB, m2i = mA, \n    sd1i = sdB, sd2i = sdA,\n    n1i = n_i, n2i = n_i\n  )\n  \n  res\n}\n\nset.seed(20211024)\ndat &lt;- sim_props(k = 60, alpha = 12, beta = 4, \n                 Lambda = 0.7, tau = .05,\n                 t_min = 5, t_max = 18,\n                 n_min = 10, n_max = 40)\n\nhead(dat)\n\n\n      pi_Ai     pi_Bi  lambda_i T_i    delta_i n_i        mA       sdA \n1 0.7584480 0.5836965 0.7695933  11 -1.3540950  24 0.7500000 0.1080650 \n2 0.7359047 0.4950740 0.6727420  16 -2.1851474  24 0.7786458 0.1222235 \n3 0.7132014 0.4773027 0.6692398  12 -1.8068471  10 0.7333333 0.1097134 \n4 0.6223653 0.4627406 0.7435193   9 -0.9877857  30 0.6666667 0.1399386 \n5 0.5916619 0.4205407 0.7107787   6 -0.8527716  28 0.5833333 0.2103299 \n6 0.7266748 0.5014601 0.6900751   9 -1.5160305  35 0.7619048 0.1209466 \n         mB       sdB     lRR  V_lRR       d    V_d \n1 0.6174242 0.1285066 -0.1945 0.0027 -1.0983 0.0959 \n2 0.5260417 0.1275776 -0.3922 0.0035 -1.9888 0.1245 \n3 0.3583333 0.1622089 -0.7161 0.0227 -2.5934 0.3681 \n4 0.4555556 0.1943213 -0.3808 0.0075 -1.2306 0.0793 \n5 0.4583333 0.2060055 -0.2412 0.0119 -0.5921 0.0746 \n6 0.4920635 0.1793349 -0.4372 0.0045 -1.7447 0.0789 \n\n\nFor the specified parameter values, there is only a small amount of true heterogeneity in the log of the response ratios (the blue density). Of course, there is further heterogeneity in the log response ratio estimates (the green density) due to sampling error:\n\nlibrary(ggplot2)\nggplot(dat) + \n  geom_density(aes(log(lambda_i), ..scaled..), fill = \"blue\", alpha = 0.5) + \n  geom_density(aes(lRR, ..scaled..), fill = \"green\", alpha = 0.2) + \n  theme_minimal()\n\n\n\n\n\n\n\n\nA random effects meta-analysis confirms that there is only a modest degree of true heterogeneity in the log response ratios:\n\nlibrary(metafor)\nrma(yi = lRR, vi = V_lRR, data = dat)\n\n\nRandom-Effects Model (k = 60; tau^2 estimator: REML)\n\ntau^2 (estimated amount of total heterogeneity): 0.0028 (SE = 0.0013)\ntau (square root of estimated tau^2 value):      0.0529\nI^2 (total heterogeneity / total variability):   42.01%\nH^2 (total variability / sampling variability):  1.72\n\nTest for Heterogeneity:\nQ(df = 59) = 100.6304, p-val = 0.0006\n\nModel Results:\n\nestimate      se      zval    pval    ci.lb    ci.ub      \n -0.3498  0.0111  -31.5751  &lt;.0001  -0.3715  -0.3281  *** \n\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nContrast this with what we get from using the standardized mean difference metric. The distributions of true effect sizes (blue) and of effect size estimates (light purple) have large spread as well as strong left skew:\n\nlibrary(ggplot2)\nggplot(dat) + \n  geom_density(aes(delta_i, ..scaled..), fill = \"blue\", alpha = 0.2) + \n  geom_density(aes(d, ..scaled..), fill = \"purple\", alpha = 0.5) + \n  theme_minimal()\n\n\n\n\n\n\n\n\nA random effects meta-analysis of the standardized mean differences shows a greater degree of true heterogeneity, both in terms of the estimated \\(\\tau\\) and in \\(I^2\\), or the proportion of total variance in the effect size estimates that is attributable to true heterogeneity:\n\nrma(yi = d, vi = V_d, data = dat)\n\n\nRandom-Effects Model (k = 60; tau^2 estimator: REML)\n\ntau^2 (estimated amount of total heterogeneity): 0.2838 (SE = 0.0743)\ntau (square root of estimated tau^2 value):      0.5327\nI^2 (total heterogeneity / total variability):   72.61%\nH^2 (total variability / sampling variability):  3.65\n\nTest for Heterogeneity:\nQ(df = 59) = 203.0513, p-val &lt; .0001\n\nModel Results:\n\nestimate      se      zval    pval    ci.lb    ci.ub      \n -1.5967  0.0824  -19.3771  &lt;.0001  -1.7582  -1.4352  *** \n\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"
  },
  {
    "objectID": "posts/mean-variance-relationships-and-SMDs/index.html#diagnostics",
    "href": "posts/mean-variance-relationships-and-SMDs/index.html#diagnostics",
    "title": "Implications of mean-variance relationships for standardized mean differences",
    "section": "Diagnostics",
    "text": "Diagnostics\nThe code above more-or-less implements the workflow I suggested for deciding between the standardized mean difference or response ratio metric (for proportions, we could also add further comparisons with log odds ratios and with raw differences in proportions). But is there further diagnostic information in the data that could provide a better sense of what is going on? I think there are a few things that might be helpful to consider.\nFirst, the issues I’m concerned with here will arise when there are mean-variance relationships in the outcomes. To get at that, we can simply plot the means and SDs of each group. In the code below, I re-structure the data so that there is one row per group per study. I then plot the SD versus the mean of each group:\n\nlibrary(dplyr)\nlibrary(tidyr)\n\nlong_summary_stats &lt;- \n  dat %&gt;%\n  select(n_i, T_i, mA, sdA, mB, sdB) %&gt;%\n  pivot_longer(cols = c(mA, sdA, mB, sdB), \n               names_to = c(\".value\",\"group\"),\n               names_pattern = \"(m|sd)(A|B)\")\n\nggplot(long_summary_stats,\n       aes(m, sd, color = group)) + \n  geom_point() + \n  geom_smooth(se = FALSE) + \n  scale_x_continuous(limits = c(0, 1), expand = c(0,0)) + \n  expand_limits(y = 0) + \n  theme_minimal() + \n  theme(legend.position = c(0.1, 0.9))\n\n\n\n\n\n\n\n\nThe plot above does suggest a mean-variance relationship, though it’s a bit messy. We can do better by using the scaled SD, after adjusting for the degree of spread that we would expect given \\(T_i\\):\n\nlong_summary_stats %&gt;%\n  mutate(\n    sd_scaled = sd * sqrt(T_i)\n  ) %&gt;%\n  ggplot(aes(m, sd_scaled, color = group)) + \n  geom_point() + \n  geom_smooth(se = FALSE) + \n  geom_function(fun = function(x) sqrt(x * (1 - x)),\n                color = \"black\") + \n  scale_x_continuous(limits = c(0, 1), expand = c(0,0)) + \n  expand_limits(y = 0) + \n  theme_minimal() + \n  theme(legend.position = c(0.1, 0.9))\n\n\n\n\n\n\n\n\nFrom the above, it does appear that there could be a relationship between the scaled SD and the mean. The black curve indicates the theoretical mean-variance relationship that would be expected under the binomial distribution, and indeed the empirical relationship appears to be quite similar. This suggests that mean-variance relationships might be at play (a correct supposition, since of course we know the true data-generating process here).\nSecond, since the outcomes in each group are all proportions, we can simply plot the mean in group \\(B\\) versus the mean in group \\(A\\):\n\nggplot(dat, aes(mA, mB)) + \n  geom_point() + \n  geom_smooth(se = FALSE, color = \"green\") + \n  geom_smooth(method = \"lm\", formula = y ~ x) + \n  coord_cartesian(xlim = c(0,1), ylim = c(0,1), expand = FALSE) + \n  theme_minimal()\n\n\n\n\n\n\n\n\nThis plot shows that there is a strong linear relationship between the two means, with a best-fit line that might go through the origin. This suggests that the response ratio might be an appropriate metric (although the difference in proportions might also be appropriate here, since a line with unit slope would probably fit quite well).\nThird (and most speculatively/hand-wavily), I think exploratory moderator analysis can be useful here, but interpreted in a non-typical way. Under the model I’ve sketched, we would expect that the standardized mean difference estimates should be systematically associated with the group-\\(A\\) means, as well as with the number of trials used to assess outcomes. The scatter-plots below show that this is indeed the case (the right-hand plot shows \\(d_i\\) versus \\(\\sqrt{T_i}\\)).\n\nlibrary(patchwork)\nmA_d_plot &lt;- \n  ggplot(dat, aes(mA, d)) + \n  geom_point() + \n  geom_smooth(se = FALSE, color = \"green\") + \n  geom_smooth(method = \"lm\") + \n  scale_x_continuous(limits = c(0, 1), expand = c(0,0)) + \n  theme_minimal()\n\nTi_d_plot &lt;- \n  ggplot(dat, aes(sqrt(T_i), d)) + \n  geom_point() + \n  geom_smooth(se = FALSE, color = \"green\") + \n  geom_smooth(method = \"lm\") + \n  theme_minimal()\n\nmA_d_plot + Ti_d_plot\n\n\n\n\n\n\n\n\nThis impression is also born out by a meta-regression that includes the group-\\(A\\) means and \\(\\sqrt{T_i}\\) as moderators:\n\nrma(d ~ mA + sqrt(T_i), vi = V_d, data = dat)\n\n\nMixed-Effects Model (k = 60; tau^2 estimator: REML)\n\ntau^2 (estimated amount of residual heterogeneity):     0.0238 (SE = 0.0238)\ntau (square root of estimated tau^2 value):             0.1544\nI^2 (residual heterogeneity / unaccounted variability): 18.12%\nH^2 (unaccounted variability / sampling variability):   1.22\nR^2 (amount of heterogeneity accounted for):            91.60%\n\nTest for Residual Heterogeneity:\nQE(df = 57) = 68.9706, p-val = 0.1330\n\nTest of Moderators (coefficients 2:3):\nQM(df = 2) = 110.9125, p-val &lt; .0001\n\nModel Results:\n\n           estimate      se     zval    pval    ci.lb    ci.ub      \nintrcpt      2.5225  0.3964   6.3632  &lt;.0001   1.7455   3.2995  *** \nmA          -2.8326  0.4336  -6.5321  &lt;.0001  -3.6825  -1.9827  *** \nsqrt(T_i)   -0.6109  0.0756  -8.0855  &lt;.0001  -0.7590  -0.4628  *** \n\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nHere are the same plots as above, but using the log of the response ratio as the effect size metric:\n\nmA_lRR_plot &lt;- \n  ggplot(dat, aes(mA, lRR)) + \n  geom_point() + \n  geom_smooth(se = FALSE, color = \"green\") + \n  geom_smooth(method = \"lm\") + \n  scale_x_continuous(limits = c(0, 1), expand = c(0,0)) + \n  theme_minimal()\n\nTi_lRR_plot &lt;- \n  ggplot(dat, aes(sqrt(T_i), lRR)) + \n  geom_point() + \n  geom_smooth(se = FALSE, color = \"green\") + \n  geom_smooth(method = \"lm\") + \n  theme_minimal()\n\nmA_lRR_plot + Ti_lRR_plot\n\n\n\n\n\n\n\n\nIn the left-hand plot, there does not appear to be any relationship between the effect size estimates and the group-\\(A\\) means. In the right-hand plot, there does seem to be a mild relationship between the effect size estimates and \\(\\sqrt{T_i}\\), which is a bit surprising, although the strength of the relationship is much weaker than what we saw with the standardized mean differences. Meta-regression analysis supports these interpretations:\n\nrma(lRR ~  mA + sqrt(T_i), vi = V_lRR, data = dat)\n\n\nMixed-Effects Model (k = 60; tau^2 estimator: REML)\n\ntau^2 (estimated amount of residual heterogeneity):     0.0019 (SE = 0.0011)\ntau (square root of estimated tau^2 value):             0.0439\nI^2 (residual heterogeneity / unaccounted variability): 32.87%\nH^2 (unaccounted variability / sampling variability):   1.49\nR^2 (amount of heterogeneity accounted for):            31.30%\n\nTest for Residual Heterogeneity:\nQE(df = 57) = 84.4977, p-val = 0.0105\n\nTest of Moderators (coefficients 2:3):\nQM(df = 2) = 10.6344, p-val = 0.0049\n\nModel Results:\n\n           estimate      se     zval    pval    ci.lb    ci.ub     \nintrcpt     -0.2362  0.0950  -2.4864  0.0129  -0.4224  -0.0500   * \nmA           0.1061  0.0948   1.1196  0.2629  -0.0796   0.2918     \nsqrt(T_i)   -0.0553  0.0179  -3.0852  0.0020  -0.0904  -0.0202  ** \n\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nNow, you might think that a meta-analyst should get excited about the standardized mean difference results, since they’ve uncovered two systematic predictors of effect size magnitude. However, both of these factors are purely operational, arbitrary features of the (simulated) study designs, rather than theoretically or substantively interesting features of the studies. Considered in this light, the finding that they each moderate the magnitude of the standardized mean differences is, more than anything else, annoying. If we wanted to examine other more theoretically interesting moderators, we’d have to do so in a way that accounts for these methodological predictors. At minimum, that would mean including them all in a meta-regression (leading to a model with 3+ predictors). Further, we would have to worry about whether the functional form of the regression is reasonable. Simply adding the theoretical moderator to the model amounts to assuming that it predicts effect size magnitude in a linear, additive fashion, but what if that’s not the right model? Since we know the true data-generating process here, we can see that the linear, additive model would not be correct. But in practice, when we don’t know the true process, this would be much murkier.\nThe general principle that I’m suggesting here is that effect sizes should ideally be on a metric that is independent of arbitrary methodological factors because this should reduce overall heterogeneity and simplify the model, making it easier to detect real relations of interest. If one has a choice between several different effect size metrics, then a metric that shows clear associations with methodological factors should be discounted in favor of metrics that do not show such associations or show them only weakly. How to fully operationalize this sort of decision (as one would need to when writing a protocol for a meta-analysis, for example), I’m not yet sure about. It seems like a useful avenue for further methodological work."
  },
  {
    "objectID": "posts/mean-variance-relationships-and-SMDs/index.html#footnotes",
    "href": "posts/mean-variance-relationships-and-SMDs/index.html#footnotes",
    "title": "Implications of mean-variance relationships for standardized mean differences",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nYes, there are other ways to define the SMD. Yes, usually we use the standard deviation pooled across both groups. I’m going to use the standard deviation in group \\(A\\) alone because it simplifies some of the mathy bits. Please feel free to work through the case with a pooled SD for yourself.↩︎\nOne of the vexing things about simulations is that you often end up needing to specify a bunch of assumptions about auxiliary quantities, beyond those of the model you’re actually interested in investigating.↩︎"
  },
  {
    "objectID": "posts/MI-with-clubSandwich/index.html",
    "href": "posts/MI-with-clubSandwich/index.html",
    "title": "Pooling clubSandwich results across multiple imputations",
    "section": "",
    "text": "A colleague recently asked me about how to apply cluster-robust hypothesis tests and confidence intervals, as calculated with the clubSandwich package, when dealing with multiply imputed datasets. Standard methods (i.e., Rubin’s rules) for pooling estimates from multiple imputed datasets are developed under the assumption that the full-data estimates are approximately normally distributed. However, this might not be reasonable when working with test statistics based on cluster-robust variance estimators, which can be imprecise when the number of clusters is small or the design matrix of predictors is unbalanced in certain ways. Barnard and Rubin (1999) proposed a small-sample correction for tests and confidence intervals based on multiple imputed datasets. In this post, I’ll show how to implement their technique using the output of clubSandwich, with multiple imputations generated using the mice package.\n\nSetup\nTo begin, let me create missingness in a dataset containing multiple clusters of observations:\n\nlibrary(mlmRev)\nlibrary(mice)\nlibrary(dplyr)\n\ndata(bdf)\n\nbdf &lt;- bdf %&gt;%\n  select(schoolNR, IQ.verb, IQ.perf, sex, ses, langPRET, aritPRET, aritPOST) %&gt;%\n  mutate(\n    schoolNR = factor(schoolNR),\n    sex = as.numeric(sex)\n    ) %&gt;%\n  filter(as.numeric(schoolNR) &lt;= 30) %&gt;%\n  droplevels()\n\nbdf_missing &lt;- \n  bdf %&gt;% \n  select(-schoolNR) %&gt;%\n  ampute(run = TRUE)\n\nbdf_missing &lt;- \n  bdf_missing$amp %&gt;%\n  mutate(schoolNR = bdf$schoolNR)\n\nsummary(bdf_missing)\n\n    IQ.verb         IQ.perf            sex             ses      \n Min.   : 4.00   Min.   : 5.333   Min.   :1.000   Min.   :10.0  \n 1st Qu.:10.50   1st Qu.: 9.333   1st Qu.:1.000   1st Qu.:20.0  \n Median :11.50   Median :10.667   Median :1.000   Median :28.0  \n Mean   :11.64   Mean   :10.758   Mean   :1.473   Mean   :28.8  \n 3rd Qu.:13.00   3rd Qu.:12.333   3rd Qu.:2.000   3rd Qu.:38.0  \n Max.   :18.00   Max.   :16.667   Max.   :2.000   Max.   :50.0  \n NA's   :46      NA's   :32       NA's   :39      NA's   :35    \n    langPRET        aritPRET        aritPOST        schoolNR  \n Min.   :15.00   Min.   : 1.00   Min.   : 2.00   40     : 35  \n 1st Qu.:30.00   1st Qu.: 9.00   1st Qu.:12.00   54     : 31  \n Median :34.00   Median :11.00   Median :18.00   55     : 30  \n Mean   :33.89   Mean   :11.63   Mean   :17.74   38     : 28  \n 3rd Qu.:39.00   3rd Qu.:14.00   3rd Qu.:23.00   1      : 25  \n Max.   :48.00   Max.   :20.00   Max.   :30.00   18     : 24  \n NA's   :40      NA's   :34      NA's   :39      (Other):354  \n\n\nNow I’ll use mice to create 10 multiply imputed datasets:\n\nImpute_bdf &lt;- mice(bdf_missing, m=10, meth=\"norm.nob\", seed=24)\n\nAm I imputing while ignoring the hierarchical structure of the data? Yes, yes I am. Is this is a good way to do imputation? Probably not. But this is a quick and dirty example, so we’re going to have to live with it.\n\n\nModel\nSuppose that the goal of our analysis is to estimate the coefficients of the following regression model:\n\\[\n\\text{aritPOST}_{ij} = \\beta_0 + \\beta_1 \\text{aritPRET}_{ij} + \\beta_2 \\text{langPRET}_{ij} + \\beta_3 \\text{sex}_{ij} + \\beta_4 \\text{SES}_{ij} + e_{ij},\n\\]\nwhere \\(i\\) indexes students and \\(j\\) indexes schools, and where we allow for the possibility that errors from the same cluster are correlated in an unspecified way. With complete data, we could estimate the model by ordinary least squares and then use clubSandwich to get standard errors that are robust to within-cluster dependence and heteroskedasticity. The code for this is as follows:\n\nlibrary(clubSandwich)\n\nRegistered S3 method overwritten by 'clubSandwich':\n  method    from    \n  bread.mlm sandwich\n\nlm_full &lt;- lm(aritPOST ~ aritPRET + langPRET + sex + ses, data = bdf)\ncoef_test(lm_full, cluster = bdf$schoolNR, vcov = \"CR2\")\n\n       Coef. Estimate     SE t-stat d.f. (Satt) p-val (Satt) Sig.\n (Intercept)  -2.1921 1.3484 -1.626        22.9       0.1177     \n    aritPRET   1.0053 0.0833 12.069        23.4       &lt;0.001  ***\n    langPRET   0.2758 0.0294  9.371        24.1       &lt;0.001  ***\n         sex  -1.2040 0.4706 -2.559        23.8       0.0173    *\n         ses   0.0233 0.0266  0.876        20.5       0.3909     \n\n\nIf cluster dependence were no concern, we could simply use the model-based standard errors and test statistics. The mice package provides functions that will fit the model to each imputed dataset and then combine them by Rubin’s rules. The code is simply:\n\nwith(data = Impute_bdf, \n     lm(aritPOST ~ aritPRET + langPRET + sex + ses)\n     ) %&gt;%\n  pool() %&gt;%\n  summary()\n\n         term    estimate  std.error statistic       df      p.value\n1 (Intercept) -2.72087690 1.18042535 -2.304997 185.0618 2.227631e-02\n2    aritPRET  0.96811774 0.06791871 14.254065 343.3291 1.548626e-36\n3    langPRET  0.30949123 0.03621462  8.546030 306.6763 6.093470e-16\n4         sex -1.43119901 0.41744953 -3.428436 194.0302 7.413229e-04\n5         ses  0.03042239 0.01969233  1.544885 206.0501 1.239085e-01\n\n\nHowever, this approach ignores the possibility of correlation in the errors of units in the same cluster, which is clearly a concern in this dataset:\n\n# ratio of CRVE to conventional variance estimates\ndiag(vcovCR(lm_full, cluster = bdf$schoolNR, type = \"CR2\")) / \n  diag(vcov(lm_full))\n\n(Intercept)    aritPRET    langPRET         sex         ses \n  1.5296837   1.5493134   0.6938735   1.4567650   2.0053186 \n\n\nSo we need a way to pool results based on the cluster-robust variance estimators, while also accounting for the relatively small number of clusters in this dataset.\n\n\nBarnard & Rubin (1999)\nBarnard and Rubin (1999) proposed a small-sample correction for tests and confidence intervals based on multiple imputed datasets that seems to work in this context. Rather than using large-sample normal approximations for inference, they derive an approximate degrees-of-freedom that combines uncertainty in the standard errors calculated from each imputed dataset with between-imputation uncertainty. The method is as follows.\nSuppose that we have \\(m\\) imputed datasets. Let \\(\\hat\\beta_{(j)}\\) be the estimated regression coefficient from imputed dataset \\(j\\), with (in this case cluster-robust) sampling variance estimate \\(V_{(j)}\\). Further, let \\(\\eta_{(j)}\\) be the degrees of freedom corresponding to \\(V_{(j)}\\). To combine these estimates, calculate the averages across multiply imputed datasets:\n\\[\n\\bar\\beta = \\frac{1}{m}\\sum_{j=1}^m \\hat\\beta_{(j)}, \\qquad \\bar{V} = \\frac{1}{m}\\sum_{j=1}^m V_{(j)}, \\qquad \\bar\\eta = \\frac{1}{m}\\sum_{j=1}^m \\eta_{(j)}.\n\\]\nAlso calculate the between-imputation variance\n\\[\nB = \\frac{1}{m - 1} \\sum_{j=1}^m \\left(\\hat\\beta_{(j)} - \\bar\\beta\\right)^2\n\\]\nAnd then combine the between- and within- variance estimates using Rubin’s rules:\n\\[\nV_{total} = \\bar{V} + \\frac{m + 1}{m} B.\n\\]\nThe degrees of freedom associated with \\(V_{total}\\) modify the estimated complete-data degrees of freedom \\(\\bar\\eta\\) using quantities that depend on the fraction of missing information in a coefficient. The fraction of missing information is given by\n\\[\n\\hat\\gamma_m = \\frac{(m+1)B}{m V_{total}}\n\\]\nThe degrees of freedom are then given by\n\\[\n\\nu_{total} = \\left(\\frac{1}{\\nu_m} + \\frac{1}{\\nu_{obs}}\\right)^{-1},\n\\]\nwhere\n\\[\n\\nu_m = \\frac{(m - 1)}{\\hat\\gamma_m^2}, \\quad \\text{and} \\quad \\nu_{obs} = \\frac{\\bar\\eta (\\bar\\eta + 1) (1 - \\hat\\gamma)}{\\bar\\eta + 3}.\n\\]\nHypothesis tests and confidence intervals are based on the approximation\n\\[\n\\frac{\\bar\\beta - \\beta_0}{\\sqrt{V_{total}}} \\ \\stackrel{\\cdot}{\\sim} \\ t(\\nu_{total})\n\\]\n\n\nImplementation\nHere is how to carry out these calculations using the results of clubSandwich::coef_test and a bit of dplyr:\n\n# fit results with clubSandwich standard errors\n\nmodels_robust &lt;- with(data = Impute_bdf, \n                      lm(aritPOST ~ aritPRET + langPRET + sex + ses) %&gt;% \n                         coef_test(cluster=bdf$schoolNR, vcov=\"CR2\")\n                      ) \n\n\n# pool results with clubSandwich standard errors\n\nrobust_pooled &lt;- \n  models_robust$analyses %&gt;%\n  \n  # add coefficient names as a column\n  lapply(function(x) {\n    x$coef &lt;- row.names(x)\n    x\n  }) %&gt;%\n  bind_rows() %&gt;%\n  as.data.frame() %&gt;%\n  \n  # summarize by coefficient\n  group_by(coef) %&gt;%\n  summarise(\n    m = n(),\n    B = var(beta),\n    beta_bar = mean(beta),\n    V_bar = mean(SE^2),\n    eta_bar = mean(df)\n  ) %&gt;%\n  \n  mutate(\n    \n    # calculate intermediate quantities to get df\n    V_total = V_bar + B * (m + 1) / m,\n    gamma = ((m + 1) / m) * B / V_total,\n    df_m = (m - 1) / gamma^2,\n    df_obs = eta_bar * (eta_bar + 1) * (1 - gamma) / (eta_bar + 3),\n    df = 1 / (1 / df_m + 1 / df_obs),\n    \n    # calculate summary quantities for output\n    se = sqrt(V_total),\n    t = beta_bar / se,\n    p_val = 2 * pt(abs(t), df = df, lower.tail = FALSE),\n    crit = qt(0.975, df = df),\n    lo95 = beta_bar - se * crit,\n    hi95 = beta_bar + se * crit\n  )\n\nWarning: There were 5 warnings in `summarise()`.\nThe first warning was:\nℹ In argument: `eta_bar = mean(df)`.\nℹ In group 1: `coef = \"(Intercept)\"`.\nCaused by warning in `mean.default()`:\n! argument is not numeric or logical: returning NA\nℹ Run `dplyr::last_dplyr_warnings()` to see the 4 remaining warnings.\n\nrobust_pooled %&gt;%\n  select(coef, est = beta_bar, se, t, df, p_val, lo95, hi95, gamma) %&gt;%\n  mutate_at(vars(est:gamma), round, 3)\n\n# A tibble: 5 × 9\n  coef           est    se     t    df p_val  lo95  hi95 gamma\n  &lt;chr&gt;        &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 (Intercept) -2.72  1.34  -2.03    NA    NA    NA    NA 0.129\n2 aritPRET     0.968 0.088 11.0     NA    NA    NA    NA 0.051\n3 langPRET     0.309 0.032  9.54    NA    NA    NA    NA 0.125\n4 ses          0.03  0.025  1.23    NA    NA    NA    NA 0.096\n5 sex         -1.43  0.518 -2.76    NA    NA    NA    NA 0.104\n\n\nIt is instructive to compare the calculated df to eta_bar and df_m:\n\nrobust_pooled %&gt;%\n  select(coef, df, df_m, eta_bar) %&gt;%\n  mutate_at(vars(df, df_m, eta_bar), round, 1)\n\n# A tibble: 5 × 4\n  coef           df  df_m eta_bar\n  &lt;chr&gt;       &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;\n1 (Intercept)    NA  539.      NA\n2 aritPRET       NA 3449.      NA\n3 langPRET       NA  574       NA\n4 ses            NA  971.      NA\n5 sex            NA  829.      NA\n\n\nHere, eta_bar is the average of the complete data degrees of freedom, and it can be seen that the total degrees of freedom are somewhat less than the average complete-data degrees of freedom. This is by construction. Further df_m is the conventional degrees of freedom used in multiple-imputation, which assume that the complete-data estimates are normally distributed, and in this example they are way far off.\n\n\nFurther thoughts\nHow well does this method perform in practice? I’m not entirely sure—I’m just trusting that Barnard and Rubin’s approximation is sound and would work in this setting (I mean, they’re smart people!). Are there other, better approaches? Totally possible. I have done zero literature review beyond the Barnard and Rubin paper. In any case, exploring the performance of this method (and any other alternatives) seems like it would make for a very nice student project.\nThere’s also the issue of how to do tests of multi-dimensional constraints (i.e., F-tests). The clubSandwich package implements Wald-type tests for multi-dimensional constraints, using a small-sample correction that we developed (Tipton & Pustejovsky, 2015; Pustejovsky & Tipton, 2016). But it would take some further thought to figure out how to handle multiply imputed data with this type of test…\n\n\n\n\n\n\nReuseCC BY 4.0CitationBibTeX citation:@online{pustejovsky2017,\n  author = {Pustejovsky, James E.},\n  title = {Pooling {clubSandwich} Results Across Multiple Imputations},\n  date = {2017-09-27},\n  url = {https://jepusto.com/posts/MI-with-clubSandwich},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nPustejovsky, James E. 2017. “Pooling clubSandwich Results Across\nMultiple Imputations.” September 27, 2017. https://jepusto.com/posts/MI-with-clubSandwich."
  },
  {
    "objectID": "posts/NAP-SEs-and-CIs/index.html",
    "href": "posts/NAP-SEs-and-CIs/index.html",
    "title": "Standard errors and confidence intervals for NAP",
    "section": "",
    "text": "Parker and Vannest (2009) proposed non-overlap of all pairs (NAP) as an effect size index for use in single-case research. NAP is defined in terms of all pair-wise comparisons between the data points in two different phases for a given case (i.e., a treatment phase versus a baseline phase). For an outcome that is desirable to increase, NAP is the proportion of all such pair-wise comparisons where the treatment phase observation exceeds the baseline phase observation, with pairs that are exactly tied getting a weight of 1/2. NAP belongs to the family of non-overlap measures, which also includes the percentage of non-overlapping data, the improvement rate difference, and several other indices. It is exactly equivalent to Vargha and Delaney’s (2000) modified Common Language Effect Size and has been proposed as an effect size index in other contexts too (e.g., Acion, Peterson, Temple, & Arndt, 2006).\nThe developers of NAP have created a web-based tool for calculating it (as well as several other non-overlap indices), and I have the impression that the tool is fairly widely used. For example, Roth, Gillis, and DiGennaro Reed (2014) and Whalon, Conroy, Martinez, and Welch (2015) both used NAP in their meta-analyses of single-case research, and both noted that they used singlecaseresearch.org for calculating the effect size measure. Given that the web tool is being used, it is worth scrutinizing the methods behind the calculations it reports. As of this writing, the standard error and confidence intervals reported along with the NAP statistic are incorrect, and should not be used. After introducing a bit of notation, I’ll explain why the existing methods are deficient. I’ll also suggest some methods for calculating standard errors and confidence intervals that are potentially more accurate."
  },
  {
    "objectID": "posts/NAP-SEs-and-CIs/index.html#preliminaries",
    "href": "posts/NAP-SEs-and-CIs/index.html#preliminaries",
    "title": "Standard errors and confidence intervals for NAP",
    "section": "Preliminaries",
    "text": "Preliminaries\nSuppose that we have data from the baseline phase and treatment phase for a single case. Let \\(m\\) denote the number of baseline observations and \\(n\\) denote the number of treatment phase observations. Let \\(y^A_1,...,y^A_m\\) denote the baseline phase data and \\(y^B_1,...,y^B_n\\) denote the treatment phase data. Then NAP is calculated as\n\\[\n\\text{NAP} = \\frac{1}{m n} \\sum_{i=1}^m \\sum_{j=1}^n \\left[I\\left(y^B_j &gt; y^A_i\\right) + 0.5 I\\left(y^B_j = y^A_i\\right)\\right]\n\\]\nWhat is NAP an estimate of? The parameter of interest is the probability that a randomly selected treatment phase observation will exceed a randomly selected baseline phase observation (again, with an adjustment for ties):\n\\[\n\\theta = \\text{Pr}(Y^B &gt; Y^A) + 0.5 \\text{Pr}(Y^B = Y^A).\n\\]\nVargha and Delaney call \\(\\theta\\) the measure of stochastic superiority.\nNAP is very closely related to another non-overlap index called Tau (Parker, Vannest, Davis, & Sauber, 2011). Tau is nothing more than a linear re-scaling of NAP to the range of [-1, 1]:\n\\[\n\\text{Tau} = \\frac{S}{m n} = 2 \\times \\text{NAP} - 1,\n\\]\nwhere\n\\[\nS = \\sum_{i=1}^m \\sum_{j=1}^n \\left[I\\left(y^B_j &gt; y^A_i\\right) - I\\left(y^B_j &lt; y^A_i\\right)\\right].\n\\]\nThe \\(S\\) is Kendall’s S statistic, which is closely related to the Mann-Whitney \\(U\\) test.\nHere is an R function for calculating NAP:\n\nNAP &lt;- function(yA, yB) {\n  m &lt;- length(yA)\n  n &lt;- length(yB)\n  U &lt;- sum(sapply(yA, function(i) sapply(yB, function(j) (j &gt; i) + 0.5 * (j == i))))\n  U / (m * n)\n}\n\nUsing the data from the worked example in Parker and Vannest (2009), the function result agrees with their reported NAP of 0.96:\n\nyA &lt;- c(4, 3, 4, 3, 4, 7, 5, 2, 3, 2)\nyB &lt;- c(5, 9, 7, 9, 7, 5, 9, 11, 11, 10, 9)\nNAP(yA, yB)\n\n[1] 0.9636364"
  },
  {
    "objectID": "posts/NAP-SEs-and-CIs/index.html#standard-errors",
    "href": "posts/NAP-SEs-and-CIs/index.html#standard-errors",
    "title": "Standard errors and confidence intervals for NAP",
    "section": "Standard errors",
    "text": "Standard errors\nThe webtool at singlecaseresearch.org reports a standard error for NAP (it is labelled as “SDnap”), which from what I can tell is based on the formula\n\\[\n\\text{SE}_{\\text{Tau}} = \\sqrt{\\frac{m + n + 1}{3 m n}}.\n\\]\nThis formula appears to actually be the standard error for Tau, rather than for NAP. Since \\(\\text{NAP} = \\left(\\text{Tau} + 1\\right) / 2\\), the standard error for NAP should be half as large:\n\\[\n\\text{SE}_{null} = \\sqrt{\\frac{m + n + 1}{12 m n}}\n\\]\n(cf. Grissom & Kim, 2001, p. 141). However, even the latter formula is not always correct. It is valid only when the observations are all mutually independent and when the treatment phase data are drawn from the same distribution as the baseline phase data—that is, when the treatment has no effect on the outcome. I’ve therefore denoted it as \\(\\text{SE}_{null}\\).\n\nOther standard error estimators\nBecause an equivalent effect size measure is used in other contexts like clinical medicine, there has actually been a fair bit of research into better approaches for assessing the uncertainty in NAP. Hanley and McNeil (1982) proposed an estimator for the sampling variance of NAP that is designed for continuous outcome measures, where exact ties are impossible. Modifying it slightly (and in entirely ad hoc fashion) to account for ties, let\n\\[\n\\begin{aligned}\nQ_1 &= \\frac{1}{m n^2}\\sum_{i=1}^m \\left[\\sum_{j=1}^n I\\left(y^B_j &gt; y^A_i\\right) + 0.5 I\\left(y^B_j = y^A_i\\right)\\right]^2 \\\\\nQ_2 &= \\frac{1}{m^2 n}\\sum_{j=1}^n \\left[\\sum_{i=1}^m I\\left(y^B_j &gt; y^A_i\\right) + 0.5 I\\left(y^B_j = y^A_i\\right)\\right]^2.\n\\end{aligned}\n\\]\nThen the Hanley-McNeil variance estimator is\n\\[\nV_{HM} = \\frac{1}{mn} \\left[\\text{NAP}\\left(1 - \\text{NAP}\\right) + (n - 1)\\left(Q_1 - \\text{NAP}^2\\right) + (m - 1)\\left(Q_2 - \\text{NAP}^2\\right)\\right],\n\\]\nwith \\(\\text{SE}_{HM} = \\sqrt{V_{HM}}\\).\nThe same authors also propose a different estimator, which is based on the assumption that the outcome data are exponentially distributed. Even though this is a strong and often inappropriate assumption, there is evidence that this estimator works even for other, non-exponential distributions. Newcombe (2006) suggested a further modification of their estimator, and I’ll describe his version. Let \\(h = (m + n) / 2 - 1\\). Then\n\\[\nV_{New} = \\frac{h}{mn} \\text{NAP}\\left(1 - \\text{NAP}\\right)\\left[\\frac{1}{h} + \\frac{1 - \\text{NAP}}{2 - \\text{NAP}} + \\frac{\\text{NAP}}{1 + \\text{NAP}}\\right],\n\\]\nwith \\(\\text{SE}_{New} = \\sqrt{V_{New}}\\).\nHere are R functions to calculate each of these variance estimators.\n\nV_HM &lt;- function(yA, yB) {\n  m &lt;- length(yA)\n  n &lt;- length(yB)\n  U &lt;- sapply(yB, function(j) (j &gt; yA) + 0.5 * (j == yA))\n  t &lt;- sum(U) / (m * n)\n  Q1 &lt;- sum(rowSums(U)^2) / (m * n^2)\n  Q2 &lt;- sum(colSums(U)^2) / (m^2 * n)\n  (t * (1 - t) + (n - 1) * (Q1 - t^2) + (m - 1) * (Q2 - t^2)) / (m * n)\n}\n\nV_New &lt;- function(yA, yB) {\n  m &lt;- length(yA)\n  n &lt;- length(yB)\n  t &lt;- NAP(yA, yB)\n  h &lt;- (m + n) / 2 - 1\n  t * (1 - t) * (1 + h * (1 - t) / (2 - t) + h * t / (1 + t)) / (m * n)\n}\n\nsqrt(V_HM(yA, yB))\n\n[1] 0.03483351\n\nsqrt(V_New(yA, yB))\n\n[1] 0.04370206\n\n\nFor the worked example dataset from Parker and Vannest, the Newcombe estimator yields a standard error that is about 25% larger than the Hanley-McNeil estimator. Both of these are substantially smaller than the null standard error, which in this example is \\(\\text{SE}_{null} = 0.129\\).\n\n\nA small simulation\nSimulation methods can be used to examine how well these various standard error formulas estimate the actual sampling variation of NAP. For simplicity, I’ll simulate normally distributed data where\n\\[\nY^A \\sim N(0, 1) \\qquad \\text{and} \\qquad Y^B \\sim N\\left(\\sqrt{2}\\Phi^{-1}(\\theta), 1\\right)\n\\]\nfor varying values of the effect size estimand (\\(\\theta\\)) and a couple of different sample sizes.\n\nsample_NAP &lt;- function(delta, m, n, iterations) {\n  NAPs &lt;- replicate(iterations, {\n    yA &lt;- rnorm(m)\n    yB &lt;- rnorm(n, mean = delta)\n    c(NAP = NAP(yA, yB), V_HM = V_HM(yA, yB), V_New = V_New(yA, yB))\n  })\n  data.frame(sd = sd(NAPs[\"NAP\",]), \n             SE_HM = sqrt(mean(NAPs[\"V_HM\",])), \n             SE_New = sqrt(mean(NAPs[\"V_New\",])))\n}\n\nlibrary(dplyr)\nlibrary(tidyr)\ntheta &lt;- seq(0.5, 0.95, 0.05)\nm &lt;- c(5, 10, 15, 20, 30)\nn &lt;- c(5, 10, 15, 20, 30)\n\nexpand.grid(theta = theta, m = m, n = n) %&gt;%\n  group_by(theta, m, n) %&gt;% \n  mutate(delta = sqrt(2) * qnorm(theta)) -&gt;\n  params \n\nparams %&gt;%\n  do(sample_NAP(.$delta, .$m, .$n, iterations = 2000)) %&gt;%\n  mutate(se_null = sqrt((m + n + 1) / (12 * m * n))) %&gt;%\n  gather(\"sd\",\"val\", sd, SE_HM, SE_New, se_null) -&gt;\n  NAP_sim\n\n\nlibrary(ggplot2)\nggplot(NAP_sim, aes(theta, val, color = sd)) + \n  facet_grid(n ~ m, labeller = \"label_both\") + \n  geom_line() + \n  theme_bw() + theme(legend.position = \"bottom\")\n\n\n\n\n\n\n\n\nIn the above figure, the actual sampling standard deviation of NAP (in red) and the value of \\(\\text{SE}_{null}\\) (in purple) are plotted against the true value of \\(\\theta\\), with separate plots for various combinations of \\(m\\) and \\(n\\). The expected value of the standard errors \\(\\text{SE}_{HM}\\) and \\(\\text{SE}_{New}\\) (actually the square root of the expectation of the variance estimators) are depicted in green and blue, respectively. The value of \\(\\text{SE}_{null}\\) agrees with the actual standard error when \\(\\delta = 0\\), but the two diverge when there is a positive treatment effect. It appears that \\(\\text{SE}_{HM}\\) and \\(\\text{SE}_{New}\\) both under-estimate the actual standard error when \\(m\\) or \\(n\\) is equal to 5, and over-estimate for the largest values of \\(\\theta\\). However, both of these estimators offer a marked improvement over \\(\\text{SE}_{null}\\)."
  },
  {
    "objectID": "posts/NAP-SEs-and-CIs/index.html#confidence-intervals",
    "href": "posts/NAP-SEs-and-CIs/index.html#confidence-intervals",
    "title": "Standard errors and confidence intervals for NAP",
    "section": "Confidence intervals",
    "text": "Confidence intervals\nThe webtool at singlecaseresearch.org also reports 85% and 90% confidence intervals for NAP. These confidence intervals appear to have the same two problems as the standard errors. First, they are constructed as CIs for Tau rather than for NAP. For the \\(100\\% \\times (1 - \\alpha)\\) CI, let \\(z_{\\alpha / 2}\\) be the appropriate critical value from a standard normal distribution. The CIs reported by the webtool are given by\n\\[\n\\text{Tau} \\pm \\text{SE}_{\\text{Tau}} \\times z_{\\alpha / 2}.\n\\]\nThis is probably just an oversight in the programming, which could be corrected by instead using\n\\[\n\\text{NAP} \\pm \\text{SE}_{null} \\times z_{\\alpha / 2}.\n\\]\nIn parallel with the standard error formulas, I’ll call this formula the null confidence interval. Funnily enough, the upper bound of the null CI is the same as the upper bound of the Tau CI. However, the lower bound is going to be quite a bit larger than the lower bound for the Tau CI, so that the null CI will be much narrower.\nThe second problem is that even the null CI has poor coverage properties because it is based on \\(\\text{SE}_{null}\\), which can drastically over-estimate the standard error of NAP for non-null values.\n\nOther confidence intervals\nAs I noted above, there has been a fair amount of previous research into how to construct CIs for \\(\\theta\\), the parameter estimated by NAP. As is often the case with these sorts of problems, there are many different methods available, scattered across the literature. Fortunately, there are two (at least) fairly comprehensive simulation studies that compare the performance of various methods under a wide range of conditions. Newcombe (2006) examined a range of methods based on inverting Wald-type test statistics (which give CIs of the form \\(\\text{estimate} \\pm \\text{SE} \\times z_{\\alpha / 2}\\), where \\(\\text{SE}\\) is some standard error estimate) and score-based methods (in which the standard error is estimated using the candidate parameter value). Based on an extensive simulation, he suggested a score-based method in which the end-points of the CI are defined the values of \\(\\theta\\) that satisfy:\n\\[\n(\\text{NAP} - \\theta)^2 = \\frac{z^2_{\\alpha / 2} h \\theta (1 - \\theta)}{mn}\\left[\\frac{1}{h} + \\frac{1 - \\theta}{2 - \\theta} + \\frac{\\theta}{1 + \\theta}\\right],\n\\]\nwhere \\(h = (m + n) / 2 - 1\\). This equation is a fourth-degree polynomial in \\(\\theta\\), easily solved using a numerical root-finding algorithm.\nIn a different simulation study, Ruscio and Mullen (2012) examined the performance of a selection of different confidence intervals for \\(\\theta\\), including several methods not considered by Newcombe. Among the methods that they examined, they find that the bias-corrected, accelerated (BCa) bootstrap CI performs particularly well (and seems to outperform the score-based CI recommended by Newcombe).\nNeither Newcombe (2006) nor Ruscio and Mullen (2012) considered constructing a confidence interval by directly pivoting the Mann-Whitney U test (the same technique used to construct confidence intervals for the Hodges-Lehmann estimator of location shift), although it seems to me that this would be possible and potentially an attractive approach in the context of SCDs. The main caveat is that such a CI would require stronger distributional assumptions than those studied in the simulations, such as that the distributions of \\(Y^A\\) and \\(Y^B\\) differ by an additive (or multiplicative) constant. In any case, it seems like it would be worth exploring this approach too.\n\n\nAnother small simulation\nHere is an R function for calculating several different CIs for \\(\\theta\\), including the null CI, Wald-type CIs based on \\(V_{HM}\\) and \\(V_{New}\\), and the score-type CI recommended by Newcombe (2006). I haven’t programmed the BCa bootstrap because it would take a bit more thought to figure out how to simulate it efficiently.\nThe following code simulates the coverage rates of nominal 90% CIs based on each of these methods, following the same simulation set-up as above.\n\nNAP_CIs &lt;- function(yA, yB, alpha = .05) {\n  m &lt;- length(yA)\n  n &lt;- length(yB)\n  U &lt;- sapply(yB, function(j) (j &gt; yA) + 0.5 * (j == yA))\n  t &lt;- sum(U) / (m * n)\n  \n  # variance estimators\n  V_null &lt;- (m + n + 1) / (12 * m * n)\n  \n  Q1 &lt;- sum(rowSums(U)^2) / (m * n^2)\n  Q2 &lt;- sum(colSums(U)^2) / (m^2 * n)\n  V_HM &lt;- (t * (1 - t) + (n - 1) * (Q1 - t^2) + (m - 1) * (Q2 - t^2)) / (m * n)\n  \n  h &lt;- (m + n) / 2 - 1\n  V_New &lt;- t * (1 - t) * (1 + h * (1 - t) / (2 - t) + h * t / (1 + t)) / (m * n)\n  \n  # Wald-type confidence intervals\n  z &lt;- qnorm(1 - alpha / 2)\n  SEs &lt;- sqrt(c(null = V_null, HM = V_HM, Newcombe = V_New))\n  Wald_lower &lt;- t - z * SEs\n  Wald_upper &lt;- t + z * SEs\n  \n  # score-type confidence interval\n  f &lt;- function(x) m * n * (t - x)^2 * (2 - x) * (1 + x) - \n    z^2 * x * (1 - x) * (2 + h + (1 + 2 * h) * x * (1 - x))\n  score_lower &lt;- if (t &gt; 0) uniroot(f, c(0, t))$root else 0\n  score_upper &lt;- if (t &lt; 1) uniroot(f, c(t, 1))$root else 1\n  list(NAP = t, \n       CI = data.frame(lower = c(Wald_lower, score = score_lower), \n                       upper = c(Wald_upper, score = score_upper)))\n}\n\nNAP_CIs(yA, yB)\n\n$NAP\n[1] 0.9636364\n\n$CI\n             lower     upper\nnull     0.7106061 1.2166666\nHM       0.8953639 1.0319088\nNewcombe 0.8779819 1.0492908\nscore    0.7499741 0.9950729\n\n\n\nsample_CIs &lt;- function(delta, m, n, alpha = .05, iterations) {\n  NAPs &lt;- replicate(iterations, {\n    yA &lt;- rnorm(m)\n    yB &lt;- rnorm(n, mean = delta)\n    NAP_CIs(yA, yB, alpha = alpha)\n  }, simplify = FALSE)\n  theta &lt;- mean(sapply(NAPs, function(x) x$NAP))\n  coverage &lt;- rowMeans(sapply(NAPs, function(x) (x$CI$lower &lt; theta) & (theta &lt; x$CI$upper)))\n  data.frame(CI = rownames(NAPs[[1]]$CI), coverage = coverage)\n}\n\nparams %&gt;% \n  do(sample_CIs(delta = .$delta, m = .$m, n = .$n, alpha = .10, iterations = 5000)) -&gt;\n  NAP_CI_sim\n\n\nggplot(NAP_CI_sim, aes(theta, coverage, color = CI)) + \n  facet_grid(n ~ m, labeller = \"label_both\", scales = \"free_y\") + \n  geom_line() + \n  labs(y = \"SE\") + \n  geom_hline(yintercept=.90, linetype=\"dashed\") +\n  theme_bw() + theme(legend.position = \"bottom\")\n\n\n\n\n\n\n\n\nThe figure above plots the coverage rates of several different confidence intervals for \\(\\theta\\): the naive CI (in blue), the HM Wald CI (red), the Newcombe Wald CI (green), and the Newcombe score CI (purple). The dashed horizontal line is the nominal coverage rate of 90%. It can be seen that the null CI has the correct coverage only when \\(\\theta \\leq .6\\); for larger values of \\(\\theta\\), its coverage becomes too conservative (tending towards 100%). The Wald-type CIs have below-nominal coverage rates, which improve as the sample size in each phase increases but remain too liberal even at the largest sample size considered. Finally, Newcombe’s score CI maintains close-to-nominal coverage over a wider range of \\(\\theta\\) values. Although these CIs have below-nominal coverage for the smallest sample sizes, they generally have good coverage for \\(\\theta &lt; .9\\) and when the sample size in each phase is 10 or more. It is also notable that their coverage rates appear to become more accurate as the sample size in a given group increases, even if the sample size in the other group is fairly small and remains constant."
  },
  {
    "objectID": "posts/NAP-SEs-and-CIs/index.html#caveats",
    "href": "posts/NAP-SEs-and-CIs/index.html#caveats",
    "title": "Standard errors and confidence intervals for NAP",
    "section": "Caveats",
    "text": "Caveats\nMy aim in this post was to highlight the problems with how singlecaseresearch.org calculates standard errors and CIs for the NAP statistic. Some of these issues could easily be resolved by correcting the relevant formulas so that they are appropriate for NAP rather than Tau. However, even with these corrections, better approaches exist for calculating standard errors and CIs. I’ve highlighted some promising ones above, which seem worthy of further investigation. But I should also emphasize that these methods do come with some important caveats too.\nFirst, all of the methods I’ve discussed are premised on having mutually independent observations. In the presence of serial correlation, I would anticipate that any of these standard errors will be too small and any of the confidence intervals will be too narrow. (This could readily be verified through simulation, although I have not done so here.)\nSecond, my small simulations are based on the assumption of normally distributed, homoskedastic observations in each phase, which is not a particularly good model for the types of outcome measures commonly used in single case research. In some of my other work, I’ve developed statistical models for data collected by systematic direct observation of behavior, which is the most prevalent type of outcome data in single-case research. Before recommending any particular method, the performance of the standard error formulas (e.g., the Hanley-McNeil and Newcombe estimators) and CI methods (such as Newcombe’s score CI) should be examined under more realistic models for behavioral observation data."
  },
  {
    "objectID": "posts/package-downloads/index.html",
    "href": "posts/package-downloads/index.html",
    "title": "CRAN downloads of my packages",
    "section": "",
    "text": "At AERA this past weekend, one of the recurring themes was how software availability (and its usability and default features) influences how people conduct meta-analyses. That got me thinking about the R packages that I’ve developed, how to understand the extent to which people are using them, how they’re being used, and so on. I’ve had badges on my github repos for a while now:\n\nclubSandwich: \nARPobservation: \nscdhlm: \nSingleCaseES: \n\nThese statistics come from the METACRAN site, which makes available data on daily downloads of all packages on CRAN (one of the main repositories for sharing R packages). The downloads are from the RStudio mirror of CRAN, which is only one of many mirrors around the world. Although the data do not represent complete tallies of all package downloads, they are nonetheless the best available source that I’m aware of.\nThe thing is, the download numbers are rather hard to interpret. Beyond knowing that somebody out there is at least trying to use the tools I’ve made, it’s pretty hard to gauge whether 300 or 3000 or 3 million downloads a month is a good usage level. In this post, I’ll attempt to put just a little bit of context around these numbers. Emphasis on little bit, as I’m not all that satisfied with what I’ll show below, but at least it’s something beyond four numbers floating in the air.\n\nGetting package download data\nI used the cranlogs package to get daily download counts of all currently available CRAN packages over the period 2018-04-05 18:00:00 through 2019-04-06. I then limited the sample to packages that had been downloaded at least once between 2018-04-05 18:00:00 and 2018-10-05. This had the effect of excluding about 1000 packages that were either only recently added to CRAN or that had been discontinued but were still sitting on CRAN.\n\n\nCode\nlibrary(tidyverse)\nlibrary(lubridate)\nlibrary(cranlogs)\n\nto_date &lt;- \"2019-04-06\"\nfrom_date &lt;- as.character(as_date(to_date) - duration(1, \"year\"))\nfile_name &lt;- paste0(\"CRAN package downloads \", to_date, \".rds\")\n\npkg_downloads &lt;-\n  available.packages() %&gt;%\n  as_tibble() %&gt;%\n  select(Package, Version) %&gt;%\n  mutate(grp = 1 + trunc((row_number() - 1) / 100)) %&gt;%\n  nest(Package, Version) %&gt;%\n  mutate(downloads = map(.$data, ~ cran_downloads(packages = .$Package, \n                                                  from = from_date, \n                                                  to = to_date))) %&gt;%\n  select(-data) %&gt;%\n  unnest()\n\n\n\n\nCode\ndownloaded_last_yr &lt;- \n  pkg_downloads %&gt;%\n  filter(date &lt;= as_date(as_date(to_date) - duration(6, \"months\"))) %&gt;%\n  group_by(package) %&gt;%\n  summarise(\n    count = sum(count),\n    .groups = \"drop\"\n  ) %&gt;%\n  filter(count &gt; 0) %&gt;%\n  select(package)\n\n\nThis yielded 12925 packages. For each of these packages, I then calculated the average monthly download rate over the most recent six months, along with where that rate falls as a percentile of all packages in the sample.\n\n\nCode\ndownloads_past_six &lt;-\n  pkg_downloads %&gt;%\n  filter(date &gt; as_date(as_date(to_date) - duration(6, \"months\"))) %&gt;%\n  semi_join(downloaded_last_yr, by = \"package\") %&gt;%\n  group_by(package) %&gt;%\n  summarise(\n    count = sum(count) / 6,\n    .groups = \"drop\"\n  ) %&gt;%\n  mutate(\n    package = fct_reorder(factor(package), count),\n    pct_less = cume_dist(count)\n  )\n\n\n\n\nPusto’s packages\nI have developed four packages that are currently available on CRAN:\n\nThe clubSandwich package provides cluster-robust variance estimators for a variety of different linear models (including meta-regression, hierarchical linear models, panel data models, etc.), as well as (more recently) some instrumental variables models. The package has received some attention in connection with estimating meta-analysis and meta-regression models, and it’s also relevant to applied micro-economics, field experiments, and other fields.\nThe scdhlm and SingleCaseES packages provide functions and interactive web apps for calculating various effect sizes for single-case experimental designs. The SingleCaseES package is fairly new and I haven’t yet written any articles that feature it. Both it and scdhlm are relevant in fairly specialized fields where single-case experimental designs are commonly used—and where there is a need to meta-analyze results from such designs—and so I would not expect them to be widely downloaded.\nThe ARPobservation package provides tools for simulating behavioral observation data based on an alternating renewal process model. I developed this package for my own dissertation work, and my students and I have used it in some subsequent work. I think of it mostly as a tool for my group’s work on statistical methods for single-case experimental designs, and so would not expect to be widely downloaded or used outside of this area.\n\nAs points of comparison to my contributions, it is perhaps useful to look at two popular packages for conducting meta-analysis, the metafor package and the robumeta package:\n\nThe metafor package, developed by Wolfgang Viechtbauer, has been around for 10 years and includes all sorts of incredible tools for calculating effect sizes, estimating meta-analysis and meta-regression models, investigating fitted models, and representing the results graphically. In contrast, the clubSandwich package is narrower in scope—it just calculates robust standard errors, confidence intervals, etc.—so metafor is not a perfect point of comparison.\nThe robumeta package, by Zachary Fisher and Elizabeth Tipton, is a closer match in terms of scope. It is used for estimating meta-regression models with robust variance estimation, using specific methods proposed by Hedges, Tipton, and Johnson (2010).\n\nI am having a harder time thinking of good comparables for the scdhlm, SingleCaseES, and ARPobservation packages due to their specialized focus. (Ideas? Suggestions? I’m all ears!)\nWith that background, here are the average monthly download rates (over the past six months) for each of my four packages, along with metafor and robumeta:\n\n\nCode\nlibrary(knitr)\nlibrary(kableExtra)\n\nPusto_pkgs &lt;- c(\"ARPobservation\",\"scdhlm\",\"SingleCaseES\",\"clubSandwich\")\nmeta_pkgs &lt;- c(\"metafor\",\"robumeta\")\n\nfocal_downloads &lt;- \n  downloads_past_six %&gt;%\n  filter(package %in% c(Pusto_pkgs, meta_pkgs)) %&gt;%\n  mutate(\n    count = round(count),\n    pct_less = round(100 * pct_less, 1)\n  ) %&gt;%\n  arrange(desc(count))\n\nfocal_downloads %&gt;%\n  rename(`Average monthly downloads` = count, \n         `Percentile of CRAN packages` = pct_less) %&gt;%\n  kable() %&gt;%\n  kable_styling(bootstrap_options = c(\"hover\", \"condensed\"), full_width = FALSE)\n\n\n\n\n\n\npackage\nAverage monthly downloads\nPercentile of CRAN packages\n\n\n\n\nmetafor\n7348\n94.0\n\n\nclubSandwich\n2992\n90.3\n\n\nrobumeta\n2025\n87.9\n\n\nARPobservation\n387\n55.5\n\n\nSingleCaseES\n306\n36.4\n\n\nscdhlm\n229\n7.5\n\n\n\n\n\n\n\n\nThus, clubSandwich sits in between metafor and robumeta, at the 90th percentile among all active packages on CRAN. The other packages are much less widely downloaded, averaging between 200 and 400 downloads per month. The distribution of monthly download rates is highly skewed, as can be seen in the figure below. About 68% of packages are downloaded 500 times or fewer per month, while only 7% of packages get more than 5000 downloads per month.\n\n\nCode\nlibrary(colorspace)\nlibrary(ggrepel)\n\ndownloads_sample &lt;- \n  downloads_past_six %&gt;%\n  arrange(count) %&gt;%\n  mutate(\n    focal = package %in% c(Pusto_pkgs,meta_pkgs),\n    tenth = (row_number(count) %% 10) == 1\n  ) %&gt;%\n  filter(focal | tenth)\n\nfocal_pkg_dat &lt;- \n  downloads_sample %&gt;%\n  filter(focal) %&gt;%\n  mutate(Pusto = if_else(package %in% Pusto_pkgs, \"Pusto\",\"comparison\"))\n\ntitle_str &lt;- paste(\"Average monthly downloads of R packages from\", as_date(as_date(to_date) - duration(6, \"months\")),\"through\",to_date)\n\nqualitative_hcl(n = 2, h = c(140, -30), c = 90, l = 40, register = \"custom-qual\")\n\nggplot(downloads_sample, aes(x = package, y = count)) +\n  geom_col() + \n  geom_col(data = focal_pkg_dat, aes(color = Pusto, fill = Pusto), size = 1.5) + \n  geom_label_repel(\n    data = focal_pkg_dat, aes(color = Pusto, label = package),\n    segment.size = 0.4,\n    segment.color = \"grey50\",\n    nudge_y = 0.5,\n    point.padding = 0.3\n  ) + \n  scale_y_log10(breaks = c(20, 50, 200, 500, 2000, 5000, 20000, 50000, 200000), labels = scales::comma) + \n  scale_fill_discrete_qualitative(palette = \"custom-qual\") + \n  scale_color_discrete_qualitative(palette = \"custom-qual\") + \n  labs(x = \"\", y = \"Downloads (per month)\", title = title_str) + \n  theme(legend.position = \"none\", axis.line.x = element_blank(), axis.ticks.x = element_blank(), axis.text.x = element_blank())\n\n\n\n\n\n\n\n\n\n\n\nDownloads over time\nHere are the weekly download rates for each of my packages over the past two years. (Note that the vertical scales of the graphs differ.)\n\n\nCode\nweekly_downloads &lt;- \n  pkg_downloads %&gt;%\n  mutate(\n    yr = year(date),\n    wk = week(date)\n  ) %&gt;%\n  group_by(package, yr, wk) %&gt;%\n  mutate(\n    date = max(date)\n  ) %&gt;%\n  group_by(package, date) %&gt;%\n  summarise(\n    count = sum(count),\n    days = n(),\n    .groups = \"drop\"\n  )\n\nweekly_downloads %&gt;%\n  filter(\n    days == 7,\n    package %in% Pusto_pkgs\n  ) %&gt;%\n  ggplot(aes(date, count, color = package)) + \n  geom_line() + \n  expand_limits(y = 0) + \n  facet_wrap(~ package, scales = \"free\", ncol = 2) + \n  theme_minimal() + \n  labs(x = \"\", y = \"Downloads (per month)\") + \n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\n\n\nThere are a couple of curious features in these plots. For one, there are big spikes in downloads of ARPobservation and SingleCaseES. The ARPobservation spike was in mid-June of 2018, when I was at the IES Single-Case Design training institute and demonstrated some of the package’s tools. The SingleCaseES spike was in early January, 2019. Perhaps someone was teaching a class in single-case research and demonstrated the package? Or something at the IES PI meeting (January 9-10, 2019)?\nAnother interesting pattern is in the download rate of scdhlm, which looks like it increased systematically starting in September, 2018. I wonder if this was the result of someone demonstrating or incorporating use of the package into a course. Lacking details about where the downloads are coming from, it’s hard to do anything but speculate.\n\n\nCaveats and musings\nClearly, download counts are only a very rough proxy for package usage. In marketing-speak, they might be more like leads than conversion, in that people might be downloading a package only to discover that it’s not good for anything and then never use it to accomplish anything. Downloads are also not one-time events. If they use it in their work, a single person will likely download a package many times, over a span of time as new versions are released, onto multiple machines that they might use, by accident in the process of trying to install some other package, and so on. Downloads of inter-related packages are likely to be highly correlated too, as they will be with release of new major versions of R, which probably makes it a bit tricky to do event studies.\nUltimately, I don’t know that knowing where my packages stand in terms of download rankings is all that useful. The packages that I’ve developed are all aimed at fairly academic audiences, which means that citations would probably be a better measure of contribution. The problem is, many people don’t know that they should be citing software, or how to do it. As usual, there’s an R function for that. Here’s how to get the citation for clubSandwich:\n\n\nCode\ncitation(package=\"clubSandwich\") %&gt;%\n  print(style = \"textVersion\")\n\n\nwhich returns the following:\n\n\n\n\n\n\n\n\n\nReuseCC BY 4.0CitationBibTeX citation:@online{pustejovsky2019,\n  author = {Pustejovsky, James E.},\n  title = {CRAN Downloads of My Packages},\n  date = {2019-04-09},\n  url = {https://jepusto.com/posts/package-downloads},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nPustejovsky, James E. 2019. “CRAN Downloads of My\nPackages.” April 9, 2019. https://jepusto.com/posts/package-downloads."
  },
  {
    "objectID": "posts/parallel-R-on-TACC-update/index.html",
    "href": "posts/parallel-R-on-TACC-update/index.html",
    "title": "Update: parallel R on the TACC",
    "section": "",
    "text": "I have learned from Mr. Yaakoub El Khamra that he and the good folks at TACC have made some modifications to TACC’s custom MPI implementation and R build in order to correct bugs in Rmpi and snow that were causing crashes. My earlier post has been updated to reflect the modifications. The main changes are:\n\nThe version of MVAPICH2 has changed to 2.0b\nChanges to the Rmpi and snow packages necessitate using the latest version of R (Warm Puppy, 3.0.3). This version is available in the Rstats module.\nFor improved reproducibility, I modified the R code so that the simulation driver function uses a seed value.\nI had to switch from maply to mdply as a result of (3).\n\n\n\n\n\n\nReuseCC BY 4.0CitationBibTeX citation:@online{pustejovsky2014,\n  author = {Pustejovsky, James E.},\n  title = {Update: Parallel {R} on the {TACC}},\n  date = {2014-04-08},\n  url = {https://jepusto.com/posts/parallel-R-on-TACC-update},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nPustejovsky, James E. 2014. “Update: Parallel R on the\nTACC.” April 8, 2014. https://jepusto.com/posts/parallel-R-on-TACC-update."
  },
  {
    "objectID": "posts/PIR-overestimates-prevalence/index.html",
    "href": "posts/PIR-overestimates-prevalence/index.html",
    "title": "To what extent does partial interval recording over-estimate prevalence?",
    "section": "",
    "text": "It is well known that the partial interval recording procedure produces an over-estimate of the prevalence of a behavior. Here I will demonstrate how to use the ARPobservation package to study the extent of this bias. First though, I’ll need to define the terms prevalence and incidence and also take a detour through continuous duration recording."
  },
  {
    "objectID": "posts/PIR-overestimates-prevalence/index.html#prevalence-and-incidence",
    "href": "posts/PIR-overestimates-prevalence/index.html#prevalence-and-incidence",
    "title": "To what extent does partial interval recording over-estimate prevalence?",
    "section": "Prevalence and incidence",
    "text": "Prevalence and incidence\nFirst off, what do I mean by prevalence? In an alternating renewal process, prevalence is the long-run proportion of time that the behavior occurs. I’ll call prevalence \\(\\phi\\) (“phi”). So far, I’ve described alternating renewal processes in terms of their average event duration (which I’ll call \\(\\mu\\) or “mu”) and the average interim time (which I’ll call \\(\\lambda\\) or “lambda”). Prevalence is related to these quantities mathematically as follows:\n\\[ \\phi = \\frac{\\mu}{\\mu + \\lambda}. \\]\nSo given \\(\\mu\\) and \\(\\lambda\\), we can figure out \\(\\phi\\).\nAnother characteristic of behavior that can be determined by the average event duration and average interim time is incidence, or the rate of event occurrence per unit of time. I’ll call incidence \\(\\zeta\\) (“zeta”). In an alternating renewal process,\n\\[ \\zeta = \\frac{1}{\\mu + \\lambda}. \\]\nThis makes intuitive sense, because \\(\\mu + \\lambda\\) is the average time in between the start of each event, so its inverse should be the average number of times that an event starts per unit of time. (Note that though this is quite intuitive, it’s also very difficult to prove mathematically.) Given \\(\\mu\\) and \\(\\lambda\\), we can figure out \\(\\zeta\\). Conversely, if we know \\(\\phi\\) and \\(\\zeta\\), we can solve for \\(\\mu = \\phi / \\zeta\\) and \\(\\lambda = (1 - \\phi) / \\zeta\\)."
  },
  {
    "objectID": "posts/PIR-overestimates-prevalence/index.html#continuous-duration-recording",
    "href": "posts/PIR-overestimates-prevalence/index.html#continuous-duration-recording",
    "title": "To what extent does partial interval recording over-estimate prevalence?",
    "section": "Continuous duration recording",
    "text": "Continuous duration recording\nIt can be shown mathematically that, on average, data produced by continuous duration recording (CDR) will be equal to the prevalence of the behavior. In statistical parlance, CDR data produces an unbiased estimate of prevalence. Since this is a mathematical fact, it’s a good idea to check that the software gives the same result (if it doesn’t, there must be something wrong with the code).\nIn order to simulate behavior streams, the software needs values for the average event duration and average interim time. But I want to think in terms of prevalence and incidence, so I’ll first pick a value for incidence. Say that a new behavioral event starts once per minute on average, so incidence (in events per second) would be \\(\\zeta = 1 / 60\\). I’ll then vary prevalence across the range from zero to one. For each value of prevalence, I’ll generate 10 behavior streams (if you’d like to do more, go ahead!).\n\nlibrary(ARPobservation)\nset.seed(8)\nzeta &lt;- 1 / 60\nphi &lt;- rep(seq(0.01, 0.99, 0.01), each = 10)\n\n# Now solve for mu and lambda\nmu &lt;- phi / zeta\nlambda &lt;- (1 - phi) / zeta\n\niterations &lt;- length(phi) # total number of behavior streams to generate\n\nTwo last elements are needed before I can get to the simulating: I need to decide what distributions to use for event durations and interim times, and I need to decide how long the observation session should last. To keep things simple, for the time being I’ll use exponential distributions. I’ll also suppose that we observe for 10 min = 600 s, so that on average we should observe 10 events per session. Now I can simulate a bunch of behavior streams and apply the CDR procedure to them.\n\nBS &lt;- r_behavior_stream(n = iterations, mu = mu, lambda = lambda, F_event = F_exp(), F_interim = F_exp(), stream_length = 600)\nCDR &lt;- continuous_duration_recording(BS)\n\nTo check that the CDR procedure is unbiased, I’ll plot the CDR data versus the true value of prevalence, and run a smoothing line through the cloud of data-points:\n\nlibrary(ggplot2)\nqplot(x = phi, y = CDR, geom = \"point\") + geom_smooth(method = \"loess\")\n\nWarning: `qplot()` was deprecated in ggplot2 3.4.0.\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nThe blue line is nearly identical to the line y = x, meaning that the average of CDR data is equal to prevalence. Good news–the software appears to be working correctly!"
  },
  {
    "objectID": "posts/PIR-overestimates-prevalence/index.html#partial-interval-recording",
    "href": "posts/PIR-overestimates-prevalence/index.html#partial-interval-recording",
    "title": "To what extent does partial interval recording over-estimate prevalence?",
    "section": "Partial interval recording",
    "text": "Partial interval recording\nNow to partial interval recording (PIR). There are two different ways to think about how PIR data over-estimates prevalence. The conventional statistical approach follows the same logic as above, comparing the average value of PIR data to the true value of prevalence, \\(\\phi\\). Using the same simulated data streams as above, with 15 s intervals and 5 s of rest time after each interval…\n\nPIR &lt;- interval_recording(BS, interval_length = 20, rest_length = 5)\n\nqplot(x = phi, y = PIR, geom = \"point\", ylim = c(-0.02,1.02)) + \n  geom_smooth(method = \"loess\", se = FALSE) + \n  geom_abline(intercept = 0, slope = 1, linetype = \"dashed\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nThe blue line indicates the average value of PIR data across the simulations for a given value of prevalence. The dashed line indicates y = x, so clearly PIR data over-estimates prevalence.\nPrevious studies in the Applied Behavior Analysis literature have taken a slightly different approach to thinking about over-estimation. Rather than comparing PIR data to the prevalence parameter \\(\\phi\\), PIR data is instead compared to the sample value of prevalence, which is equivalent to the CDR proportion. Following this logic, I apply the PIR and CDR procedures to the same simulated behavior streams, then plot PIR versus CDR.\n\nobs_data &lt;- reported_observations(BS, data_types = c(\"C\",\"P\"), interval_length = 20, rest_length = 5)\n\nqplot(x = CDR, y = PIR, data = obs_data, geom = \"point\", ylim = c(-0.02,1.02)) + \n  geom_smooth(method = \"loess\", se = FALSE) + \n  geom_abline(intercept = 0, slope = 1, linetype = \"dashed\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nThe blue fitted line is slightly different than with the other approach, but the general conclusion is the same: PIR data over-estimates prevalence.\nBut by how much? That’s actually a tricky question to answer, because the extent of the bias depends on a bunch of factors:\n\nthe true prevalence \\(\\phi\\),\nthe true incidence \\(\\zeta\\),\nthe length of the intervals, and\nthe distribution of interim times F_lambda.\n\n(Curiously enough, the bias doesn’t depend on the distribution of event durations F_mu.)\n\nInterval length\nTo see that the bias depends on the length of intervals used, I’ll compare 15 s intervals with 5 s rest times versus 25 s intervals with 5 s rest times. For a session of length 600 s, the latter procedure will yield 20 intervals.\n\nPIR_25 &lt;- interval_recording(BS, interval_length = 30, rest_length = 5)\nobs_data &lt;- cbind(obs_data, PIR_25)\nqplot(x = CDR, y = PIR, data = obs_data, geom = \"smooth\", method = \"loess\", ylim = c(-0.02,1.02)) + \n  geom_smooth(aes(y = PIR_25), method = \"loess\", se = FALSE, col = \"red\") + \n  geom_abline(intercept = 0, slope = 1, linetype = \"dashed\")\n\n`geom_smooth()` using formula = 'y ~ x'\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nThe red line indicates that the longer interval time leads to a larger degree of over-estimation. (For clarity, I’ve removed the points in the scatter-plot.)\n\n\nInterim time distribution\nIt isn’t terribly troubling that the bias of PIR data depends on the interval length, because the observer will generally know (and will hopefully report in any write-up of their experiment) the interval length that was used. Much more troubling is the fact that the bias depends on the distribution of interim times, because this is something that the observer or analyst won’t usually have much information about. To see how this bias works, I’ll compare behavior streams generated using an exponential distribution for the interim times with thos generated using a gamma distribution with shape parameter 3 (this distribution is much less dispersed than the exponential).\n\nBS_exp &lt;- r_behavior_stream(n = iterations, mu = mu, lambda = lambda, F_event = F_exp(), F_interim = F_exp(), stream_length = 600)\nobs_exp &lt;- reported_observations(BS_exp, data_types = c(\"C\",\"P\"), interval_length = 20, rest_length = 5)\nobs_exp$F_lambda &lt;- \"Exponential\"\n\nBS_gam &lt;- r_behavior_stream(n = iterations, mu = mu, lambda = lambda, F_event = F_exp(), F_interim = F_gam(shape = 3), stream_length = 600)\nobs_gam &lt;- reported_observations(BS_gam, data_types = c(\"C\",\"P\"), interval_length = 20, rest_length = 5)\nobs_gam$F_lambda &lt;- \"Gamma(3)\"\n\nobs_data &lt;- rbind(obs_exp, obs_gam)\nqplot(x = C, y = P, color = F_lambda, \n      data = obs_data, geom = \"smooth\", method = \"loess\", se = FALSE, ylim = c(-0.02, 1.02))\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nThe gamma(3) interim time distribution leads to a slightly larger positive bias."
  },
  {
    "objectID": "posts/Pusto-Tipton-2018-Theorem-2/index.html",
    "href": "posts/Pusto-Tipton-2018-Theorem-2/index.html",
    "title": "Corrigendum to Pustejovsky and Tipton (2018)",
    "section": "",
    "text": "\\[\n\\def\\Pr{{\\text{Pr}}}\n\\def\\E{{\\text{E}}}\n\\def\\Var{{\\text{Var}}}\n\\def\\Cov{{\\text{Cov}}}\n\\def\\bm{\\mathbf}\n\\def\\bs{\\boldsymbol}\n\\] In my 2018 paper with Beth Tipton, published in the Journal of Business and Economic Statistics, we considered how to do cluster-robust variance estimation in fixed effects models estimated by weighted (or unweighted) least squares. A careful reader, Dr. Michael Pfaffermayr, recently alerted us to a problem with Theorem 2 in the paper, which concerns a computational short cut for a certain cluster-robust variance estimator in models with cluster-specific fixed effects. The theorem is incorrect as stated, and we are currently working on issuing a correction for the published version of the paper. In the interim, this post details the problem with Theorem 2. I’ll first review the CR2 variance estimator, then describe the assertion of the theorem, and then provide a numerical counter-example demonstrating that the assertion is not correct as stated.\n\nA fixed effects model\nFor data that can be grouped into \\(m\\) clusters of observations, we considered the model \\[\n\\bm{y}_i = \\bm{R}_i \\bs\\beta + \\bm{S}_i \\bs\\gamma + \\bm{T}_i \\bs\\mu + \\bs\\epsilon_i, (\\#eq:regression)\n\\] where \\(\\bm{y}_i\\) is an \\(n_i \\times 1\\) vector of responses for cluster \\(i\\), \\(\\bm{R}_i\\) is an \\(n_i \\times r\\) matrix of focal predictors, \\(\\bm{S}_i\\) is an \\(n_i \\times s\\) matrix of additional covariates that vary across multiple clusters, and \\(\\bm{T}_i\\) is an \\(n_i \\times t\\) matrix encoding cluster-specific fixed effects, all for \\(i = 1,...,m\\). The cluster-specific fixed effects satisfy \\(\\bm{T}_h \\bm{T}_i' = \\bm{0}\\) for \\(h \\neq i\\). Interest centers on inference for the coefficients on the focal predictors \\(\\bs\\beta\\).\nWe considered estimation of Model @ref(eq:regression) by weighted least squares (WLS), possibly under a working model for the distribution of \\(\\bs\\epsilon_i\\). Let \\(\\bm{W}_1,...,\\bm{W}_m\\) be a set of symmetric weight matrices used for WLS estimation. Sometimes, these weight matrices may be diagonal, consisting of sampling weights for each observation. Other times, the weight matrices may involve off-diagonal terms as well. Consider a working model \\(\\Var\\left(\\bs\\epsilon_i | \\bm{R}_i, \\bm{S}_i, \\bm{T}_i\\right) = \\sigma^2 \\bs\\Phi_i\\) where \\(\\bs\\Phi_i\\) is a symmetric \\(n_i \\times n_i\\) matrix that may be a function of a low-dimensional, estimable parameter. Based on this working model, the weight matrices might be taken as \\(\\bm{W}_i = \\bs{\\hat\\Phi}_i^{-1}\\), where \\(\\bs{\\hat\\Phi}_i\\) is an estimate of \\(\\bs\\Phi_i\\).\n\n\nThe CR2 variance estimator\nIn the paper, we provide a generalization of the bias-reduced linearization estimator introduced by McCaffrey et al. (2001) and Bell & McCaffrey (2002) that can be applied to Model @ref(eq:regression). The variance estimator is effectively a generalization of the HC2 correction for heteroskedasticity-robust standard errors, but that works for models with within-cluster dependence and cluster-specific fixed effects, and so we refer to it the “CR2” estimator.\nIn order to define the CR2 variance estimator and explain the issue with Theorem 2, I’ll need to lay down a bit more notation. Let \\(N = \\sum_{i=1}^m n_i\\) be the total sample size. Let \\(\\bm{U}_i = \\left[ \\bm{R}_i \\ \\bm{S}_i \\right]\\) be the set of predictors that vary across clusters and \\(\\bm{X}_i = \\left[ \\bm{R}_i \\ \\bm{S}_i \\ \\bm{T}_i \\right]\\) be the full set of predictors. Let \\(\\bm{R}\\), \\(\\bm{S}\\), \\(\\bm{T}\\), \\(\\bm{U}\\), and \\(\\bm{X}\\) denote the stacked versions of the cluster-specific matrices (i.e., \\(\\bm{R} = \\left[\\bm{R}_1' \\ \\bm{R}_2' \\ \\cdots \\ \\bm{R}_m'\\right]'\\), etc.). Let \\(\\bm{W} = \\bigoplus_{i=1}^m \\bm{W}_i\\) and \\(\\bs\\Phi = \\bigoplus_{i=1}^m \\bs\\Phi_i\\). For a generic matrix \\(\\bm{Z}\\), let \\(\\bm{M}_{Z} = \\left(\\bm{Z}'\\bm{W}\\bm{Z}\\right)^{-1}\\) and \\(\\bm{H}_{\\bm{Z}} = \\bm{Z} \\bm{M}_{\\bm{Z}}\\bm{Z}'\\bm{W}\\). Let \\(\\bm{C}_i\\) be the \\(n_i \\times N\\) matrix that selects the rows of cluster \\(i\\) from the full set of observations, such that \\(\\bm{X}_i = \\bm{C}_i \\bm{X}\\). These operators provide an easy way to define absorbed versions of the predictors. Specifically, let \\(\\bm{\\ddot{S}} = \\left(\\bm{I} - \\bm{H}_{\\bm{T}}\\right) \\bm{S}\\) be the covariates after absorbing (i.e., partialling out) the cluster-specific effects, let \\(\\bm{\\ddot{U}} = \\left(\\bm{I} - \\bm{H}_{\\bm{T}}\\right) \\bm{U}\\) be an absorbed version of the focal predictors and the covariates, and let \\(\\bm{\\ddot{R}} = \\left(\\bm{I} - \\bm{H}_{\\bm{\\ddot{S}}}\\right)\\left(\\bm{I} - \\bm{H}_{\\bm{T}}\\right) \\bm{R}\\) be the focal predictors after absorbing the covariates and the cluster-specific fixed effects.\nWith this notation established, the CR2 variance estimator has the form \\[\n\\bm{V}^{CR2} = \\bm{M}_{\\bm{\\ddot{R}}} \\left(\\sum_{i=1}^m \\bm{\\ddot{R}}_i' \\bm{W}_i \\bm{A}_i \\bm{e}_i \\bm{e}_i' \\bm{A}_i \\bm{W}_i \\bm{\\ddot{R}}_i \\right) \\bm{M}_{\\bm{\\ddot{R}}},\n\\] where \\(\\bm{\\ddot{R}}_i = \\bm{C}_i \\bm{\\ddot{R}}\\) is the cluster-specific matrix of absorbed focal predictors, \\(\\bm{e}_i\\) is the vector of weighted least squares residuals from cluster \\(i\\), and \\(\\bm{A}_1,...,\\bm{A}_m\\) are a set of adjustment matrices that correct the bias of the residual cross-products. The adjustment matrices are calculated as follows. Let \\(\\bm{D}_i\\) be the upper-right Cholesky factorization of \\(\\bm{\\Phi}_i\\) and define the matrices \\[\n\\bm{B}_i = \\bm{D}_i \\bm{C}_i \\left(\\bm{I} - \\bm{H}_{\\bm{X}}\\right) \\bs\\Phi \\left(\\bm{I} - \\bm{H}_{\\bm{X}}\\right)'\\bm{C}_i' \\bm{D}_i'\n(#eq:B-matrix)\n\\] for \\(i = 1,...,m\\). The adjustment matrices are then calculated as \\[\n\\bm{A}_i = \\bm{D}_i' \\bm{B}_i^{+1/2} \\bm{D}_i,\n(#eq:A-matrix)\n\\] where \\(\\bm{B}_i^{+1/2}\\) is the symmetric square root of the Moore-Penrose inverse of \\(\\bm{B}_i\\). Theorem 1 in the paper shows that, if the working model \\(\\bs\\Phi\\) is correctly specified and some conditions on the rank of \\(\\bm{U}\\) are satisfied, then the CR2 estimator is exactly unbiased for the sampling variance of the weighted least squares estimator of \\(\\bs\\beta\\). Across multiple simulation studies, it’s been observed that the CR2 estimator also works well and outperforms alternative sandwich estimators even when the working model is not correctly specified.\n\n\nTheorem 2\nThe adjustment matrices given in @ref(eq:A-matrix) can be expensive to compute directly because the \\(\\bm{B}_i\\) matrices involve computing a “residualized” version of the \\(N \\times N\\) matrix \\(\\bs\\Phi\\) involving the full set of predictors \\(\\bm{X}\\)—including the cluster-specific fixed effects \\(\\bm{T}_1,...,\\bm{T}_m\\). Theorem 2 considered whether one can take a computational short cut by omitting the cluster-specific fixed effects from the calculation of the \\(\\bm{B}_i\\) matrices. Specifically, define the modified matrices \\[\n\\bm{\\tilde{B}}_i = \\bm{D}_i \\bm{C}_i \\left(\\bm{I} - \\bm{H}_{\\bm{\\ddot{U}}}\\right) \\bs\\Phi \\left(\\bm{I} - \\bm{H}_{\\bm{\\ddot{U}}}\\right)'\\bm{C}_i' \\bm{D}_i'\n(#eq:B-modified)\n\\] and \\[\n\\bm{\\tilde{A}}_i = \\bm{D}_i' \\bm{\\tilde{B}}_i^{+1/2} \\bm{D}_i,\n(#eq:A-modified).\n\\] Theorem 2 claims that if the weight matrices are inverse of the working model, such that \\(\\bm{W}_i = \\bs\\Phi_i^{-1}\\) for \\(i = 1,...,m\\), then \\(\\bm{\\tilde{B}}_i^{+1/2} = \\bm{B}_i^{+1/2}\\) and hence \\(\\bm{\\tilde{A}}_i = \\bm{A}_i\\). The implication is that the cluster-specific fixed effects can be ignored when calculating the adjustment matrices. However, the claimed equivalence does not actually hold.\nHere is a simple numerical example that contradicts the assertion of Theorem 2. I first create a predictor matrix consisting of 4 clusters, a single focal predictor, and cluster-specific fixed effects.\n\n\nCode\nset.seed(20220926)\nm &lt;- 4                                             # number of clusters\nni &lt;- 2 + rpois(m, 3.5)                            # cluster sizes\nN &lt;- sum(ni)                                       # total sample size\nid &lt;- factor(rep(LETTERS[1:m], ni))                # cluster ID\nR &lt;- rnorm(N)                                      # focal predictor\ndat &lt;- data.frame(R, id)                           # create raw data frame\nX &lt;- model.matrix(~ R + id + 0, data = dat)        # full predictor matrix\nUi &lt;- tapply(R, id, \\(x) x - mean(x))              # absorbed version of R\nU &lt;- unsplit(Ui, id)\n\n\nConsider a model estimated by ordinary least squares, where the assumed working model is homoskedastic and independent errors, so \\(\\bs\\Phi_i = \\bm{I}_i\\), an \\(n_i \\times n_i\\) identity matrix (with no parameters to estimate). In this case, the adjustment matrices simplify considerably, to \\[\n\\bm{A}_i = \\left(\\bm{I}_i - \\bm{X}_i \\bm{M}_{X} \\bm{X}_i' \\right)^{+1/2} \\qquad \\text{and} \\qquad \\bm{\\tilde{A}}_i = \\left(\\bm{I}_i - \\bm{\\ddot{U}}_i \\bm{M}_{\\ddot{U}} \\bm{\\ddot{U}}_i' \\right)^{+1/2}.\n\\] I calculate these directly as follows:\n\n\nCode\nmatrix_power &lt;- function(x, p) {\n  eig &lt;- eigen(x, symmetric = TRUE)\n  val_p &lt;- with(eig, ifelse(values &gt; 10^-12, values^p, 0))\n  with(eig, vectors %*% (val_p * t(vectors)))\n}\n\nMX &lt;- solve(crossprod(X))\nB &lt;- \n  by(X, id, as.matrix) |&gt;\n  lapply(\\(x) diag(nrow(x)) - x %*% MX %*% t(x))\nA &lt;- lapply(B, matrix_power, p = -1/2)\n  \nMU &lt;- 1 / crossprod(U)\nBtilde &lt;- lapply(Ui, \\(x) diag(length(x)) - x %*% MU %*% t(x))\nAtilde &lt;- lapply(Btilde, matrix_power, p = -1/2)\n\n\nHere are the adjustment matrices based on the full predictor matrix \\(\\bm{X}\\):\n\n\nCode\nprint(A, digits = 3)\n\n\n$A\n       [,1]   [,2]   [,3]   [,4]   [,5]\n[1,]  0.853 -0.198 -0.207 -0.191 -0.257\n[2,] -0.198  0.800 -0.200 -0.200 -0.202\n[3,] -0.207 -0.200  0.801 -0.201 -0.192\n[4,] -0.191 -0.200 -0.201  0.802 -0.210\n[5,] -0.257 -0.202 -0.192 -0.210  0.860\n\n$B\n       [,1]   [,2]   [,3]\n[1,]  0.668 -0.338 -0.330\n[2,] -0.338  0.683 -0.345\n[3,] -0.330 -0.345  0.675\n\n$C\n       [,1]   [,2]   [,3]   [,4]   [,5]   [,6]\n[1,]  0.873 -0.206 -0.163 -0.105 -0.233 -0.166\n[2,] -0.206  0.873 -0.171 -0.229 -0.100 -0.167\n[3,] -0.163 -0.171  0.834 -0.160 -0.173 -0.167\n[4,] -0.105 -0.229 -0.160  0.931 -0.271 -0.166\n[5,] -0.233 -0.100 -0.173 -0.271  0.946 -0.168\n[6,] -0.166 -0.167 -0.167 -0.166 -0.168  0.833\n\n$D\n       [,1]   [,2]   [,3]\n[1,]  0.797 -0.342 -0.455\n[2,] -0.342  0.667 -0.325\n[3,] -0.455 -0.325  0.780\n\n\nCompare the above with the adjustment matrices based on the absorbed predictors only:\n\n\nCode\nprint(Atilde, digits = 3)\n\n\n$A\n         [,1]      [,2]     [,3]      [,4]     [,5]\n[1,]  1.05313  0.001860 -0.00742  0.008995 -0.05657\n[2,]  0.00186  1.000065 -0.00026  0.000315 -0.00198\n[3,] -0.00742 -0.000260  1.00104 -0.001257  0.00790\n[4,]  0.00900  0.000315 -0.00126  1.001523 -0.00958\n[5,] -0.05657 -0.001980  0.00790 -0.009576  1.06022\n\n$B\n         [,1]     [,2]     [,3]\n[1,]  1.00139 -0.00478  0.00339\n[2,] -0.00478  1.01642 -0.01163\n[3,]  0.00339 -0.01163  1.00824\n\n$C\n          [,1]      [,2]      [,3]      [,4]     [,5]      [,6]\n[1,]  1.039180 -0.039378  4.00e-03  0.061921 -0.06632  5.94e-04\n[2,] -0.039378  1.039577 -4.02e-03 -0.062234  0.06665 -5.97e-04\n[3,]  0.003999 -0.004019  1.00e+00  0.006320 -0.00677  6.07e-05\n[4,]  0.061921 -0.062234  6.32e-03  1.097861 -0.10481  9.39e-04\n[5,] -0.066317  0.066651 -6.77e-03 -0.104808  1.11225 -1.01e-03\n[6,]  0.000594 -0.000597  6.07e-05  0.000939 -0.00101  1.00e+00\n\n$D\n         [,1]     [,2]    [,3]\n[1,]  1.13078 -0.00914 -0.1216\n[2,] -0.00914  1.00064  0.0085\n[3,] -0.12165  0.00850  1.1131\n\n\nThe matrices differ:\n\n\nCode\nall.equal(A, Atilde)\n\n\n[1] \"Component \\\"A\\\": Mean relative difference: 0.6073885\"\n[2] \"Component \\\"B\\\": Mean relative difference: 0.7403564\"\n[3] \"Component \\\"C\\\": Mean relative difference: 0.5671847\"\n[4] \"Component \\\"D\\\": Mean relative difference: 0.6682793\"\n\n\nThus, Theorem 2 is incorrect as stated. (I have yet to identify the mis-step in the proof as given in the supplementary materials of the paper.)\n\n\nFurther thoughts\nFor this particular model specification, it is interesting to note that \\(\\bm{\\tilde{A}}_i = \\bm{A}_i + \\bm{T}_i \\bm{M}_{\\bm{T}} \\bm{T}_i'\\). Because \\(\\bm{\\ddot{U}}_i' \\bm{T}_i = \\bm{0}\\), it follows that \\[\n\\bm{\\ddot{U}}_i' \\bm{\\tilde{A}}_i = \\bm{\\ddot{U}}_i' \\left(\\bm{A}_i + \\bm{T}_i \\bm{M}_{\\bm{T}} \\bm{T}_i' \\right) = \\bm{\\ddot{U}}_i' \\bm{A}_i.\n\\] This holds in the numerical example:\n\n\nCode\nUiAtilde &lt;- mapply(\\(u, a) t(u) %*% a, u = Ui, a = Atilde, SIMPLIFY = FALSE)\nUiA &lt;- mapply(\\(u, a) t(u) %*% a, u = Ui, a = A, SIMPLIFY = FALSE)\nall.equal(UiAtilde, UiA)\n\n\n[1] TRUE\n\n\nThus, although the exact statement of Theorem 2 is incorrect, the substantive implication actually still holds. For this particular example, computing the CR2 variance estimator using the short-cut adjustment matrices \\(\\bm{\\tilde{A}}_1,...,\\bm{\\tilde{A}}_m\\) is equivalent to computing the CR2 variance estimator using the full model adjustment matrices \\(\\bm{A}_1,...,\\bm{A}_m\\). However, I have not yet been able to work out the general conditions under which this equivalence holds. It may require stricter conditions than those assumed in Theorem 2.\n\n\n\n\n\n\n\n\n\n\nReferences\n\nBell, R. M., & McCaffrey, D. F. (2002). Bias reduction in standard errors for linear regression with multi-stage samples. Survey Methodology, 28(2), 169–181.\n\n\nMcCaffrey, D. F., Bell, R. M., & Botts, C. H. (2001). Generalizations of biased reduced linearization. Proceedings of the Annual Meeting of the American Statistical Association.\n\nReuseCC BY 4.0CitationBibTeX citation:@online{pustejovsky2022,\n  author = {Pustejovsky, James E.},\n  title = {Corrigendum to {Pustejovsky} and {Tipton} (2018)},\n  date = {2022-09-28},\n  url = {https://jepusto.com/posts/Pusto-Tipton-2018-Theorem-2},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nPustejovsky, J. E. (2022, September 28). Corrigendum to Pustejovsky\nand Tipton (2018). https://jepusto.com/posts/Pusto-Tipton-2018-Theorem-2"
  },
  {
    "objectID": "posts/rdd-interactions/index.html",
    "href": "posts/rdd-interactions/index.html",
    "title": "Regression discontinuities with covariate interactions in the rdd package",
    "section": "",
    "text": "NOTE (2019-09-24): This post pertains to version 0.56 of the rdd package. The problems described in this post have been corrected in version 0.57 of the package, which was posted to CRAN on 2016-03-14.\nThe rdd package in R provides a set of methods for analysis of regression discontinuity designs (RDDs), including methods to estimate marginal average treatment effects by local linear regression. I was working with the package recently and obtained some rather counter-intuitive treatment effect estimates in a sharp RDD model. After digging around a bit, I found that my perplexing results were the result of a subtle issue of model specification. Namely, in models with additional covariates (beyond just the running variable, treatment indicator, and interaction), the main estimation function in rdd uses a specification in which covariates are always interacted with the treatment indicator. In this post, I’ll demonstrate the issue and comment on potential work-arounds.\n\nA simulated example\nTo make things more concrete, here’s a hypothetical RDD. I’ll use \\(R\\) to denote the running variable, with the threshold set at zero; \\(T\\) for the treatment indicator; and \\(Y\\) for the outcome. \\(X_1\\) is a continuous covariate that is correlated with \\(R\\). \\(X_2\\) is a categorical covariate with four levels that is independent of \\(X_1\\) and \\(R\\). In order to illustrate the issue with covariate-by-treatment interactions, I use a model in which the effect of the treatment varies with \\(R\\), \\(X_1\\), and \\(X_2\\):\n\nset.seed(20160124)\n\nsimulate_RDD &lt;- function(n = 2000, R = rnorm(n, mean = qnorm(.2))) {\n  n &lt;- length(R)\n  T &lt;- as.integer(R &gt; 0)\n  X1 &lt;- 10 + 0.6 * (R - qnorm(.2)) + rnorm(n, sd = sqrt(1 - 0.6^2))\n  X2 &lt;- sample(LETTERS[1:4], n, replace = TRUE, prob = c(0.2, 0.3, 0.35, 0.15))\n  Y0 &lt;- 0.4 * R + 0.1 * (X1 - 10) + c(A = 0, B = 0.30, C = 0.40, D = 0.55)[X2] + rnorm(n, sd = 0.9)\n  Y1 &lt;- 0.35 + 0.3 * R + 0.18 * (X1 - 10) + c(A = -0.50, B = 0.30, C = 0.20, D = 0.60)[X2] + rnorm(n, sd = 0.9)\n  Y &lt;- (1 - T) * Y0 + T * Y1\n  data.frame(R, T, X1, X2, Y0, Y1, Y)\n}\n\nRD_data &lt;- simulate_RDD(n = 2000)\n\n\n\nSimple RDD analysis\nThe main estimand in a sharp RDD is the marginal average treatment effect (MATE)—that is, the average effect of treatment assignment for units right at/near the threshold of eligibility. Even though I simulated a treatment response surface that depends on the covariates \\(X_1,X_2\\), it is not necessary to control for them in order to identify the MATE. Rather, it is sufficient to use a local linear regression of the outcome on the running variable, treatment indicator, and their interaction:\n\\[Y_i = \\beta_0 + \\beta_1 R_i + \\beta_2 T_i + \\beta_3 R_i T_i + \\epsilon_i\\]\nTypically, this regression is estimated using the observations within a certain bandwidth of the threshold, and using weights defined on the basis of some kernel. The default in the rdd package is to use a triangular edge kernel, with bandwidth chosen using a formula proposed by Imbens and Kalyanaraman. The following code uses rdd to estimate the MATE without controlling for covariates:\n\nlibrary(rdd)\nbw &lt;- with(RD_data, IKbandwidth(R, Y, cutpoint = 0))\nrdd_simple &lt;- RDestimate(Y ~ R, data = RD_data, cutpoint = 0, bw = bw)\nsummary(rdd_simple)\n\n\nCall:\nRDestimate(formula = Y ~ R, data = RD_data, cutpoint = 0, bw = bw)\n\nType:\nsharp \n\nEstimates:\n           Bandwidth  Observations  Estimate  Std. Error  z value  Pr(&gt;|z|)    \nLATE       1.0894     1177          0.3035    0.11323     2.680    0.007355  **\nHalf-BW    0.5447      611          0.2308    0.15471     1.492    0.135722    \nDouble-BW  2.1787     1832          0.2699    0.08968     3.010    0.002613  **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nF-statistics:\n           F       Num. DoF  Denom. DoF  p        \nLATE        37.73  3         1173        0.000e+00\nHalf-BW     12.64  3          607        1.006e-07\nDouble-BW  104.74  3         1828        0.000e+00\n\n\nUsing a bandwidth of 1.09, the estimated marginal average treatment effect is 0.303. The figure below illustrates the discontinuity:\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\n\nRDD with covariates\nIn practice, it is quite common for analysts to include additional covariates in the model specification. Doing so is not necessary for treatment effect identification, but can be useful for purposes of improving precision. For example, Cortes, Goodman, and Nomi (2015) use an RDD to estimate the effects of assigning low-performing 9th graders to double-dose algebra. Their main specifications include controls for student gender, race/ethnicity, free/reduced-price lunch status, etc. In the analysis that I’m working on, the data come from students nested within multiple schools, and so it seems sensible to include fixed effects for each school. There’s a direct analogy here to simple randomized experiments: the basic difference in means provides a randomization-unbiased estimate of the sample average treatment effect, but in practice it can be awfully useful to use an estimate from a model with additional covariates.\nReturning to my simulated example, the following table reports the estimates generated by RDestimate when controlling for neither, one, or both covariates.\n\nRD_est &lt;- function(mod, covariates) {\n  RD_fit &lt;- RDestimate(as.formula(paste(mod, covariates)), \n                       data = RD_data, cutpoint = 0)\n  with(RD_fit, c(est = est[[1]], se = se[1], p = p[1]))\n}\n\ncovariates &lt;- list(\"No covariates\" = \"\",\n                \"X1 only\" = \"| X1\",\n                \"X2 only\" = \"| X2\",\n                \"X1 + X2\" = \"| X1 + X2\")\n\nlibrary(plyr)\nldply(covariates, RD_est, mod = \"Y ~ R\", .id = \"Specification\")\n\n  Specification        est        se           p\n1 No covariates  0.3034839 0.1132266 0.007355079\n2       X1 only -0.6861864 0.8077039 0.395574210\n3       X2 only -0.2269958 0.1626996 0.162960539\n4       X1 + X2 -1.2529313 0.7315106 0.086749345\n\n\nDespite using identical bandwidths, the estimates are drastically different from each other, with standard errors that are much larger than for the simple estimate without covariates.\n\n\nWhat’s going on?\nIt is known that introducing covariates into an RDD analysis should have little effect on the MATE estimate (see, e.g., Lee and Lemieux, 2010). It is therefore quite perplexing that the estimates in my example (and in the real study I was analyzing) were so sensitive. It turns out that this puzzling behavior arises because, for sharp RDDs only, RDestimate always interacts the covariate(s) with the treatment indicator. Here is the relevant section of the function:\n\nbody(RDestimate)[[39]][[4]][[7]][[3]][[3]]\n\nif (!is.null(covs)) {\n    data &lt;- data.frame(Y, Tr, Xl, Xr, covs, w)\n    form &lt;- as.formula(paste(\"Y~Tr+Xl+Xr+\", paste(\"Tr*\", names(covs), \n        collapse = \"+\", sep = \"\"), sep = \"\"))\n} else {\n    data &lt;- data.frame(Y, Tr, Xl, Xr, w)\n    form &lt;- as.formula(Y ~ Tr + Xl + Xr)\n}\n\n\nFor a generic covariate \\(X\\), the function uses the specification:\n\\[Y_i = \\beta_0 + \\beta_1 R_i + \\beta_2 T_i + \\beta_3 R_i T_i + \\beta_4 X_i + \\beta_5 X_i T_i + \\epsilon_i, \\]\nwhile still taking \\(\\beta_2\\) to represent the MATE. This is problematic because, as soon as the \\(X_i T_i\\) term is introduced into the model, \\(\\beta_2\\) represents the difference between treated and untreated units at the threshold (where \\(R_i = 0\\)) and where \\(X_i = 0\\). Thus, including the \\(X_1\\) interaction in the model means that \\(\\beta_2\\) is a difference extrapolated way outside the support of the data, as in the following scatterplot of the outcome versus the covariate \\(X_1\\):\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\nWarning: The `guide` argument in `scale_*()` cannot be `FALSE`. This was deprecated in\nggplot2 3.3.4.\nℹ Please use \"none\" instead.\n\n\n\n\n\n\n\n\n\nRDestimate returns as the MATE estimate the difference between the regression lines when \\(X_1 = 0\\), which in this example is -0.69. Similarly, including the \\(X_2\\) interaction in the model means that \\(\\beta_2\\) will represent the marginal average treatment effect for only one of the categories of \\(X_2\\), rather than as some sort of average across all four categories.\n\n\nWhat to do about this\nIf you’ve been using the rdd package to analyze your data, I can think of a couple of ways to handle this issue, depending on whether you want to use a model that interacts the covariates with the treatment indicator. Here are some options:\nFirst, suppose that you want to estimate a model that does NOT include covariate-by-treatment interactions. The most transparent (and thus probably safest) approach is to do the estimation “by hand,” so to speak. Specifically, Use the rdd package to get kernel weights, but then estimate the outcome model using plain-old lm. Here’s an example:\n\nlibrary(sandwich)\nlibrary(lmtest)\nRD_data$wt &lt;- kernelwts(RD_data$R, center = 0, bw = bw)\nMATE_model &lt;- lm(Y ~ R + T + R * T + X1 + X2, weights = wt, data = subset(RD_data, wt &gt; 0))\ncoeftest(MATE_model, vcov. = vcovHC(MATE_model, type = \"HC1\"))\n\n\nt test of coefficients:\n\n             Estimate Std. Error t value  Pr(&gt;|t|)    \n(Intercept) -1.586191   0.374247 -4.2384 2.429e-05 ***\nR            0.183542   0.136025  1.3493 0.1774938    \nT            0.292284   0.107689  2.7142 0.0067422 ** \nX1           0.130973   0.034704  3.7739 0.0001688 ***\nX2B          0.474403   0.091835  5.1658 2.813e-07 ***\nX2C          0.549125   0.084991  6.4610 1.523e-10 ***\nX2D          0.713331   0.096855  7.3649 3.338e-13 ***\nR:T          0.283663   0.222801  1.2732 0.2032105    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nBy default, RDestimate uses the HC1 variant of heteroskedasticity-robust standard errors. To exactly replicate its behavior, I used coeftest from the lmtest package, combined with vcovHC from the sandwich package. Note that it is also necessary to estimate the model based on the subset of observations with positive weight (otherwise the sandwich standard errors will misbehave).\nAn alternative to the first approach is to “trick” RDestimate into using the desired model specification by using 2SLS estimation with \\(T\\) instrumenting itself. Because the function does not use covariate-by-treatment interactions for “fuzzy” RDDs, you get the correct model specification:\n\nsummary(RDestimate(Y ~ R + T| X1 + X2, data = RD_data, cutpoint = 0))\n\n\nCall:\nRDestimate(formula = Y ~ R + T | X1 + X2, data = RD_data, cutpoint = 0)\n\nType:\nfuzzy \n\nEstimates:\n           Bandwidth  Observations  Estimate  Std. Error  z value  Pr(&gt;|z|)    \nLATE       1.0894     1177          0.2923    0.10769     2.714    0.006644  **\nHalf-BW    0.5447      611          0.2041    0.14911     1.369    0.171103    \nDouble-BW  2.1787     1832          0.2703    0.08447     3.200    0.001374  **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nF-statistics:\n           F      Num. DoF  Denom. DoF  p        \nLATE       31.24  7         1169        7.490e-40\nHalf-BW    13.84  7          603        1.110e-16\nDouble-BW  68.36  7         1824        7.919e-88\n\n\nThe results based on the first bandwidth agree with the results from lm.\nNow, suppose that you DO want to retain the covariate-by-treatment interactions in the model, while also estimating the MATE. To do this, you can use what I call “the centering trick,” which entails centering each covariate at the sample average (in this case, the locally-weighted sample average). For a generic covariate \\(X\\), let\n\\[\\bar{x} = \\frac{\\sum_{i=1}^n w_i X_i}{\\sum_{i=1}^n w_i},\\]\nwhere \\(w_i\\) is the kernel weight for unit \\(i\\). Then estimate the model\n\\[Y_i = \\beta_0 + \\beta_1 R_i + \\beta_2 T_i + \\beta_3 R_i T_i + \\beta_4 \\left(X_i - \\bar{x}\\right) + \\beta_5 \\left(X_i - \\bar{x}\\right) T_i + \\epsilon_i, \\]\nThe coefficient on \\(T\\) now corresponds to the MATE. Here’s R code that implements this approach:\n\ncovariate_mat &lt;- model.matrix(~ X1 + X2, data = RD_data)[,-1]\ncovariate_cent &lt;- apply(covariate_mat, 2, function(x) x - weighted.mean(x, w = RD_data$wt))\nRD_data &lt;- data.frame(subset(RD_data, select = c(R, Y, T)), covariate_cent)\n\ncovariates_cent &lt;- list(\"No covariates\" = \"\",\n                \"X1 only\" = \"| X1\",\n                \"X2 only\" = \"| X2B + X2C + X2D\",\n                \"X1 + X2\" = \"| X1 + X2B + X2C + X2D\")\n\nldply(covariates_cent, RD_est, mod = \"Y ~ R\", .id = \"Specification\")\n\n  Specification       est        se           p\n1 No covariates 0.3034839 0.1132266 0.007355079\n2       X1 only 0.2913246 0.1125398 0.009635680\n3       X2 only 0.3107688 0.1071302 0.003721488\n4       X1 + X2 0.2981428 0.1065888 0.005155864\n\n\nThe estimates are now insensitive to the inclusion of the (properly centered) covariates, just as in the no-interactions model. In this example, the standard errors from the model that includes covariate-by-treatment interactions are just ever so slightly smaller than those from the model without interactions.\nWhy does this third approach work? I’ll explain more in a later post…\n\n\n\n\n\n\nReuseCC BY 4.0CitationBibTeX citation:@online{pustejovsky2016,\n  author = {Pustejovsky, James E.},\n  title = {Regression Discontinuities with Covariate Interactions in the\n    Rdd Package},\n  date = {2016-01-25},\n  url = {https://jepusto.com/posts/rdd-interactions},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nPustejovsky, James E. 2016. “Regression Discontinuities with\nCovariate Interactions in the Rdd Package.” January 25, 2016. https://jepusto.com/posts/rdd-interactions."
  },
  {
    "objectID": "posts/Robust-meta-analysis-1/index.html",
    "href": "posts/Robust-meta-analysis-1/index.html",
    "title": "A meta-sandwich",
    "section": "",
    "text": "A common problem arising in many areas of meta-analysis is how to synthesize a set of effect sizes when the set includes multiple effect size estimates from the same study. It’s often not possible to obtain all of the information you’d need in order to estimate the sampling covariances between those effect sizes, yet without that information, established approaches to modeling dependent effect sizes become inaccurate. Hedges, Tipton, & Johnson (2010, HTJ hereafter) proposed the use of cluster-robust standard errors for multi-variate meta-analysis. (These are also called “sandwich” standard errors, which is up there on the list of great and evocative names for statistical procedures.) The great advantage of the sandwich approach is that it permits valid inferences for average effect sizes and meta-regression coefficients even if you don’t have correct covariance estimates (or variance estimates, for that matter).\nI recently heard from Beth Tipton (who’s a graduate-school buddy) that she and her student have written an R package implementing the HTJ methods, including moment estimators for the between-study variance components. I want to try out the cluster-robust standard errors for a project I’m working on, but I also need to use REML estimators rather than the moment estimators. It turns out, it’s easy enough to do that by writing a couple of short functions. Here’s how.\nFirst, the metafor package contains a very rich suite of meta-analytic methods, including for multi-variate meta-analysis. The only thing it lacks is sandwich standard errors. However, the sandwich package provides an efficient, well-structured framework for calculating all sorts of robust standard errors. All that’s needed are a few functions to make the packages talk to each other. Each of the functions described below takes as input a fitted multi-variate meta-analysis model, which is represented in R by an object of class rma.mv.\nFirst load up the packages:\n\nlibrary(metafor)\nlibrary(sandwich)\nlibrary(lmtest)\n\nNext, I need a bread method for objects of class rma.mv, which is a function that returns the \\(p \\times p\\) matrix \\(\\displaystyle{m \\left(\\sum_{i=1}^m \\mathbf{X}_j' \\mathbf{W}_j \\mathbf{X}_j\\right)^{-1}}\\). The bread function is straight-forward because it is just a multiple of the model-based covariance matrix, which rma.mv objects store in the vb component:\n\nbread.rma.mv &lt;- function(obj) {\n  cluster &lt;- findCluster(obj)\n  length(unique(cluster)) * obj$vb  \n}\n\nI also need an estfun method for objects of class rma.mv, which is a function that returns an \\(m \\times p\\) matrix where row \\(j\\) is equal to \\(\\mathbf{e}_j' \\mathbf{W}_j \\mathbf{X}_j\\), \\(j = 1,...,m\\). The necessary pieces for the estfun method can also be pulled out of the components of rma.mv:\n\nestfun.rma.mv &lt;- function(obj) {\n  cluster &lt;- droplevels(as.factor(findCluster(obj)))\n  res &lt;- residuals(obj)\n  WX &lt;- chol2inv(chol(obj$M)) %*% obj$X\n  rval &lt;- by(cbind(res, WX), cluster, \n             function(x) colSums(x[,1] * x[,-1, drop = FALSE]))\n  rval &lt;- matrix(unlist(rval), length(unique(cluster)), obj$p, byrow=TRUE)\n  colnames(rval) &lt;- colnames(obj$X)\n  rval\n}\n\nThe remaining question is how to determine which of the components in the model should be used to define independent clusters. This is a little bit tricky because there are several different methods of specifying random effects in the rma.mv function. One way involves providing a list of formulas, each containing a factor associated with a unique random effect, such as random = list( ~ 1 | classroom, ~ 1 | school). If this method of specifying random effects is used, the rma.mv object will have the component withS set to TRUE, and my approach is to simply take the factor with the smallest number of unique levels. This is perhaps a little bit presumptious, because the withS method could potentially be used to specify arbitrary random effects, where one level is not strictly nested inside another. However, probably the most common use will involve nested factors, so my assumption seems like a good starting point at least.\nAnother approach to specifying random effects is to use a formula of the form random = inner | outer, in which case the rma.mv object will have the component withG set to TRUE. Here, it seems reasonable to use the outer factor for defining clusters. If both the withS and withG methods are used together, I’ll assume that the withS factors contain the outermost level.\nFinally, if rma.mv is used to estimate a fixed effects model without any random components, the clustering factor will have to be manually added to the rma.mv object in a component called cluster. For example, if you want to cluster on the variable studyID in the dataframe dat:\n\nrma_fit$cluster &lt;- dat$studyID\n\nHere’s code that implements these assumptions:\n\nfindCluster &lt;- function(obj) {\n  if (is.null(obj$cluster)) {\n    if (obj$withS) {\n      r &lt;- which.min(obj$s.nlevels)\n      cluster &lt;- obj$mf.r[[r]][[obj$s.names[r]]]\n    } else if (obj$withG) {\n      cluster &lt;- obj$mf.r[[1]][[obj$g.names[2]]]\n    } else {\n        stop(\"No clustering variable specified.\")\n    }\n  } else {\n    cluster &lt;- obj$cluster\n  }\n  cluster\n}\n\nWith these three functions, you can then use metafor to fit a random effects model, sandwich to calculate the standard errors, and functions like coeftest from the package lmtest to run \\(t\\)-tests. As a little bonus, here’s a function for probably the most common case of how you’d use the sandwich standard errors:\n\nRobustResults &lt;- function(obj, adjust = TRUE) {\n  cluster &lt;- findCluster(obj)  \n  vcov. &lt;- sandwich(obj, adjust = adjust)\n  df. &lt;- length(unique(cluster)) - obj$p\n  coeftest(obj, vcov. = vcov., df = df.)\n}\n\nSee here for a file containing the full code.\n\nExample\nTanner-Smith & Tipton (2013) provide an application of the cluster-robust method to a fictional dataset with 68 effect sizes nested within 15 studies. They call this a “hierarchical” dependence example because each effect size estimate is drawn from an independent sample, but dependence is induced because the experiments were all done in the same lab. For comparison purposes, here are the results produced by robumeta:\n\nlibrary(grid)\nlibrary(robumeta)\ndata(hierdat)\n\nHTJ &lt;- robu(effectsize ~ 1,\n       data = hierdat, modelweights = \"HIER\",\n       studynum = studyid,\n       var.eff.size = var, small = FALSE)\nHTJ\n\nRVE: Hierarchical Effects Model  \n\nModel: effectsize ~ 1 \n\nNumber of clusters = 15 \nNumber of outcomes = 68 (min = 1 , mean = 4.53 , median = 2 , max = 29 )\nOmega.sq = 0.1560802 \nTau.sq = 0.06835547 \n\n               Estimate StdErr t-value dfs  P(|t|&gt;) 95% CI.L 95% CI.U Sig\n1 X.Intercept.     0.25 0.0598    4.18  14 0.000925    0.122    0.378 ***\n---\nSignif. codes: &lt; .01 *** &lt; .05 ** &lt; .10 *\n---\n\n\nTo exactly re-produce the results with metafor, I’ll need to use the weights proposed by HTJ. In their approach, effect size \\(i\\) from study \\(j\\) receives weight equal to \\(\\left(v_{ij} + \\hat\\omega^2 + \\hat\\tau^2\\right)^{-1}\\), where \\(v_{ij}\\) is the sampling variance of the effect size, \\(\\hat\\omega^2\\) is an estimate of the between-sample within-study variance, and \\(\\hat\\tau^2\\) is an estimate of the between-study variance. After calculating these weights, I fit the model in metafor, calculate the sandwich covariance matrix, and replay the results:\n\nhierdat$var_HTJ &lt;- hierdat$var + HTJ$mod_info$omega.sq + HTJ$mod_info$tau.sq # calculate weights\n\nWarning in hierdat$var + HTJ$mod_info$omega.sq: Recycling array of length 1 in vector-array arithmetic is deprecated.\n  Use c() or as.vector() instead.\n\n\nWarning in hierdat$var + HTJ$mod_info$omega.sq + HTJ$mod_info$tau.sq: Recycling array of length 1 in vector-array arithmetic is deprecated.\n  Use c() or as.vector() instead.\n\nmeta1 &lt;- rma.mv(yi = effectsize ~ 1, V = var_HTJ, data = hierdat, method = \"FE\")\nmeta1$cluster &lt;- hierdat$studyid # add clustering variable to the fitted model\nRobustResults(meta1)\n\n\nt test of coefficients:\n\n        Estimate Std. Error t value  Pr(&gt;|t|)    \nintrcpt 0.249826   0.059762  4.1803 0.0009253 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nThe HTJ weights are not the only alternative–one could instead use weights that are exactly inverse variance under the posited model. For effect \\(i\\) from study \\(j\\), these weights would be closer to \\(\\left(v_{ij} + \\hat\\omega^2 + k_j \\hat\\tau^2 \\right)^{-1}\\). For \\(\\hat\\tau^2 &gt; 0\\), the inverse-variance weights put proportionately less weight on studies containing many effects. These weights can be calculated in metafor as follows:\n\nmeta2 &lt;- rma.mv(yi = effectsize ~ 1, V = var, \n                 random = list(~ 1 | esid, ~ 1 | studyid), \n                 sigma2 = c(HTJ$mod_info$omega.sq, HTJ$mod_info$tau.sq),\n                 data = hierdat)\nRobustResults(meta2)\n\n\nt test of coefficients:\n\n        Estimate Std. Error t value Pr(&gt;|t|)   \nintrcpt 0.264422   0.086688  3.0503 0.008645 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nCuriously, the robust standard error increases under a weighting scheme that is more efficient if the model is correct.\nFinally, metafor provides ML and REML estimators for the between-sample and between-study random effects (the HTJ moment estimators are not available though). Here are the results based on REML estimators and the corresponding inverse-variance weights:\n\nmeta3 &lt;- rma.mv(yi = effectsize ~ 1, V = var, \n                 random = list(~ 1 | esid, ~ 1 | studyid), \n                 data = hierdat,\n                method = \"REML\")\nmeta3\n\n\nMultivariate Meta-Analysis Model (k = 68; method: REML)\n\nVariance Components:\n\n            estim    sqrt  nlvls  fixed   factor \nsigma^2.1  0.2263  0.4757     68     no     esid \nsigma^2.2  0.0000  0.0000     15     no  studyid \n\nTest for Heterogeneity:\nQ(df = 67) = 370.1948, p-val &lt; .0001\n\nModel Results:\n\nestimate      se    zval    pval   ci.lb   ci.ub      \n  0.2501  0.0661  3.7822  0.0002  0.1205  0.3797  *** \n\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nRobustResults(meta3)\n\n\nt test of coefficients:\n\n        Estimate Std. Error t value  Pr(&gt;|t|)    \nintrcpt 0.250071   0.059796  4.1821 0.0009222 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nThe between-study variance estimate is tiny, particularly when compared to the between-sample within-study estimate. Despite the difference in variance estimates, the average effect size estimate is nearly identical to the estimate based on the HTJ approach.\nSee here for the full code to reproduce this example.\n\n\nNotes\nIt would be straight-forward to add a few more functions that provide robust standard errors for univariate meta-analysis models as well. All that it would take is to write bread and estfun methods for the class rma.uni.\nAlso, Beth has recently proposed small-sample corrections to the cluster-robust estimators, based on the bias-reduced linearization (BRL) approach of McCaffrey, Bell, & Botts (2001). It seems to me that these small-sample corrections could also be implemented using an approach similar to what I’ve done here, by building out the estfun method to provide BRL results. It would take a little more thought, but actually it would be worth doing–and treating the general case–because BRL seems like it would be useful for all sorts of models besides multi-variate meta-analysis.\n\n\n\n\n\n\nReuseCC BY 4.0CitationBibTeX citation:@online{pustejovsky2014,\n  author = {Pustejovsky, James E.},\n  title = {A Meta-Sandwich},\n  date = {2014-04-21},\n  url = {https://jepusto.com/posts/Robust-meta-analysis-1},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nPustejovsky, James E. 2014. “A Meta-Sandwich.” April 21,\n2014. https://jepusto.com/posts/Robust-meta-analysis-1."
  },
  {
    "objectID": "posts/Robust-meta-analysis-3/index.html",
    "href": "posts/Robust-meta-analysis-3/index.html",
    "title": "Meta-sandwich with extra mustard",
    "section": "",
    "text": "In an earlier post about sandwich standard errors for multi-variate meta-analysis, I mentioned that Beth Tipton has recently proposed small-sample corrections for the covariance estimators and t-tests, based on the bias-reduced linearization approach of McCaffrey, Bell, and Botts (2001). You can find her forthcoming paper on the adjustments here. My understanding is that these small-sample corrections are important because the uncorrected sandwich estimators can lead to under-statement of uncertainty and inflated type I error rates when a given meta-regression coefficient is estimated from only a small or moderately sized sample of independent studies (or clusters of studies). Moreover, it can be difficult to determine exactly when you have a large enough sample to trust the uncorrected sandwiches.\nI wanted to try out these small-sample corrected sandwich estimators for a meta-analyses project that I’m working on. Beth and one of her students have written an R package called robumeta that implements the sandwich covariance estimator and small-sample corrections as described in her paper. However, for my project I want to use the metafor package, which doesn’t provide these methods. I’ve therefore created a set of functions that implement the sandwich covariance estimators and small-sample corrections for models estimated using the rma.mv function in metafor. Here is the complete code. Sorry, there’s no further documentation at the moment (beyond the rest of this post).\n\nConsistency with robumeta\nIn order to check that the functions are correct, I compared the results generated by robumeta with the results from metafor plus my functions. Here’s one example (I looked at a few others as well). First, the robumeta results:\n\nlibrary(grid)\nlibrary(robumeta)\ndata(hierdat)\n\nrobu_hier &lt;- robu(effectsize ~ males + binge,\n            data = hierdat, modelweights = \"HIER\",\n            studynum = studyid,\n            var.eff.size = var, small = TRUE)\nrobu_hier\n\nRVE: Hierarchical Effects Model with Small-Sample Corrections \n\nModel: effectsize ~ males + binge \n\nNumber of clusters = 15 \nNumber of outcomes = 68 (min = 1 , mean = 4.53 , median = 2 , max = 29 )\nOmega.sq = 0.1146972 \nTau.sq = 0.06797866 \n\n               Estimate  StdErr t-value  dfs P(|t|&gt;) 95% CI.L 95% CI.U Sig\n1 X.Intercept.  -0.0989 0.32140  -0.308 1.79 0.79045  -1.6511   1.4533    \n2        males   0.0020 0.00441   0.454 1.88 0.69689  -0.0182   0.0222    \n3        binge   0.6799 0.12156   5.594 4.18 0.00439   0.3482   1.0117 ***\n---\nSignif. codes: &lt; .01 *** &lt; .05 ** &lt; .10 *\n---\nNote: If df &lt; 4, do not trust the results\n\n\nTo maintain consistency, I first need to calculate the approximate weights used in robumeta and then fit the model in metafor using these fixed weights.\n\ndevtools::source_gist(id = \"11302318\", filename = \"metafor-BRL.R\")\n\nhierdat$var_HTJ &lt;- hierdat$var + as.numeric(robu_hier$mod_info$omega.sq) + as.numeric(robu_hier$mod_info$tau.sq)\n\nmeta_hier &lt;- rma.mv(yi = effectsize ~ males + binge, \n                V = var_HTJ, \n                data = hierdat, method = \"FE\")\nmeta_hier$cluster &lt;- hierdat$studyid\n\nRobustResults(meta_hier)\n\n            Estimate  Std. Error    t value       df    Pr(&gt;|t|)\nintrcpt -0.098869582 0.321400179 -0.3076214 1.788350 0.790446059\nmales    0.002002043 0.004410552  0.4539212 1.879142 0.696887075\nbinge    0.679929801 0.121556887  5.5935111 4.182783 0.004385654\n\n\nThe estimated covariance matrices match:\n\nall.equal(sandwich(meta_hier, meat.=meatBRL), \n          robu_hier$VR.r, \n          check.attributes=FALSE)\n\n[1] TRUE\n\n\nIt can also be verified that the p-values based on the Satterthwaite degrees of freedom agree.\n\n\nUse with metafor\nOf course, the point of writing functions that work with rma.mv objects is not to replicate robumeta results, but to take advantage of metafor’s flexibility. Rather than estimate the model with robumeta, typically one would estimate the variance components in metafor and then calculate the sandwich covariance estimates and small-sample corrections. For instance:\n\nmeta_REML &lt;- rma.mv(yi = effectsize ~ males + binge, \n                V = var, random = list(~ 1 | esid, ~ 1 | studyid), \n                data = hierdat,\n                method = \"REML\")\nmeta_REML\n\n\nMultivariate Meta-Analysis Model (k = 68; method: REML)\n\nVariance Components:\n\n            estim    sqrt  nlvls  fixed   factor \nsigma^2.1  0.1566  0.3957     68     no     esid \nsigma^2.2  0.0000  0.0000     15     no  studyid \n\nTest for Residual Heterogeneity:\nQE(df = 65) = 297.0172, p-val &lt; .0001\n\nTest of Moderators (coefficients 2:3):\nQM(df = 2) = 27.2659, p-val &lt; .0001\n\nModel Results:\n\n         estimate      se     zval    pval    ci.lb   ci.ub      \nintrcpt   -0.1118  0.2474  -0.4520  0.6513  -0.5966  0.3730      \nmales      0.0022  0.0034   0.6467  0.5178  -0.0044  0.0088      \nbinge      0.6744  0.1313   5.1349  &lt;.0001   0.4170  0.9319  *** \n\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nRobustResults(meta_REML)\n\n            Estimate  Std. Error    t value       df    Pr(&gt;|t|)\nintrcpt -0.111796564 0.318156355 -0.3513888 1.794988 0.762200367\nmales    0.002173683 0.004380026  0.4962718 1.882842 0.671549040\nbinge    0.674435042 0.121660936  5.5435628 4.167780 0.004585142\n\n\nOne advantage here is that it’s possible to compare the model-based standard errors to the robust ones. In this instance, the two are fairly similar. However, the degrees of freedom estimated in the robust results indicate that the model-based standard errors (based on normal approximations) may be much too narrow.\n\n\nDifferences between robumeta and my implementation\nThere are two important differences between the approach implemented in robumeta and the approach based on metafor and the code that I’ve provided. The first is that robumeta uses moment estimators for the variance components, whereas metafor uses restricted- or full maximum likelihood. The estimated between-study heterogeneity (and for the hierarchical effects model, the within-study heterogeneity as well) will therefore differ to some degree.\nThe second, and perhaps more crucial, distinction has to do with the choice of weights. Weights are used for two purposes: to estimate the fixed effects and to calculate the small-sample correction. The robumeta package uses diagonal weights for both purposes. Using diagonal weights in calculating the fixed effects means that the resulting point estimates will be equivalent to those from a weighted ordinary least squares regression:\n\nWOLS &lt;- lm(effectsize ~ males + binge, data = hierdat, weights = 1 / var_HTJ)\ncoef(WOLS)\n\n (Intercept)        males        binge \n-0.098869582  0.002002043  0.679929801 \n\nall.equal(coef(WOLS), as.numeric(robu_hier$b.r), check.attributes = FALSE)\n\n[1] TRUE\n\n\nA subtler point is that robumeta uses the inverse weights for purposes of calculating the small sample-correction. The small sample correction involves choosing a “working” or “target” covariance matrix towards which to adjust the sandwich estimator. If the working covariance model is correct, then the BRL covariance estimator is exactly unbiased. The working matrix is also used to determine the Satterthwaite degrees of freedom. In robumeta, the working covariance matrix is taken to be inverse of the weights, which is also a diagonal matrix. Thus, the BRL correction amounts to assuming independence among all of the effect sizes. This may sound somewhat counter-intuitive, but some simulation results (reported in Beth’s paper, referenced above) suggest that the resulting estimators perform well even when the working independence assumption is not correct.\nIn contrast to the robumeta weights, metafor calculates the fixed effects based on a weighting matrix that is exactly inverse variance for given estimates of the variance components. Typically, the weighting matrix will be block-diagonal but may have off-diagonal entries corresponding to effect sizes drawn from the same study. Furthermore, my implementation of BRL uses the estimated covariance matrix derived from the posited random effects structure; in other words, the working covariance structure is taken to be the same as the model specified in the metafor call. This seems sensible to me, although I do not have any evidence regarding its performance relative to the alternatives. It is possible that any gains in asymptotic efficiency from using exactly inverse variance weights are outweighed by some sort of instability in small samples. It’s also possible that the performance of the different approaches to weighting might depend on which variance component estimators are used (i.e., MOM vs. REML).\nNeither implementation that I’ve described above is fully general. Following the generalized estimating equation framework, a fully general implementation would allow the user to specify an arbitrary weight matrix in addition to a working covariance structure. The weighting matrix would be used for purposes of estimating the fixed effects. The working covariance model would be estimated (based on MOM or REML or what-not) and then used for purposes of BRL adjustment. Of course, this fully general formulation may well be more complicated than what most analysts would actually need or use (especially for linear mixed models), except perhaps when dealing with complex survey data.\n\n\n\n\n\n\nReuseCC BY 4.0CitationBibTeX citation:@online{pustejovsky2014,\n  author = {Pustejovsky, James E.},\n  title = {Meta-Sandwich with Extra Mustard},\n  date = {2014-04-26},\n  url = {https://jepusto.com/posts/Robust-meta-analysis-3},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nPustejovsky, James E. 2014. “Meta-Sandwich with Extra\nMustard.” April 26, 2014. https://jepusto.com/posts/Robust-meta-analysis-3."
  },
  {
    "objectID": "posts/SCD-effect-size-sensitivities/index.html",
    "href": "posts/SCD-effect-size-sensitivities/index.html",
    "title": "New working paper: Procedural sensitivities of SCD effect sizes",
    "section": "",
    "text": "I’ve just posted a new version of my working paper, Procedural sensitivities of effect sizes for single-case designs with behavioral outcome measures. The abstract is below. This version is a major update of an earlier paper that focused only on the non-overlap measures. The new version also includes analysis of two other effect sizes (the within-case standardized mean difference and the log response ratio) as well as additional results and more succinct summaries of the main findings.\nThe paper itself is available on the Open Science Framework (here), as are the supplementary materials and Source code. I also created interaction versions of the graphics from the main paper and the supplementary materials, which can be viewed in this shiny app. I would welcome any comments, questions, or feedback that readers may have.\n\nAbstract\nA wide variety of effect size indices have been proposed for quantifying the magnitude of treatment effects in single-case designs. Commonly used measures include parametric indices such as the standardized mean difference, as well as non-overlap measures, such as the percentage of non-overlapping data, improvement rate difference, and non-overlap of all pairs. Currently, little is known about the properties of these indices when applied to behavioral data collected by systematic direct observation, even though systematic direct observation is the most common approach to outcome measurement in single-case research. This study uses computer simulation to investigate the properties of several single-case effect size measures when applied to systematic direct observation data. Results indicate that the magnitude of the non-overlap measures and of the standardized mean difference can be strongly influenced by procedural details of the study’s design, which is a significant limitation to using these indices as effect sizes for meta-analysis of single-case designs. A less widely used parametric index, the log-response ratio, has the advantage of being insensitive to sample size and observation session length, although its magnitude is influenced by the use of partial interval recording.\n\n\n\n\n\n\nReuseCC BY 4.0CitationBibTeX citation:@online{pustejovsky2016,\n  author = {Pustejovsky, James E.},\n  title = {New Working Paper: {Procedural} Sensitivities of {SCD} Effect\n    Sizes},\n  date = {2016-10-17},\n  url = {https://jepusto.com/posts/SCD-effect-size-sensitivities},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nPustejovsky, James E. 2016. “New Working Paper: Procedural\nSensitivities of SCD Effect Sizes.” October 17, 2016. https://jepusto.com/posts/SCD-effect-size-sensitivities."
  },
  {
    "objectID": "posts/Shogren-reliability-analysis/index.html",
    "href": "posts/Shogren-reliability-analysis/index.html",
    "title": "Reliability of UnGraphed single-case data: An example using the Shogren dataset",
    "section": "",
    "text": "In one example from my dissertation, I re-analyzed a systematic review by Shogren and colleagues, titled “The effect of choice-making as an intervention for problem behavior” (Shogren, et al., 2004). In order to do the analysis, I retrieved all of the original articles identified by the review, scanned in all of the graphs depicting the data, and used (actually, had an undergraduate use) a computer program called UnGraph to capture the data-points off of the graphs (see Shadish, et al., 2009 for details on this procedure).\nAs it turned out, Wim Van Den Noortgate and Patrick Onghena followed a similar procedure in analyzing the same systematic review (reported in Van Den Noorgate & Onghena, 2008). Wim and Patrick were kind enough to share their data so that I could calculate the reliability of this data extraction procedure, based on the two independent replications. After some initial data-munging, I arrived at a clean, merged dataset:\n\nShogren &lt;- read.csv(\"http://jepusto.com/data/Shogren_data_merged.csv\")\nhead(Shogren)\n\n    Study Case Setting  Measure time choice Phase A B lowIntAxis\n1 Bambara   Al Dessert Protests    1      0     A 5 5          1\n2 Bambara   Al Dessert Protests    2      0     A 7 7          1\n3 Bambara   Al Dessert Protests    3      0     A 4 4          1\n4 Bambara   Al Dessert Protests    4      1     B 1 1          1\n5 Bambara   Al Dessert Protests    5      1     B 0 0          1\n6 Bambara   Al Dessert Protests    6      1     B 1 1          1\n\n\nThe variables are as follows:\n\nStudy - First author of original study included in the meta-analysis;\nCase - Name of individual case;\nSetting - some of the studies used multiple baselines on single individuals across multiple settings;\nMeasure - some of the studies used multiple outcome measures on each case;\ntime - sequential measurement occasion;\nchoice - indicator equal to one if the treatment condition allowed for choice;\nPhase - Factor indicating sequential phases (some of the designs were treatment reversals, such as ABA or ABAB or ABABAB);\nA - Wim’s outcome measurement;\nB - my outcome measurement;\nlowIntAxis - an idicator equal to one if the vertical axis of the graph was labeled with integers, and the axis maximum was &lt;= 20.\n\nThe final variable distinguishes graphs that are particularly easy to capture. Wim/Patrick and I used slightly different exclusion criteria, so there are a total of 0 cases across 0 studies included in the merged dataset. To begin, here’s a plot of A versus B by study:\n\nlibrary(ggplot2)\nqplot(A, B, geom = \"point\", color = Case, data = Shogren) + facet_wrap( ~ Study, scales = \"free\") + theme(legend.position=\"none\")\n\nWarning: `qplot()` was deprecated in ggplot2 3.4.0.\n\n\n\n\n\n\n\n\n\nClearly the two measurements are very correlated. You’ll notice that the studies (and sometimes cases within studies) used several different outcome measurement scales, so the overall correlation between A and B (r = 0.999767) isn’t really the best approach. Furthermore, some of the variation in the outcomes is presumably due to differences between phases, and it would be better to calculate a reliability based on the residual variation within phases.\nI accomplish this with a simple hierarchical model, fit separately to the data from each case. Denote the outcome as \\(y_{ijk}\\) for phase \\(i = 1,...,P\\), measurement occasion \\(j = 1,...,n_i\\), and replicate \\(k = 1,2\\). I model these outcomes as\n\\[y_{ijk} = \\beta_i + \\epsilon_{ij} + \\nu_{ijk}\\]\nwith the \\(\\beta\\)’s fixed, \\(\\epsilon_{ij} \\sim (0, \\tau^2)\\), and \\(\\nu_{ijk} \\sim (0, \\sigma^2)\\). Reliability is then captured by the intra-class correlation \\(\\rho = \\tau^2 / (\\tau^2 + \\sigma^2)\\).\nI calculate the reliabilities from each case using restricted maximum likelihood, then apply Fisher’s Z-transform:\n\nlibrary(reshape)\nlibrary(plyr)\n\nShogren_long &lt;- melt(Shogren, measure.vars = c(\"A\",\"B\"), variable_name = \"observer\")\n\nFisher_Z &lt;- function(x) 0.5 * (log(1 + x) - log(1 - x))\n\nlibrary(nlme)\nZ_ICC &lt;- function(x, formula = value ~ Phase){\n  fit &lt;- lme(formula, random = ~ 1 | time, data = x)\n  tau.sq.ratio &lt;- as.double(coef(fit$modelStruct$reStruct, FALSE))\n  rho &lt;- tau.sq.ratio / (tau.sq.ratio + 1)\n  Z &lt;- Fisher_Z(rho)\n  df &lt;- dim(x)[1] / 2 - length(fit$coefficients$fixed)\n  return(c(rho = rho, Z = Z, df = df))\n}\nICC &lt;- ddply(Shogren_long, .(Study, Case, Setting, Measure, lowIntAxis), Z_ICC)\n\nIt turns out that 5 of 6 cases with lowIntAxis==1 are perfectly correlated. The remainder of my analysis focuses on the cases with lowIntAxis==0. Here’s a histogram of the Z-transformed correlations:\n\nwith(subset(ICC, lowIntAxis==0), hist(Z))\n\n\n\n\n\n\n\n\nWith only 2 replicates per measurement occasion, the large-sample variance of the intra-class correlation is equivalent to that of the usual Pearson correlation (see Hedges, Hedberg, & Kuyper, 2013), except that I use \\(N - P\\) in the denominator to account for the fact that separate means are estimated for each of the \\(P\\) phases: \\[Var(\\hat\\rho) \\approx \\frac{(1 - \\rho^2)^2}{N - P},\\] where \\(N = \\sum_i n_i\\). Applying Fisher’s Z transform stabilizes the variance, so that it is appropriate to use inverse variance weights of simply \\(N - P\\). Turning to a random-effects meta-analysis:\n\nlibrary(metafor)\nsummary(rma_Z &lt;- rma(yi = Z, vi = 1 / df, data = ICC, subset = lowIntAxis==0))\n\n\nRandom-Effects Model (k = 27; tau^2 estimator: REML)\n\n  logLik  deviance       AIC       BIC      AICc   \n-26.1156   52.2313   56.2313   58.7475   56.7530   \n\ntau^2 (estimated amount of total heterogeneity): 0.3778 (SE = 0.1198)\ntau (square root of estimated tau^2 value):      0.6146\nI^2 (total heterogeneity / total variability):   88.15%\nH^2 (total variability / sampling variability):  8.44\n\nTest for Heterogeneity:\nQ(df = 26) = 211.1324, p-val &lt; .0001\n\nModel Results:\n\nestimate      se     zval    pval   ci.lb   ci.ub      \n  3.2596  0.1265  25.7670  &lt;.0001  3.0117  3.5075  *** \n\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nThe average effect size corresponds to a reliability of 0.9970546 (95% CI: [0.9951684,0.9982051]). The reliabilities are heterogeneous, but because they are all at the extreme of the scale, the heterogeneity has little practical implication: approximating the population of reliabilities by a normal distribution, and based on the RML estimates, 84 percent of reliabilities will be greater than 0.9900. Though one could certainly imagine factors that might explain the variation in reliabilities–the resolution of the image file from which the data were captured, the size of the points used to graph each measurement, the number of outcomes represented on the same graph–it hardly seems worth exploring further because all of the reliabilities are so high. These results are very similar to those reported by Shadish, et al. (2009), who found a median reliability of 0.9993 based on a similar study of 91 single-case graphs.\n\nReferences\n\nHedges, L. V, Hedberg, E. C., & Kuyper, A. M. (2012). The variance of intraclass correlations in three- and four-level models. Educational and Psychological Measurement. doi:10.1177/0013164412445193\nShadish, W. R., Brasil, I. C. C., Illingworth, D. A., White, K. D., Galindo, R., Nagler, E. D., & Rindskopf, D. M. (2009). Using UnGraph to extract data from image files: Verification of reliability and validity. Behavior Research Methods, 41(1), 177-83. doi:10.3758/BRM.41.1.177\nShogren, K. A., Faggella-Luby, M. N., Bae, S. J., & Wehmeyer, M. L. (2004). The effect of choice-making as an intervention for problem behavior. Journal of Positive Behavior Interventions, 6(4), 228-237.\nVan den Noortgate, W., & Onghena, P. (2008). A multilevel meta-analysis of single-subject experimental design studies. Evidence-Based Communication Assessment and Intervention, 2(3), 142-151. doi:10.1080/17489530802505362\n\n\n\n\n\n\n\nReuseCC BY 4.0CitationBibTeX citation:@online{pustejovsky2013,\n  author = {Pustejovsky, James E.},\n  title = {Reliability of {UnGraphed} Single-Case Data: {An} Example\n    Using the {Shogren} Dataset},\n  date = {2013-10-23},\n  url = {https://jepusto.com/posts/Shogren-reliability-analysis},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nPustejovsky, James E. 2013. “Reliability of UnGraphed Single-Case\nData: An Example Using the Shogren Dataset.” October 23, 2013. https://jepusto.com/posts/Shogren-reliability-analysis."
  },
  {
    "objectID": "posts/Simulation-studies-in-R-2016/index.html",
    "href": "posts/Simulation-studies-in-R-2016/index.html",
    "title": "Simulation studies in R (Fall, 2016 version)",
    "section": "",
    "text": "In today’s Quant Methods colloquium, I gave an introduction to the logic and purposes of Monte Carlo simulation studies, with examples written in R.\n\nHere are the slides from my presentation.\nYou can find the code that generates the slides here.\nHere is my presentation on the same topic from a couple of years ago.\nDavid Robinson’s blog has a much more in-depth discussion of beta-binomial regression.\nThe data I used is from Lahman’s baseball database.\n\n\n\n\n\n\nReuseCC BY 4.0CitationBibTeX citation:@online{pustejovsky2016,\n  author = {Pustejovsky, James E.},\n  title = {Simulation Studies in {R} {(Fall,} 2016 Version)},\n  date = {2016-09-28},\n  url = {https://jepusto.com/posts/Simulation-studies-in-R-2016},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nPustejovsky, James E. 2016. “Simulation Studies in R (Fall, 2016\nVersion).” September 28, 2016. https://jepusto.com/posts/Simulation-studies-in-R-2016."
  },
  {
    "objectID": "posts/SMDs-in-single-group/index.html",
    "href": "posts/SMDs-in-single-group/index.html",
    "title": "Standardized mean differences in single-group, repeated measures designs",
    "section": "",
    "text": "I received a question from a colleague about computing variances and covariances for standardized mean difference effect sizes from a design involving a single group, measured repeatedly over time. Deriving these quantities is a little exercise in normal distribution theory, which I find kind of relaxing sometimes (hey, we all have our coping mechanisms!).\n\nThe set-up\nConsider a study in which a single group of \\(n\\) participants was measured at each of \\(T + 1\\) time-points, indexed as \\(t = 0,...,T\\). At the first time-point, there has not yet been any exposure to an intervention. At the second and subsequent time-points, there is some degree of exposure, and so we are interested in describing change between time point \\(t &gt; 0\\) and time-point 0. For each time-point, we have a sample mean \\(\\bar{y}_t\\) and a sample standard deviation \\(s_{t}\\). For now, assume that there is complete response. Let \\(\\mu_t\\) denote the population mean and \\(\\sigma_t\\) denote the population standard deviation, both at time \\(t\\). Let \\(\\rho_{st}\\) denote the correlation between outcomes measured at time \\(s\\) and time \\(t\\), for \\(s,t = 0,..,T\\), where \\(\\rho_{tt} = 1\\). We might also have sample correlations for each time point, denoted \\(r_{st}\\). We calculate a standardized mean difference for each time-point \\(t &gt; 0\\) by taking \\[\nd_t = \\frac{\\bar{y}_t - \\bar{y}_0}{s_P},\n\\] where \\(s_p\\) is the sample standard deviation pooled across all time-points: \\[\ns_P^2 = \\frac{1}{T+1}\\sum_{t=0}^T s_t^2.\n\\] The question is then, what is \\(\\text{Var}(d_t)\\) and what is \\(\\text{Cov}(d_s, d_t)\\), for \\(s,t = 1,...,T\\)?\n\n\nThe results\nDefine the unstandardized mean difference between time-point \\(t\\) and time-point 0 as \\(D_t = \\bar{y}_t - \\bar{y}_0\\). Then, from the algebra of variances and covariances, we have \\[\n\\text{Var}(D_t) = \\frac{1}{n}\\left(\\sigma_0^2 + \\sigma_t^2 - 2 \\rho_{t0} \\sigma_0 \\sigma_t\\right)\n\\] and \\[\\text{Cov}(D_s, D_t) = \\frac{1}{n}\\left[\\sigma_0^2 + \\rho_{st} \\sigma_s \\sigma_t - \\sigma_0 \\left(\\rho_{s0} \\sigma_s + \\rho_{t0} \\sigma_t\\right) \\right].\\] From a previous post about the distribution of sample variances, we have that \\[\n\\text{Cov}(s_s^2, s_t^2) = \\frac{2 \\left(\\rho_{st} \\sigma_s \\sigma_t\\right)^2}{n - 1}.\n\\] Consequently, \\[\n\\begin{aligned}\n\\text{Var}(s_P^2) &= \\frac{1}{(T + 1)^2} \\sum_{s=0}^T \\sum_{t=0}^T \\text{Cov}(s_s^2, s_t^2) \\\\\n&= \\frac{2}{(n-1)(T + 1)^2} \\sum_{s=0}^T \\sum_{t=0}^T \\left(\\rho_{st} \\sigma_s \\sigma_t\\right)^2.\n\\end{aligned}\n\\] Let \\(\\sigma_P^2 = \\frac{1}{T+1}\\sum_{t=0}^T \\sigma_t^2\\) denote the average population variance across all \\(T + 1\\) time-points, and let \\(\\delta_t\\) denote the standardized mean difference parameter at time \\(t\\). Then, following the multivariate delta method, \\[\n\\text{Var}(d_t) \\approx \\frac{\\text{Var}(D_t)}{\\sigma_P^2} + \\frac{\\delta_t^2}{2 \\nu} \\qquad \\text{and} \\qquad \\text{Cov}(d_s, d_t) \\approx \\frac{\\text{Cov}(D_s, D_t)}{\\sigma_P^2} + \\frac{\\delta_s \\delta_t}{2 \\nu},\n\\] where \\(\\displaystyle{\\nu = \\frac{2 \\sigma_P^4}{\\text{Var}(s_P^2)}}\\).\nWithout imposing further assumptions, and assuming that we have access to the sample correlations between time-points, a feasible estimator of the sampling variance of \\(d_t\\) is \\[\nV_t = \\frac{s_0^2 + s_t^2 - 2 r_{t0} s_0 s_t}{n s_P^2} + \\frac{d_t^2}{2 \\hat\\nu},\n\\] where \\[\n\\hat\\nu = \\frac{(n-1) s_p^4}{\\frac{1}{(T + 1)^2}\\sum_{s=0}^T \\sum_{t=0}^T r_{st}^2 s_s^2 s_t^2}.\n\\] Similarly, a feasible estimator for the covariance between \\(d_s\\) and \\(d_t\\) is \\[\nC_{st} = \\frac{s_0^2 + r_{st} s_s s_t - s_0 \\left(r_{s0} s_s + r_{t0} s_t\\right)}{n s_P^2} + \\frac{d_s d_t}{2 \\hat\\nu}.\n\\]\nIn some cases, it might be reasonable to use further assumptions about distributional structure in order to simplify these approximations. In particular, suppose we assume that the population variances are constant across time-points, \\(\\sigma_0 = \\sigma_1 = \\cdots = \\sigma_T\\). In this case, the variances and covariances no longer depend on the scale of the outcome, and we have \\[\n\\hat\\nu = \\frac{(n - 1)(T + 1)}{T R + 1}, \\qquad \\text{where} \\qquad R = \\frac{2}{T (T + 1)}\\sum_{s=0}^{T-1} \\sum_{t=s+1}^T r_{st}^2\n\\] (here, \\(R\\) is the average of the squared correlations between pairs of distinct time-points). Since \\(R\\) will always be less than 1, \\(\\hat\\nu\\) will always be larger than \\(n - 1\\). If sample correlations aren’t reported or available, it would seem fairly reasonable to use \\(\\hat\\nu = n - 1\\), or to make a rough assumption about the average squared correlation \\(R\\). With the approximate degrees of freedom \\(\\hat\\nu\\), the variances and covariances are then given by \\[\nV_t = \\frac{2(1 - r_{t0})}{n} + \\frac{d_t^2}{2 \\hat\\nu} \\qquad \\text{and} \\qquad C_{st} = \\frac{1 + r_{st} - r_{s0} - r_{t0}}{n} + \\frac{d_s d_t}{2 \\hat\\nu}.\n\\]\n\n\nExtension\nIn some contexts, one might encounter a design that uses over-lapping but not identical samples at each time-point. For instance, in a rotating panel survey, each participant is measured repeatedly for some small number of time-points \\(p &lt; T + 1\\) (say \\(p = 2\\) or \\(p = 3\\)), and new participants are added to the sample with each new time-point. The simple repeated measures set-up that I described in this post is an imperfect approximation for such designs. In dealing with such a design, suppose that one knew the total number of observations at each time-point, denoted \\(n_t\\) for \\(t = 0,...,T\\), as well as the number of observations that were common across any pair of time-points, denoted as \\(n_{st}\\) for \\(s,t = 0,...,T\\). Further suppose that the drop-outs and additions are ignorable (missing completely at random), so that any subset of participants defined by a pattern of response or non-response is still representative of the full population. I leave it as an exercise for the reader (a relaxing and fun one!) to derive \\(\\text{Var}(d_t)\\) and \\(\\text{Cov}(d_s, d_t)\\) under such a model.\n\n\n\n\n\n\nReuseCC BY 4.0CitationBibTeX citation:@online{pustejovsky2021,\n  author = {Pustejovsky, James E.},\n  title = {Standardized Mean Differences in Single-Group, Repeated\n    Measures Designs},\n  date = {2021-10-06},\n  url = {https://jepusto.com/posts/SMDs-in-single-group},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nPustejovsky, James E. 2021. “Standardized Mean Differences in\nSingle-Group, Repeated Measures Designs.” October 6, 2021. https://jepusto.com/posts/SMDs-in-single-group."
  },
  {
    "objectID": "posts/SPED-Pro-Sem/index.html",
    "href": "posts/SPED-Pro-Sem/index.html",
    "title": "Special Education Pro-Sem",
    "section": "",
    "text": "Dr. Marcia Barnes from the department of Special Education invited me to visit her pro-seminar this afternoon and talk about some of my work on meta-analytic methods for single-case research. Thanks very much to the students for asking such thoughtful and engaging questions. Here are the slides, which include some additional material that we didn’t get to talk about.\n\n\n\n\n\nReuseCC BY 4.0CitationBibTeX citation:@online{pustejovsky2014,\n  author = {Pustejovsky, James E.},\n  title = {Special {Education} {Pro-Sem}},\n  date = {2014-04-10},\n  url = {https://jepusto.com/posts/SPED-Pro-Sem},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nPustejovsky, James E. 2014. “Special Education Pro-Sem.”\nApril 10, 2014. https://jepusto.com/posts/SPED-Pro-Sem."
  },
  {
    "objectID": "posts/Tau-U/index.html",
    "href": "posts/Tau-U/index.html",
    "title": "Tau-U",
    "section": "",
    "text": "Parker, Vannest, Davis, and Sauber (2011) proposed Tau-U as an effect size measure for use in single-case designs that exhibit baseline trend. In their original paper, they actually conceptualize Tau-U as a family of four distinct indices, distinguished by a) whether the index includes an adjustment for the presence of baseline trend and b) whether the index incorporates information about trend during the intervention phase. However, in subsequent presentations the authors seem to have focused exclusively on the index that adjusts for baseline trend but not for intervention phase trend, and so I’ll do the same here. (This version is also the one available in the web-tool at singlecaseresearch.org.)\nTau-U is an elaboration on their previously proposed effect sizes NAP and Tau, which do not account for baseline trends. The index is calculated as follows. Suppose that we have data from A and B phases from a single case, where the baseline phase has \\(m\\) observations and treatment phase has \\(n\\) observations. Let \\(y^A_1,...,y^A_m\\) denote the baseline phase data and \\(y^B_1,...,y^B_n\\) denote the treatment phase data. Tau-U is then calculated as\n\\[\n\\text{Tau-U} = \\frac{S_P - S_B}{mn}\n\\]\nwhere \\(S_P\\) is Kendall’s S statistic calculated for the comparison between phases and \\(S_B\\) is Kendall’s S statistic calculated on the baseline trend. More precisely,\n\\[\n\\begin{aligned}\nS_P &= \\sum_{i=1}^m \\sum_{j=1}^n \\left[I\\left(y^B_j &gt; y^A_i\\right) - I\\left(y^B_j &lt; y^A_i\\right)\\right] \\\\\nS_B &= \\sum_{i=1}^{m - 1} \\sum_{j = i + 1}^m \\left[I\\left(y^A_j &gt; y^A_i\\right) - I\\left(y^A_j &lt; y^A_i\\right)\\right].\n\\end{aligned}\n\\]\nNote that the first term in Tau-U is equivalent to \\(\\text{Tau} = S_P / (m n)\\), which in turn is a re-scaling of NAP. The second term is related to the rank-correlation between the measurement occasions and outcomes in the baseline phase. Subtracting the second from the first thus adjusts for baseline trend, in the sense that more pronounced baseline trends will lead to smaller values of Tau-U. But looking at the measure a bit more deeply, it has some very odd features. In this post, I’ll show that the distribution of Tau-U is sensitive to the number of observations in each phase."
  },
  {
    "objectID": "posts/Tau-U/index.html#sample-size-sensitivity",
    "href": "posts/Tau-U/index.html#sample-size-sensitivity",
    "title": "Tau-U",
    "section": "Sample size sensitivity",
    "text": "Sample size sensitivity\nConsider first the logical range of Tau-U. The minimum and maximum possible values of \\(S_P\\) are \\(-m n\\) and \\(m n\\); the minimum and maximum of \\(S_B\\) are \\(-m (m-1) / 2\\) and \\(m (m - 1) / 2\\). Consequently, the logical range of Tau-U is from \\(-(2n + m - 1) / (2n)\\) to \\((2n + m - 1) / (2n)\\). If the treatment phase is quite long compared to the baseline phase, then this range will be close to [-1, 1]. On the other hand, in a study with a baseline that is twice as long as the treatment phase, the range of Tau-U will be closer to [-2, 2]. That’s a very odd property.\nThe average magnitude of Tau-U is similarly influenced by the lengths of each phase. To see this, it’s helpful to think first about its target parameter–the quantity that is estimated when calculating Tau-U based on a sample of data. Since Tau-U is not defined in parametric terms, I will assume that the Tau-U statistic is an unbiased estimator of its target parameter \\(\\tau_U = \\text{E}\\left(\\text{Tau-U}\\right)\\). It follows that\n\\[\n\\tau_U = \\tau_P - \\frac{m - 1}{2n} \\tau_B,\n\\]\nwhere \\(\\tau_P\\) is Kendall’s rank correlation between the outcomes and an indicator for the treatment phase and \\(\\tau_B\\) is Kendall’s rank correlation between the measurement occasions and outcomes during baseline:\n\\[\n\\begin{aligned}\n\\tau_P &= \\frac{1}{mn}\\sum_{i=1}^m \\sum_{j=1}^n \\left[\\text{Pr}\\left(Y^B_j &gt; Y^A_i\\right) - \\text{Pr}\\left(Y^B_j &lt; Y^A_i\\right)\\right] \\\\\n\\tau_B &= \\frac{2}{m(m-1)} \\sum_{i=1}^{m - 1} \\sum_{j = i + 1}^m \\left[\\text{Pr}\\left(Y^A_j &gt; Y^A_i\\right) - \\text{Pr}\\left(Y^A_j &lt; Y^A_i\\right)\\right].\n\\end{aligned}\n\\]\nNow consider a positive a baseline trend, so that \\(\\tau_B &gt; 0\\), and assume that \\(\\tau_P\\) is constant. A longer baseline phase will then lead to smaller values of Tau-U (on average), while a longer treatment phase will lead to larger values of Tau-U (on average). Again, that’s really weird. This is not a good feature for an effect size measure because it means that Tau-U values from different cases are only on the same scale if the cases have identical baseline and treatment phase lengths. In a multiple baseline study, each case is necessarily observed for a different number of occasions in baseline (otherwise it wouldn’t be a multiple baseline). Thus, it seems inadvisable to use Tau-U to quantify the magnitude of treatment effects in a multiple baseline study."
  },
  {
    "objectID": "posts/Tau-U/index.html#sensitivity-under-a-parametric-model",
    "href": "posts/Tau-U/index.html#sensitivity-under-a-parametric-model",
    "title": "Tau-U",
    "section": "Sensitivity under a parametric model",
    "text": "Sensitivity under a parametric model\nThings may be different if we allow for the magnitude of \\(\\tau_P\\) to change along with the sample size. Such would be the case under a model where the intervention phase also exhibits a trend. For example, let’s suppose that the outcome follows a linear model with a non-zero trend and the intervention leads to an immediate shift in the outcome, as in the model:\n\\[\ny_t = \\beta_0 + \\beta_1 t + \\beta_2 I(t &gt; m) + \\epsilon_t.\n\\]\nFor simplicity, I’ll assume that the errors in this model are normally distributed with unit variance. Under this model,\n\\[\n\\begin{aligned}\n\\tau_B &= \\frac{4}{m (m - 1)} \\left[\\sum_{i=1}^{m-1} \\sum_{j=i+1}^m \\Phi\\left[\\beta_1\\left(j - i\\right) / \\sqrt{2}\\right]\\right] - 1, \\\\\n\\tau_P &= \\frac{2}{m n} \\left[\\sum_{i=1}^m \\sum_{j=1}^n \\Phi\\left[\\left(\\beta_1 (m + j - i) + \\beta_2\\right) / \\sqrt{2}\\right]\\right] - 1,\n\\end{aligned}\n\\]\nwhere \\(\\Phi()\\) is the standard normal cumulative distribution function. I can use the above formulas to calculate the average value of Tau-U for various degrees of baseline trend \\((\\beta_1)\\), level shift \\((\\beta_2)\\), and phase lengths \\((m,n)\\).\n\nE_TauU &lt;- function(b1, b2, m, n) {\n  tau_B &lt;- sum(sapply(1:(m - 1), function(i) \n    sum(pnorm(b1 * ((i+1):m - i) / sqrt(2))))) * 4 / (m * (m - 1)) - 1\n  tau_P &lt;- sum(sapply(1:m, function(i) \n    sum(pnorm((b1 * (m + 1:n - i) + b2) / sqrt(2))))) * 2 / (m * n) - 1\n  tau_P - tau_B * (m - 1) / (2 * n)\n}\n\nlibrary(dplyr)\nlibrary(tidyr)\nb1 &lt;- c(-0.2, -0.1, 0, 0.1, 0.2)\nb2 &lt;- c(0, 0.5, 1.0, 2.0)\nm &lt;- c(5, 10, 15, 20)\nn &lt;- 5:20\n\nexpand.grid(b1 = b1, b2 = b2, m = m, n = n) %&gt;%\n  group_by(b1, b2, m, n) %&gt;% \n  mutate(TauU = E_TauU(b1, b2, m, n)) -&gt;\n  TauU_values\nex &lt;- filter(TauU_values, b1 == -0.2 & b2 == 0)\n\n\nlibrary(ggplot2)\nggplot(TauU_values, aes(n, TauU, color = factor(m))) + \n  facet_grid(b1 ~ b2, labeller = \"label_both\") + \n  geom_line() + \n  labs(y = \"Expected magnitude of Tau-U\", color = \"m\") + \n  theme_bw() + theme(legend.position = \"bottom\")\n\n\n\n\n\n\n\n\nIn the figure above, each plot corresponds to a different value of the baseline slope (\\(\\beta_1\\), ranging from -0.2 in the top row to 0.2 in the bottom row) and treatment shift (\\(\\beta_2\\), ranging from 0 in the first column to 2 in the last column). Within each plot, the x axis corresponds to treatment phase length and the different lines correspond to different baseline phase lengths. The thing to note is that, when the baseline slope is non-zero, the expected value of Tau-U ranges quite widely within each plot, depending on the values of \\(m\\) and \\(n\\). For example, when \\(\\beta_2 = 0\\) (in the first column), the data follow a simple linear trend with no shift. If the slope of the trend is equal to -0.2 (the first row), then the expected magnitude of Tau-U ranges from -0.8 to 0.3 depending on the phase lengths, which is quite a wide range.\nGenerally, the degree of sample size sensitivity depends on the absolute magnitude of the baseline slope, with steeper slopes leading to increased sensitivity. For steeper values of slope, it appears that the degree to which the measure is affected by sample size even swamps the degree to which the measure is sensitive to the magnitude of the treatment effect. Very peculiar."
  },
  {
    "objectID": "posts/Tau-U/index.html#a-final-thought",
    "href": "posts/Tau-U/index.html#a-final-thought",
    "title": "Tau-U",
    "section": "A final thought",
    "text": "A final thought\nOf course, these results are contingent on the particular model under which I derived the expected magnitude of Tau-U. If the data followed some other model, such as a log-linear model with Poisson-distributed outcomes, then the behavior described above might change. Still, I think all of this raises the reasonable question: under what model (i.e., what sort of patterns of baseline trend, what sort of patterns of response to the intervention) does Tau-U provide a meaningful effect size measure that clearly quantifies the magnitude of treatment effects without being strongly affected by phase lengths? Unless and until such a model can be identified, I would be wary of interpreting Tau-U as a measure of treatment effect magnitude."
  },
  {
    "objectID": "posts/using-log-response-ratios/index.html",
    "href": "posts/using-log-response-ratios/index.html",
    "title": "New working paper: Using log response ratios for meta-analyzing SCDs with behavioral outcomes",
    "section": "",
    "text": "One of the papers that came out of my dissertation work (Pustejovsky, 2015) introduced an effect size metric called the log response ratio (or LRR) for use in meta-analysis of single-case research—particularly for single-case studies that measure behavioral outcomes through systematic direct observation. The original paper was pretty technical since it focused mostly on a formal measurement model for behavioral observation data. I’ve just completed a tutorial paper that demonstrates how to use the LRR for meta-analyzing single-case studies with behavioral outcomes. In this paper, I’ve tried to present the methods in as accessible a manner as I could muster, to provide a sort of “user’s guide” for researchers wanting to apply the LRR for their own work. You can find the working paper and supplementary materials (including data and replication code) on the Open Science Framework. I would welcome your feedback and questions about this work!\n\n\n\n\n\nReuseCC BY 4.0CitationBibTeX citation:@online{pustejovsky2017,\n  author = {Pustejovsky, James E.},\n  title = {New Working Paper: {Using} Log Response Ratios for\n    Meta-Analyzing {SCDs} with Behavioral Outcomes},\n  date = {2017-04-26},\n  url = {https://jepusto.com/posts/using-log-response-ratios},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nPustejovsky, James E. 2017. “New Working Paper: Using Log Response\nRatios for Meta-Analyzing SCDs with Behavioral Outcomes.” April\n26, 2017. https://jepusto.com/posts/using-log-response-ratios."
  },
  {
    "objectID": "posts/variance-components-with-misspecified-correlation/index.html",
    "href": "posts/variance-components-with-misspecified-correlation/index.html",
    "title": "Variance component estimates in meta-analysis with mis-specified sampling correlation",
    "section": "",
    "text": "\\[\n\\def\\Pr{{\\text{Pr}}}\n\\def\\E{{\\text{E}}}\n\\def\\Var{{\\text{Var}}}\n\\def\\Cov{{\\text{Cov}}}\n\\]\nIn a recent paper with Beth Tipton, we proposed new working models for meta-analyses involving dependent effect sizes. The central idea of our approach is to use a working model that captures the main features of the effect size data, such as by allowing for both between- and within-study heterogeneity in the true effect sizes (rather than only between-study heterogeneity). Doing so will lead to more precise estimates of overall average effects or, in models that include predictor variables, more precise estimates of meta-regression coefficients. Further, one can combine this working model with robust variance estimation methods to provide protection against the possibility that some of the model’s assumptions could be mis-specified.\nIn order to estimate these new working models, the analyst must first make some assumption about the degree of correlation between effect size estimates that come from the same sample. In typical applications, it can be difficult to obtain good empirical information about the correlation between effect size estimates, and so it is common to impose some simplifying assumptions and use rough guesses about the degree of correlation. There’s a sense that this might not matter much—particularly because robust variance estimation should protect the inferences if the assumptions about the correlation are wrong. However, I’m still curious about the extent to which these assumptions about the correlation structure matter for anything.\nThere’s a few reasons to wonder about how much the correlation matters. One is that the analyst might actually care about the variance component estimates from the working model, if they’re substantively interested in the extent of heterogeneity or if they’re trying to make predictions about the distribution of effect sizes that could be expected in a new study. Compared to earlier working models, the variance component estimates of the models that we proposed in the paper seem to be relatively more sensitive to the assumed correlation. Second, one alternative analytic strategy that’s been proposed (and applied) for meta-analysis of dependent effect sizes is to use a multi-level meta-analysis (MLMA) model. The MLMA is a special case of the correlated-and-hierarchical effects model that we described in the paper, the main difference being that MLMA ignores any correlations between effect size estimates (at the level of the sampling errors), or equivalently, assumes that the correlations are all zero. Thus, MLMA is one specific way that this correlation assumption might be mis-specified. There’s some simulation evidence that inferences based on MLMA may be robust (even without using robust variance estimation), but it’s not clear how general this robustness property might be.\nIn this post, I’m going to look at the implications of using a mis-specified assumption about the sampling correlation for the variance components in the correlated-and-hierarchical effects working model. As in my previous post on weights in multivariate meta-analysis, I’m going to mostly limit consideration to the simple (but important!) case of an intercept-only model, without any further predictors of effect size, to see what can be learned about how the variance components can go wrong."
  },
  {
    "objectID": "posts/variance-components-with-misspecified-correlation/index.html#completely-balanced-designs",
    "href": "posts/variance-components-with-misspecified-correlation/index.html#completely-balanced-designs",
    "title": "Variance component estimates in meta-analysis with mis-specified sampling correlation",
    "section": "3.1 Completely balanced designs",
    "text": "3.1 Completely balanced designs\nThings simplify considerably in the special case that the sample of studies is completely balanced, such that \\(k_1 = k_2 = \\cdots = k_J\\) and \\(\\sigma_1^2 = \\sigma_2^2 = \\cdots = \\sigma_J^2\\). In such a design, the log-likelihood depends on \\(\\tau^2\\) and \\(\\omega^2\\) only through the quantities \\(a = \\tau^2 + \\rho \\sigma^2\\) and \\(b = \\omega^2 + (1 - \\rho) \\sigma^2\\). It follows that \\[\nl_R\\left(\\tau^2, \\omega^2, \\phi\\right) = l_R\\left(\\tilde\\tau^2, \\tilde\\omega^2, \\rho\\right)\n\\] so long as \\[\n\\begin{aligned}\n\\tau^2 + \\phi \\sigma^2 &= \\tilde\\tau^2 + \\rho \\sigma^2 \\\\\n\\omega^2 + (1 - \\phi)\\sigma^2 &= \\tilde\\omega^2 + (1 - \\rho) \\sigma^2.\n\\end{aligned}\n\\] If we assume that \\((\\rho - \\phi)\\sigma^2 &lt; \\tau^2\\) and \\((\\phi - \\rho)\\sigma^2 &lt; \\omega^2\\), then we can set \\[\n\\begin{aligned}\n\\tilde\\tau^2 &= \\tau^2 - \\left(\\rho - \\phi\\right) \\sigma^2 \\\\\n\\tilde\\omega^2 &= \\omega^2 + \\left(\\rho - \\phi\\right) \\sigma^2\n\\end{aligned}\n\\] and achieve the exact same likelihood.1 Because the Kullback-Liebler divergence is minimized at the log likelihood of the true parameter values, setting \\(\\tilde\\tau^2\\) and \\(\\tilde\\omega^2\\) equal to the above quantities will also minimize the K-L divergence.\nThe relationships here are fairly intuitive, I think. When \\(\\rho\\) is an over-estimate of the true correlation \\(\\phi\\), then the between-study variance will be under-estimated and the within-study variance will be over-estimated, each to an extent that depends on a) the difference between \\(\\rho\\) and \\(\\phi\\) and b) the size of the (average) sampling variance. When \\(\\rho\\) is an under-estimate of the true correlation \\(\\phi\\), then the between-study variance will be over-estimated and the within-study variance will be under-estimated, each to an extent that depends on the same components. It’s also rather intriguing to see that the total variance (the sum of \\(\\tau^2\\) and \\(\\omega^2\\)) is totally invariant to \\(\\rho\\) and will be preserved no matter what assumption we make regarding the sample correlation.\nIn practice, of course, it’s pretty unlikely to have a meta-analytic dataset that is completely balanced. Still, the formulas for this completely balanced case might nonetheless be useful as heuristics for the direction of the biases in the parameter estimates—perhaps even as rough guides for the magnitude of bias that could be expected."
  },
  {
    "objectID": "posts/variance-components-with-misspecified-correlation/index.html#finding-tildetau2-and-tildeomega2-in-imbalanced-designs",
    "href": "posts/variance-components-with-misspecified-correlation/index.html#finding-tildetau2-and-tildeomega2-in-imbalanced-designs",
    "title": "Variance component estimates in meta-analysis with mis-specified sampling correlation",
    "section": "3.2 Finding \\(\\tilde\\tau^2\\) and \\(\\tilde\\omega^2\\) in imbalanced designs",
    "text": "3.2 Finding \\(\\tilde\\tau^2\\) and \\(\\tilde\\omega^2\\) in imbalanced designs\nIn imbalanced designs, we can find \\(\\tilde\\tau^2\\) and \\(\\tilde\\omega^2\\) by direct minimization of \\(\\mathcal{KL}\\), given design information \\(k_1,...,k_J\\) and \\(\\sigma_1^2,...,\\sigma_J^2\\); true parameter values \\(\\tau^2\\), \\(\\omega^2\\), \\(\\phi\\); and assumed correlation \\(\\rho\\). The plot below depicts how \\(\\tilde\\tau\\), \\(\\tilde\\omega\\), and the total SD \\(\\sqrt{\\tilde\\tau^2 + \\tilde\\omega^2}\\) change as a function of the assumed correlation \\(\\rho\\), for various levels of true correlation \\(\\phi\\), when the design is imbalanced. As previously, I use \\(\\tau = 0.2\\) and \\(\\omega = 0.1\\).\n\n\nCode\nfind_tau_omega &lt;- function(tau, omega, phi, rho, k_j, sigmasq_j) {\n\n  res &lt;- optim(par = c(tau + 0.001, omega + 0.001), fn = CHE_KL, \n                tau = tau, omega = omega, phi = phi, rho = rho,\n                k_j = k_j, sigmasq_j = sigmasq_j,\n                lower = c(0,0), method = \"L-BFGS-B\")\n\n  data.frame(tau_tilde = res$par[1], omega_tilde = res$par[2])\n}\n\nsigmasq_bar &lt;- mean(sigmasq_j)\n\nopt_params &lt;- \n  cross_df(list(tau = tau,\n                omega = omega,\n                phi = seq(0.2,0.8,0.2),\n                rho = seq(0,0.95,0.05))) %&gt;%\n  mutate(\n    res = pmap(., .f = find_tau_omega, k_j = k_j, sigmasq_j = sigmasq_j),\n  ) %&gt;%\n  unnest(res) %&gt;%\n  mutate(\n    total_tilde = sqrt(tau_tilde^2 + omega_tilde^2),\n    tau_pred = sqrt(pmax(0,tau^2 + (phi - rho) * sigmasq_bar)),\n    omega_pred = sqrt(pmax(0, omega^2 - (phi - rho) * sigmasq_bar)),\n    total_pred = sqrt(tau_pred^2+ omega_pred^2),\n    phi_lab = paste(\"phi ==\", phi)\n  )\n\nopt_params %&gt;% \n  pivot_longer(c(ends_with(\"_tilde\"), ends_with(\"_pred\")),\n               names_to = \"q\", values_to = \"p\") %&gt;%\n  separate(q, into = c(\"param\",\"type\")) %&gt;%\n  mutate(\n    type = recode(type, tilde = \"exact\", pred = \"balanced\"),\n    type = factor(type, levels = c(\"exact\",\"balanced\")),\n    param = factor(param, levels = c(\"tau\",\"omega\",\"total\"),\n                   labels = c(\"tau\",\"omega\",\"sqrt(tau^2 + omega^2)\"))\n  ) %&gt;%\n  ggplot(aes(rho, p, color = type, linetype = type)) + \n  geom_hline(yintercept = 0) + \n  geom_line() + \n  scale_color_brewer(type = \"qual\", palette = 2) + \n  facet_grid(param ~ phi_lab, labeller = \"label_parsed\") + \n  theme_minimal() + \n  labs(x = expression(rho), y = \"Parameter\", color = \"\", linetype = \"\") +\n  theme(legend.position = \"bottom\")\n\n\n\n\n\n\n\n\n\nThe top row of the figure shows \\(\\tilde\\tau\\), the middle row shows \\(\\tilde\\omega\\), and the bottom row shows the total SD, for varying levels of assumed correlation \\(\\rho\\). The solid green lines represent the values that actually minimize the KL divergence. The dashed orange lines correspond to the minimizing values assuming complete balance (and using the average value of the \\(\\sigma_j^2\\)’s to evaluate the bias). The “balanced” approximations are fairly close—close enough to use as heuristics, at least—although they’re not perfect. In particular, the balanced approximation becomes discrepant from the real minimizing values when \\(\\tilde\\tau\\) or \\(\\tilde\\omega\\) gets closer to zero. It’s also notable that the total variance is nearly constant (except when one or the other variance component is zero) and the balanced approximation is quite close to the real minimizing values."
  },
  {
    "objectID": "posts/variance-components-with-misspecified-correlation/index.html#footnotes",
    "href": "posts/variance-components-with-misspecified-correlation/index.html#footnotes",
    "title": "Variance component estimates in meta-analysis with mis-specified sampling correlation",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nConsequently, \\(\\phi\\) is not identifiable (in the statistical sense) in the completely balanced design.↩︎"
  },
  {
    "objectID": "posts/wanted-PIR-data/index.html",
    "href": "posts/wanted-PIR-data/index.html",
    "title": "Wanted: PIR data",
    "section": "",
    "text": "Partial interval recording (PIR) is one method for recording data during systematic direct observation of a behavior. While a convenient method, PIR has the key drawback that it systematically over-states the prevalence of the behavior under observation. When used in single-case research to measure changes in behavior resulting from intervention, the systematic bias in PIR data can lead to deceptive results, such as inferring that an intervention reduces the prevalence of a problem behavior when in fact the opposite is true.\nWith my student Daniel Swan, I am currently working on developing methods for analyzing partial interval recording data that take its systematic bias into account. Some of these methods can be used with session-level summary PIR measurements (i.e., the percentage of intervals with the behavior), which are easily extracted from published single-case graphs. See here for the paper describing these methods.\nWe are now turning our attention to methods that use the finer-grained, interval-by-interval PIR data to obtain better estimates of the prevalence and incidence (frequency per unit time) of the behavior. For instance, if the observer uses 15 s partial interval recording, with 5 s for recording, for a 20 min session, this is a total of 60 intervals, for each of which the presence or absence of the behavior is recorded. The methods we’re working on make use of the full set of 60 ordered data points from the session. The general idea our work is similar to the post-hoc correction techniques proposed by Suen & Ary (1986), but we think we can greatly improve on their proposal.\nTo fully validate the methods we are developing, we need to test them out on real-world data. If you, dear reader, have access to PIR data and would be willing to share it with us, I would love to hear from you. We are looking specifically for:\n\nFine-grained (interval-by-interval) PIR data collected in real research contexts, such as single-case studies or observational studies involving students with behavioral disorders, children with autism-spectrum disorders, etc.\nAlternately, continuously-recorded behavioral observation data (e.g., as collected through MOOSES, the Direct Assessment Tracking Application, or ProCoderDV) that we could then convert into PIR data.\nAlong with either type of behavioral observation data, a brief (or lengthier) description of the participant(s) whose behavior was measured and the context in which the measurements were collected.\n\nWe can work with data in whatever format you might be willing to provide–whether that means photo-copied, paper observation forms, an Excel workbook, or a bunch of ProCoderDV data files. In return for sharing data, we will share with you the examples that we develop based on the data, which could also provide a basis for further collaboration. If you are interested in seeing your data analyzed and helping to advance this methodological work, please contact me.\n\n\n\n\n\nReuseCC BY 4.0CitationBibTeX citation:@online{pustejovsky2014,\n  author = {Pustejovsky, James E.},\n  title = {Wanted: {PIR} Data},\n  date = {2014-09-03},\n  url = {https://jepusto.com/posts/wanted-PIR-data},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nPustejovsky, James E. 2014. “Wanted: PIR Data.” September\n3, 2014. https://jepusto.com/posts/wanted-PIR-data."
  },
  {
    "objectID": "posts/what-does-multivariate-mean/index.html",
    "href": "posts/what-does-multivariate-mean/index.html",
    "title": "What do meta-analysts mean by ‘multivariate’ meta-analysis?",
    "section": "",
    "text": "If you’ve ever had class with me or attended one of my presentations, you’ve probably heard me grouse about how statisticians are mostly awful about naming things.1 A lot of the terminology in our field is pretty bad and ineloquent. As a leading example, look no further than Rubin and Little’s classification of missing data mechanisms as missing completely at random (MCAR), missing at random (MAR), or missing not at random (MNAR). Clear as mud, and the last one sounds like something you’d see on a handmade sign with a picture of someone’s pet puppy who wandered off last week.\nAs another example, consider that introductory statistics students always struggle to distinguish between no less than three different concepts that are all called “variance”: population variance, sample variance, and sampling variance.2 Unless the instructor also took diction training from the Royal Shakespeare Company, it’s no wonder that a fair number of students are left confused.\nIn this post, I will try to clarify (at least a little bit) another mess of terminology that crops up a lot in my work on meta-analysis: what do we mean when we say a model or method is “multivariate”? In the context of meta-analysis methods, I think there are at least three distinct senses in which this term is used:\nLet me explain what I mean by each of these."
  },
  {
    "objectID": "posts/what-does-multivariate-mean/index.html#multivariate-handwaving",
    "href": "posts/what-does-multivariate-mean/index.html#multivariate-handwaving",
    "title": "What do meta-analysts mean by ‘multivariate’ meta-analysis?",
    "section": "Multivariate handwaving",
    "text": "Multivariate handwaving\nIn the context of meta-analysis, the broadest meaning of “multivariate” is any method used for modeling data that includes more than one effect size estimate in some or all of the included studies. Formally, the term would apply to any model appropriate for a set of \\(k\\) studies, where study \\(j\\) includes \\(n_j\\) effect size estimates, and where the effect size estimates would be denoted \\(T_{ij}\\), for \\(i = 1,...,n_j\\) and \\(j = 1,...,k\\).\nAs it is used here, “multivariate” is really an umbrella term that could encompass a wide variety of methods and models, including multi-level meta-analysis or meta-regression models, multivariate methods in the narrower senses I will describe subsequently, and even robust variance estimation methods. It would also encompass techniques for handling this sort of data structure that aren’t strictly models, such as aggregating effect size estimates to the level of the study or using Harris Cooper’s “shifting unit-of-analysis” method (Cooper, 1998). This usage of “multivariate” involves a bit too much hand-waving for my taste (although I’ve been guilty of using the term this way in the past). I think a better, clearer term for this broad class of methods would be to call them methods for meta-analysis of dependent effect sizes."
  },
  {
    "objectID": "posts/what-does-multivariate-mean/index.html#multivariate-sampling-errors",
    "href": "posts/what-does-multivariate-mean/index.html#multivariate-sampling-errors",
    "title": "What do meta-analysts mean by ‘multivariate’ meta-analysis?",
    "section": "Multivariate sampling errors",
    "text": "Multivariate sampling errors\nAnother sense in which “multivariate” is used pertains to a certain class of models for dependent effect sizes. In particular, “multivariate meta-analysis” sometimes means a model where the sampling variances and covariances of the effect size estimates are treated as fully known. Say that each effect size estimate \\(T_{ij}\\) has a corresponding true effect size parameter \\(\\theta_{ij}\\), so that the sampling error is \\(e_{ij} = T_{ij} - \\theta_{ij}\\), or \\[\nT_{ij} = \\theta_{ij} + e_{ij}.\n\\] Typically, meta-analysis techniques treat the sampling errors as having known variances, \\(\\text{Var}(e_{ij}) = \\sigma_{ij}^2\\) for known \\(\\sigma_{ij}^2\\). Here, a multivariate meta-analysis would go a step further and make assumptions that \\(\\text{Cov}(e_{hj}, e_{ij}) = \\rho_{hij}\\sigma_{hj} \\sigma_{ij}\\) for known correlations \\(\\rho_{hij}\\), \\(h,i = 1,...,n_j\\) and \\(j=1,...,k\\). Typically, the sampling variances and covariances would play into how the model is estimated and how one conducts inference and gets standard errors on things, etc.\nBecker (2000) and Gleser & Olkin (2009) describe a whole slew of different situations where meta-analysts will encounter multiple effect size estimates within a given study, and both provide formulas for the covariances between those effect sizes. In some situations, these covariances can be calculated just based on primary study sample sizes or other information readily available from study reports. In other situations (such as when one calculates standardized mean differences for each of several outcomes on a common set of participants), the information needed to calculate covariances might not be available, which is where methods like robust variance estimation come in. With this meaning of the term, multivariate meta-analysis methods are those that both directly model the dependent effects structure and that treat the sampling covariances as known. They are therefore distinct from methods, such as robust variance estimation, that do not rely on knowing the exact variance-covariance structure of the sampling errors. In my own work, I find it helpful to be able to draw this distinction, so I rather like this usage of “multivariate.” This will surely irritate some statisticians, though, who prefer the third, stricter meaning of the term."
  },
  {
    "objectID": "posts/what-does-multivariate-mean/index.html#strictly-multivariate-models",
    "href": "posts/what-does-multivariate-mean/index.html#strictly-multivariate-models",
    "title": "What do meta-analysts mean by ‘multivariate’ meta-analysis?",
    "section": "Strictly multivariate models",
    "text": "Strictly multivariate models\nA third meaning of multivariate is to denote a class of models for multivariate data, meaning data where each unit is measured on several dimensions or characteristics. In the meta-analysis context, multivariate effect sizes are ones where, for each included study or sample, we have effect sizes describing outcomes (e.g., treatment effects) on one or more dimensions. For example, say that we have a bunch of studies examining some sort of educational intervention, and each study reports effect sizes describing the intervention’s impact on a) reading performance, b) social studies achievement, and/or c) language arts achievement. What differentiates this sort of multivariate data from the first, “umbrella” sense of the term is that with strictly multivariate data, no study has more than one effect size within a given dimension. In contrast, meta-analysis of dependent effect sizes deal with data structures that are not necessarily so tidy and organized, such that we might not be able to classify each effect size into one of a finite and exhaustive set of categories.\nWhen working with strictly multivariate data like this, a multivariate meta-analysis (or meta-regression) model would entail estimating average effects (or regression coefficients) for each dimension rather than aggregating across dimensions. This class of models was discussed extensively in an excellent article by Jackson et al. (2011).3 With my example of educational intervention studies, we would estimate average impacts on reading performance, social studies achievement, and language arts achievement. Estimating an overall aggregate effect on academic achievement would make little sense here, because we’d be mixing apples, oranges, and kiwis.\nFormally, this sort of data structure and model can be described as follows. As previously, say that we have a set of \\(k\\) studies, where study \\(j\\) has \\(n_j\\) effect sizes, \\(T_{ij}\\), and correspoding sampling variances \\(\\sigma_{ij}^2\\), both for \\(i = 1,...,n_j\\) and \\(j = 1,...k\\). Effect size \\(i\\) from study \\(j\\) can be classified into one of \\(C\\) dimensions. Let \\(d^c_{ij}\\) be an indicator for whether effect \\(i\\) falls into dimension \\(c\\), for \\(c = 1,...,C\\). With a strictly multivariate structure, there is never more than one effect per category, so \\(\\sum_{i=1}^{n_j} d^c_{ij} \\leq 1\\) for each \\(c = 1,...,C\\) and \\(j = 1,...,k\\). A typical multivariate random effects model would then be \\[\nT_{ij} = \\sum_{c=1}^C \\left(\\mu_c + v_{cj}\\right) d^c_{ij} + e_{ij},\n\\] where \\(\\mu_c\\) is the average effect size for category \\(c\\), \\(v_{cj}\\) is a random effect for category \\(c\\) in study \\(j\\), and \\(e_{ij}\\) is the sampling error term. The classic assumption about the random effects is that they are dependent within study, so \\[\n\\text{Var}(v_{cj}) = \\tau^2_c \\qquad \\text{and} \\qquad \\text{Cov}(v_{bj}, v_{cj}) = \\tau_{bc}\n\\] for \\(b,c = 1,...,C\\). Typically, these sorts of models would also rely on assumptions about the correlations between the sampling errors, just as with the second meaning of multivariate. Thus, to complete the model, we would have \\(\\text{Cov}(e_{hj}, e_{ij}) = \\rho_{hij}\\sigma_{hj}\\sigma_{ij}\\) for known \\(\\rho_{hij}\\). In practice, we might want to impose some common structure to the correlations across studies, such as using \\(\\rho_{hij}\\)’s that depend on the dimensions being correlated but are common across studies. Formally, we would have \\[\n\\rho_{hij} = \\sum_{b=1}^C \\sum_{c=1}^C d^b_{ij} \\ d^c_{ij} \\ \\rho_{bc}.\n\\] Of course, even getting this level of detail about correlations between effect sizes might often be pretty challenging.\nIn a strictly multivariate meta-regression model, we would also allow the coefficients for each predictor to be specific to each category, so that \\[\nT_{ij} = \\sum_{c=1}^C \\left(\\mathbf{x}_{ij}\\boldsymbol\\beta_c + v_{cj}\\right) d^c_{ij} + e_{ij},\n\\] In my example of educational intervention impact studies, say that are interested in whether the effects differ between quasi-experimental studies and true randomized control trials, and whether the effects differ based on the proportion of the sample that was economically disadvantaged. The strictly multivariate model would always involve interacting these predictors with the outcome category. In R’s equation notation, the meta-regression specification would be\n\nES ~ 0 + Cat + Cat:RCT + Cat:disadvantaged_pct\n\nIn contrast, in a generic meta-regression for dependent effect sizes, we might not include all of the interactions, and instead assume that the associations of the predictors were constant across outcome dimensions, as in\n\nES ~ 0 + outcome_cat + RCT + college_pct\n\nIn the strict sense of the term, the model without interactions is no longer really a multivariate meta-regression."
  },
  {
    "objectID": "posts/what-does-multivariate-mean/index.html#remarks",
    "href": "posts/what-does-multivariate-mean/index.html#remarks",
    "title": "What do meta-analysts mean by ‘multivariate’ meta-analysis?",
    "section": "Remarks",
    "text": "Remarks\nAn interesting property of strict multivariate meta-analysis models is that they involve partial pooling—or “borrowing of strength”—across dimensions (R. D. Riley et al., 2007; Richard D. Riley et al., 2017). Even though the model has separate coefficients for each dimension, the estimates for a given dimension are influenced by the available effect sizes for all dimensions. For instance, in the meta-analysis of educational intervention studies, the average impact on reading performance outcomes is based in part on the effect size estimates for the social studies and language arts performance. This happens because the model treats all of the dimensions as correlated—through the correlated sampling errors and, potentially, through the correlated random effects structure. Copas et al. (2018) examine how this works and propose a diagnostic plot to understand how it happens in application. Kirkham et al. (2012) also show that the borrowing of strength phenomenon can partially mitigate bias from selective outcome reporting. These concepts could be quite relevant even beyond the “strict” multivariate meta-analysis context in which they have been explored. It strikes me that it would be useful to investigate them in the more general context of meta-analysis with dependent effect sizes—that is, multivariate meta-analysis in the first, broadest sense."
  },
  {
    "objectID": "posts/what-does-multivariate-mean/index.html#footnotes",
    "href": "posts/what-does-multivariate-mean/index.html#footnotes",
    "title": "What do meta-analysts mean by ‘multivariate’ meta-analysis?",
    "section": "Footnotes",
    "text": "Footnotes\n\n\n“Mostly” rather than “uniformly” due to exceptions like Brad Efron (a.k.a. Mr. Bootstrap) and Rob Tibshirani (a.k.a. Mr. Lasso).↩︎\nAnd then consider the square roots of these quantities, respectively: population standard deviation, sample standard deviation, and standard error. WTF?↩︎\nRead this article! It’s essential. And it comes with pages and pages of commentary by other statisticans.↩︎"
  },
  {
    "objectID": "posts/Woodbury-identity/index.html",
    "href": "posts/Woodbury-identity/index.html",
    "title": "The Woodbury identity",
    "section": "",
    "text": "As in many parts of life, statistics is full of little bits of knowledge that are useful if you happen to know them, but which hardly anybody ever bothers to mention. You would think, if something is so useful, perhaps your professors would spend a fair bit of time explaining it to you. But maybe the stuff seems trivial, obvious, or simple to them, so they don’t bother.\nOne example of this is Excel keyboard shortcuts. In a previous life, I was an Excel jockey so I learned all the keyboard shortcuts, such as how to move the cursor to the last cell in a continuous block of entries (ctrl + an arrow key). Whenever I do this while sharing a screen in a meeting, someone is invariably astounded and wants to know what dark sorcery I’m conjuring. It’s a simple trick, but a useful one—especially if you’re working with a really large dataset with thousands of rows. But it’s also something that there’s no reason to expect anyone to figure out on their own, and that no stats or quant methods professor is going to spend class time demonstrating.\nLet me explain another, slightly more involved example, involving one of my favorite pieces of matrix algebra. There’s a thing called the Woodbury identity, also known as the Sherman-Morrison-Woodbury identity, that is a little life hack for inverting certain types of matrices. It has a Wikipedia page, which I have visited many times. It is a very handy bit of math, if you happen to be a statistics student working with hierarchical models (such as meta-analytic models). I’ll give a statement of the identity, then explain a bit about the connection to hierarchical models."
  },
  {
    "objectID": "posts/Woodbury-identity/index.html#random-intercepts",
    "href": "posts/Woodbury-identity/index.html#random-intercepts",
    "title": "The Woodbury identity",
    "section": "Random intercepts",
    "text": "Random intercepts\nAs an example, consider a very simple model that includes only random intercepts, so \\(\\mathbf{Z}_j = \\mathbf{1}_j\\), an \\(n_j \\times 1\\) vector with every entry equal to 1, and \\(\\mathbf{T}\\) is simply \\(\\tau^2\\), the variance of the random intercepts. For simplicity, let’s also assume that the level-1 errors are independent, so \\(\\boldsymbol\\Sigma_j = \\sigma^2 \\mathbf{I}_j\\) and \\(\\boldsymbol\\Sigma_j^{-1} = \\sigma^{-2} \\mathbf{I}_j\\). Applying the Woodbury identity, \\[\n\\begin{aligned}\n\\mathbf{V}_j^{-1} &= \\boldsymbol\\Sigma_j^{-1} - \\boldsymbol\\Sigma_j^{-1} \\mathbf{1}_j \\left(\\mathbf{T}^{-1} + \\mathbf{1}_j'\\boldsymbol\\Sigma_j^{-1}\\mathbf{1}_j \\right)^{-1} \\mathbf{1}_j' \\boldsymbol\\Sigma_j^{-1} \\\\\n&= \\sigma^{-2} \\mathbf{I}_j - \\sigma^{-4} \\mathbf{1}_j \\left(\\tau^{-2} + \\sigma^{-2} \\mathbf{1}_j'\\mathbf{1}_j \\right)^{-1} \\mathbf{1}_j' \\\\\n&= \\sigma^{-2} \\mathbf{I}_j - \\sigma^{-4} \\left(\\tau^{-2} + \\sigma^{-2} n_j \\right)^{-1} \\mathbf{1}_j \\mathbf{1}_j' \\\\\n&= \\sigma^{-2} \\left(\\mathbf{I}_j - \\frac{\\tau^2} {\\sigma^2 + n_j \\tau^2} \\mathbf{1}_j \\mathbf{1}_j'\\right).\n\\end{aligned}\n\\] Try checking this for yourself by carrying through the matrix algebra for \\(\\mathbf{V}_j \\mathbf{V}_j^{-1}\\), which should come out equal to \\(\\mathbf{I}_j\\).\nNow suppose that the design matrix is also quite simple, consisting of just an intercept term \\(\\mathbf{X}_j = \\mathbf{1}_j\\), so that \\(\\boldsymbol\\beta = \\beta\\) is simply a population mean. How precise is the estimate of the population mean from this hierarchical model? Well, the sampling variance of the estimator \\(\\hat\\beta\\) is approximately \\[\n\\begin{aligned}\n\\text{Var}(\\hat\\beta) &\\approx \\left(\\sum_{j=1}^J \\mathbf{1}_j'\\mathbf{V}_j^{-1} \\mathbf{1}_j \\right)^{-1} \\\\\n&= \\left(\\sigma^{-2}\\sum_{j=1}^J \\mathbf{1}_j' \\left(\\mathbf{I}_j - \\frac{\\tau^2} {\\sigma^2 + n_j \\tau^2} \\mathbf{1}_j \\mathbf{1}_j'\\right) \\mathbf{1}_j \\right)^{-1} \\\\\n&= \\left(\\sigma^{-2} \\sum_{j=1}^J n_j \\left(1 - \\frac{n_j \\tau^2} {\\sigma^2 + n_j \\tau^2} \\right)  \\right)^{-1} \\\\\n&= \\left( \\sigma^{-2} \\sum_{j=1}^J \\frac{n_j \\sigma^2} {\\sigma^2 + n_j \\tau^2} \\right)^{-1} \\\\\n&= \\left(\\sum_{j=1}^J \\frac{n_j} {\\sigma^2 + n_j \\tau^2} \\right)^{-1} \\\\\n&= \\left(\\sigma^2 + \\tau^2\\right) \\left(\\sum_{j=1}^J \\frac{n_j} {1 + (n_j - 1) \\rho} \\right)^{-1},\n\\end{aligned}\n\\] where \\(\\rho = \\tau^2 / (\\tau^2 + \\sigma^2)\\) is the intra-class correlation. Squint at this expression for a bit and you can see how the ICC influences the varince. If \\(\\rho\\) is near zero, then the sampling variance will be close to \\(\\left(\\sigma^2 + \\tau^2\\right) / N\\), which is what you would get if you treated every observation as independent. If \\(\\rho\\) is near 1, then the sampling variance ends up being nearly \\(\\left(\\sigma^2 + \\tau^2\\right) / J\\), which is what you would get if you treated every cluster as a single observation. For intermediate ICCs, the sample size from cluster \\(j\\) (in the numerator of the fraction inside the summation) gets cut down to size accordingly.\nThe estimator of the population mean is a weighted average of the outcomes. Specifically, \\[\n\\hat\\beta = \\left(\\sum_{j=1}^J \\mathbf{1}_j'\\mathbf{\\hat{V}}_j^{-1} \\mathbf{1}_j \\right)^{-1} \\sum_{j=1}^J \\mathbf{1}_j'\\mathbf{\\hat{V}}_j^{-1} \\mathbf{Y}_j,\n\\] where \\(\\mathbf{\\hat{V}}_j\\) is an estimator of \\(\\mathbf{V}_j\\). If you carry through the matrix algebra, you’ll find that \\[\n\\begin{aligned}\n\\hat\\beta &= \\left(\\sum_{j=1}^J \\frac{n_j} {\\sigma^2 + n_j \\tau^2} \\right)^{-1} \\sum_{j=1}^J \\frac{\\mathbf{1}_j'\\mathbf{Y}_j}{\\sigma^2 + n_j \\tau^2} \\\\\n&= \\frac{1}{W} \\sum_{j=1}^J \\sum_{i=1}^{n_j} w_j y_{ij},\n\\end{aligned}\n\\] where \\(w_j = \\frac{1}{1 + (n_j - 1) \\rho}\\) and \\(\\displaystyle{W = \\sum_{j=1}^J n_j w_j}\\). From this, we can see that the weight of a given observation depends on the ICC and the size of the cluster. If the ICC is low, then weights will all be close to 1. For higher ICCs, observations in smaller clusters get proportionately more weight than observations in larger clusters."
  },
  {
    "objectID": "posts/Woodbury-identity/index.html#a-meta-analysis-example",
    "href": "posts/Woodbury-identity/index.html#a-meta-analysis-example",
    "title": "The Woodbury identity",
    "section": "A meta-analysis example",
    "text": "A meta-analysis example\nIn a previous post on multi-variate meta-analysis, I examined how weighting works in some multi-variate meta-analysis models, where you have multiple effect size estimates nested within a study. Letting \\(T_{ij}\\) denote effect size estimate \\(i\\) in study \\(j\\), for \\(i = 1,...,n_j\\) and \\(j = 1,...,J\\). The first model I considered in the previous post was \\[\nT_{ij} = \\mu + \\eta_j + \\nu_{ij} + e_{ij},\n\\] where \\(\\text{Var}(\\eta_j) = \\tau^2\\), \\(\\text{Var}(\\nu_{ij}) = \\omega^2\\), \\(\\text{Var}(e_{ij}) = V_j\\), treated as known, and \\(\\text{cor}(e_{hj}, e_{ij}) = \\rho\\) for some specified value of \\(\\rho\\).1 This model makes the simplifying assumptions that the effect sizes within a given study all have the same sampling variance, \\(V_j\\), and that there is a single correlation between pairs of outcomes from the same study, that is constant across all pairs of outcomes and across all studies.\nYou can write this model in matrix form as \\[\n\\mathbf{T}_j = \\mu \\mathbf{1}_j + \\eta_j \\mathbf{1}_j + \\boldsymbol\\nu_j + \\mathbf{e}_j,\n\\] where \\(\\text{Var}(\\boldsymbol\\nu_j) = \\omega^2 \\mathbf{I}_j\\) and \\(\\text{Var}(\\mathbf{e}_j) = V_j \\left[\\rho \\mathbf{1}_j \\mathbf{1}_j' + (1 - \\rho) \\mathbf{I}_j\\right]\\). It follows that \\[\n\\text{Var}(\\mathbf{T}_j) = (\\tau^2 + V_j\\rho) \\mathbf{1}_j \\mathbf{1}_j' + [\\omega^2 + V_j (1 - \\rho)] \\mathbf{I}_j.\n\\] The Woodbury identity comes in handy here again, if we want to examine the weights implied by this model or the sampling variance of the overall average effect size estimator.2 I’ll leave it as an exercise to find an expression for the weight assigned to effect size \\(T_{ij}\\) under this model.3 You could also try finding an expression for the variance of the overall average effect size estimator \\(\\hat\\mu\\), based on inverse-variance weighting, when the model is correctly specified."
  },
  {
    "objectID": "posts/Woodbury-identity/index.html#another-meta-analysis-example",
    "href": "posts/Woodbury-identity/index.html#another-meta-analysis-example",
    "title": "The Woodbury identity",
    "section": "Another meta-analysis example",
    "text": "Another meta-analysis example\nIn the previous post, I also covered weighting in a bit more general model, where the sampling variances and correlations are no longer quite so constrained. As before, we have \\[\n\\mathbf{T}_j = \\mu \\mathbf{1}_j + \\eta_j \\mathbf{1}_j + \\boldsymbol\\nu_j + \\mathbf{e}_j,\n\\] where \\(\\text{Var}(\\eta_j) = \\tau^2\\) and \\(\\text{Var}(\\boldsymbol\\nu_j) = \\omega^2 \\mathbf{I}_j\\). But now let \\(\\text{Var}(\\mathbf{e}_j) = \\boldsymbol\\Sigma_j\\) for some arbitrary, symmetric, invertible matrix \\(\\boldsymbol\\Sigma_j\\). The marginal variance of \\(\\mathbf{T}_j\\) is therefore \\[\n\\text{Var}(\\mathbf{T}_j) = \\tau^2\\mathbf{1}_j \\mathbf{1}_j' + \\omega^2 \\mathbf{I}_j + \\boldsymbol\\Sigma_j.\n\\] Let \\(\\mathbf{S}_j = \\left(\\omega^2 \\mathbf{I}_j + \\boldsymbol\\Sigma_j\\right)^{-1}\\). Try applying the Woodbury identity to invert \\(\\text{Var}(\\mathbf{T}_j)\\) in terms of \\(\\tau^2\\), \\(n_j\\), and \\(\\mathbf{S}_j\\). Then see if you can derive the weight assigned to effect \\(i\\) in study \\(j\\) under this model. See the previous post for the solution.4"
  },
  {
    "objectID": "posts/Woodbury-identity/index.html#footnotes",
    "href": "posts/Woodbury-identity/index.html#footnotes",
    "title": "The Woodbury identity",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThis model is what we call the “correlated-and-hierarchical effects model” in my paper (with Beth Tipton) on extending working models for robust variance estimation.↩︎\nOr squint hard at the formula for the variance of \\(\\mathbf{T}_j\\), and you’ll see that it has the same form as the random intercepts model in the previous example. Just replace the \\(\\tau^2\\) in that model with \\(\\tau^2 + V_j \\rho\\) and replace the \\(\\sigma^2\\) in that model with \\(\\omega^2 + V_j (1 - \\rho)\\).↩︎\nSee the previous post for the answer.↩︎\nIn the previous post, I expressed the weights in terms of \\(s_{ij}\\), the sum of the entries in row \\(i\\) of the \\(\\mathbf{S}_j\\) matrix. In vector form, \\(\\mathbf{s}_j = \\left(s_{1j} \\ s_{2j} \\ \\cdots \\ s_{n_j j}\\right)' = \\mathbf{S}_j \\mathbf{1}_j\\).↩︎"
  },
  {
    "objectID": "presentations/WISCC-2023.html",
    "href": "presentations/WISCC-2023.html",
    "title": "Determining the Timing of Phase Changes: Some Statistical Perspective",
    "section": "",
    "text": "Wing Institute Single Case Conference 2023, Nashville, TN\n Slides"
  },
  {
    "objectID": "presentations/UTAustin-2018-combining-RVE-with-models.html",
    "href": "presentations/UTAustin-2018-combining-RVE-with-models.html",
    "title": "Combining robust variance estimation with models for dependent effect sizes",
    "section": "",
    "text": "UT Austin, Developmental & Clinical Psychology Brownbag, Austin, TX\n Slides  Code"
  },
  {
    "objectID": "presentations/TUESAP-2014-construct-invalidity-of-PIR.html",
    "href": "presentations/TUESAP-2014-construct-invalidity-of-PIR.html",
    "title": "Addressing construct invalidity in partial interval recording data",
    "section": "",
    "text": "Texas Universities Educational Statistics and Psychometrics Meeting, Texas A&M University, College Station, TX\n Slides"
  },
  {
    "objectID": "presentations/SSCC-2022-State-of-SCD-synthesis.html",
    "href": "presentations/SSCC-2022-State-of-SCD-synthesis.html",
    "title": "The state of single case synthesis: Premises, tools, and possibilities",
    "section": "",
    "text": "Spencer Single Case Conference, Vanderbilt University, Nashville, TN\n Slides"
  },
  {
    "objectID": "presentations/SRSM-2019-generalized-excess-significance-test.html",
    "href": "presentations/SRSM-2019-generalized-excess-significance-test.html",
    "title": "A generalized excess significance test for selective outcome reporting with dependent effect sizes",
    "section": "",
    "text": "Society for Research Synthesis Methodology annual meeting, Chicago, IL\n Slides"
  },
  {
    "objectID": "presentations/SRSM-2015-small-sample-adjustments.html",
    "href": "presentations/SRSM-2015-small-sample-adjustments.html",
    "title": "Small-sample adjustments for multiple-contrast hypothesis tests of meta-regressions using robust variance estimation",
    "section": "",
    "text": "Society for Research Synthesis Methods annual meeting, Nashville, TN\n PDF  Slides"
  },
  {
    "objectID": "presentations/SREE-2017-small-sample-corrections.html",
    "href": "presentations/SREE-2017-small-sample-corrections.html",
    "title": "Small sample corrections for use of cluster-robust standard errors in the analysis of school-based experiments",
    "section": "",
    "text": "Society for Research on Educational Effectiveness Conference, Washington, DC\n Slides"
  },
  {
    "objectID": "presentations/SREE-2014-internal-validity-of-MBD.html",
    "href": "presentations/SREE-2014-internal-validity-of-MBD.html",
    "title": "On internal validity in multiple baseline designs",
    "section": "",
    "text": "Society for Research on Educational Effectiveness Conference, Washington, DC\n Slides"
  },
  {
    "objectID": "presentations/Small-is-Beautiful-2023-workshop.html",
    "href": "presentations/Small-is-Beautiful-2023-workshop.html",
    "title": "Calculating Effect Sizes for Single-Case Research: An Introduction to the SingleCaseES and scdhlm Web Applications and R Packages",
    "section": "",
    "text": "This workshop will provide an introduction to effect size calculations for single-case research designs, focused on two interactive web applications (or “apps”) and accompanying R packages. I will begin by describing how to organize raw data from a single-case or n-of-1 study for purposes of using the apps, as well as for archiving and sharing with the research community. I will then introduce the SingleCaseES app, which provides tools for calculating case-specific effect size indices such as the non-overlap of all pairs, within-case standardized mean difference, and log-response ratio. All of these effect sizes describe intervention effects at the level of the individual participant. I will demonstrate use of the SingleCaseES app for calculating an effect size from a single data series as well as for calculating one or multiple effect sizes across multiple data series (with the latter being especially useful when conducting meta-analyses across multiple cases and studies). In the final section of the workshop, I will introduce the scdhlm app, which provides an interface for calculating between-case standardized mean difference (BC-SMD, also known as “design-comparable”) effect sizes. BC-SMDs are study-level summary effect sizes that are theoretically comparable to effect size indices commonly used in between-group experimental designs. BC-SMDs are defined based on a hierarchical model for the data from a single-case design that includes multiple participants. I will discuss the data requirements, model-building process, and choice of summary effect size for calculating a BC-SMD, while demonstrating how to use the scdhlm app. In each section of the workshop, I will also show how the interactive apps can facilitate learning to carry out effect size calculations using reproducible R code."
  },
  {
    "objectID": "presentations/SIPS-2021-statistical-frontiers-for-selective-reporting.html",
    "href": "presentations/SIPS-2021-statistical-frontiers-for-selective-reporting.html",
    "title": "Statistical frontiers for selective reporting and publication bias",
    "section": "",
    "text": "This workshop will cover methods to investigate selective reporting in meta-analysis of statistically dependent effect sizes, which are a common feature of systematic reviews in psychology. The workshop is organized into two sections. In the first section, we will describe situations where dependent effect sizes occur and review methods for summarizing findings in the presence of dependent effects. We will then describe methods for creating and interpreting funnel plots, including tests of asymmetry, with dependent effect sizes. In the second section, we will present new statistical sensitivity analyses for publication bias, which perform well in small meta-analyses, those with non-normal or dependent effect sizes, and those with heterogeneity. The sensitivity analyses enable statements such as “For publication bias to shift the observed point estimate to the null, ‘significant’ results would need to be at least 10-fold more likely to be published than negative or ‘non-significant’ results” or “no amount of publication bias could explain away the average effect.” In both sections, we will demonstrate methods using R code and examples from real meta-analyses."
  },
  {
    "objectID": "presentations/OsloRUG-2022-clubSandwich.html",
    "href": "presentations/OsloRUG-2022-clubSandwich.html",
    "title": "Easy, cluster-robust standard errors with the clubSandwich package",
    "section": "",
    "text": "Cluster-robust variance estimation methods (also known as sandwich estimators, linearization estimators, or simply “clustered” standard errors) are a standard inferential tool in many different areas of applied statistics. They are appealing because they provide a means to do inference for regression models without relying on strong assumptions about the distribution or dependence structure of errors. However, standard cluster-robust variance estimators are based on large-sample approximations and can perform poorly when based on a small number of clusters. In this talk, I will provide an overview of some refinements to cluster-robust variance estimators, as implemented on the clubSandwich package (https://CRAN.R-project.org/package=clubSandwich), that perform well even with a limited number of clusters. I will provide a brief, high-level sketch of the theory behind the refined methods, discuss the practical rationale for using the methods, and demonstrate their application with the clubSandwich package, focusing in particular on linear mixed models. In addition to linear mixed models, the methods are available for a range of regression models and estimation methods, including ordinary least squares, weighted least squares, two-stage least squares, generalized linear models, and meta-regression models."
  },
  {
    "objectID": "presentations/NU-stats-2013-markov-models-for-direct-observation.html",
    "href": "presentations/NU-stats-2013-markov-models-for-direct-observation.html",
    "title": "Some Markov models for direct observation of behavior",
    "section": "",
    "text": "Department of Statistics, Northwestern University, Evanston, IL\n Slides"
  },
  {
    "objectID": "presentations/ISSNA-2008-question-order-effects.html",
    "href": "presentations/ISSNA-2008-question-order-effects.html",
    "title": "Question-order effects in social network name generators",
    "section": "",
    "text": "International Sunbelt Social Network Conference, St. Petersburg Beach, FL\n PDF"
  },
  {
    "objectID": "presentations/IES-2018-randomization-inference.html",
    "href": "presentations/IES-2018-randomization-inference.html",
    "title": "Randomization inference for single-case experimental designs",
    "section": "",
    "text": "Institute of Education Sciences 2018 Principal Investigators Meeting, Washington, DC\n Slides"
  },
  {
    "objectID": "presentations/IES-2016-single-case-effect-sizes.html",
    "href": "presentations/IES-2016-single-case-effect-sizes.html",
    "title": "Effect sizes for single-case research",
    "section": "",
    "text": "Institute of Education Sciences 2016 Principal Investigators Meeting, Washington, DC\n Slides"
  },
  {
    "objectID": "presentations/ESMARConf2021-RVE-with-metafor-and-clubSandwich.html",
    "href": "presentations/ESMARConf2021-RVE-with-metafor-and-clubSandwich.html",
    "title": "Synthesis of dependent effect sizes: Versatile models through metafor and clubSandwich",
    "section": "",
    "text": "Across scientific fields, large meta-analyses often involve dependent effect size estimates. Robust variance estimation (RVE) methods provide a way to include all dependent effect sizes in a single meta-analysis model, even when the nature of the dependence is unknown. RVE uses a working model of the dependence structure, but the two currently available working models (available in the robumeta package) are limited to each describing a single type of dependence. We describe a workflow combining two existing packages, metafor and clubSandwich, that can be used to implement an expanded set of working models, offering benefits in terms of better capturing the types of data structures that occur in practice and improving the efficiency of meta-analytic model estimates."
  },
  {
    "objectID": "presentations/AIR-2016-when-large-samples-act-small.html",
    "href": "presentations/AIR-2016-when-large-samples-act-small.html",
    "title": "When large samples act small: The importance of small-sample adjustments for cluster-robust inference in impact evaluations",
    "section": "",
    "text": "American Institutes for Research Impact Working Group Lecture Series, Austin, TX\n Slides"
  },
  {
    "objectID": "presentations/AERA-2023-replication-discussion.html",
    "href": "presentations/AERA-2023-replication-discussion.html",
    "title": "Discussion of ‘Moving from What Works to What Replicates: Promoting the Systematic Replication of Results.’",
    "section": "",
    "text": "American Educational Research Association annual convention, Chicago, IL\n Slides"
  },
  {
    "objectID": "presentations/AERA-2019-ORB-dependence.html",
    "href": "presentations/AERA-2019-ORB-dependence.html",
    "title": "Evaluating meta-analytic methods to detect outcome reporting bias in the presence of dependent effect sizes",
    "section": "",
    "text": "American Educational Research Association annual convention, Toronto, Ontario\n PDF"
  },
  {
    "objectID": "presentations/AERA-2018-dependent-effects.html",
    "href": "presentations/AERA-2018-dependent-effects.html",
    "title": "Meta-analysis of dependent effects: A review and consolidation of methods",
    "section": "",
    "text": "American Educational Research Association annual convention, New York, NY\n Slides"
  },
  {
    "objectID": "presentations/AERA-2017-intervention-analysis.html",
    "href": "presentations/AERA-2017-intervention-analysis.html",
    "title": "A nonlinear intervention analysis model for treatment reversal single-case designs",
    "section": "",
    "text": "American Educational Research Association annual convention, San Antonio, TX\n PDF"
  },
  {
    "objectID": "presentations/AERA-2015-small-sample-adjustments.html",
    "href": "presentations/AERA-2015-small-sample-adjustments.html",
    "title": "Small-sample adjustments for F-tests using robust variance estimation in meta-regression",
    "section": "",
    "text": "American Educational Research Association annual convention, Chicago, IL\n PDF  Slides"
  },
  {
    "objectID": "presentations/AERA-2015-observation-procedures.html",
    "href": "presentations/AERA-2015-observation-procedures.html",
    "title": "Observation procedures and Markov Chain models for estimating the prevalence and incidence of a state behavior",
    "section": "",
    "text": "American Educational Research Association annual convention, Chicago, IL\n PDF  Slides"
  },
  {
    "objectID": "presentations/AERA-2013-observation-procedures.html",
    "href": "presentations/AERA-2013-observation-procedures.html",
    "title": "Observation procedures and Markov chain models for estimating the prevalence and incidence of a behavior",
    "section": "",
    "text": "American Educational Research Association annual convention, San Francisco, CA\n PDF"
  },
  {
    "objectID": "presentations/ABAI-2019-log-response-ratios.html",
    "href": "presentations/ABAI-2019-log-response-ratios.html",
    "title": "Log response ratio effect sizes: Rationale and methods for single case designs with behavioral outcomes",
    "section": "",
    "text": "Association for Behavior Analysis International Annual Convention, Chicago, IL\n Slides"
  },
  {
    "objectID": "presentations/AERA-2014-four-methods-for-PIR.html",
    "href": "presentations/AERA-2014-four-methods-for-PIR.html",
    "title": "Four methods of analyzing partial interval recording data, with application to single-case research",
    "section": "",
    "text": "American Educational Research Association annual convention, Philadelphia, PA\n PDF"
  },
  {
    "objectID": "presentations/AERA-2015-operational-sensitivities.html",
    "href": "presentations/AERA-2015-operational-sensitivities.html",
    "title": "Operational sensitivities of non-overlap effect sizes for single-case experimental designs",
    "section": "",
    "text": "American Educational Research Association annual convention, Chicago, IL\n PDF"
  },
  {
    "objectID": "presentations/AERA-2017-HC-t-tests.html",
    "href": "presentations/AERA-2017-HC-t-tests.html",
    "title": "Heteroskedasticity-robust tests in linear regression: A review and evaluation of small-sample corrections",
    "section": "",
    "text": "American Educational Research Association annual convention, San Antonio, TX\n Slides"
  },
  {
    "objectID": "presentations/AERA-2017-response-ratios.html",
    "href": "presentations/AERA-2017-response-ratios.html",
    "title": "Using response ratios for meta-analyzing single-case designs with behavioral outcomes",
    "section": "",
    "text": "American Educational Research Association annual convention, San Antonio, TX\n PDF"
  },
  {
    "objectID": "presentations/AERA-2018-meta-analysis-of-single-case-research.html",
    "href": "presentations/AERA-2018-meta-analysis-of-single-case-research.html",
    "title": "Meta-analysis of single-case research: A brief and breezy tour",
    "section": "",
    "text": "American Educational Research Association annual convention, New York, NY\n Slides"
  },
  {
    "objectID": "presentations/AERA-2023-BCSMD-benchmarks.html",
    "href": "presentations/AERA-2023-BCSMD-benchmarks.html",
    "title": "Empirical benchmarks for between-case standardized mean differences from single-case multiple baseline designs examining academic interventions.",
    "section": "",
    "text": "The between-case standardized mean difference (BC-SMD) is an effect size measure for single-case designs (SCDs) that it puts findings on the same scale as standardized mean differences for between-group designs. Although the BC-SMD is being applied in practice, the field still lacks guidelines for interpreting its magnitude. We develop empirical benchmarks for BC-SMDs from multiple baseline and multiple probe SCDs examining academic interventions. Drawing on a large corpus of SCDs identified using a systematic search process, we develop benchmarks for multiple baseline and multiple probe designs that examine the effects of math, reading, and writing-related interventions for students in school settings. We find distributions of effect sizes that are broader and more dispersed than distributions observed in group designs."
  },
  {
    "objectID": "presentations/AERA-2024-Bayes-BCSMD.html",
    "href": "presentations/AERA-2024-Bayes-BCSMD.html",
    "title": "Bayesian estimation of between-case standardized mean differences: A simulation study",
    "section": "",
    "text": "American Educational Research Association annual convention, Philadelphia, PA\n PDF"
  },
  {
    "objectID": "presentations/EdPsych-Colloquium-2021-Four-Things.html",
    "href": "presentations/EdPsych-Colloquium-2021-Four-Things.html",
    "title": "Four things every quantitative social scientist should know about meta-analysis",
    "section": "",
    "text": "Meta-analysis is a set of statistical tools for synthesizing results across multiple sources of evidence. Meta-analyses of intervention research are often taken as a gold standard for informing evidence-based practice, yet they are also frequently misinterpreted. In this talk, Pustejovsky will highlight four conceptual issues that arise in conducting and interpreting findings from contemporary meta-analyses: a) the interpretation of heterogeneous effects, b) the challenge of defining inclusion criteria, c) prospects and limitations of moderator analysis, and d) limitations of aggregated data. Understanding these issues will help social scientists both to be more critical consumers of research syntheses and to improve how they design and conduct their own syntheses. On each issue, he will also highlight outstanding methodological challenges in need of further investigation."
  },
  {
    "objectID": "presentations/ESMARConf2023-clustered-bootstrap-selection-model.html",
    "href": "presentations/ESMARConf2023-clustered-bootstrap-selection-model.html",
    "title": "Clustered bootstrapping for selective reporting models in meta-analysis with dependent effects",
    "section": "",
    "text": "In many fields, quantitative meta-analyses involve dependent effect sizes, which occur when primary studies included in a synthesis contain more than one relevant estimate of the relation between constructs. When using meta-analysis methods to summarize findings or examine moderators, analysts can now apply well-established methods for handling dependent effect sizes. However, very few methods are available for examining publication bias issues when the data also include dependent effect sizes. Furthermore, applying existing tools for publication bias assessment without accounting for effect size dependency can produce misleading conclusions (e.g., too-narrow confidence intervals, hypothesis tests with inflated Type I error). In this presentation, we explore a potential solution: clustered bootstrapping, a general-purpose technique for quantifying uncertainty in data with clustered structures, which can be combined with many existing analytic models. We demonstrate how to implement the clustered bootstrap in combination with existing publication bias assessment techniques like selection models, PET-PEESE, trim-and-fill, or kinked meta-regression. After providing a brief introduction to the theory of bootstrapping, we will develop and demonstrate example code using existing R packages, including boot and metafor. Time permitting, we will also share findings from ongoing methodological studies on the performance of clustered bootstrap selection models."
  },
  {
    "objectID": "presentations/IES-2018-gradual-effects-model.html",
    "href": "presentations/IES-2018-gradual-effects-model.html",
    "title": "A gradual effects model for single case designs",
    "section": "",
    "text": "Institute of Education Sciences 2018 Principal Investigators Meeting, Washington, DC\n PDF"
  },
  {
    "objectID": "presentations/JSM-2016-small-sample-CRVE.html",
    "href": "presentations/JSM-2016-small-sample-CRVE.html",
    "title": "Small-sample methods for cluster-robust variance estimation and hypothesis testing in fixed effects models",
    "section": "",
    "text": "Joint Statistical Meetings, Chicago, IL\n Slides"
  },
  {
    "objectID": "presentations/OsloRUG-2021-RVE-with-metafor-and-clubSandwich.html",
    "href": "presentations/OsloRUG-2021-RVE-with-metafor-and-clubSandwich.html",
    "title": "Synthesis of dependent effect sizes: Robust variance estimation with clubSandwich",
    "section": "",
    "text": "Large meta-analyses often involve dependent effect sizes, but where the exact form of the dependence is unknown. Meta-analysis with robust variance estimation handles this problem through specification of a working model for the dependence, which need not be correct. However, the two currently available working models are limited to each describing a single type of dependence. James will demonstrate a workflow for implementing an expanded set of working models by combining the metafor and clubSandwich R packages."
  },
  {
    "objectID": "presentations/PRC-2016-when-large-samples-act-small.html",
    "href": "presentations/PRC-2016-when-large-samples-act-small.html",
    "title": "When large samples act small: Cluster-robust variance estimation and hypothesis testing with few clusters",
    "section": "",
    "text": "Population Research Center colloquium, Austin, TX\n Slides"
  },
  {
    "objectID": "presentations/Small-is-Beautiful-2023-keynote.html",
    "href": "presentations/Small-is-Beautiful-2023-keynote.html",
    "title": "Effect size measures for single-case research: Conceptual, practical, and statistical considerations",
    "section": "",
    "text": "Quantitative analysis of single-case research and n-of-1 experiments often focuses on calculation of effect size measures, or numerical indices describing the direction and strength of an intervention’s effect on an outcome for the participating cases. Many different effect size measures have been proposed specifically for use with single-case research designs—so many, in fact, that researchers may find it difficult to choose among the wide array of options that have been proposed. In this talk, I will first discuss the purpose of using effect size measures and highlight some conceptual considerations that should inform the choice among available measures. I argue that effect size measures should be selected by considering the properties of the outcome variable, the anticipated form of intervention’s effect on that outcome, and the level of analysis (i.e., individual-level or study-level) in order to identify an index that is interpretable and can be meaningfully compared from one study to another. I then survey some of the available effect size measures, highlight practical tools for carrying out the calculations, and discuss some of the statistical issues that arise in applying effect sizes to data from single-case and n-of-1 designs."
  },
  {
    "objectID": "presentations/SREE-2013-operationally-comparable-effect-sizes.html",
    "href": "presentations/SREE-2013-operationally-comparable-effect-sizes.html",
    "title": "Operationally comparable effect sizes for meta-analysis of single-case research",
    "section": "",
    "text": "Society for Research on Educational Effectiveness Conference, Washington, DC\n Slides"
  },
  {
    "objectID": "presentations/SREE-2015-small-sample-adjustments.html",
    "href": "presentations/SREE-2015-small-sample-adjustments.html",
    "title": "Small-sample adjustments for tests of moderators and model fit using robust variance estimation in meta-regression",
    "section": "",
    "text": "Society for Research on Educational Effectiveness Conference, Washington, DC\n Slides"
  },
  {
    "objectID": "presentations/SREE-2019-2SLS-CRVE.html",
    "href": "presentations/SREE-2019-2SLS-CRVE.html",
    "title": "Small-sample cluster-robust variance estimators for two-stage least squares models",
    "section": "",
    "text": "Society for Research on Educational Effectiveness Conference, Washington, DC\n Slides"
  },
  {
    "objectID": "presentations/SREE-2023-Stabilizing-performance-measures-discussion.html",
    "href": "presentations/SREE-2023-Stabilizing-performance-measures-discussion.html",
    "title": "Discussion of Stabilizing measures to reconcile accuracy and equity in performance measurement",
    "section": "",
    "text": "Society for Research on Educational Effectiveness Conference, Arlington, VA\n Slides"
  },
  {
    "objectID": "presentations/SRSM-2018-combining-RVE-with-models.html",
    "href": "presentations/SRSM-2018-combining-RVE-with-models.html",
    "title": "Combining robust variance estimation with models for dependent effect sizes",
    "section": "",
    "text": "Society for Research Synthesis Methodology annual meeting, Bristol, UK\n Slides"
  },
  {
    "objectID": "presentations/SRSM-2022-matter-of-emphasis.html",
    "href": "presentations/SRSM-2022-matter-of-emphasis.html",
    "title": "A matter of emphasis: Comparison of working models for meta-analysis of dependent effect sizes",
    "section": "",
    "text": "Society for Research Synthesis Methodology annual meeting, Portland, OR (and online)\n Slides"
  },
  {
    "objectID": "presentations/Stanford-QSU-2022-selective-reporting.html",
    "href": "presentations/Stanford-QSU-2022-selective-reporting.html",
    "title": "Selective reporting in meta-analysis of dependent effect size estimates",
    "section": "",
    "text": "Publication bias and other forms of selective outcome reporting are important threats to the validity of findings from research syntheses—even undermining their special status for informing evidence-based practice and policy guidance. An array of methods have been proposed for detecting selective outcome reporting, but nearly all of the available statistical tests are premised on the assumption that each study contributes a single effect size, which is statistically independent of the other effect sizes in the analysis. In practice, however, it is very common for meta-analyses to include studies that contribute multiple, statistically dependent effect sizes (e.g., effect sizes for multiple, related outcome measures, effect sizes at different follow-up times, or effect sizes from multiple replications based on a common protocol). In this talk, I will review these issues and describe the range of methods that synthesists currently use to examine selective reporting issues under effect size dependence. I then describe a new test for diagnosing selective reporting by comparing the observed number of statistically significant effect sizes to the number expected based on the power of included studies to detect the estimated average effect. This test generalizes the Test of Excess Significance (TES; Ioannidis & Trikalinos, 2007) and is closely related to the score test under a simple version of the Vevea and Hedges (1995) selection model. It uses cluster-robust sandwich estimation methods to handle dependence of effect sizes nested within studies. I report some simulation evidence on the power of this new test relative to existing alternatives and discuss further directions for investigating selective reporting issues in meta-analysis."
  },
  {
    "objectID": "presentations/UT-Austin-2012-meta-analysis-of-single-case-research.html",
    "href": "presentations/UT-Austin-2012-meta-analysis-of-single-case-research.html",
    "title": "Some implications of behavioral observation procedures for meta-analysis of single-case research",
    "section": "",
    "text": "Presentation at the Department of Educational Psychology, College of Education, University of Texas at Austin, Austin, TX\n Slides"
  },
  {
    "objectID": "people/Bethany-H-Bhat/index.html",
    "href": "people/Bethany-H-Bhat/index.html",
    "title": "Bethany Hamilton Bhat",
    "section": "",
    "text": "Bethany Hamilton Bhat is a doctoral candidate in the Quantitative Methods area in the Educational Psychology Department at the University of Texas at Austin. Her research interests include meta-analysis, hierarchical linear modeling, and Bayesian inference.\n\n\n\n\nPhD in Quantitative Methods | 2025 (expected)  University of Texas at Austin\nBA in Plan II Honors | 2014  The University of Texas at Austin\n\n\n\n\n\nMeta-analysis\nHierarchical linear modeling\nBayesian inference"
  },
  {
    "objectID": "people/Bethany-H-Bhat/index.html#education",
    "href": "people/Bethany-H-Bhat/index.html#education",
    "title": "Bethany Hamilton Bhat",
    "section": "",
    "text": "PhD in Quantitative Methods | 2025 (expected)  University of Texas at Austin\nBA in Plan II Honors | 2014  The University of Texas at Austin"
  },
  {
    "objectID": "people/Bethany-H-Bhat/index.html#interests",
    "href": "people/Bethany-H-Bhat/index.html#interests",
    "title": "Bethany Hamilton Bhat",
    "section": "",
    "text": "Meta-analysis\nHierarchical linear modeling\nBayesian inference"
  },
  {
    "objectID": "teaching/Quasi-Experimental/index.html",
    "href": "teaching/Quasi-Experimental/index.html",
    "title": "Design & Analysis of Quasi-Experiments for Causal Inference",
    "section": "",
    "text": "The course begins with an introduction to the potential outcomes framework for expressing causal quantities, followed by an examination of (idealized) simple and block randomized experiments as prototypes for learning about causal effects. The remainder of the course covers theory and data-analysis strategies for drawing causal inferences from four quasi-experimental designs: instrumental variables approaches, regression discontinuity designs, non-equivalent control group designs (using techniques such as matching and propensity score weighting), and comparative interrupted time series designs. For each design, we will consider (i) the core strategy for identifying a causal effect, (ii) corresponding statistical approaches for estimating the effect, and (iii) strategies and design elements for strengthening the design. Further, advanced topics will be covered based on student interest.\n\n2023 (Fall) syllabus and reading list\n2021 (Fall) syllabus and reading list"
  },
  {
    "objectID": "people/Melissa-A-Rodgers/index.html",
    "href": "people/Melissa-A-Rodgers/index.html",
    "title": "Melissa A. Rodgers",
    "section": "",
    "text": "Melissa A. Rodgers is a doctoral student in the Quantitative Methods area in the Educational Psychology Department at The University of Texas at Austin. Her research interests include meta-analysis, selective reporting detection tests and adjustment methods, and handling dependent effects in meta-analysis. She is also a quantitative researcher at the American Institutes for Research.\n\n\n\n\nPhD in Quantitative Methods | 2024 (expected)  The University of Texas at Austin\nMEd in Quantitative Methods | 2016  The University of Texas at Austin\n\n\n\n\n\nMeta-analysis\nSelective reporting detection tests and adjustment methods\nDependent effect sizes"
  },
  {
    "objectID": "people/Melissa-A-Rodgers/index.html#education",
    "href": "people/Melissa-A-Rodgers/index.html#education",
    "title": "Melissa A. Rodgers",
    "section": "",
    "text": "PhD in Quantitative Methods | 2024 (expected)  The University of Texas at Austin\nMEd in Quantitative Methods | 2016  The University of Texas at Austin"
  },
  {
    "objectID": "people/Melissa-A-Rodgers/index.html#interests",
    "href": "people/Melissa-A-Rodgers/index.html#interests",
    "title": "Melissa A. Rodgers",
    "section": "",
    "text": "Meta-analysis\nSelective reporting detection tests and adjustment methods\nDependent effect sizes"
  },
  {
    "objectID": "posts/Designing-simulation-studies-using-R/index.html",
    "href": "posts/Designing-simulation-studies-using-R/index.html",
    "title": "Designing simulation studies using R",
    "section": "",
    "text": "Here are the slides from my presentation at this afternoon’s Quant. Methods brown bag. I gave a very quick introduction to using R for conducting simulation studies. I hope it was enough to get people intrigued about the possibilities of using R in their own work.\nThe second half of the presentation sketched out a quick-and-dirty simulation of the Behrens-Fisher problem, or more specifically the coverage rates of 95% confidence intervals using Welch’s degrees of freedom approximation, given independent samples with unequal variances. Here is the complete code. As I mentioned in the talk, there’s lots of room for improvement. The main point that I was trying to illustrate is that simulations have five distinct pieces:\n\na data generating model,\nan estimation procedure,\nperformance criteria,\nan experimental design (parameter values and sample dimensions), and\nanalysis and results.\n\nIt is useful to write simulation code that reflects the structure, so that it is easy for you (or other people) to read, revise, extend, or re-run it. And then post it on your blog.\n\n\n\n\n\nReuseCC BY 4.0CitationBibTeX citation:@online{pustejovsky2013,\n  author = {Pustejovsky, James E.},\n  title = {Designing Simulation Studies Using {R}},\n  date = {2013-12-06},\n  url = {https://jepusto.com/posts/Designing-simulation-studies-using-R},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nPustejovsky, James E. 2013. “Designing Simulation Studies Using\nR.” December 6, 2013. https://jepusto.com/posts/Designing-simulation-studies-using-R."
  },
  {
    "objectID": "temp-posts.html",
    "href": "temp-posts.html",
    "title": "Posts",
    "section": "",
    "text": "Posts\n\n\n\n\n\n\n\n\n\n\n\nJames E. Pustejovsky\n\n\n\n\n\n\n\n\n\n\n\n\nDistribution of the number of significant effect sizes\n\n\nIn a study reporting multiple outcomes\n\n\n\neffect size\n\n\ndistribution theory\n\n\n\n\n\n\n\n\n\nMar 28, 2024\n\n\nJames E. Pustejovsky\n\n\n\n\n\n\n\n\n\n\n\n\nApproximating the distribution of cluster-robust Wald statistics\n\n\n\n\n\n\nrobust variance estimation\n\n\ndistribution theory\n\n\n\n\n\n\n\n\n\nMar 24, 2024\n\n\nJames E. Pustejovsky\n\n\n\n\n\n\n\n\n\n\n\n\nImplementing Consul’s generalized Poisson distribution in Stan\n\n\n\n\n\n\nBayes\n\n\nsimulation\n\n\ndistribution-theory\n\n\ngeneralized linear model\n\n\nprogramming\n\n\nRstats\n\n\n\n\n\n\n\n\n\nDec 6, 2023\n\n\nJames E. Pustejovsky\n\n\n\n\n\n\n\n\n\n\n\n\nImplementing Efron’s double Poisson distribution in Stan\n\n\n\n\n\n\nBayes\n\n\nsimulation\n\n\ndistribution-theory\n\n\ngeneralized linear model\n\n\nprogramming\n\n\nRstats\n\n\n\n\n\n\n\n\n\nSep 15, 2023\n\n\nJames E. Pustejovsky\n\n\n\n\n\n\n\n\n\n\n\n\nCluster-Bootstrapping a meta-analytic selection model\n\n\n\n\n\n\nbootstrap\n\n\ndependent effect sizes\n\n\nmeta-analysis\n\n\npublication bias\n\n\nprogramming\n\n\nRstats\n\n\n\nIn this post, we will sketch out what we think is a promising and pragmatic method for examining selective reporting while also accounting for effect size dependency. The method is to use a cluster-level bootstrap, which involves re-sampling clusters of observations to approximate the sampling distribution of an estimator. To illustrate this technique, we will demonstrate how to bootstrap a Vevea-Hedges selection model.\n\n\n\n\n\nMar 30, 2023\n\n\nJames E. Pustejovsky, James E. Pustejovsky, Megha Joshi\n\n\n\n\n\n\n\n\n\n\n\n\nCohen’s \\(d_z\\) makes me dizzy when considering measurement error\n\n\n\n\n\n\neffect size\n\n\nstandardized mean difference\n\n\ndesign-comparable SMD\n\n\nmeasurement-error\n\n\n\nMeta-analyses in education, psychology, and related fields rely heavily of Cohen’s \\(d\\), or the standardized mean difference effect size, for quantitatively describing the magnitude and direction of intervention effects. In these fields, Cohen’s \\(d\\) is so pervasive that its use is nearly automatic, and analysts rarely question its utility or consider alternatives (response ratios, anyone? POMP?). Despite this state of affairs, working with Cohen’s \\(d\\) is theoretically challenging because the standardized mean difference metric does not have a singular definition. Rather, its definition depends on the choice of the standardizing variance used in the denominator.\n\n\n\n\n\nFeb 17, 2023\n\n\nJames E. Pustejovsky\n\n\n\n\n\n\n\n\n\n\n\n\nCorrigendum to Pustejovsky and Tipton (2018), redux\n\n\nA revised version of Theorem 2\n\n\n\nrobust variance estimation\n\n\neconometrics\n\n\nmatrix algebra\n\n\n\nIn my 2018 paper with Beth Tipton, published in the Journal of Business and Economic Statistics, we considered how to do cluster-robust variance estimation in fixed effects models estimated by weighted (or unweighted) least squares. We were recently alerted that Theorem 2 in the paper is incorrect as stated. It turns out, the conditions in the original version of the theorem are too general. A more limited version of the Theorem does actually hold, but only for models estimated using ordinary (unweighted) least squares, under a working model that assumes independent, homoskedastic errors. In this post, I’ll give the revised theorem, following the notation and setup of the previous post (so better read that first, or what follows won’t make much sense!).\n\n\n\n\n\nNov 7, 2022\n\n\nJames E. Pustejovsky\n\n\n\n\n\n\n\n\n\n\n\n\nCorrigendum to Pustejovsky and Tipton (2018)\n\n\nTheorem 2 is incorrect as stated\n\n\n\nrobust variance estimation\n\n\neconometrics\n\n\nmatrix algebra\n\n\n\nIn my 2018 paper with Beth Tipton, published in the Journal of Business and Economic Statistics, we considered how to do cluster-robust variance estimation in fixed effects models estimated by weighted (or unweighted) least squares. A careful reader recently alerted us to a problem with Theorem 2 in the paper, which concerns a computational short cut for a certain cluster-robust variance estimator in models with cluster-specific fixed effects. The theorem is incorrect as stated, and we are currently working on issuing a correction for the published version of the paper. In the interim, this post details the problem with Theorem 2. I’ll first review the CR2 variance estimator, then describe the assertion of the theorem, and then provide a numerical counter-example demonstrating that the assertion is not correct as stated.\n\n\n\n\n\nSep 28, 2022\n\n\nJames E. Pustejovsky\n\n\n\n\n\n\n\n\n\n\n\n\nVariance component estimates in meta-analysis with mis-specified sampling correlation\n\n\n\n\n\n\nmeta-analysis\n\n\ndependent effect sizes\n\n\ndistribution theory\n\n\nhierarchical models\n\n\n\n\n\n\n\n\n\nNov 28, 2021\n\n\nJames E. Pustejovsky\n\n\n\n\n\n\n\n\n\n\n\n\nImplications of mean-variance relationships for standardized mean differences\n\n\n\n\n\n\nstandardized mean difference\n\n\nresponse ratio\n\n\ndistribution theory\n\n\nmeta-analysis\n\n\n\nA question came up on the R-SIG-meta-analysis listserv about whether it was reasonable to use the standardized mean difference metric for synthesizing studies where the outcomes are measured as proportions. I think this is an interesting question because, while the SMD could work perfectly fine as an effect size metric for proportions, there are also other alternatives that could be considered, such as odds ratios or response ratios or raw differences in proportions. Further, there are some situations where the SMD has disadvantages for synthesizing contrasts between proportions. Thus, it’s a situation where one has to make a choice about the effect size metric, and where the most common metric (the SMD) might not be the right answer. In this post, I want to provide a bit more detail regarding why I think mean-variance relationships in raw data can signal that the standardized mean differences might be less useful as an effect size metric compared to alternatives.\n\n\n\n\n\nNov 2, 2021\n\n\nJames E. Pustejovsky\n\n\n\n\n\n\n\n\n\n\n\n\nInverting partitioned matrices\n\n\n\n\n\n\nmatrix algebra\n\n\n\nThere’s lots of linear algebra out there that’s quite useful for statistics, but that I never learned in school or never had cause to study in depth. In the same spirit as my previous post on the Woodbury identity, I thought I would share my notes on another helpful bit of math about matrices. At some point in high school or college, you might have learned how to invert a small matrix by hand. It turns out that there’s a straight-forward generalization of this formula to matrices of arbitrary size, but that are partitioned into four pieces.\n\n\n\n\n\nOct 20, 2021\n\n\nJames E. Pustejovsky\n\n\n\n\n\n\n\n\n\n\n\n\nStandardized mean differences in single-group, repeated measures designs\n\n\n\n\n\n\neffect size\n\n\nstandardized mean difference\n\n\ndistribution theory\n\n\n\n\n\n\n\n\n\nOct 6, 2021\n\n\nJames E. Pustejovsky\n\n\n\n\n\n\n\n\n\n\n\n\nFinding the distribution of significant effect sizes\n\n\nIn a study reporting multiple outcomes\n\n\n\neffect size\n\n\ndistribution theory\n\n\n\n\n\n\n\n\n\nApr 27, 2021\n\n\nJames E. Pustejovsky\n\n\n\n\n\n\n\n\n\n\n\n\nThe Woodbury identity\n\n\nA life-hack for analyzing hierarchical models\n\n\n\nhierarchical models\n\n\nmatrix algebra\n\n\n\n\n\n\n\n\n\nDec 4, 2020\n\n\nJames E. Pustejovsky\n\n\n\n\n\n\n\n\n\n\n\n\nAn ANCOVA puzzler\n\n\n\n\n\n\nmeta-analysis\n\n\neffect size\n\n\nstandardized mean difference\n\n\n\n\n\n\n\n\n\nNov 24, 2020\n\n\nJames E. Pustejovsky\n\n\n\n\n\n\n\n\n\n\n\n\nFrom Longhorn to Badger\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAug 28, 2020\n\n\nJames E. Pustejovsky\n\n\n\n\n\n\n\n\n\n\n\n\nWhat do meta-analysts mean by ‘multivariate’ meta-analysis?\n\n\n\n\n\n\nmeta-analysis\n\n\nmultivariate\n\n\ndependent effect sizes\n\n\n\n\n\n\n\n\n\nJun 27, 2020\n\n\nJames E. Pustejovsky\n\n\n\n\n\n\n\n\n\n\n\n\nWeighting in multivariate meta-analysis\n\n\n\n\n\n\nmeta-analysis\n\n\nweighting\n\n\n\nOne common question about multivariate/multi-level meta-analysis is how such models assign weight to individual effect size estimates. When a version of the question came up recently on the R-sig-meta-analysis listserv, Dr. Wolfgang Viechtbauer offered a whole blog post in reply, demonstrating how weights work in simpler fixed effect and random effects meta-analysis and then how things get more complicated in multivariate models. In this post, I’ll try to add some further intuition on how weights work in certain multivariate meta-analysis models. Most of the discussion will apply to models that include multiple level of random effects, but no predictors. I’ll also comment briefly on meta-regression models with only study-level predictor variables, and finally give some pointers to work on more complicated models.\n\n\n\n\n\nJun 9, 2020\n\n\nJames E. Pustejovsky\n\n\n\n\n\n\n\n\n\n\n\n\nAn update on code folding with blogdown + Academic theme\n\n\n\n\n\n\nprogramming\n\n\nRstats\n\n\n\n\n\n\n\n\n\nMay 3, 2020\n\n\nJames E. Pustejovsky\n\n\n\n\n\n\n\n\n\n\n\n\nSimulating correlated standardized mean differences for meta-analysis\n\n\n\n\n\n\neffect size\n\n\nstandardized mean difference\n\n\nmeta-analysis\n\n\nsimulation\n\n\nprogramming\n\n\ndistribution theory\n\n\n\n\n\n\n\n\n\nSep 30, 2019\n\n\nJames E. Pustejovsky\n\n\n\n\n\n\n\n\n\n\n\n\nSometimes, aggregating effect sizes is fine\n\n\n\n\n\n\neffect size\n\n\nmeta-analysis\n\n\ndependent effect sizes\n\n\n\n\n\n\n\n\n\nJul 2, 2019\n\n\nJames E. Pustejovsky\n\n\n\n\n\n\n\n\n\n\n\n\nCode folding with blogdown + Academic theme\n\n\n\n\n\n\nprogramming\n\n\nRstats\n\n\n\n\n\n\n\n\n\nApr 14, 2019\n\n\nJames E. Pustejovsky\n\n\n\n\n\n\n\n\n\n\n\n\nCRAN downloads of my packages\n\n\n\n\n\n\nprogramming\n\n\nRstats\n\n\nmeta-analysis\n\n\n\n\n\n\n\n\n\nApr 9, 2019\n\n\nJames E. Pustejovsky\n\n\n\n\n\n\n\n\n\n\n\n\nSystematic Reviews and Meta-analysis SIG at AERA 2019\n\n\n\n\n\n\nmeta-analysis\n\n\nAERA\n\n\n\n\n\n\n\n\n\nMar 26, 2019\n\n\nJames E. Pustejovsky\n\n\n\n\n\n\n\n\n\n\n\n\nA handmade clubSandwich for multi-site trials\n\n\n\n\n\n\nsandwiches\n\n\nrobust variance estimation\n\n\neconometrics\n\n\nweighting\n\n\n\n\n\n\n\n\n\nMar 9, 2019\n\n\nJames E. Pustejovsky\n\n\n\n\n\n\n\n\n\n\n\n\nEffective sample size aggregation\n\n\n\n\n\n\neconometrics\n\n\ncausal inference\n\n\nweighting\n\n\n\n\n\n\n\n\n\nJan 22, 2019\n\n\nJames E. Pustejovsky\n\n\n\n\n\n\n\n\n\n\n\n\nEasily simulate thousands of single-case designs\n\n\n\n\n\n\nbehavioral observation\n\n\nsimulation\n\n\nsingle-case design\n\n\n\n\n\n\n\n\n\nJun 21, 2018\n\n\nJames E. Pustejovsky\n\n\n\n\n\n\n\n\n\n\n\n\nNew paper: A gradual effects model for single-case designs\n\n\n\n\n\n\nsingle-case design\n\n\neffect size\n\n\ngeneralized linear model\n\n\n\n\n\n\n\n\n\nMay 14, 2018\n\n\nJames E. Pustejovsky\n\n\n\n\n\n\n\n\n\n\n\n\nclubSandwich at the Austin R User Group Meetup\n\n\n\n\n\n\nRstats\n\n\nrobust variance estimation\n\n\nsandwiches\n\n\n\n\n\n\n\n\n\nApr 26, 2018\n\n\nJames E. Pustejovsky\n\n\n\n\n\n\n\n\n\n\n\n\nSampling variance of Pearson r in a two-level design\n\n\n\n\n\n\neffect size\n\n\ncorrelation\n\n\nmeta-analysis\n\n\ndelta method\n\n\ndistribution theory\n\n\n\n\n\n\n\n\n\nApr 19, 2018\n\n\nJames E. Pustejovsky\n\n\n\n\n\n\n\n\n\n\n\n\nThe multivariate delta method\n\n\n\n\n\n\ndelta method\n\n\ndistribution theory\n\n\n\n\n\n\n\n\n\nApr 11, 2018\n\n\nJames E. Pustejovsky\n\n\n\n\n\n\n\n\n\n\n\n\nNew paper: Using response ratios for meta-analyzing SCDs with behavioral outcomes\n\n\n\n\n\n\nsingle-case design\n\n\neffect size\n\n\nresponse ratio\n\n\nmeta-analysis\n\n\n\n\n\n\n\n\n\nMar 16, 2018\n\n\nJames E. Pustejovsky\n\n\n\n\n\n\n\n\n\n\n\n\nNew paper: procedural sensitivities of effect size measures for SCDs\n\n\n\n\n\n\nsingle-case design\n\n\neffect size\n\n\nresponse ratio\n\n\nnon-overlap measures\n\n\nsimulation\n\n\n\n\n\n\n\n\n\nJan 11, 2018\n\n\nJames E. Pustejovsky\n\n\n\n\n\n\n\n\n\n\n\n\nBack from the IES PI meeting\n\n\n\n\n\n\nsingle-case design\n\n\nhypothesis testing\n\n\n\n\n\n\n\n\n\nJan 10, 2018\n\n\nJames E. Pustejovsky\n\n\n\n\n\n\n\n\n\n\n\n\n2SLS standard errors and the delta-method\n\n\n\n\n\n\ninstrumental variables\n\n\ncausal inference\n\n\ndelta method\n\n\ndistribution theory\n\n\n\n\n\n\n\n\n\nOct 7, 2017\n\n\nJames E. Pustejovsky\n\n\n\n\n\n\n\n\n\n\n\n\nPooling clubSandwich results across multiple imputations\n\n\n\n\n\n\nmissing data\n\n\nsandwiches\n\n\nsmall-sample\n\n\nRstats\n\n\n\n\n\n\n\n\n\nSep 27, 2017\n\n\nJames E. Pustejovsky\n\n\n\n\n\n\n\n\n\n\n\n\nImputing covariance matrices for meta-analysis of correlated effects\n\n\n\n\n\n\nmeta-analysis\n\n\nsandwiches\n\n\nrobust variance estimation\n\n\nRstats\n\n\n\n\n\n\n\n\n\nAug 10, 2017\n\n\nJames E. Pustejovsky\n\n\n\n\n\n\n\n\n\n\n\n\nThe siren song of significance\n\n\n\n\n\n\npre-registration\n\n\nhypothesis testing\n\n\n\n\n\n\n\n\n\nJun 19, 2017\n\n\nJames E. Pustejovsky\n\n\n\n\n\n\n\n\n\n\n\n\nYou wanna PEESE of d’s?\n\n\n\n\n\n\nmeta-analysis\n\n\npublication bias\n\n\nstandardized mean difference\n\n\nsimulation\n\n\n\n\n\n\n\n\n\nApr 27, 2017\n\n\nJames E. Pustejovsky\n\n\n\n\n\n\n\n\n\n\n\n\nNew working paper: Using log response ratios for meta-analyzing SCDs with behavioral outcomes\n\n\n\n\n\n\nsingle-case design\n\n\neffect size\n\n\nresponse ratio\n\n\nmeta-analysis\n\n\n\n\n\n\n\n\n\nApr 26, 2017\n\n\nJames E. Pustejovsky\n\n\n\n\n\n\n\n\n\n\n\n\nPresentation at IES 2016 PI meeting\n\n\n\n\n\n\nsingle-case design\n\n\neffect size\n\n\n\n\n\n\n\n\n\nDec 19, 2016\n\n\nJames E. Pustejovsky\n\n\n\n\n\n\n\n\n\n\n\n\nNew tutorial paper on BC-SMD effect sizes\n\n\n\n\n\n\nsingle-case design\n\n\neffect size\n\n\ndesign-comparable SMD\n\n\n\n\n\n\n\n\n\nDec 19, 2016\n\n\nJames E. Pustejovsky\n\n\n\n\n\n\n\n\n\n\n\n\nBug in nlme::lme with fixed sigma and REML estimation\n\n\n\n\n\n\nRstats\n\n\nprogramming\n\n\nhierarchical models\n\n\nnlme\n\n\n\n\n\n\n\n\n\nNov 7, 2016\n\n\nJames E. Pustejovsky\n\n\n\n\n\n\n\n\n\n\n\n\nWhat is Tau-U?\n\n\n\n\n\n\neffect size\n\n\nsingle-case design\n\n\nnon-overlap measures\n\n\n\n\n\n\n\n\n\nNov 3, 2016\n\n\nJames E. Pustejovsky\n\n\n\n\n\n\n\n\n\n\n\n\nNew working paper: Procedural sensitivities of SCD effect sizes\n\n\n\n\n\n\neffect size\n\n\nsingle-case design\n\n\nresponse ratio\n\n\nnon-overlap measures\n\n\nsimulation\n\n\n\n\n\n\n\n\n\nOct 17, 2016\n\n\nJames E. Pustejovsky\n\n\n\n\n\n\n\n\n\n\n\n\nSimulation studies in R (Fall, 2016 version)\n\n\n\n\n\n\nRstats\n\n\nsimulation\n\n\nprogramming\n\n\n\n\n\n\n\n\n\nSep 28, 2016\n\n\nJames E. Pustejovsky\n\n\n\n\n\n\n\n\n\n\n\n\nBug in nlme::getVarCov\n\n\n\n\n\n\nRstats\n\n\nprogramming\n\n\nhierarchical models\n\n\nnlme\n\n\n\n\n\n\n\n\n\nAug 10, 2016\n\n\nJames E. Pustejovsky\n\n\n\n\n\n\n\n\n\n\n\n\nAlternative formulas for the standardized mean difference\n\n\n\n\n\n\nmeta-analysis\n\n\neffect size\n\n\ndistribution theory\n\n\nstandardized mean difference\n\n\n\n\n\n\n\n\n\nJun 3, 2016\n\n\nJames E. Pustejovsky\n\n\n\n\n\n\n\n\n\n\n\n\nAssigning after dplyr\n\n\n\n\n\n\nRstats\n\n\nprogramming\n\n\n\n\n\n\n\n\n\nMay 13, 2016\n\n\nJames E. Pustejovsky\n\n\n\n\n\n\n\n\n\n\n\n\nUnlucky randomization\n\n\n\n\n\n\nexperimental design\n\n\n\n\n\n\n\n\n\nMay 11, 2016\n\n\nJames E. Pustejovsky\n\n\n\n\n\n\n\n\n\n\n\n\nThe sampling distribution of sample variances\n\n\n\n\n\n\ndistribution theory\n\n\n\n\n\n\n\n\n\nApr 25, 2016\n\n\nJames E. Pustejovsky\n\n\n\n\n\n\n\n\n\n\n\n\nTau-U\n\n\n\n\n\n\neffect size\n\n\nsingle-case design\n\n\nnon-overlap measures\n\n\n\n\n\n\n\n\n\nMar 23, 2016\n\n\nJames E. Pustejovsky\n\n\n\n\n\n\n\n\n\n\n\n\nStandard errors and confidence intervals for NAP\n\n\n\n\n\n\neffect size\n\n\nsingle-case design\n\n\nnon-overlap measures\n\n\ndistribution theory\n\n\n\n\n\n\n\n\n\nFeb 28, 2016\n\n\nJames E. Pustejovsky\n\n\n\n\n\n\n\n\n\n\n\n\nEstimating average effects in regression discontinuities with covariate interactions\n\n\n\n\n\n\neconometrics\n\n\nRstats\n\n\ncausal inference\n\n\nregression discontinuity\n\n\n\n\n\n\n\n\n\nJan 27, 2016\n\n\nJames E. Pustejovsky\n\n\n\n\n\n\n\n\n\n\n\n\nRegression discontinuities with covariate interactions in the rdd package\n\n\n\n\n\n\neconometrics\n\n\nRstats\n\n\ncausal inference\n\n\nregression discontinuity\n\n\n\n\n\n\n\n\n\nJan 25, 2016\n\n\nJames E. Pustejovsky\n\n\n\n\n\n\n\n\n\n\n\n\nClustered standard errors and hypothesis tests in fixed effects models\n\n\n\n\n\n\neconometrics\n\n\nfixed effects\n\n\nsandwiches\n\n\nRstats\n\n\n\n\n\n\n\n\n\nJan 10, 2016\n\n\nJames E. Pustejovsky\n\n\n\n\n\n\n\n\n\n\n\n\nSpecial Education Pro-Sem\n\n\n\n\n\n\nmeta-analysis\n\n\nsingle-case design\n\n\neffect size\n\n\n\n\n\n\n\n\n\nNov 24, 2015\n\n\nJames E. Pustejovsky\n\n\n\n\n\n\n\n\n\n\n\n\nCorrelations between standardized mean differences\n\n\n\n\n\n\nmeta-analysis\n\n\neffect size\n\n\nstandardized mean difference\n\n\ndistribution theory\n\n\n\n\n\n\n\n\n\nSep 17, 2015\n\n\nJames E. Pustejovsky\n\n\n\n\n\n\n\n\n\n\n\n\nFatal crashes in Austin/Travis County\n\n\n\n\n\n\ntransportation\n\n\n\n\n\n\n\n\n\nAug 20, 2015\n\n\nJames E. Pustejovsky\n\n\n\n\n\n\n\n\n\n\n\n\nThe clubSandwich package for meta-analysis with RVE\n\n\n\n\n\n\nmeta-analysis\n\n\nrobust variance estimation\n\n\nsandwiches\n\n\nRstats\n\n\n\n\n\n\n\n\n\nJul 10, 2015\n\n\nJames E. Pustejovsky\n\n\n\n\n\n\n\n\n\n\n\n\nNew article: Four methods for analyzing PIR data\n\n\n\n\n\n\nbehavioral observation\n\n\nsingle-case design\n\n\n\n\n\n\n\n\n\nFeb 11, 2015\n\n\nJames E. Pustejovsky\n\n\n\n\n\n\n\n\n\n\n\n\nGetting started with scdhlm\n\n\n\n\n\n\nsingle-case design\n\n\ndesign-comparable SMD\n\n\nRstats\n\n\n\n\n\n\n\n\n\nOct 19, 2014\n\n\nJames E. Pustejovsky\n\n\n\n\n\n\n\n\n\n\n\n\nWanted: PIR data\n\n\n\n\n\n\nbehavioral observation\n\n\n\n\n\n\n\n\n\nSep 3, 2014\n\n\nJames E. Pustejovsky\n\n\n\n\n\n\n\n\n\n\n\n\nNew article: Design-comparable effect sizes in multiple baseline designs: A general modeling framework\n\n\n\n\n\n\nsingle-case design\n\n\neffect size\n\n\nhierarchical models\n\n\ndesign-comparable SMD\n\n\n\n\n\n\n\n\n\nJul 20, 2014\n\n\nJames E. Pustejovsky\n\n\n\n\n\n\n\n\n\n\n\n\nARPobservation now on CRAN\n\n\n\n\n\n\nbehavioral observation\n\n\nalternating renewal process\n\n\nRstats\n\n\n\n\n\n\n\n\n\nMay 31, 2014\n\n\nJames E. Pustejovsky\n\n\n\n\n\n\n\n\n\n\n\n\nMeta-sandwich with extra mustard\n\n\n\n\n\n\nmeta-analysis\n\n\nsandwiches\n\n\nRstats\n\n\nrobust variance estimation\n\n\n\n\n\n\n\n\n\nApr 26, 2014\n\n\nJames E. Pustejovsky\n\n\n\n\n\n\n\n\n\n\n\n\nAnother meta-sandwich\n\n\n\n\n\n\nmeta-analysis\n\n\nsandwiches\n\n\nRstats\n\n\nrobust variance estimation\n\n\n\n\n\n\n\n\n\nApr 23, 2014\n\n\nJames E. Pustejovsky\n\n\n\n\n\n\n\n\n\n\n\n\nA meta-sandwich\n\n\n\n\n\n\nmeta-analysis\n\n\nsandwiches\n\n\nRstats\n\n\nrobust variance estimation\n\n\n\n\n\n\n\n\n\nApr 21, 2014\n\n\nJames E. Pustejovsky\n\n\n\n\n\n\n\n\n\n\n\n\nSpecial Education Pro-Sem\n\n\n\n\n\n\nmeta-analysis\n\n\nsingle-case design\n\n\neffect size\n\n\n\n\n\n\n\n\n\nApr 10, 2014\n\n\nJames E. Pustejovsky\n\n\n\n\n\n\n\n\n\n\n\n\nUpdate: parallel R on the TACC\n\n\n\n\n\n\nRstats\n\n\nprogramming\n\n\nsimulation\n\n\nTACC\n\n\n\n\n\n\n\n\n\nApr 8, 2014\n\n\nJames E. Pustejovsky\n\n\n\n\n\n\n\n\n\n\n\n\nNew article: Measurement-comparable effect sizes for single-case studies of free-operant behavior\n\n\n\n\n\n\nbehavioral observation\n\n\nsingle-case design\n\n\neffect size\n\n\nresponse ratio\n\n\n\n\n\n\n\n\n\nFeb 4, 2014\n\n\nJames E. Pustejovsky\n\n\n\n\n\n\n\n\n\n\n\n\nRunning R in parallel on the TACC\n\n\n\n\n\n\nRstats\n\n\nprogramming\n\n\nsimulation\n\n\nTACC\n\n\n\n\n\n\n\n\n\nDec 20, 2013\n\n\nJames E. Pustejovsky\n\n\n\n\n\n\n\n\n\n\n\n\nDesigning simulation studies using R\n\n\n\n\n\n\nRstats\n\n\nsimulation\n\n\n\n\n\n\n\n\n\nDec 6, 2013\n\n\nJames E. Pustejovsky\n\n\n\n\n\n\n\n\n\n\n\n\nTo what extent does partial interval recording over-estimate prevalence?\n\n\n\n\n\n\nbehavioral observation\n\n\nsimulation\n\n\nalternating renewal process\n\n\n\n\n\n\n\n\n\nOct 26, 2013\n\n\nJames E. Pustejovsky\n\n\n\n\n\n\n\n\n\n\n\n\nARPobservation: Basic use\n\n\n\n\n\n\nbehavioral observation\n\n\nalternating renewal process\n\n\nRstats\n\n\n\n\n\n\n\n\n\nOct 25, 2013\n\n\nJames E. Pustejovsky\n\n\n\n\n\n\n\n\n\n\n\n\nGetting started with ARPobservation\n\n\n\n\n\n\nbehavioral observation\n\n\nsimulation\n\n\nRstats\n\n\n\n\n\n\n\n\n\nOct 24, 2013\n\n\nJames E. Pustejovsky\n\n\n\n\n\n\n\n\n\n\n\n\nReliability of UnGraphed single-case data: An example using the Shogren dataset\n\n\n\n\n\n\nsingle-case design\n\n\ninter-rater reliability\n\n\n\n\n\n\n\n\n\nOct 23, 2013\n\n\nJames E. Pustejovsky\n\n\n\n\n\n\n\n\n\n\n\n\nAnother project idea: Meta-analytic methods for correlational data\n\n\n\n\n\n\nmeta-analysis\n\n\nrobust variance estimation\n\n\ncorrelation\n\n\n\n\n\n\n\n\n\nSep 13, 2013\n\n\nJames E. Pustejovsky\n\n\n\n\n\n\n\n\n\n\n\n\nCurrent projects\n\n\n\n\n\n\nbehavioral observation\n\n\nsingle-case design\n\n\n\n\n\n\n\n\n\nAug 20, 2013\n\n\nJames E. Pustejovsky\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "teaching/index.html#at-uw-madison",
    "href": "teaching/index.html#at-uw-madison",
    "title": "Teaching",
    "section": "",
    "text": "Course Title\n\n\n\n\n\n\nDesign & Analysis of Quasi-Experiments for Causal Inference\n\n\n\n\nField Experiments in Education Research\n\n\n\n\nMeta-analysis\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "teaching/index.html#at-ut-austin",
    "href": "teaching/index.html#at-ut-austin",
    "title": "Teaching",
    "section": "At UT Austin",
    "text": "At UT Austin\n\n\n\n\n\nCourse Title\n\n\n\n\n\n\nCausal Inference\n\n\n\n\nData Analysis, Simulation, and Programming in R\n\n\n\n\nResearch Design and Methods for Psychology and Education\n\n\n\n\nStatistical Analysis of Experimental Data\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "publication/Taking-a-distributed-perspective/index.html",
    "href": "publication/Taking-a-distributed-perspective/index.html",
    "title": "Taking a distributed perspective: epistemological and methodological tradeoffs in operationalizing the leader-plus aspect",
    "section": "",
    "text": "Purpose\nThis paper is concerned with the epistemological and methodological challenges involved in studying the distribution of leadership across people within the school-–-the leader-plus aspect of a distributed perspective, which it aims to investigate.\n\n\nDesign/methodology/approach\nThe paper examines the entailments of the distributed perspective for collecting and analyzing data on school leadership and management. It considers four different operationalizations of the leader-plus aspect of the distributed perspective and examines the results obtained from these different operationalizations. The research reported in this paper is part of a larger study, an efficacy trial of a professional development program intended to prepare principals to improve their practice. The study involved a mixed method design. For the purpose of this paper a combination of qualitative and quantitative data, including an experience sampling method (ESM) principal log, a principal questionnaire (PQ), and a school staff questionnaire (SSQ) was used.\n\n\nFindings\nWhile acknowledging broad similarities among the various approaches, the different approaches also surfaced some divergence that has implications for thinking about the epistemological and methodological challenges in measuring leadership from a distributed perspective. Approaches that focus on the lived organization as distinct from the designed organization, for example, unearth the role of individuals with no formal leadership designations in leading and managing the school.\n\n\nResearch limitations/implications\nLimited by the data set, the paper focuses on only four operationalizations of the leader-plus aspect of the distributed perspective rather than taking a more comprehensive look at how the leader-plus aspect might be operationalized.\n\n\nOriginality/value\nThe primary value of this paper is that it will prompt scholars to think about the entailments of different ways of operationalizing the leader-plus aspect when using a distributed perspective."
  },
  {
    "objectID": "publication/Understanding-teacher-leadership/index.html",
    "href": "publication/Understanding-teacher-leadership/index.html",
    "title": "Understanding teacher leadership in middle school mathematics: A collaborative research effort",
    "section": "",
    "text": "We report ﬁndings from a collaborative research effort designed to examine how teachers act as leaders in their schools. We ﬁnd that teachers educated by the Math in the Middle Institute act as key sources of advice for colleagues within their schools while drawing support from a network consisting of other teachers in the program and university-level advisors. In addition to reporting on our ﬁndings, we reﬂect on our research process, noting some of the practical challenges involved, as well as some of the beneﬁts of collaboration."
  },
  {
    "objectID": "publication/Organizing-for-instruction/index.html",
    "href": "publication/Organizing-for-instruction/index.html",
    "title": "Organizing for instruction: A comparative study of public, charter, & Catholic schools",
    "section": "",
    "text": "Guided by theories of institutions, organizations, and sense-making, this manuscript examines how public, charter, and Catholic school staff in a large urban area organize for instruction and respond to educational change. To build theory about institutional processes of “organizing” from participants’ perspectives, data included a survey regarding staff networks \\((N = 271)\\) and semi-structured, qualitative interviews \\((n = 49)\\). Findings demonstrate that all 11 schools in this study reflected the current reform environment with its focus on managing instruction. However, staff from different kinds of schools organized in distinct ways. Most charter and Catholic school staff described obtaining information about instruction through “organic” relationships using the metaphor of family to define their work situations. Alternatively, public schools tended to be “mechanistic,” with staff viewing themselves as professionals who were focused on standards and testing. One charter school, however, combined organic and mechanistic characteristics demonstrating the contagion that occurs among organizations in the same institutional sector and the reach that institutional policy scripts, such as No Child Left Behind, have in changing instructional practice at all kinds of schools."
  },
  {
    "objectID": "publication/integrating-word-reading-and-word-meaning/index.html",
    "href": "publication/integrating-word-reading-and-word-meaning/index.html",
    "title": "The relative effects of integrating word reading and word meaning instruction to word reading instruction alone on the accuracy, fluency, and word meaning knowledge of 4th-5th grade students with dyslexia",
    "section": "",
    "text": "This within-subjects experimental study investigated the relative effects of word reading and word meaning instruction (WR+WM) compared to word-reading instruction alone (WR) on the accuracy, fluency, and word meaning knowledge of 4th-5th graders with dyslexia. We matched word lists on syllables, phonemes, frequency, number of definitions, and concreteness. We assigned half the words to WR and half to WR+WM. Word reading accuracy, word reading fluency, and word meaning knowledge were measured at pretest, immediately following each intervention session, and at posttest, administered immediately following the 12, 45-minute, daily instructional sessions. Compared to WR instruction alone, WR+WM significantly improved accuracy \\((d = 0.65)\\), fluency \\((d = 0.43)\\), and word meaning knowledge \\((d = 1.92)\\) immediately following intervention, and significantly improved accuracy \\((d = 0.74)\\), fluency \\((d = 0.84)\\), and word meaning knowledge \\((d = 1.03)\\) at posttest. Findings support the premise that word meaning knowledge facilitates accurate and fluent word reading, and that instruction explicitly integrating word reading and word meaning may be an effective support for upper elementary students with dyslexia."
  },
  {
    "objectID": "publication/AAC-social-validity/index.html",
    "href": "publication/AAC-social-validity/index.html",
    "title": "Social validity, cost, acceptability, and feasibility of Augmentative and Alternative Communication devices used for individuals with autism spectrum disorder and intellectual disability: A systematic review",
    "section": "",
    "text": "Purpose: The authors conducted a systematic review of single-case experimental designs that included individuals with autism spectrum disorder and/or intellectual disability who used speech-generating devices (SGDs) for communication. The purpose of this study was to review subjective and normative pre- and post-intervention social validity data, in addition to the cost, acceptability, and feasibility of the SGDs used in the studies. The authors also studied trends in the reporting of pre- and post-intervention data over time.\nMethod: A systematic review of 7,327 articles resulted in 86 articles that met design quality criteria and included participants who used SGDs. A group of raters completed interrater reliability for all stages of the review.\nResults: Researchers reported more subjective than normative data. Few studies reported on the price of the SGD or the person who purchased the SGD. More researchers reported using an SGD with more than one use, but few solicited feedback about the SGD used during the intervention. Few researchers reported information about the portability of the device or the operation effort.\nConclusions: Reporting on social validity represents a substantial limitation in experimental studies. Future work incorporating less biased/more objective measures is important for the creation of socially valid interventions."
  },
  {
    "objectID": "publication/AAC-increased-rigor/index.html",
    "href": "publication/AAC-increased-rigor/index.html",
    "title": "A case for increased rigor in AAC research: A methodological quality review",
    "section": "",
    "text": "This comprehensive review reports on methodological quality of 162 single-case studies on augmentative and alternative communication interventions for communication and challenging behavior in individuals diagnosed with autism or intellectual disabilities and with complex communication needs. Following review for inclusion criteria, documents were excluded if they failed to meet basic methodological standards. Each remaining study was evaluated for 10 detailed quality criteria. No studies met all standards without reservations. Only three of the included studies met all of the standards with reservations and the remainder met some but not all standards, with or without reservations. The included studies reported adequate detail for half of the quality indicators, but insufficient details for participant, setting, maintenance, generalization, and social validity descriptions. An increased quantity and quality of research were found in over four decades. More recent studies have adequately reported half of the criteria investigated, including describing the materials, defining the outcome variables, describing baseline and intervention procedures, and evaluating procedural integrity. After identifying quality features, the authors report in more detail on low-rated quality indicators particularly relevant to studies addressing social-communication interventions. The literature infrequently reported race, ethnicity, or home language. Future research should report characteristics of participants to ensure that research becomes representative of the population."
  },
  {
    "objectID": "publication/More-is-not-necessarily-better/index.html",
    "href": "publication/More-is-not-necessarily-better/index.html",
    "title": "Determining associations between intervention amount and outcomes for young autistic children: A systematic review and meta-analysis",
    "section": "",
    "text": "Importance\nHealth professionals routinely recommend “intensive interventions” (i.e., 20-40 hours per week) for autistic children. However, primary research backing this recommendation is sparse and plagued by methodological flaws.\n\n\nObjective\nWe examined whether different metrics of intervention amount are associated with intervention effects on any developmental domain for young autistic children.\n\n\nData Sources\nWe used a large corpus of studies \\((n = 144)\\) taken from a recent meta-analysis (with a search date of November 2021) of early interventions for autistic children.\n\n\nStudy Selection\nStudies were eligible if they reported a quasi-experimental or randomized controlled trial testing the effects of a nonpharmacological intervention on any outcome in participant samples comprising &gt;50% autistic children age 8 or younger.\n\n\nData Extraction and Synthesis\nData were independently extracted by multiple coders. We constructed meta-regression models to determine whether each index of intervention amount was associated with effect sizes for each intervention type, while controlling for outcome domain, outcome proximity, age of participants, study design, and risk of detection bias.\n\n\nMain Outcomes and Measures\nThe primary predictor of interest was intervention amount, quantified using three different metrics (daily intensity, duration, and cumulative intensity). The primary outcomes of interest were gains in any developmental domain, quantified by Hedges \\(g\\) effect sizes.\n\n\nResults\nNone of the meta-regression models evidenced a significant, positive association between any index of intervention amount and intervention effect size when considered within intervention type.\n\n\nConclusions and Relevance\nOur findings do not support the assertion that intervention effects increase with increasing amounts of intervention. Health professionals recommending supports should be advised that there is little robust evidence supporting the provision of “intensive” intervention. Our prior meta-analyses suggest that some interventions have the potential to improve some outcomes for autistic children during early childhood. The present results, however, suggest that providing more of such interventions will not necessarily result in larger effects on outcomes of interest. There is a pressing need for future primary research to be designed and conducted in a manner that allows us to determine the most supportive amounts of interventions by child characteristics, intervention type, and the outcomes targeted by the intervention."
  },
  {
    "objectID": "presentations/AERA-2022-synthesis-of-NAP.html",
    "href": "presentations/AERA-2022-synthesis-of-NAP.html",
    "title": "Synthesis of non-overlap of all pairs using logistic transformation or binomial generalized linear mixed model",
    "section": "",
    "text": "Available methods for meta-analysis of findings from single-case designs include one-stage methods involving modeling of raw data from across multiple studies and two-stage methods involving calculation of effect sizes and subsequent meta-analysis. The two-stage approach works well for some effect size measures, such as log response ratios, but performs inadequately for the non-overlap of all pairs index. NAP is an effect size in the family of non-overlap measures, which quantify effect magnitude in terms of pairwise rank comparisons of outcomes under different treatment conditions, and is thus a useful metric for outcomes that are not normally distributed and not on a ratio metric. We examine two alternative approaches to meta-analysis of NAP, based on either transforming the effect size estimates or on a binomial generalized linear mixed model. We demonstrate the approaches by re-analyzing data from a meta-analysis of SCEDs examining augmentative and alternative communication interventions and evaluate the performance of the approaches using an extensive simulation study. We find that neither approach performs adequately for synthesis of single-case data series with limited numbers of observations in the baseline and intervention phases."
  },
  {
    "objectID": "presentations/AERA-2021-multivariate-effect-sizes-SCDs.html",
    "href": "presentations/AERA-2021-multivariate-effect-sizes-SCDs.html",
    "title": "On the multivariate distribution of effect size estimates from single-case experimental designs",
    "section": "",
    "text": "Many different effect size metrics have been proposed for use with single-case experimental designs (SCEDs). Metrics that have known sampling variances and are suitable for meta-analysis include the within-case standardized mean difference (SMD), the log response ratio (LRR), and the non-overlap of all pairs (NAP). These within-case effect size metrics can be used to make comparisons between pairs of phases within an SCED for a single outcome. However, in practice, many SCEDs include multiple outcomes, multiple phases, or both multiple outcomes and multiple phases. In such studies, it may be useful to estimate multiple effect sizes for inclusion in a meta-analysis. This requires calculation not only of effect size estimates and their sampling variances, but also the covariances between effect size estimates. Formulas for the covariances between effect size estimates are available but scattered around the methodological literature. This paper reviews and consolidates available formulas and demonstrates their relevance in the context of meta-analysis of SCEDs. I describe methods for estimating multiple effect sizes, along with corresponding sampling variances and covariances, for the within-case SMD, LRR, and NAP indices. An empirical example is included to illustrate the calculations."
  },
  {
    "objectID": "presentations/AERA-2021-meta-analysis-of-SCDs.html",
    "href": "presentations/AERA-2021-meta-analysis-of-SCDs.html",
    "title": "Meta-analysis of single-case experimental designs using robust variance estimation",
    "section": "",
    "text": "Many different effect size metrics have been proposed for use with single-case experimental designs (SCEDs). Metrics that have known sampling variances and are suitable for meta-analysis include the within-case standardized mean difference (SMD), the log response ratio (LRR), and the non-overlap of all pairs (NAP). These within-case effect size metrics can be used to make comparisons between pairs of phases within an SCED for a single outcome. However, in practice, many SCEDs include multiple outcomes, multiple phases, or both multiple outcomes and multiple phases. In such studies, it may be useful to estimate multiple effect sizes for inclusion in a meta-analysis. This requires calculation not only of effect size estimates and their sampling variances, but also the covariances between effect size estimates. Formulas for the covariances between effect size estimates are available but scattered around the methodological literature. This paper reviews and consolidates available formulas and demonstrates their relevance in the context of meta-analysis of SCEDs. I describe methods for estimating multiple effect sizes, along with corresponding sampling variances and covariances, for the within-case SMD, LRR, and NAP indices. An empirical example is included to illustrate the calculations."
  }
]