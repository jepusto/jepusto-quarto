<?xml version="1.0" encoding="UTF-8"?>
<rss  xmlns:atom="http://www.w3.org/2005/Atom" 
      xmlns:media="http://search.yahoo.com/mrss/" 
      xmlns:content="http://purl.org/rss/1.0/modules/content/" 
      xmlns:dc="http://purl.org/dc/elements/1.1/" 
      version="2.0">
<channel>
<title>James E. Pustejovsky</title>
<link>https://jepusto.com/publication/</link>
<atom:link href="https://jepusto.com/publication/index.xml" rel="self" type="application/rss+xml"/>
<description>Education Statistics and Meta-Analysis</description>
<generator>quarto-1.4.552</generator>
<lastBuildDate>Thu, 07 Mar 2024 06:00:00 GMT</lastBuildDate>
<item>
  <title>Equivalences between ad hoc strategies and meta-analytic models for dependent effect sizes</title>
  <dc:creator>James E. Pustejovsky</dc:creator>
  <dc:creator>Man Chen</dc:creator>
  <link>https://jepusto.com/publication/Equivalences-between-ad-hoc-strategies-and-models/</link>
  <description><![CDATA[ 






<p>Meta-analyses of educational research findings frequently involve statistically dependent effect size estimates. Meta-analysts have often addressed dependence issues using ad hoc approaches that involve modifying the data to conform to the assumptions of models for independent effect size estimates, such as aggregating estimates to obtain one summary estimate per study, conducting separate analyses of distinct subgroups of estimates, or combinations thereof. We demonstrate that these ad hoc approaches correspond exactly to certain multivariate models for dependent effect sizes. Specifically, we describe classes of multivariate random effects models that have likelihoods equivalent to those of models for effect sizes that have been averaged by study, classified into subgroups, or both. The equivalence also applies to robust variance estimation methods.</p>



 ]]></description>
  <category>meta-analysis</category>
  <category>meta-regression</category>
  <category>dependent effect sizes</category>
  <category>robust variance estimation</category>
  <guid>https://jepusto.com/publication/Equivalences-between-ad-hoc-strategies-and-models/</guid>
  <pubDate>Thu, 07 Mar 2024 06:00:00 GMT</pubDate>
</item>
<item>
  <title>High replicability of newly-discovered social-behavioral findings is achievable.</title>
  <dc:creator>John Protzko</dc:creator>
  <dc:creator>Jon Krosnick</dc:creator>
  <dc:creator>Leif Nelson</dc:creator>
  <dc:creator>Brian Nosek</dc:creator>
  <dc:creator>Jordan Axt</dc:creator>
  <dc:creator>Matt Berent</dc:creator>
  <dc:creator>Nicholas Buttrick</dc:creator>
  <dc:creator>Matthew DeBell</dc:creator>
  <dc:creator>Charles R. Ebersole</dc:creator>
  <dc:creator>Sebastian Lundmark</dc:creator>
  <dc:creator>Bo MacInnis</dc:creator>
  <dc:creator>Michael O&#39;Donnell</dc:creator>
  <dc:creator>Hannah Perfecto</dc:creator>
  <dc:creator>James E. Pustejovsky</dc:creator>
  <dc:creator>Scott Roeder</dc:creator>
  <dc:creator>Jan Walleczek</dc:creator>
  <dc:creator>Jonathan W. Schooler</dc:creator>
  <link>https://jepusto.com/publication/Decline-effects/</link>
  <description><![CDATA[ 






<p>Failures to replicate evidence of new discoveries have forced scientists to ask whether this unreliability is due to suboptimal implementation of optimal methods or whether presumptively optimal methods are not, in fact, optimal. This paper reports an investigation by four coordinated laboratories of the prospective replicability of 16 novel experimental findings using current optimal practices: high statistical power, preregistration, and complete methodological transparency. In contrast to past systematic replication efforts that reported replication rates averaging 50%, replication attempts here produced the expected effects with significance testing (p&lt;.05) in 86% of attempts, slightly exceeding maximum expected replicability based on observed effect size and sample size. When one lab attempted to replicate an effect discovered by another lab, the effect size in the replications was 97% that of the original study. This high replication rate justifies confidence in rigor enhancing methods and suggests that past failures to replicate may be attributable to departures from optimal procedures.</p>



 ]]></description>
  <category>replication</category>
  <category>meta-analysis</category>
  <guid>https://jepusto.com/publication/Decline-effects/</guid>
  <pubDate>Thu, 09 Nov 2023 06:00:00 GMT</pubDate>
  <media:content url="https://jepusto.com/publication/Decline-effects/Effect sizes and 95% CI from 16 new discoveries (yellow marks) in the social-behavioral sciences, each with four replications. Each lab is designated by a unique shape for observed effect size; blue marks correspond to self-replications, green marks to independent replications." medium="image"/>
</item>
<item>
  <title>Conducting power analysis for meta-analysis of dependent effect sizes: Common guidelines and an introduction to the POMADE R package</title>
  <dc:creator>Mikkel H. Vembye</dc:creator>
  <dc:creator>James E. Pustejovsky</dc:creator>
  <dc:creator>Terri D. Pigott</dc:creator>
  <link>https://jepusto.com/publication/Conducting-POMADE/</link>
  <description><![CDATA[ 






<p>Sample size and statistical power are important factors to consider when planning a research synthesis. Power analysis methods have been developed for fixed effect or random effects models, but until recently these methods were limited to simple data structures with a single, independent effect per study. Recent work has provided power approximation formulas for meta-analyses involving studies with multiple, dependent effect size estimates, which are common in syntheses of social science research. Prior work focused on developing and validating the approximations, but did not address the practice challenges encountered in applying them for purposes of planning a synthesis involving dependent effect sizes. We aim to facilitate application of these recent developments by providing practical guidance on how to conduct power analysis for planning a meta-analysis of dependent effect sizes and by introducing a new R package, POMADE, designed for this purpose. We present a comprehensive overview of resources for finding information about the study design features and model parameters needed to conduct power analysis, along with detailed worked examples using the POMADE package. For presenting power analysis findings, we emphasize graphical tools that can depict power under a range of pausible assumptions and introduce a novel plot, the traffic light power plot, for conveying the degree of certainty in one’s assumptions.</p>



 ]]></description>
  <category>power</category>
  <category>meta-analysis</category>
  <category>dependent effect sizes</category>
  <category>robust variance estimation</category>
  <guid>https://jepusto.com/publication/Conducting-POMADE/</guid>
  <pubDate>Fri, 21 Jul 2023 05:00:00 GMT</pubDate>
  <media:content url="https://jepusto.com/software/POMADE/featured.png" medium="image" type="image/png" height="144" width="144"/>
</item>
<item>
  <title>The efficacy of combining cognitive training and non-invasive brain stimulation: A transdiagnostic systematic review and meta-analysis</title>
  <dc:creator>Anika Poppe</dc:creator>
  <dc:creator>Franziska D. E. Ritter</dc:creator>
  <dc:creator>Leonie Bais</dc:creator>
  <dc:creator>James E. Pustejovsky</dc:creator>
  <dc:creator>Marie-José van Tol</dc:creator>
  <dc:creator>Branislava Ćurčić-Blake</dc:creator>
  <dc:creator>Gerdina H.M. Pijnenborg</dc:creator>
  <dc:creator>Lisette van der Meer</dc:creator>
  <link>https://jepusto.com/publication/Efficacy-of-combining-cognitive-training-and-NIBS/</link>
  <description><![CDATA[ 






<p>Over the past decade, an increasing number of studies investigated the innovative approach of supplementing cognitive training (CT) with non-invasive brain stimulation (NIBS) to increase the effects on outcomes. In this review, we aim to summarize the evidence for this treatment combination. We identified 72 published and unpublished studies (reporting 773 effect sizes) including 2518 participants from healthy and clinical populations indexed in PubMed, Medline, PsycINFO, ProQuest, Web of Science, and ClinicalTrials.gov (last search: 8/8/2022) that compared the effects of NIBS combined with CT on cognitive, symptoms and everyday functioning to CT alone at post-intervention and/or follow-up. We performed random-effects meta-analyses with robust variance estimation and assessed risk of bias with the Cochrane R.O.B. tool. Only four studies had low risk of bias in all domains, and many studies lacked standard controls such as keeping the outcome assessor and trainer unaware of the treatment condition. Following sensitivity analyses, only learning/memory robustly improved significantly more when CT was combined with NIBS compared to CT only (<img src="https://latex.codecogs.com/png.latex?g"> = 0.18, 95% CI [0.07, 0.29]) at post-intervention, but not in the long-term. The effect was small and limited by substantial heterogeneity. The other seven cognitive outcome domains, symptoms, and everyday functioning did not benefit from adding NIBS to CT. Given the methodological limitation of prior studies, more high-quality trials that focus on the potential of combining NIBS and CT to enhance benefits in everyday functioning in the short- and long-term are needed to evaluate whether combining NIBS and CT is relevant for clinical practice.</p>



 ]]></description>
  <category>meta-analysis</category>
  <category>systematic review</category>
  <category>cognitive remediation</category>
  <category>non-invasive brain stimulation</category>
  <guid>https://jepusto.com/publication/Efficacy-of-combining-cognitive-training-and-NIBS/</guid>
  <pubDate>Fri, 21 Jul 2023 05:00:00 GMT</pubDate>
</item>
<item>
  <title>Systematic review of variables related to instruction in augmentative and alternative communication implementation: Group and single-case design</title>
  <dc:creator>Joe Reichle</dc:creator>
  <dc:creator>James E. Pustejovsky</dc:creator>
  <dc:creator>Kimberly J. Vannest</dc:creator>
  <dc:creator>Margaret Foster</dc:creator>
  <dc:creator>Lauren M. Pierson</dc:creator>
  <dc:creator>Sanikan Wattanawongwan</dc:creator>
  <dc:creator>Man Chen</dc:creator>
  <dc:creator>Marcus Fuller</dc:creator>
  <dc:creator>April N. Haas</dc:creator>
  <dc:creator>Bethany H. Bhat</dc:creator>
  <dc:creator>Mary R. Sallese</dc:creator>
  <dc:creator>S. D. Smith</dc:creator>
  <dc:creator>Valeria Yllades</dc:creator>
  <dc:creator>Daira Rodriguez</dc:creator>
  <dc:creator>Amara Yoro</dc:creator>
  <dc:creator>Jay B. Ganz</dc:creator>
  <link>https://jepusto.com/publication/AAC-group-and-SCD/</link>
  <description><![CDATA[ 






<p>Purpose: This article provides a systematic review and analysis of group and single-case studies addressing augmentative and alternative communication (AAC) intervention with school-aged persons having autism spectrum disorder (ASD) and/or intellectual/developmental disabilities resulting in complex communication needs (CCNs). Specifically, we examined participant characteristics in group-design studies reporting AAC intervention outcomes and how these compared to those reported in single-case experimental designs (SCEDs). In addition, we compared the status of intervention features reported in group and SCED studies with respect to instructional strategies utilized. Participants: Participants included school-aged individuals with CCNs who also experienced ASD or ASD with an intellectual delay who utilized aided or unaided AAC. Method: A systematic review using descriptive statistics and effect sizes was implemented. Results: Findings revealed that participant features such as race, ethnicity, and home language continue to be underreported in both SCED and group-design studies. Participants in SCED investigations more frequently used multiple communication modes when compared to participants in group studies. The status of pivotal skills such as imitation was sparsely reported in both types of studies. With respect to instructional features, group-design studies were more apt to utilize clinical rather than educational or home settings when compared with SCED studies. In addition, SCED studies were more apt to utilize instructional methods that closely adhered to instructional features more typically characterized as being associated with behavioral approaches. Conclusion: The authors discuss future research needs, practice implications, and a more detailed specification of treatment intensity parameters for future research.</p>



 ]]></description>
  <category>single-case design</category>
  <category>systematic review</category>
  <category>Augmentative and alternative communication</category>
  <guid>https://jepusto.com/publication/AAC-group-and-SCD/</guid>
  <pubDate>Fri, 26 May 2023 05:00:00 GMT</pubDate>
</item>
<item>
  <title>Comparison of competing approaches to analyzing cross-classified data: Random effects models, ordinary least squares, or fixed effects with cluster robust standard errors</title>
  <dc:creator>Young Ri Lee</dc:creator>
  <dc:creator>James E. Pustejovsky</dc:creator>
  <link>https://jepusto.com/publication/competing-approaches-for-cross-classified-data/</link>
  <description><![CDATA[ 






<p>Cross-classified random effects modeling (CCREM) is a common approach for analyzing cross-classified data in education. However, when the focus of a study is on the regression coefficients at level one rather than on the random effects, ordinary least squares regression with cluster robust variance estimators (OLS-CRVE) or fixed effects regression with CRVE (FE-CRVE) could be appropriate approaches. These alternative methods may be advantageous because they rely on weaker assumptions than what is required by CCREM. We conducted a Monte Carlo Simulation study to compare the performance of CCREM, OLS-CRVE, and FE-CRVE in models with crossed random effects, including conditions where homoscedasticity assumptions and exogeneity assumptions held and conditions where they were violated. We found that CCREM performed the best when its assumptions are all met. However, when homoscedasticity assumptions are violated, OLS-CRVE and FE-CRVE provided similar or better performance than CCREM. FE-CRVE showed the best performance when the exogeneity assumption is violated. Thus, we recommend two-way FE-CRVE as a good alternative to CCREM, particularly if the homoscedasticity or exogeneity assumptions of the CCREM might be in doubt.</p>



 ]]></description>
  <category>simulation</category>
  <category>random effects</category>
  <category>robust variance estimation</category>
  <category>fixed effects</category>
  <guid>https://jepusto.com/publication/competing-approaches-for-cross-classified-data/</guid>
  <pubDate>Thu, 09 Mar 2023 06:00:00 GMT</pubDate>
</item>
<item>
  <title>Between-case standardized mean differences: Flexible methods for single-case designs</title>
  <dc:creator>Man Chen</dc:creator>
  <dc:creator>James E. Pustejovsky</dc:creator>
  <dc:creator>David A. Klingbeil</dc:creator>
  <dc:creator>Ethan R. Van Norman</dc:creator>
  <link>https://jepusto.com/publication/Three-level-BC-SMD/</link>
  <description><![CDATA[ 






<p>Single-case designs (SCDs) are a class of research methods for evaluating the effects of academic and behavioral interventions in educational and clinical settings. Although visual analysis is typically the first and main method for primary analysis of data from SCDs, quantitative methods are useful for synthesizing results and drawing systematic generalizations across bodies of single-case research. Researchers who are interested in synthesizing findings across SCDs and between-group designs might consider using the between-case standardized mean difference (BC-SMD) effect size, which aims to put results from both types of studies into a common metric. Current BC-SMD methods are limited to treatment reversal design and across-participant multiple baseline design, yet more complex designs are used in practice. In this study, we extend available BC-SMD methods to several variations of the multiple baseline design, including the replicated multiple baseline across behaviors or settings, the clustered multiple baseline design, and the multivariate multiple baseline across participants. For each variation, we describe methods for estimating BC-SMD effect sizes and illustrate our proposed approach by re-analyzing data from a published SCD study.</p>



 ]]></description>
  <category>single-case design</category>
  <category>effect size</category>
  <category>standardized mean difference</category>
  <category>design-comparable SMD</category>
  <category>hierarchical models</category>
  <category>multivariate</category>
  <guid>https://jepusto.com/publication/Three-level-BC-SMD/</guid>
  <pubDate>Wed, 08 Mar 2023 06:00:00 GMT</pubDate>
  <media:content url="https://jepusto.com/publication/Three-level-BC-SMD/Clustered multiple baseline design data from Bryant et al. (2018)" medium="image"/>
</item>
<item>
  <title>Single case design research in Special Education: Next generation standards and considerations</title>
  <dc:creator>Jennifer R. Ledford</dc:creator>
  <dc:creator>Joseph Lambert</dc:creator>
  <dc:creator>James E. Pustejovsky</dc:creator>
  <dc:creator>Kathleen N. Zimmerman</dc:creator>
  <dc:creator>Erin E. Barton</dc:creator>
  <link>https://jepusto.com/publication/Single-case-next-generation-standards/</link>
  <description><![CDATA[ 






<p>Single case design has a long history of use for assessing intervention effectiveness for children with disabilities. Although these designs have been widely employed for more than 50 years, recent years have been especially dynamic in terms of growth in the use of single case design and application of standards designed to improve the validity and applicability of findings. This growth expanded possibilities and inspired new questions about the contributions this methodology can make to generalizable knowledge about intervention in special education. In this paper, we discuss and extend previous standards for studies using single case designs (i.e., Horner et al., 2005). We identify new suggestions for internal validity, generality and acceptability, and reporting. We also provide considerations for single case synthesis and discuss the complexities of assessing accumulating evidence for a given practice.</p>



 ]]></description>
  <category>single-case design</category>
  <category>meta-analysis</category>
  <category>effect size</category>
  <guid>https://jepusto.com/publication/Single-case-next-generation-standards/</guid>
  <pubDate>Tue, 22 Nov 2022 06:00:00 GMT</pubDate>
</item>
<item>
  <title>Power approximations for overall average effects in meta-analysis of dependent effect sizes</title>
  <dc:creator>Mikkel H. Vembye</dc:creator>
  <dc:creator>James E. Pustejovsky</dc:creator>
  <dc:creator>Terri D. Pigott</dc:creator>
  <link>https://jepusto.com/publication/Power-approximations-for-dependent-effects/</link>
  <description><![CDATA[ 






<p>Meta-analytic models for dependent effect sizes have grown increasingly sophisticated over the last few decades, which has created challenges for a priori power calculations. We introduce power approximations for tests of average effect sizes based upon several common approaches for handling dependent effect sizes. In a Monte Carlo simulation, we show that the new power formulas can accurately approximate the true power of meta-analytic models for dependent effect sizes. Lastly, we investigate the Type I error rate and power for several common models, finding that tests using robust variance estimation provide better Type I error calibration than tests with model-based variance estimation. We consider implications for practice with respect to selecting a working model and an inferential approach.</p>



 ]]></description>
  <category>power</category>
  <category>meta-analysis</category>
  <category>dependent effect sizes</category>
  <category>robust variance estimation</category>
  <guid>https://jepusto.com/publication/Power-approximations-for-dependent-effects/</guid>
  <pubDate>Mon, 17 Oct 2022 05:00:00 GMT</pubDate>
  <media:content url="https://jepusto.com/publication/Power-approximations-for-dependent-effects/Power difference between approximated and true (simulated) power versus approximated power for the C(H)E working models, across different methods of sampling study characteristics." medium="image"/>
</item>
<item>
  <title>Investigating narrative performance in children with developmental language disorder: A systematic review and meta-analysis</title>
  <dc:creator>Katherine L. Winters</dc:creator>
  <dc:creator>Javier Jasso</dc:creator>
  <dc:creator>James E. Pustejovsky</dc:creator>
  <dc:creator>Courtney Byrd</dc:creator>
  <link>https://jepusto.com/publication/investigating-narrative-performance/</link>
  <description><![CDATA[ 






<p><strong>Purpose</strong>: Speech-language pathologists (SLPs) typically examine narrative performance when completing a comprehensive language assessment. However, there is significant variability in the methodologies used to evaluate narration. The primary aims of this systematic review and meta-analysis were to a) investigate how narrative assessment type (e.g., macrostructure, microstructure, internal state language) differentiates typically developing (TD) children from children with developmental language disorder (DLD), or, TD–DLD group differences, b) identify specific narrative assessment measures (e.g., number of different words) that result in greater TD–DLD differences, and, c) evaluate participant and sample characteristics (e.g., DLD inclusionary criteria) that may uniquely influence performance differences. <strong>Method</strong>: Three electronic databases (PsychInfo, ERIC, and PubMed) and ASHAWire were searched on July 30, 2019 to locate studies that reported oral narrative language measures for both DLD and TD groups between ages 4 and 12 years; studies focusing on written narration or other developmental disorders only were excluded. Thirty-seven primary studies were identified via a three-step study selection procedure. We extracted data related to the sample participants, the narrative task(s) and assessment measures, and research design. Standardized mean differences using a bias-corrected Hedges’ <img src="https://latex.codecogs.com/png.latex?g"> were the calculated effect sizes (<img src="https://latex.codecogs.com/png.latex?N%20=%20382">). Research questions were analyzed using mixed-effects meta-regression with robust variance estimation to account for effect size dependencies. <strong>Results</strong>: Searches identified eligible studies published between 1987 and 2019. An overall meta-analysis using 382 effect sizes obtained across 37 studies showed that children with DLD had decreased narrative performance relative to TD peers, with summary estimates ranging from -0.850, 95% CI [-1.016, -0.685] to -0.794, 95% CI [-0.963, -0.624], depending on the correlation assumed. Across all models, effect size estimates showed significant heterogeneity both between and within studies, even after accounting for effect size-, sample-, and study-level predictors. Grammatical accuracy (microstructure) and story grammar (macrostructure) yielded the most consistent evidence of significant TD–DLD group differences across statistical models. <strong>Conclusions</strong>: Present findings suggest some narrative assessment measures may yield significantly different performance between children with and without DLD. However, researchers need to be consistent in their inclusionary criteria, their description of sample characteristics, and in their reporting of the correlations of measures, in order to determine which assessment measures are more likely to yield group differences.</p>



 ]]></description>
  <category>narrative assessment</category>
  <category>developmental language disorder</category>
  <category>meta-analysis</category>
  <category>dependent effect sizes</category>
  <category>robust variance estimation</category>
  <guid>https://jepusto.com/publication/investigating-narrative-performance/</guid>
  <pubDate>Fri, 30 Sep 2022 05:00:00 GMT</pubDate>
  <media:content url="https://jepusto.com/publication/investigating-narrative-performance/Forest Plot of TD-DLD Group Differences in Narrative Performance" medium="image"/>
</item>
<item>
  <title>Multi-level meta-analysis of single-case experimental designs using robust variance estimation</title>
  <dc:creator>Man Chen</dc:creator>
  <dc:creator>James E. Pustejovsky</dc:creator>
  <link>https://jepusto.com/publication/SCED-MLMA-RVE/</link>
  <description><![CDATA[ 






<p>Single-case experimental designs (SCEDs) are used to study the effects of interventions on the behavior of individual cases, by making comparisons between repeated measurements of an outcome under different conditions. In research areas where SCEDs are prevalent, there is a need for methods to synthesize results across multiple studies. One approach to synthesis uses a multi-level meta-analysis (MLMA) model to describe the distribution of effect sizes across studies and across cases within studies. However, MLMA relies on having accurate sampling variances of effect size estimates for each case, which may not be possible due to auto-correlation in the raw data series. One possible solution is to combine MLMA with robust variance estimation (RVE), which provides valid assessments of uncertainty even if the sampling variances of effect size estimates are inaccurate. Another possible solution is to forgo MLMA and use simpler, ordinary least squares (OLS) methods with RVE. This study evaluates the performance of effect size estimators and methods of synthesizing SCEDs in the presence of auto-correlation, for several different effect size metrics, via a Monte Carlo simulation designed to emulate the features of real data series. Results demonstrate that the MLMA model with RVE performs properly in terms of bias, accuracy, and confidence interval coverage for estimating overall average log response ratios. The OLS estimator corrected with RVE performs the best in estimating overall average Tau effect sizes. None of the available methods perform adequately for meta-analysis of within-case standardized mean differences.</p>



 ]]></description>
  <category>single-case design</category>
  <category>meta-analysis</category>
  <category>robust variance estimation</category>
  <category>response ratio</category>
  <category>non-overlap measures</category>
  <category>standardized mean difference</category>
  <guid>https://jepusto.com/publication/SCED-MLMA-RVE/</guid>
  <pubDate>Fri, 29 Apr 2022 05:00:00 GMT</pubDate>
</item>
<item>
  <title>Augmentative and Alternative Communication intervention targets for school-aged participants with ASD and ID: A single-case systematic review and meta-analysis</title>
  <dc:creator>Jay B. Ganz</dc:creator>
  <dc:creator>James E. Pustejovsky</dc:creator>
  <dc:creator>Joe Reichle</dc:creator>
  <dc:creator>Kimberly J. Vannest</dc:creator>
  <dc:creator>Margaret Foster</dc:creator>
  <dc:creator>Marcus Fuller</dc:creator>
  <dc:creator>Lauren M. Pierson</dc:creator>
  <dc:creator>Sanikan Wattanawongwan</dc:creator>
  <dc:creator>Amando Bernal</dc:creator>
  <dc:creator>Man Chen</dc:creator>
  <dc:creator>April N. Haas</dc:creator>
  <dc:creator>Rachel Skov</dc:creator>
  <dc:creator>S. D. Smith</dc:creator>
  <dc:creator>Valeria Yllades</dc:creator>
  <link>https://jepusto.com/publication/AAC-communication-outcomes/</link>
  <description><![CDATA[ 






<p>Objective: This meta-analysis reviews the literature on communication modes, communicative functions, and types of augmentative and alternative communication (AAC) interventions for school-age participants with autism spectrum disorders and/or intellectual disabilities who experience complex communication needs. Considering potential differences related to outcomes that were targeted for intervention could help identify the most effective means of individualizing AAC interventions. Methods: We performed a systematic literature search using Academic Search Ultimate, ERIC, PsycINFO, Web of Science, and Proquest Dissertations &amp; Theses Global to retrieve research conducted between 1978 and the beginning of 2020. Studies included in the synthesis are (a) in English; (b) has one or more participants with an intellectual delay, developmental disability(ies); (c) reported the results of an augmentative and alternative communication (AAC) intervention to supplement or replace conventional speech for people with complex communication needs; (d) was a SCED; (e) measured social-communicative outcomes. We synthesized results across studies using multi-level meta-analyses of two case-level effect size metrics, Tau and log response ratio. We conducted moderator analyses using meta-regression with robust variance estimation. Results: Across 114 included studies with 330 participants and 767 effect size, overall Tau effects were moderate, Tau = 0.72, 95% CI [0.67, 0.77], and heterogeneous. For the subset of data series where log response ratio could be estimated, the overall average effect was LRR = 1.86, 95% CI [1.58, 2.13], and effects were highly heterogeneous. There were few statistically significant differences found between moderator categories, which included communication mode, communicative function, and type of AAC implemented. Conclusions: This meta-analysis highlights the potential differences related to outcomes that were targeted for AAC interventions for individuals with ASD and IDD. AAC intervention has been shown to improve communication outcomes in this population. However, there was a lack of sufficient data to analyze for some potential moderators such as insufficient descriptive information on participant characteristics. This is likely due to the heterogeneity of the participants and implementation factors; however, these factors were frequently underreported by original study authors which disallowed systematic analysis. That said, there is a need for more detailed participant characteristic descriptions in original research reports to support future aggregation across the literature. Sponsorship: We received funding for the review from the Institute of Education Sciences. Protocol: The review protocol was registered in the PROSPERO system (CRD42018112428).</p>



 ]]></description>
  <category>single-case design</category>
  <category>systematic review</category>
  <category>Augmentative and alternative communication</category>
  <guid>https://jepusto.com/publication/AAC-communication-outcomes/</guid>
  <pubDate>Wed, 20 Apr 2022 05:00:00 GMT</pubDate>
</item>
<item>
  <title>Considering instructional contexts in AAC interventions for people with ASD and/or IDD experiencing complex communication needs: A single-case design meta-analysis</title>
  <dc:creator>Jay B. Ganz</dc:creator>
  <dc:creator>James E. Pustejovsky</dc:creator>
  <dc:creator>Joe Reichle</dc:creator>
  <dc:creator>Kimberly J. Vannest</dc:creator>
  <dc:creator>Margaret Foster</dc:creator>
  <dc:creator>April N. Haas</dc:creator>
  <dc:creator>Lauren M. Pierson</dc:creator>
  <dc:creator>Sanikan Wattanawongwan</dc:creator>
  <dc:creator>Amando Bernal</dc:creator>
  <dc:creator>Man Chen</dc:creator>
  <dc:creator>Rachel Skov</dc:creator>
  <dc:creator>S. D. Smith</dc:creator>
  <link>https://jepusto.com/publication/AAC-instructional-contexts/</link>
  <description><![CDATA[ 






<p>For children with autism or intellectual and developmental disabilities who also have complex communication needs, communication is a necessary skill set to increase independence and quality of life. Understanding the how, where, and communication style being taught is important for identifying deficits in the field as well as which interventions are most effective. This meta-analysis sought to identify effectiveness among different settings, behavioral strategies, and moderator variables. A systematic search and screening process identified 114 eligible studies with 330 participants; overall outcomes indicate that augmentative and alternative communication interventions were effective with Tau effects ranging from 0.53 to 1.03 and log response ratio effects ranging from 0.21 to 2.90. However, no instructional context variables systematically predicted differences in intervention effectiveness.</p>



 ]]></description>
  <category>single-case design</category>
  <category>systematic review</category>
  <category>Augmentative and alternative communication</category>
  <guid>https://jepusto.com/publication/AAC-instructional-contexts/</guid>
  <pubDate>Wed, 20 Apr 2022 05:00:00 GMT</pubDate>
</item>
<item>
  <title>Participant characteristics predicting communication outcomes in AAC implementation for individuals with ASD and IDD: Meta-analysis</title>
  <dc:creator>Jay B. Ganz</dc:creator>
  <dc:creator>James E. Pustejovsky</dc:creator>
  <dc:creator>Joe Reichle</dc:creator>
  <dc:creator>Kimberly J. Vannest</dc:creator>
  <dc:creator>Margaret Foster</dc:creator>
  <dc:creator>Lauren M. Pierson</dc:creator>
  <dc:creator>Sanikan Wattanawongwan</dc:creator>
  <dc:creator>Amando Bernal</dc:creator>
  <dc:creator>Man Chen</dc:creator>
  <dc:creator>April N. Haas</dc:creator>
  <dc:creator>Mary Rose Sallese</dc:creator>
  <dc:creator>Rachel Skov</dc:creator>
  <dc:creator>S. D. Smith</dc:creator>
  <link>https://jepusto.com/publication/AAC-participant-characteristics/</link>
  <description><![CDATA[ 






<p>This meta-analysis examined social communication outcomes in augmentative and alternative communication (AAC) interventions, or those that involved aided (e.g., speech generating devices, picture point systems) or unaided AAC (e.g., gestures, manual sign language) as a component of intervention, and the extent to which communication outcomes were predicted by participant characteristics. Variables of interest included chronological age, communication mode used prior to intervention, number of words produced and imitation skills of participants prior to intervention. Investigators identified 117 primary studies that implemented AAC interventions with school-aged individuals (up to 22 years) with autism spectrum disorder and/or intellectual disability associated with complex communication needs and assessed social-communication outcomes. All included studies involved single-case experimental designs and met basic study design quality standards. We synthesized findings across studies using two complementary effect size indices, Tau(AB) and the log response ratio, and multi-level meta-analysis with robust variance estimation. With Tau(AB), the overall average effect across 338 participants was 0.72, 95% CI [0.67, 0.76], with a high degree of heterogeneity across studies. With the log response ratio, the overall average effect corresponded to a 538% increase from baseline levels of responding, 95% CI [388%, 733%], with a high degree of heterogeneity across studies and contrasts. Moderator analyses detected few differences in effectiveness when comparing across diagnoses, ages, the number and type of communication modes the participants used prior to intervention, the number of words used by the participants prior to intervention, and imitation use prior to intervention.</p>



 ]]></description>
  <category>single-case design</category>
  <category>systematic review</category>
  <category>Augmentative and alternative communication</category>
  <guid>https://jepusto.com/publication/AAC-participant-characteristics/</guid>
  <pubDate>Wed, 20 Apr 2022 05:00:00 GMT</pubDate>
</item>
<item>
  <title>Meta-Analysis with robust variance estimation: Expanding the range of working models</title>
  <dc:creator>James E. Pustejovsky</dc:creator>
  <dc:creator>Elizabeth Tipton</dc:creator>
  <link>https://jepusto.com/publication/RVE-Meta-analysis-expanding-the-range/</link>
  <description><![CDATA[ 






<p>In prevention science and related fields, large meta-analyses are common, and these analyses often involve dependent effect size estimates. Robust variance estimation (RVE) methods provide a way to include all dependent effect sizes in a single meta-regression model, even when the nature of the dependence is unknown. RVE uses a working model of the dependence structure, but the two currently available working models are limited to each describing a single type of dependence. Drawing on flexible tools from multivariate meta-analysis, this paper describes an expanded range of working models, along with accompanying estimation methods, which offer benefits in terms of better capturing the types of data structures that occur in practice and improving the efficiency of meta-regression estimates. We describe how the methods can be implemented using existing software (the ‘metafor’ and ‘clubSandwich’ packages for R) and illustrate the approach in a meta-analysis of randomized trials examining the effects of brief alcohol interventions for adolescents and young adults.</p>



 ]]></description>
  <category>meta-analysis</category>
  <category>meta-regression</category>
  <category>dependent effect sizes</category>
  <category>robust variance estimation</category>
  <guid>https://jepusto.com/publication/RVE-Meta-analysis-expanding-the-range/</guid>
  <pubDate>Fri, 01 Apr 2022 05:00:00 GMT</pubDate>
  <media:content url="https://jepusto.com/publication/RVE-Meta-analysis-expanding-the-range/Decision-tree for selecting a working model for use with RVE meta-regression" medium="image"/>
</item>
<item>
  <title>Cluster wild bootstrapping to handle dependent effect sizes in meta-analysis with a small number of studies</title>
  <dc:creator>Megha Joshi</dc:creator>
  <dc:creator>James E. Pustejovsky</dc:creator>
  <dc:creator>S. Natasha Beretvas</dc:creator>
  <link>https://jepusto.com/publication/cluster-wild-bootstrap-for-meta-analysis/</link>
  <description><![CDATA[ 






<p>The most common and well-known meta-regression models work under the assumption that there is only one effect size estimate per study and that the estimates are independent. However, meta-analytic reviews of social science research often include multiple effect size estimates per primary study, leading to dependence in the estimates. Some meta-analyses also include multiple studies conducted by the same lab or investigator, creating another potential source of dependence. An increasingly popular method to handle dependence is robust variance estimation (RVE), but this method can result in inflated Type I error rates when the number of studies is small. Small-sample correction methods for RVE have been shown to control Type I error rates adequately but may be overly conservative, especially for tests of multiple-contrast hypotheses. We evaluated an alternative method for handling dependence, cluster wild bootstrapping, which has been examined in the econometrics literature but not in the context of meta-analysis. Results from two simulation studies indicate that cluster wild bootstrapping maintains adequate Type I error rates and provides more power than extant small sample correction methods, particularly for multiple-contrast hypothesis tests. We recommend using cluster wild bootstrapping to conduct hypothesis tests for meta-analyses with a small number of studies. We have also created an R package that implements such tests.</p>



 ]]></description>
  <category>meta-analysis</category>
  <category>robust variance estimation</category>
  <category>bootstrap</category>
  <guid>https://jepusto.com/publication/cluster-wild-bootstrap-for-meta-analysis/</guid>
  <pubDate>Tue, 08 Feb 2022 06:00:00 GMT</pubDate>
</item>
<item>
  <title>Examining the effects of social stories on challenging behavior and prosocial skills in young children: A systematic review and meta-analysis</title>
  <dc:creator>Charis L. Wahman</dc:creator>
  <dc:creator>James E. Pustejovsky</dc:creator>
  <dc:creator>Michaelene M. Ostrosky</dc:creator>
  <dc:creator>Rosa Milagros Santos</dc:creator>
  <link>https://jepusto.com/publication/effects-of-social-stories-on-challenging-behavior-and-prosocial-skills/</link>
  <description><![CDATA[ 






<p>Social stories are a commonly used intervention practice in early childhood special education. Recent systematic reviews have documented the evidence-base for social stories, but findings are mixed. We examined the efficacy of social stories for young children (i.e., 3-5 years) with challenging behavior across 12 single-case studies, that included 30 participants. The What Works Clearinghouse standards for single case research design were used to evaluate the rigor of studies that included social stories as a primary intervention. For studies meeting standards, we synthesized findings on the efficacy of social stories using meta-analysis techniques and a recently developed parametric effect size measure, the log response ratio. Trends in participants’ response to treatment also were explored. Results indicate variability in rigor and efficacy for the use of social stories as an isolated intervention and in combination with other intervention approaches. Additional studies that investigate the efficacy of social stories as a primary intervention are warranted.</p>



 ]]></description>
  <category>single-case design</category>
  <category>meta-analysis</category>
  <category>systematic review</category>
  <guid>https://jepusto.com/publication/effects-of-social-stories-on-challenging-behavior-and-prosocial-skills/</guid>
  <pubDate>Tue, 01 Feb 2022 06:00:00 GMT</pubDate>
</item>
<item>
  <title>Evaluating the Transition to College Mathematics Course in Texas high schools: Examining heterogeneity across schools and student characteristics</title>
  <dc:creator>James E. Pustejovsky</dc:creator>
  <dc:creator>Megha Joshi</dc:creator>
  <link>https://jepusto.com/publication/Transition-to-College-Mathematics-Year-3/</link>
  <description><![CDATA[ 






<p>Texas House Bill 5 introduced requirements that school districts partner with institutions of higher education to provide college preparatory courses in mathematics and English language arts for high school seniors who are not yet college ready. In response to House Bill 5 requirements, the Charles A. Dana Center developed a model college preparatory mathematics course, Transition to College Mathematics Course (TCMC), which has been adopted by dozens of school districts across Texas over the past several school years. In prior work, we examined the effects of TCMC on students’ progress into post-secondary education by comparing two student cohorts who participated in TCMC to observationally similar students from the same cohort but who did not enroll in the course. In this report, we investigate the extent of heterogeneity in the effects of participating in TCMC. We find little evidence that the program was differentially effective for students from different socio-economic backgrounds, nor do we find evidence that program effects varied by the number of years that it had been offered. However, for key outcomes such as rates of passing a college-level math course, the effects of participating in TCMC may have varied across the schools that offered the course. Just as in prior work, these findings must be interpreted cautiously because we were unable to fully assess and account for students’ college-readiness status at the start of their senior year.</p>



 ]]></description>
  <category>causal inference</category>
  <category>evaluation</category>
  <category>Transition to College Mathematics</category>
  <category>heterogeneity</category>
  <guid>https://jepusto.com/publication/Transition-to-College-Mathematics-Year-3/</guid>
  <pubDate>Tue, 20 Apr 2021 05:00:00 GMT</pubDate>
</item>
<item>
  <title>A systematic review and meta-analysis of effects of psychosocial interventions on spiritual well-being in adults with cancer</title>
  <dc:creator>Laurie E. McLouth</dc:creator>
  <dc:creator>C. Graham Ford</dc:creator>
  <dc:creator>James E. Pustejovsky</dc:creator>
  <dc:creator>Crystal Park</dc:creator>
  <dc:creator>Allen C. Sherman</dc:creator>
  <dc:creator>Kelly Trevino</dc:creator>
  <dc:creator>John A. Salsman</dc:creator>
  <link>https://jepusto.com/publication/psychosocial-interventions-for-spiritual-well-being/</link>
  <description><![CDATA[ 






<p><strong>Objective</strong> Spiritual well‐being (SpWb) is an important dimension of health‐related quality of life for many cancer patients. Accordingly, an increasing number of psychosocial intervention studies have included SpWb as a study endpoint, and may improve SpWb even if not designed explicitly to do so. This meta‐analysis of randomized controlled trials (RCTs) evaluated effects of psychosocial interventions on SpWb in adults with cancer and tested potential moderators of intervention effects. <strong>Methods</strong> Six literature databases were systematically searched to identify RCTs of psychosocial interventions in which SpWb was an outcome. Doctoral‐level rater pairs extracted data using Covidence following Preferred Reporting Items for Systematic reviews and Meta‐Analyses guidelines. Standard meta‐analytic techniques were applied, including meta‐regression with robust variance estimation and risk‐of‐bias sensitivity analysis. <strong>Results</strong> Forty‐one RCTs were identified, encompassing 88 treatment effects among 3883 survivors. Interventions were associated with significant improvements in SpWb (<img src="https://latex.codecogs.com/png.latex?g%20=%200.22">, 95% CI [0.14, 0.29], <img src="https://latex.codecogs.com/png.latex?p%20%3C%200.0001">). Studies assessing the FACIT‐Sp demonstrated larger effect sizes than did those using other measures of SpWb (<img src="https://latex.codecogs.com/png.latex?g%20=%200.25">, 95% CI [0.17, 0.34], vs.&nbsp;<img src="https://latex.codecogs.com/png.latex?g%20=%200.10">, 95% CI [−0.02, 0.23], <img src="https://latex.codecogs.com/png.latex?p%20=%200.03">). No other intervention, clinical, or demographic characteristics significantly moderated effect size. <strong>Conclusions</strong> Psychosocial interventions are associated with small‐to‐medium‐sized effects on SpWb among cancer survivors. Future research should focus on conceptually coherent interventions explicitly targeting SpWb and evaluate interventions in samples that are diverse with respect to race and ethnicity, sex and cancer type.</p>



 ]]></description>
  <category>cancer</category>
  <category>meta-analysis</category>
  <category>psycho-social intervention</category>
  <category>systematic review</category>
  <guid>https://jepusto.com/publication/psychosocial-interventions-for-spiritual-well-being/</guid>
  <pubDate>Mon, 01 Feb 2021 06:00:00 GMT</pubDate>
</item>
<item>
  <title>Systematic review and meta-analysis of stay-play-talk interventions for improving social behaviors of young children</title>
  <dc:creator>Jennifer R. Ledford</dc:creator>
  <dc:creator>James E. Pustejovsky</dc:creator>
  <link>https://jepusto.com/publication/stay-play-talk-meta-analysis/</link>
  <description><![CDATA[ 






<p>Stay-play-talk (SPT) is a peer-mediated intervention which involves training peer implementers to stay in proximity to, play with, and talk to a focal child who has disabilities or lower social competence. This systematic review and meta-analysis investigated the contexts in which SPT interventions have been conducted, the methodological adequacy of the research assessing its effects, and the outcomes for both peer implementers and focal children. Studies have primarily occurred in inclusive preschool settings during free play activities, with researchers serving as facilitators. Average effects were positive for both peer implementers and focal children, although considerable heterogeneity across studies was observed. Additional research is needed to determine what peer implementer and focal child characteristics moderate intervention success, what modifications are needed for children who have complex communication needs, and optimal procedural variations (e.g., group size, training time).</p>



 ]]></description>
  <category>meta-analysis</category>
  <category>response ratio</category>
  <category>single-case design</category>
  <category>systematic review</category>
  <guid>https://jepusto.com/publication/stay-play-talk-meta-analysis/</guid>
  <pubDate>Tue, 05 Jan 2021 06:00:00 GMT</pubDate>
  <media:content url="https://jepusto.com/publication/stay-play-talk-meta-analysis/Forest plots of LRRi estimates for peer and focal participants" medium="image"/>
</item>
</channel>
</rss>
