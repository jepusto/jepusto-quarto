<?xml version="1.0" encoding="UTF-8"?>
<rss  xmlns:atom="http://www.w3.org/2005/Atom" 
      xmlns:media="http://search.yahoo.com/mrss/" 
      xmlns:content="http://purl.org/rss/1.0/modules/content/" 
      xmlns:dc="http://purl.org/dc/elements/1.1/" 
      version="2.0">
<channel>
<title>James E. Pustejovsky</title>
<link>https://mellifluous-buttercream-e2edd2.netlify.app/publication/</link>
<atom:link href="https://mellifluous-buttercream-e2edd2.netlify.app/publication/index.xml" rel="self" type="application/rss+xml"/>
<description>Education Statistics and Meta-Analysis</description>
<generator>quarto-1.4.552</generator>
<lastBuildDate>Mon, 24 Jun 2024 05:00:00 GMT</lastBuildDate>
<item>
  <title>Determining associations between intervention amount and outcomes for young autistic children: A systematic review and meta-analysis</title>
  <dc:creator>Micheal Sandbank</dc:creator>
  <dc:creator>James E. Pustejovsky</dc:creator>
  <dc:creator>Kristen Bottema-Beutel</dc:creator>
  <dc:creator>Nicolette Caldwell</dc:creator>
  <dc:creator>Jacob I. Feldman</dc:creator>
  <dc:creator>Shannon Crowley LaPoint</dc:creator>
  <dc:creator>Tiffany Woynaroski</dc:creator>
  <link>https://mellifluous-buttercream-e2edd2.netlify.app/publication/More-is-not-necessarily-better/</link>
  <description><![CDATA[ 






<section id="importance" class="level3">
<h3 class="anchored" data-anchor-id="importance">Importance</h3>
<p>Health professionals routinely recommend “intensive interventions” (i.e., 20-40 hours per week) for autistic children. However, primary research backing this recommendation is sparse and plagued by methodological flaws.</p>
</section>
<section id="objective" class="level3">
<h3 class="anchored" data-anchor-id="objective">Objective</h3>
<p>We examined whether different metrics of intervention amount are associated with intervention effects on any developmental domain for young autistic children.</p>
</section>
<section id="data-sources" class="level3">
<h3 class="anchored" data-anchor-id="data-sources">Data Sources</h3>
<p>We used a large corpus of studies <img src="https://latex.codecogs.com/png.latex?(n%20=%20144)"> taken from a recent meta-analysis (with a search date of November 2021) of early interventions for autistic children.</p>
</section>
<section id="study-selection" class="level3">
<h3 class="anchored" data-anchor-id="study-selection">Study Selection</h3>
<p>Studies were eligible if they reported a quasi-experimental or randomized controlled trial testing the effects of a nonpharmacological intervention on any outcome in participant samples comprising &gt;50% autistic children age 8 or younger.</p>
</section>
<section id="data-extraction-and-synthesis" class="level3">
<h3 class="anchored" data-anchor-id="data-extraction-and-synthesis">Data Extraction and Synthesis</h3>
<p>Data were independently extracted by multiple coders. We constructed meta-regression models to determine whether each index of intervention amount was associated with effect sizes for each intervention type, while controlling for outcome domain, outcome proximity, age of participants, study design, and risk of detection bias.</p>
</section>
<section id="main-outcomes-and-measures" class="level3">
<h3 class="anchored" data-anchor-id="main-outcomes-and-measures">Main Outcomes and Measures</h3>
<p>The primary predictor of interest was intervention amount, quantified using three different metrics (daily intensity, duration, and cumulative intensity). The primary outcomes of interest were gains in any developmental domain, quantified by Hedges <img src="https://latex.codecogs.com/png.latex?g"> effect sizes.</p>
</section>
<section id="results" class="level3">
<h3 class="anchored" data-anchor-id="results">Results</h3>
<p>None of the meta-regression models evidenced a significant, positive association between any index of intervention amount and intervention effect size when considered within intervention type.</p>
</section>
<section id="conclusions-and-relevance" class="level3">
<h3 class="anchored" data-anchor-id="conclusions-and-relevance">Conclusions and Relevance</h3>
<p>Our findings do not support the assertion that intervention effects increase with increasing amounts of intervention. Health professionals recommending supports should be advised that there is little robust evidence supporting the provision of “intensive” intervention. Our prior meta-analyses suggest that some interventions have the potential to improve some outcomes for autistic children during early childhood. The present results, however, suggest that providing more of such interventions will not necessarily result in larger effects on outcomes of interest. There is a pressing need for future primary research to be designed and conducted in a manner that allows us to determine the most supportive amounts of interventions by child characteristics, intervention type, and the outcomes targeted by the intervention.</p>


</section>

<a onclick="window.scrollTo(0, 0); return false;" id="quarto-back-to-top"><i class="bi bi-arrow-up"></i> Back to top</a> ]]></description>
  <category>systematic review</category>
  <category>autism</category>
  <category>meta-analysis</category>
  <guid>https://mellifluous-buttercream-e2edd2.netlify.app/publication/More-is-not-necessarily-better/</guid>
  <pubDate>Mon, 24 Jun 2024 05:00:00 GMT</pubDate>
</item>
<item>
  <title>Equivalences between ad hoc strategies and meta-analytic models for dependent effect sizes</title>
  <dc:creator>James E. Pustejovsky</dc:creator>
  <dc:creator>Man Chen</dc:creator>
  <link>https://mellifluous-buttercream-e2edd2.netlify.app/publication/Equivalences-between-ad-hoc-strategies-and-models/</link>
  <description><![CDATA[ 






<p>Meta-analyses of educational research findings frequently involve statistically dependent effect size estimates. Meta-analysts have often addressed dependence issues using ad hoc approaches that involve modifying the data to conform to the assumptions of models for independent effect size estimates, such as aggregating estimates to obtain one summary estimate per study, conducting separate analyses of distinct subgroups of estimates, or combinations thereof. We demonstrate that these ad hoc approaches correspond exactly to certain multivariate models for dependent effect sizes. Specifically, we describe classes of multivariate random effects models that have likelihoods equivalent to those of models for effect sizes that have been averaged by study, classified into subgroups, or both. The equivalence also applies to robust variance estimation methods.</p>



<a onclick="window.scrollTo(0, 0); return false;" id="quarto-back-to-top"><i class="bi bi-arrow-up"></i> Back to top</a> ]]></description>
  <category>meta-analysis</category>
  <category>meta-regression</category>
  <category>dependent effect sizes</category>
  <category>robust variance estimation</category>
  <guid>https://mellifluous-buttercream-e2edd2.netlify.app/publication/Equivalences-between-ad-hoc-strategies-and-models/</guid>
  <pubDate>Thu, 07 Mar 2024 06:00:00 GMT</pubDate>
</item>
<item>
  <title>High replicability of newly-discovered social-behavioral findings is achievable.</title>
  <dc:creator>John Protzko</dc:creator>
  <dc:creator>Jon Krosnick</dc:creator>
  <dc:creator>Leif Nelson</dc:creator>
  <dc:creator>Brian Nosek</dc:creator>
  <dc:creator>Jordan Axt</dc:creator>
  <dc:creator>Matt Berent</dc:creator>
  <dc:creator>Nicholas Buttrick</dc:creator>
  <dc:creator>Matthew DeBell</dc:creator>
  <dc:creator>Charles R. Ebersole</dc:creator>
  <dc:creator>Sebastian Lundmark</dc:creator>
  <dc:creator>Bo MacInnis</dc:creator>
  <dc:creator>Michael O&#39;Donnell</dc:creator>
  <dc:creator>Hannah Perfecto</dc:creator>
  <dc:creator>James E. Pustejovsky</dc:creator>
  <dc:creator>Scott Roeder</dc:creator>
  <dc:creator>Jan Walleczek</dc:creator>
  <dc:creator>Jonathan W. Schooler</dc:creator>
  <link>https://mellifluous-buttercream-e2edd2.netlify.app/publication/Decline-effects/</link>
  <description><![CDATA[ 






<p>Failures to replicate evidence of new discoveries have forced scientists to ask whether this unreliability is due to suboptimal implementation of optimal methods or whether presumptively optimal methods are not, in fact, optimal. This paper reports an investigation by four coordinated laboratories of the prospective replicability of 16 novel experimental findings using current optimal practices: high statistical power, preregistration, and complete methodological transparency. In contrast to past systematic replication efforts that reported replication rates averaging 50%, replication attempts here produced the expected effects with significance testing (p&lt;.05) in 86% of attempts, slightly exceeding maximum expected replicability based on observed effect size and sample size. When one lab attempted to replicate an effect discovered by another lab, the effect size in the replications was 97% that of the original study. This high replication rate justifies confidence in rigor enhancing methods and suggests that past failures to replicate may be attributable to departures from optimal procedures.</p>



<a onclick="window.scrollTo(0, 0); return false;" id="quarto-back-to-top"><i class="bi bi-arrow-up"></i> Back to top</a> ]]></description>
  <category>replication</category>
  <category>meta-analysis</category>
  <guid>https://mellifluous-buttercream-e2edd2.netlify.app/publication/Decline-effects/</guid>
  <pubDate>Thu, 09 Nov 2023 06:00:00 GMT</pubDate>
  <media:content url="https://mellifluous-buttercream-e2edd2.netlify.app/publication/Decline-effects/Effect sizes and 95% CI from 16 new discoveries (yellow marks) in the social-behavioral sciences, each with four replications. Each lab is designated by a unique shape for observed effect size; blue marks correspond to self-replications, green marks to independent replications." medium="image"/>
</item>
<item>
  <title>Conducting power analysis for meta-analysis of dependent effect sizes: Common guidelines and an introduction to the POMADE R package</title>
  <dc:creator>Mikkel H. Vembye</dc:creator>
  <dc:creator>James E. Pustejovsky</dc:creator>
  <dc:creator>Terri D. Pigott</dc:creator>
  <link>https://mellifluous-buttercream-e2edd2.netlify.app/publication/Conducting-POMADE/</link>
  <description><![CDATA[ 






<p>Sample size and statistical power are important factors to consider when planning a research synthesis. Power analysis methods have been developed for fixed effect or random effects models, but until recently these methods were limited to simple data structures with a single, independent effect per study. Recent work has provided power approximation formulas for meta-analyses involving studies with multiple, dependent effect size estimates, which are common in syntheses of social science research. Prior work focused on developing and validating the approximations, but did not address the practice challenges encountered in applying them for purposes of planning a synthesis involving dependent effect sizes. We aim to facilitate application of these recent developments by providing practical guidance on how to conduct power analysis for planning a meta-analysis of dependent effect sizes and by introducing a new R package, POMADE, designed for this purpose. We present a comprehensive overview of resources for finding information about the study design features and model parameters needed to conduct power analysis, along with detailed worked examples using the POMADE package. For presenting power analysis findings, we emphasize graphical tools that can depict power under a range of pausible assumptions and introduce a novel plot, the traffic light power plot, for conveying the degree of certainty in one’s assumptions.</p>



<a onclick="window.scrollTo(0, 0); return false;" id="quarto-back-to-top"><i class="bi bi-arrow-up"></i> Back to top</a> ]]></description>
  <category>power</category>
  <category>meta-analysis</category>
  <category>dependent effect sizes</category>
  <category>robust variance estimation</category>
  <guid>https://mellifluous-buttercream-e2edd2.netlify.app/publication/Conducting-POMADE/</guid>
  <pubDate>Fri, 21 Jul 2023 05:00:00 GMT</pubDate>
  <media:content url="https://mellifluous-buttercream-e2edd2.netlify.app/software/POMADE/featured.png" medium="image" type="image/png" height="144" width="144"/>
</item>
<item>
  <title>The efficacy of combining cognitive training and non-invasive brain stimulation: A transdiagnostic systematic review and meta-analysis</title>
  <dc:creator>Anika Poppe</dc:creator>
  <dc:creator>Franziska D. E. Ritter</dc:creator>
  <dc:creator>Leonie Bais</dc:creator>
  <dc:creator>James E. Pustejovsky</dc:creator>
  <dc:creator>Marie-José van Tol</dc:creator>
  <dc:creator>Branislava Ćurčić-Blake</dc:creator>
  <dc:creator>Gerdina H.M. Pijnenborg</dc:creator>
  <dc:creator>Lisette van der Meer</dc:creator>
  <link>https://mellifluous-buttercream-e2edd2.netlify.app/publication/Efficacy-of-combining-cognitive-training-and-NIBS/</link>
  <description><![CDATA[ 






<p>Over the past decade, an increasing number of studies investigated the innovative approach of supplementing cognitive training (CT) with non-invasive brain stimulation (NIBS) to increase the effects on outcomes. In this review, we aim to summarize the evidence for this treatment combination. We identified 72 published and unpublished studies (reporting 773 effect sizes) including 2518 participants from healthy and clinical populations indexed in PubMed, Medline, PsycINFO, ProQuest, Web of Science, and ClinicalTrials.gov (last search: 8/8/2022) that compared the effects of NIBS combined with CT on cognitive, symptoms and everyday functioning to CT alone at post-intervention and/or follow-up. We performed random-effects meta-analyses with robust variance estimation and assessed risk of bias with the Cochrane R.O.B. tool. Only four studies had low risk of bias in all domains, and many studies lacked standard controls such as keeping the outcome assessor and trainer unaware of the treatment condition. Following sensitivity analyses, only learning/memory robustly improved significantly more when CT was combined with NIBS compared to CT only (<img src="https://latex.codecogs.com/png.latex?g"> = 0.18, 95% CI [0.07, 0.29]) at post-intervention, but not in the long-term. The effect was small and limited by substantial heterogeneity. The other seven cognitive outcome domains, symptoms, and everyday functioning did not benefit from adding NIBS to CT. Given the methodological limitation of prior studies, more high-quality trials that focus on the potential of combining NIBS and CT to enhance benefits in everyday functioning in the short- and long-term are needed to evaluate whether combining NIBS and CT is relevant for clinical practice.</p>



<a onclick="window.scrollTo(0, 0); return false;" id="quarto-back-to-top"><i class="bi bi-arrow-up"></i> Back to top</a> ]]></description>
  <category>meta-analysis</category>
  <category>systematic review</category>
  <category>cognitive remediation</category>
  <category>non-invasive brain stimulation</category>
  <guid>https://mellifluous-buttercream-e2edd2.netlify.app/publication/Efficacy-of-combining-cognitive-training-and-NIBS/</guid>
  <pubDate>Fri, 21 Jul 2023 05:00:00 GMT</pubDate>
</item>
<item>
  <title>Systematic review of variables related to instruction in augmentative and alternative communication implementation: Group and single-case design</title>
  <dc:creator>Joe Reichle</dc:creator>
  <dc:creator>James E. Pustejovsky</dc:creator>
  <dc:creator>Kimberly J. Vannest</dc:creator>
  <dc:creator>Margaret Foster</dc:creator>
  <dc:creator>Lauren M. Pierson</dc:creator>
  <dc:creator>Sanikan Wattanawongwan</dc:creator>
  <dc:creator>Man Chen</dc:creator>
  <dc:creator>Marcus Fuller</dc:creator>
  <dc:creator>April N. Haas</dc:creator>
  <dc:creator>Bethany H. Bhat</dc:creator>
  <dc:creator>Mary R. Sallese</dc:creator>
  <dc:creator>S. D. Smith</dc:creator>
  <dc:creator>Valeria Yllades</dc:creator>
  <dc:creator>Daira Rodriguez</dc:creator>
  <dc:creator>Amara Yoro</dc:creator>
  <dc:creator>Jay B. Ganz</dc:creator>
  <link>https://mellifluous-buttercream-e2edd2.netlify.app/publication/AAC-group-and-SCD/</link>
  <description><![CDATA[ 






<p><strong>Purpose</strong>: This article provides a systematic review and analysis of group and single-case studies addressing augmentative and alternative communication (AAC) intervention with school-aged persons having autism spectrum disorder (ASD) and/or intellectual/developmental disabilities resulting in complex communication needs (CCNs). Specifically, we examined participant characteristics in group-design studies reporting AAC intervention outcomes and how these compared to those reported in single-case experimental designs (SCEDs). In addition, we compared the status of intervention features reported in group and SCED studies with respect to instructional strategies utilized.</p>
<p><strong>Participants</strong>: Participants included school-aged individuals with CCNs who also experienced ASD or ASD with an intellectual delay who utilized aided or unaided AAC.</p>
<p><strong>Method</strong>: A systematic review using descriptive statistics and effect sizes was implemented.</p>
<p><strong>Results</strong>: Findings revealed that participant features such as race, ethnicity, and home language continue to be underreported in both SCED and group-design studies. Participants in SCED investigations more frequently used multiple communication modes when compared to participants in group studies. The status of pivotal skills such as imitation was sparsely reported in both types of studies. With respect to instructional features, group-design studies were more apt to utilize clinical rather than educational or home settings when compared with SCED studies. In addition, SCED studies were more apt to utilize instructional methods that closely adhered to instructional features more typically characterized as being associated with behavioral approaches.</p>
<p><strong>Conclusion</strong>: The authors discuss future research needs, practice implications, and a more detailed specification of treatment intensity parameters for future research.</p>



<a onclick="window.scrollTo(0, 0); return false;" id="quarto-back-to-top"><i class="bi bi-arrow-up"></i> Back to top</a> ]]></description>
  <category>single-case design</category>
  <category>systematic review</category>
  <category>Augmentative and alternative communication</category>
  <guid>https://mellifluous-buttercream-e2edd2.netlify.app/publication/AAC-group-and-SCD/</guid>
  <pubDate>Fri, 26 May 2023 05:00:00 GMT</pubDate>
</item>
<item>
  <title>Comparison of competing approaches to analyzing cross-classified data: Random effects models, ordinary least squares, or fixed effects with cluster robust standard errors</title>
  <dc:creator>Young Ri Lee</dc:creator>
  <dc:creator>James E. Pustejovsky</dc:creator>
  <link>https://mellifluous-buttercream-e2edd2.netlify.app/publication/competing-approaches-for-cross-classified-data/</link>
  <description><![CDATA[ 






<p>Cross-classified random effects modeling (CCREM) is a common approach for analyzing cross-classified data in education. However, when the focus of a study is on the regression coefficients at level one rather than on the random effects, ordinary least squares regression with cluster robust variance estimators (OLS-CRVE) or fixed effects regression with CRVE (FE-CRVE) could be appropriate approaches. These alternative methods may be advantageous because they rely on weaker assumptions than what is required by CCREM. We conducted a Monte Carlo Simulation study to compare the performance of CCREM, OLS-CRVE, and FE-CRVE in models with crossed random effects, including conditions where homoscedasticity assumptions and exogeneity assumptions held and conditions where they were violated. We found that CCREM performed the best when its assumptions are all met. However, when homoscedasticity assumptions are violated, OLS-CRVE and FE-CRVE provided similar or better performance than CCREM. FE-CRVE showed the best performance when the exogeneity assumption is violated. Thus, we recommend two-way FE-CRVE as a good alternative to CCREM, particularly if the homoscedasticity or exogeneity assumptions of the CCREM might be in doubt.</p>



<a onclick="window.scrollTo(0, 0); return false;" id="quarto-back-to-top"><i class="bi bi-arrow-up"></i> Back to top</a> ]]></description>
  <category>simulation</category>
  <category>random effects</category>
  <category>robust variance estimation</category>
  <category>fixed effects</category>
  <guid>https://mellifluous-buttercream-e2edd2.netlify.app/publication/competing-approaches-for-cross-classified-data/</guid>
  <pubDate>Thu, 09 Mar 2023 06:00:00 GMT</pubDate>
</item>
<item>
  <title>Between-case standardized mean differences: Flexible methods for single-case designs</title>
  <dc:creator>Man Chen</dc:creator>
  <dc:creator>James E. Pustejovsky</dc:creator>
  <dc:creator>David A. Klingbeil</dc:creator>
  <dc:creator>Ethan R. Van Norman</dc:creator>
  <link>https://mellifluous-buttercream-e2edd2.netlify.app/publication/Three-level-BC-SMD/</link>
  <description><![CDATA[ 






<p>Single-case designs (SCDs) are a class of research methods for evaluating the effects of academic and behavioral interventions in educational and clinical settings. Although visual analysis is typically the first and main method for primary analysis of data from SCDs, quantitative methods are useful for synthesizing results and drawing systematic generalizations across bodies of single-case research. Researchers who are interested in synthesizing findings across SCDs and between-group designs might consider using the between-case standardized mean difference (BC-SMD) effect size, which aims to put results from both types of studies into a common metric. Current BC-SMD methods are limited to treatment reversal design and across-participant multiple baseline design, yet more complex designs are used in practice. In this study, we extend available BC-SMD methods to several variations of the multiple baseline design, including the replicated multiple baseline across behaviors or settings, the clustered multiple baseline design, and the multivariate multiple baseline across participants. For each variation, we describe methods for estimating BC-SMD effect sizes and illustrate our proposed approach by re-analyzing data from a published SCD study.</p>



<a onclick="window.scrollTo(0, 0); return false;" id="quarto-back-to-top"><i class="bi bi-arrow-up"></i> Back to top</a> ]]></description>
  <category>single-case design</category>
  <category>effect size</category>
  <category>standardized mean difference</category>
  <category>design-comparable SMD</category>
  <category>hierarchical models</category>
  <category>multivariate</category>
  <guid>https://mellifluous-buttercream-e2edd2.netlify.app/publication/Three-level-BC-SMD/</guid>
  <pubDate>Wed, 08 Mar 2023 06:00:00 GMT</pubDate>
  <media:content url="https://mellifluous-buttercream-e2edd2.netlify.app/publication/Three-level-BC-SMD/Clustered multiple baseline design data from Bryant et al. (2018)" medium="image"/>
</item>
<item>
  <title>A case for increased rigor in AAC research: A methodological quality review</title>
  <dc:creator>Jay B. Ganz</dc:creator>
  <dc:creator>James E. Pustejovsky</dc:creator>
  <dc:creator>Joe Reichle</dc:creator>
  <dc:creator>Kimberly J. Vannest</dc:creator>
  <dc:creator>Lauren M. Pierson</dc:creator>
  <dc:creator>Sanikan Wattanawongwan</dc:creator>
  <dc:creator>Margaret Foster</dc:creator>
  <dc:creator>Marcus Fuller</dc:creator>
  <dc:creator>April N. Haas</dc:creator>
  <dc:creator>Mary Rose Sallese</dc:creator>
  <dc:creator>S. D. Smith</dc:creator>
  <dc:creator>Valeria Yllades</dc:creator>
  <link>https://mellifluous-buttercream-e2edd2.netlify.app/publication/AAC-increased-rigor/</link>
  <description><![CDATA[ 






<p>This comprehensive review reports on methodological quality of 162 single-case studies on augmentative and alternative communication interventions for communication and challenging behavior in individuals diagnosed with autism or intellectual disabilities and with complex communication needs. Following review for inclusion criteria, documents were excluded if they failed to meet basic methodological standards. Each remaining study was evaluated for 10 detailed quality criteria. No studies met all standards without reservations. Only three of the included studies met all of the standards with reservations and the remainder met some but not all standards, with or without reservations. The included studies reported adequate detail for half of the quality indicators, but insufficient details for participant, setting, maintenance, generalization, and social validity descriptions. An increased quantity and quality of research were found in over four decades. More recent studies have adequately reported half of the criteria investigated, including describing the materials, defining the outcome variables, describing baseline and intervention procedures, and evaluating procedural integrity. After identifying quality features, the authors report in more detail on low-rated quality indicators particularly relevant to studies addressing social-communication interventions. The literature infrequently reported race, ethnicity, or home language. Future research should report characteristics of participants to ensure that research becomes representative of the population.</p>



<a onclick="window.scrollTo(0, 0); return false;" id="quarto-back-to-top"><i class="bi bi-arrow-up"></i> Back to top</a> ]]></description>
  <category>single-case design</category>
  <category>systematic review</category>
  <category>Augmentative and alternative communication</category>
  <guid>https://mellifluous-buttercream-e2edd2.netlify.app/publication/AAC-increased-rigor/</guid>
  <pubDate>Fri, 20 Jan 2023 06:00:00 GMT</pubDate>
</item>
<item>
  <title>Social validity, cost, acceptability, and feasibility of Augmentative and Alternative Communication devices used for individuals with autism spectrum disorder and intellectual disability: A systematic review</title>
  <dc:creator>Lauren M. Pierson</dc:creator>
  <dc:creator>Jay B. Ganz</dc:creator>
  <dc:creator>James E. Pustejovsky</dc:creator>
  <dc:creator>Joe Reichle</dc:creator>
  <dc:creator>Kimberly J. Vannest</dc:creator>
  <dc:creator>Sanikan Wattanawongwan</dc:creator>
  <dc:creator>Margaret Foster</dc:creator>
  <dc:creator>Marcus C. Fuller</dc:creator>
  <dc:creator>April N. Haas</dc:creator>
  <dc:creator>Mary Rose Sallese</dc:creator>
  <dc:creator>S. D. Smith</dc:creator>
  <dc:creator>Valeria Yllades</dc:creator>
  <dc:creator>Emily Kenny</dc:creator>
  <dc:creator>Peyton Morgan</dc:creator>
  <dc:creator>Scout Paterson</dc:creator>
  <link>https://mellifluous-buttercream-e2edd2.netlify.app/publication/AAC-social-validity/</link>
  <description><![CDATA[ 






<p><strong>Purpose</strong>: The authors conducted a systematic review of single-case experimental designs that included individuals with autism spectrum disorder and/or intellectual disability who used speech-generating devices (SGDs) for communication. The purpose of this study was to review subjective and normative pre- and post-intervention social validity data, in addition to the cost, acceptability, and feasibility of the SGDs used in the studies. The authors also studied trends in the reporting of pre- and post-intervention data over time.</p>
<p><strong>Method</strong>: A systematic review of 7,327 articles resulted in 86 articles that met design quality criteria and included participants who used SGDs. A group of raters completed interrater reliability for all stages of the review.</p>
<p><strong>Results</strong>: Researchers reported more subjective than normative data. Few studies reported on the price of the SGD or the person who purchased the SGD. More researchers reported using an SGD with more than one use, but few solicited feedback about the SGD used during the intervention. Few researchers reported information about the portability of the device or the operation effort.</p>
<p><strong>Conclusions</strong>: Reporting on social validity represents a substantial limitation in experimental studies. Future work incorporating less biased/more objective measures is important for the creation of socially valid interventions.</p>



<a onclick="window.scrollTo(0, 0); return false;" id="quarto-back-to-top"><i class="bi bi-arrow-up"></i> Back to top</a> ]]></description>
  <category>single-case design</category>
  <category>systematic review</category>
  <category>Augmentative and alternative communication</category>
  <guid>https://mellifluous-buttercream-e2edd2.netlify.app/publication/AAC-social-validity/</guid>
  <pubDate>Wed, 14 Dec 2022 06:00:00 GMT</pubDate>
</item>
<item>
  <title>Single case design research in Special Education: Next generation standards and considerations</title>
  <dc:creator>Jennifer R. Ledford</dc:creator>
  <dc:creator>Joseph Lambert</dc:creator>
  <dc:creator>James E. Pustejovsky</dc:creator>
  <dc:creator>Kathleen N. Zimmerman</dc:creator>
  <dc:creator>Nicole Hollins</dc:creator>
  <dc:creator>Erin E. Barton</dc:creator>
  <link>https://mellifluous-buttercream-e2edd2.netlify.app/publication/Single-case-next-generation-standards/</link>
  <description><![CDATA[ 






<p>Single case design has a long history of use for assessing intervention effectiveness for children with disabilities. Although these designs have been widely employed for more than 50 years, recent years have been especially dynamic in terms of growth in the use of single case design and application of standards designed to improve the validity and applicability of findings. This growth expanded possibilities and inspired new questions about the contributions this methodology can make to generalizable knowledge about intervention in special education. In this paper, we discuss and extend previous standards for studies using single case designs (i.e., Horner et al., 2005). We identify new suggestions for internal validity, generality and acceptability, and reporting. We also provide considerations for single case synthesis and discuss the complexities of assessing accumulating evidence for a given practice.</p>



<a onclick="window.scrollTo(0, 0); return false;" id="quarto-back-to-top"><i class="bi bi-arrow-up"></i> Back to top</a> ]]></description>
  <category>single-case design</category>
  <category>meta-analysis</category>
  <category>effect size</category>
  <guid>https://mellifluous-buttercream-e2edd2.netlify.app/publication/Single-case-next-generation-standards/</guid>
  <pubDate>Tue, 22 Nov 2022 06:00:00 GMT</pubDate>
</item>
<item>
  <title>Power approximations for overall average effects in meta-analysis of dependent effect sizes</title>
  <dc:creator>Mikkel H. Vembye</dc:creator>
  <dc:creator>James E. Pustejovsky</dc:creator>
  <dc:creator>Terri D. Pigott</dc:creator>
  <link>https://mellifluous-buttercream-e2edd2.netlify.app/publication/Power-approximations-for-dependent-effects/</link>
  <description><![CDATA[ 






<p>Meta-analytic models for dependent effect sizes have grown increasingly sophisticated over the last few decades, which has created challenges for a priori power calculations. We introduce power approximations for tests of average effect sizes based upon several common approaches for handling dependent effect sizes. In a Monte Carlo simulation, we show that the new power formulas can accurately approximate the true power of meta-analytic models for dependent effect sizes. Lastly, we investigate the Type I error rate and power for several common models, finding that tests using robust variance estimation provide better Type I error calibration than tests with model-based variance estimation. We consider implications for practice with respect to selecting a working model and an inferential approach.</p>



<a onclick="window.scrollTo(0, 0); return false;" id="quarto-back-to-top"><i class="bi bi-arrow-up"></i> Back to top</a> ]]></description>
  <category>power</category>
  <category>meta-analysis</category>
  <category>dependent effect sizes</category>
  <category>robust variance estimation</category>
  <guid>https://mellifluous-buttercream-e2edd2.netlify.app/publication/Power-approximations-for-dependent-effects/</guid>
  <pubDate>Mon, 17 Oct 2022 05:00:00 GMT</pubDate>
  <media:content url="https://mellifluous-buttercream-e2edd2.netlify.app/publication/Power-approximations-for-dependent-effects/Power difference between approximated and true (simulated) power versus approximated power for the C(H)E working models, across different methods of sampling study characteristics." medium="image"/>
</item>
<item>
  <title>Investigating narrative performance in children with developmental language disorder: A systematic review and meta-analysis</title>
  <dc:creator>Katherine L. Winters</dc:creator>
  <dc:creator>Javier Jasso</dc:creator>
  <dc:creator>James E. Pustejovsky</dc:creator>
  <dc:creator>Courtney Byrd</dc:creator>
  <link>https://mellifluous-buttercream-e2edd2.netlify.app/publication/investigating-narrative-performance/</link>
  <description><![CDATA[ 






<p><strong>Purpose</strong>: Speech-language pathologists (SLPs) typically examine narrative performance when completing a comprehensive language assessment. However, there is significant variability in the methodologies used to evaluate narration. The primary aims of this systematic review and meta-analysis were to a) investigate how narrative assessment type (e.g., macrostructure, microstructure, internal state language) differentiates typically developing (TD) children from children with developmental language disorder (DLD), or, TD–DLD group differences, b) identify specific narrative assessment measures (e.g., number of different words) that result in greater TD–DLD differences, and, c) evaluate participant and sample characteristics (e.g., DLD inclusionary criteria) that may uniquely influence performance differences.</p>
<p><strong>Method</strong>: Three electronic databases (PsychInfo, ERIC, and PubMed) and ASHAWire were searched on July 30, 2019 to locate studies that reported oral narrative language measures for both DLD and TD groups between ages 4 and 12 years; studies focusing on written narration or other developmental disorders only were excluded. Thirty-seven primary studies were identified via a three-step study selection procedure. We extracted data related to the sample participants, the narrative task(s) and assessment measures, and research design. Standardized mean differences using a bias-corrected Hedges’ <img src="https://latex.codecogs.com/png.latex?g"> were the calculated effect sizes (<img src="https://latex.codecogs.com/png.latex?N%20=%20382">). Research questions were analyzed using mixed-effects meta-regression with robust variance estimation to account for effect size dependencies.</p>
<p><strong>Results</strong>: Searches identified eligible studies published between 1987 and 2019. An overall meta-analysis using 382 effect sizes obtained across 37 studies showed that children with DLD had decreased narrative performance relative to TD peers, with summary estimates ranging from -0.850, 95% CI [-1.016, -0.685] to -0.794, 95% CI [-0.963, -0.624], depending on the correlation assumed. Across all models, effect size estimates showed significant heterogeneity both between and within studies, even after accounting for effect size-, sample-, and study-level predictors. Grammatical accuracy (microstructure) and story grammar (macrostructure) yielded the most consistent evidence of significant TD–DLD group differences across statistical models.</p>
<p><strong>Conclusions</strong>: Present findings suggest some narrative assessment measures may yield significantly different performance between children with and without DLD. However, researchers need to be consistent in their inclusionary criteria, their description of sample characteristics, and in their reporting of the correlations of measures, in order to determine which assessment measures are more likely to yield group differences.</p>



<a onclick="window.scrollTo(0, 0); return false;" id="quarto-back-to-top"><i class="bi bi-arrow-up"></i> Back to top</a> ]]></description>
  <category>narrative assessment</category>
  <category>developmental language disorder</category>
  <category>meta-analysis</category>
  <category>dependent effect sizes</category>
  <category>robust variance estimation</category>
  <guid>https://mellifluous-buttercream-e2edd2.netlify.app/publication/investigating-narrative-performance/</guid>
  <pubDate>Fri, 30 Sep 2022 05:00:00 GMT</pubDate>
  <media:content url="https://mellifluous-buttercream-e2edd2.netlify.app/publication/investigating-narrative-performance/Forest Plot of TD-DLD Group Differences in Narrative Performance" medium="image"/>
</item>
<item>
  <title>Multi-level meta-analysis of single-case experimental designs using robust variance estimation</title>
  <dc:creator>Man Chen</dc:creator>
  <dc:creator>James E. Pustejovsky</dc:creator>
  <link>https://mellifluous-buttercream-e2edd2.netlify.app/publication/SCED-MLMA-RVE/</link>
  <description><![CDATA[ 






<p>Single-case experimental designs (SCEDs) are used to study the effects of interventions on the behavior of individual cases, by making comparisons between repeated measurements of an outcome under different conditions. In research areas where SCEDs are prevalent, there is a need for methods to synthesize results across multiple studies. One approach to synthesis uses a multi-level meta-analysis (MLMA) model to describe the distribution of effect sizes across studies and across cases within studies. However, MLMA relies on having accurate sampling variances of effect size estimates for each case, which may not be possible due to auto-correlation in the raw data series. One possible solution is to combine MLMA with robust variance estimation (RVE), which provides valid assessments of uncertainty even if the sampling variances of effect size estimates are inaccurate. Another possible solution is to forgo MLMA and use simpler, ordinary least squares (OLS) methods with RVE. This study evaluates the performance of effect size estimators and methods of synthesizing SCEDs in the presence of auto-correlation, for several different effect size metrics, via a Monte Carlo simulation designed to emulate the features of real data series. Results demonstrate that the MLMA model with RVE performs properly in terms of bias, accuracy, and confidence interval coverage for estimating overall average log response ratios. The OLS estimator corrected with RVE performs the best in estimating overall average Tau effect sizes. None of the available methods perform adequately for meta-analysis of within-case standardized mean differences.</p>



<a onclick="window.scrollTo(0, 0); return false;" id="quarto-back-to-top"><i class="bi bi-arrow-up"></i> Back to top</a> ]]></description>
  <category>single-case design</category>
  <category>meta-analysis</category>
  <category>robust variance estimation</category>
  <category>response ratio</category>
  <category>non-overlap measures</category>
  <category>standardized mean difference</category>
  <guid>https://mellifluous-buttercream-e2edd2.netlify.app/publication/SCED-MLMA-RVE/</guid>
  <pubDate>Fri, 29 Apr 2022 05:00:00 GMT</pubDate>
</item>
<item>
  <title>Augmentative and Alternative Communication intervention targets for school-aged participants with ASD and ID: A single-case systematic review and meta-analysis</title>
  <dc:creator>Jay B. Ganz</dc:creator>
  <dc:creator>James E. Pustejovsky</dc:creator>
  <dc:creator>Joe Reichle</dc:creator>
  <dc:creator>Kimberly J. Vannest</dc:creator>
  <dc:creator>Margaret Foster</dc:creator>
  <dc:creator>Marcus Fuller</dc:creator>
  <dc:creator>Lauren M. Pierson</dc:creator>
  <dc:creator>Sanikan Wattanawongwan</dc:creator>
  <dc:creator>Amando Bernal</dc:creator>
  <dc:creator>Man Chen</dc:creator>
  <dc:creator>April N. Haas</dc:creator>
  <dc:creator>Rachel Skov</dc:creator>
  <dc:creator>S. D. Smith</dc:creator>
  <dc:creator>Valeria Yllades</dc:creator>
  <link>https://mellifluous-buttercream-e2edd2.netlify.app/publication/AAC-communication-outcomes/</link>
  <description><![CDATA[ 






<p><strong>Objective</strong>: This meta-analysis reviews the literature on communication modes, communicative functions, and types of augmentative and alternative communication (AAC) interventions for school-age participants with autism spectrum disorders and/or intellectual disabilities who experience complex communication needs. Considering potential differences related to outcomes that were targeted for intervention could help identify the most effective means of individualizing AAC interventions.</p>
<p><strong>Methods</strong>: We performed a systematic literature search using Academic Search Ultimate, ERIC, PsycINFO, Web of Science, and Proquest Dissertations &amp; Theses Global to retrieve research conducted between 1978 and the beginning of 2020. Studies included in the synthesis are (a) in English; (b) has one or more participants with an intellectual delay, developmental disability(ies); (c) reported the results of an augmentative and alternative communication (AAC) intervention to supplement or replace conventional speech for people with complex communication needs; (d) was a SCED; (e) measured social-communicative outcomes. We synthesized results across studies using multi-level meta-analyses of two case-level effect size metrics, Tau and log response ratio. We conducted moderator analyses using meta-regression with robust variance estimation.</p>
<p><strong>Results</strong>: Across 114 included studies with 330 participants and 767 effect size, overall Tau effects were moderate, Tau = 0.72, 95% CI [0.67, 0.77], and heterogeneous. For the subset of data series where log response ratio could be estimated, the overall average effect was LRR = 1.86, 95% CI [1.58, 2.13], and effects were highly heterogeneous. There were few statistically significant differences found between moderator categories, which included communication mode, communicative function, and type of AAC implemented.</p>
<p><strong>Conclusions</strong>: This meta-analysis highlights the potential differences related to outcomes that were targeted for AAC interventions for individuals with ASD and IDD. AAC intervention has been shown to improve communication outcomes in this population. However, there was a lack of sufficient data to analyze for some potential moderators such as insufficient descriptive information on participant characteristics. This is likely due to the heterogeneity of the participants and implementation factors; however, these factors were frequently underreported by original study authors which disallowed systematic analysis. That said, there is a need for more detailed participant characteristic descriptions in original research reports to support future aggregation across the literature. Sponsorship: We received funding for the review from the Institute of Education Sciences. Protocol: The review protocol was registered in the PROSPERO system (CRD42018112428).</p>



<a onclick="window.scrollTo(0, 0); return false;" id="quarto-back-to-top"><i class="bi bi-arrow-up"></i> Back to top</a> ]]></description>
  <category>single-case design</category>
  <category>systematic review</category>
  <category>Augmentative and alternative communication</category>
  <guid>https://mellifluous-buttercream-e2edd2.netlify.app/publication/AAC-communication-outcomes/</guid>
  <pubDate>Wed, 20 Apr 2022 05:00:00 GMT</pubDate>
</item>
<item>
  <title>Considering instructional contexts in AAC interventions for people with ASD and/or IDD experiencing complex communication needs: A single-case design meta-analysis</title>
  <dc:creator>Jay B. Ganz</dc:creator>
  <dc:creator>James E. Pustejovsky</dc:creator>
  <dc:creator>Joe Reichle</dc:creator>
  <dc:creator>Kimberly J. Vannest</dc:creator>
  <dc:creator>Margaret Foster</dc:creator>
  <dc:creator>April N. Haas</dc:creator>
  <dc:creator>Lauren M. Pierson</dc:creator>
  <dc:creator>Sanikan Wattanawongwan</dc:creator>
  <dc:creator>Amando Bernal</dc:creator>
  <dc:creator>Man Chen</dc:creator>
  <dc:creator>Rachel Skov</dc:creator>
  <dc:creator>S. D. Smith</dc:creator>
  <link>https://mellifluous-buttercream-e2edd2.netlify.app/publication/AAC-instructional-contexts/</link>
  <description><![CDATA[ 






<p>For children with autism or intellectual and developmental disabilities who also have complex communication needs, communication is a necessary skill set to increase independence and quality of life. Understanding the how, where, and communication style being taught is important for identifying deficits in the field as well as which interventions are most effective. This meta-analysis sought to identify effectiveness among different settings, behavioral strategies, and moderator variables. A systematic search and screening process identified 114 eligible studies with 330 participants; overall outcomes indicate that augmentative and alternative communication interventions were effective with Tau effects ranging from 0.53 to 1.03 and log response ratio effects ranging from 0.21 to 2.90. However, no instructional context variables systematically predicted differences in intervention effectiveness.</p>



<a onclick="window.scrollTo(0, 0); return false;" id="quarto-back-to-top"><i class="bi bi-arrow-up"></i> Back to top</a> ]]></description>
  <category>single-case design</category>
  <category>systematic review</category>
  <category>Augmentative and alternative communication</category>
  <guid>https://mellifluous-buttercream-e2edd2.netlify.app/publication/AAC-instructional-contexts/</guid>
  <pubDate>Wed, 20 Apr 2022 05:00:00 GMT</pubDate>
</item>
<item>
  <title>Participant characteristics predicting communication outcomes in AAC implementation for individuals with ASD and IDD: Meta-analysis</title>
  <dc:creator>Jay B. Ganz</dc:creator>
  <dc:creator>James E. Pustejovsky</dc:creator>
  <dc:creator>Joe Reichle</dc:creator>
  <dc:creator>Kimberly J. Vannest</dc:creator>
  <dc:creator>Margaret Foster</dc:creator>
  <dc:creator>Lauren M. Pierson</dc:creator>
  <dc:creator>Sanikan Wattanawongwan</dc:creator>
  <dc:creator>Amando Bernal</dc:creator>
  <dc:creator>Man Chen</dc:creator>
  <dc:creator>April N. Haas</dc:creator>
  <dc:creator>Mary Rose Sallese</dc:creator>
  <dc:creator>Rachel Skov</dc:creator>
  <dc:creator>S. D. Smith</dc:creator>
  <link>https://mellifluous-buttercream-e2edd2.netlify.app/publication/AAC-participant-characteristics/</link>
  <description><![CDATA[ 






<p>This meta-analysis examined social communication outcomes in augmentative and alternative communication (AAC) interventions, or those that involved aided (e.g., speech generating devices, picture point systems) or unaided AAC (e.g., gestures, manual sign language) as a component of intervention, and the extent to which communication outcomes were predicted by participant characteristics. Variables of interest included chronological age, communication mode used prior to intervention, number of words produced and imitation skills of participants prior to intervention. Investigators identified 117 primary studies that implemented AAC interventions with school-aged individuals (up to 22 years) with autism spectrum disorder and/or intellectual disability associated with complex communication needs and assessed social-communication outcomes. All included studies involved single-case experimental designs and met basic study design quality standards. We synthesized findings across studies using two complementary effect size indices, Tau(AB) and the log response ratio, and multi-level meta-analysis with robust variance estimation. With Tau(AB), the overall average effect across 338 participants was 0.72, 95% CI [0.67, 0.76], with a high degree of heterogeneity across studies. With the log response ratio, the overall average effect corresponded to a 538% increase from baseline levels of responding, 95% CI [388%, 733%], with a high degree of heterogeneity across studies and contrasts. Moderator analyses detected few differences in effectiveness when comparing across diagnoses, ages, the number and type of communication modes the participants used prior to intervention, the number of words used by the participants prior to intervention, and imitation use prior to intervention.</p>



<a onclick="window.scrollTo(0, 0); return false;" id="quarto-back-to-top"><i class="bi bi-arrow-up"></i> Back to top</a> ]]></description>
  <category>single-case design</category>
  <category>systematic review</category>
  <category>Augmentative and alternative communication</category>
  <guid>https://mellifluous-buttercream-e2edd2.netlify.app/publication/AAC-participant-characteristics/</guid>
  <pubDate>Wed, 20 Apr 2022 05:00:00 GMT</pubDate>
</item>
<item>
  <title>Meta-Analysis with robust variance estimation: Expanding the range of working models</title>
  <dc:creator>James E. Pustejovsky</dc:creator>
  <dc:creator>Elizabeth Tipton</dc:creator>
  <link>https://mellifluous-buttercream-e2edd2.netlify.app/publication/RVE-Meta-analysis-expanding-the-range/</link>
  <description><![CDATA[ 






<p>In prevention science and related fields, large meta-analyses are common, and these analyses often involve dependent effect size estimates. Robust variance estimation (RVE) methods provide a way to include all dependent effect sizes in a single meta-regression model, even when the nature of the dependence is unknown. RVE uses a working model of the dependence structure, but the two currently available working models are limited to each describing a single type of dependence. Drawing on flexible tools from multivariate meta-analysis, this paper describes an expanded range of working models, along with accompanying estimation methods, which offer benefits in terms of better capturing the types of data structures that occur in practice and improving the efficiency of meta-regression estimates. We describe how the methods can be implemented using existing software (the ‘metafor’ and ‘clubSandwich’ packages for R) and illustrate the approach in a meta-analysis of randomized trials examining the effects of brief alcohol interventions for adolescents and young adults.</p>



<a onclick="window.scrollTo(0, 0); return false;" id="quarto-back-to-top"><i class="bi bi-arrow-up"></i> Back to top</a> ]]></description>
  <category>meta-analysis</category>
  <category>meta-regression</category>
  <category>dependent effect sizes</category>
  <category>robust variance estimation</category>
  <guid>https://mellifluous-buttercream-e2edd2.netlify.app/publication/RVE-Meta-analysis-expanding-the-range/</guid>
  <pubDate>Fri, 01 Apr 2022 05:00:00 GMT</pubDate>
  <media:content url="https://mellifluous-buttercream-e2edd2.netlify.app/publication/RVE-Meta-analysis-expanding-the-range/Decision-tree for selecting a working model for use with RVE meta-regression" medium="image"/>
</item>
<item>
  <title>Cluster wild bootstrapping to handle dependent effect sizes in meta-analysis with a small number of studies</title>
  <dc:creator>Megha Joshi</dc:creator>
  <dc:creator>James E. Pustejovsky</dc:creator>
  <dc:creator>S. Natasha Beretvas</dc:creator>
  <link>https://mellifluous-buttercream-e2edd2.netlify.app/publication/cluster-wild-bootstrap-for-meta-analysis/</link>
  <description><![CDATA[ 






<p>The most common and well-known meta-regression models work under the assumption that there is only one effect size estimate per study and that the estimates are independent. However, meta-analytic reviews of social science research often include multiple effect size estimates per primary study, leading to dependence in the estimates. Some meta-analyses also include multiple studies conducted by the same lab or investigator, creating another potential source of dependence. An increasingly popular method to handle dependence is robust variance estimation (RVE), but this method can result in inflated Type I error rates when the number of studies is small. Small-sample correction methods for RVE have been shown to control Type I error rates adequately but may be overly conservative, especially for tests of multiple-contrast hypotheses. We evaluated an alternative method for handling dependence, cluster wild bootstrapping, which has been examined in the econometrics literature but not in the context of meta-analysis. Results from two simulation studies indicate that cluster wild bootstrapping maintains adequate Type I error rates and provides more power than extant small sample correction methods, particularly for multiple-contrast hypothesis tests. We recommend using cluster wild bootstrapping to conduct hypothesis tests for meta-analyses with a small number of studies. We have also created an R package that implements such tests.</p>



<a onclick="window.scrollTo(0, 0); return false;" id="quarto-back-to-top"><i class="bi bi-arrow-up"></i> Back to top</a> ]]></description>
  <category>meta-analysis</category>
  <category>robust variance estimation</category>
  <category>bootstrap</category>
  <guid>https://mellifluous-buttercream-e2edd2.netlify.app/publication/cluster-wild-bootstrap-for-meta-analysis/</guid>
  <pubDate>Tue, 08 Feb 2022 06:00:00 GMT</pubDate>
</item>
<item>
  <title>Examining the effects of social stories on challenging behavior and prosocial skills in young children: A systematic review and meta-analysis</title>
  <dc:creator>Charis L. Wahman</dc:creator>
  <dc:creator>James E. Pustejovsky</dc:creator>
  <dc:creator>Michaelene M. Ostrosky</dc:creator>
  <dc:creator>Rosa Milagros Santos</dc:creator>
  <link>https://mellifluous-buttercream-e2edd2.netlify.app/publication/effects-of-social-stories-on-challenging-behavior-and-prosocial-skills/</link>
  <description><![CDATA[ 






<p>Social stories are a commonly used intervention practice in early childhood special education. Recent systematic reviews have documented the evidence-base for social stories, but findings are mixed. We examined the efficacy of social stories for young children (i.e., 3-5 years) with challenging behavior across 12 single-case studies, that included 30 participants. The What Works Clearinghouse standards for single case research design were used to evaluate the rigor of studies that included social stories as a primary intervention. For studies meeting standards, we synthesized findings on the efficacy of social stories using meta-analysis techniques and a recently developed parametric effect size measure, the log response ratio. Trends in participants’ response to treatment also were explored. Results indicate variability in rigor and efficacy for the use of social stories as an isolated intervention and in combination with other intervention approaches. Additional studies that investigate the efficacy of social stories as a primary intervention are warranted.</p>



<a onclick="window.scrollTo(0, 0); return false;" id="quarto-back-to-top"><i class="bi bi-arrow-up"></i> Back to top</a> ]]></description>
  <category>single-case design</category>
  <category>meta-analysis</category>
  <category>systematic review</category>
  <guid>https://mellifluous-buttercream-e2edd2.netlify.app/publication/effects-of-social-stories-on-challenging-behavior-and-prosocial-skills/</guid>
  <pubDate>Tue, 01 Feb 2022 06:00:00 GMT</pubDate>
</item>
</channel>
</rss>
