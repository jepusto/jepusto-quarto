<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.4.552">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="James E. Pustejovsky">
<meta name="dcterms.date" content="2023-02-17">
<meta name="description" content="Meta-analyses in education, psychology, and related fields rely heavily of Cohen’s d, or the standardized mean difference effect size, for quantitatively describing the magnitude and direction of intervention effects. In these fields, Cohen’s d is so pervasive that its use is nearly automatic, and analysts rarely question its utility or consider alternatives (response ratios, anyone? POMP?). Despite this state of affairs, working with Cohen’s d is theoretically challenging because the standardized mean difference metric does not have a singular definition. Rather, its definition depends on the choice of the standardizing variance used in the denominator.">

<title>James E. Pustejovsky - Cohen’s \(d_z\) makes me dizzy when considering measurement error</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<style>html{ scroll-behavior: smooth; }</style>

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<meta property="og:title" content="James E. Pustejovsky - Cohen’s \(d_z\) makes me dizzy when considering measurement error">
<meta property="og:description" content="Meta-analyses in education, psychology, and related fields rely heavily of Cohen’s \(d\), or the standardized mean difference effect size, for quantitatively describing the magnitude and direction of intervention effects. In these fields, Cohen’s \(d\) is so pervasive that its use is nearly automatic, and analysts rarely question its utility or consider alternatives (response ratios, anyone? POMP?). Despite this state of affairs, working with Cohen’s \(d\) is theoretically challenging because the standardized mean difference metric does not have a singular definition. Rather, its definition depends on the choice of the standardizing variance used in the denominator.">
<meta property="og:site_name" content="James E. Pustejovsky">
<meta name="twitter:title" content="James E. Pustejovsky - Cohen’s \(d_z\) makes me dizzy when considering measurement error">
<meta name="twitter:description" content="Meta-analyses in education, psychology, and related fields rely heavily of Cohen’s \(d\), or the standardized mean difference effect size, for quantitatively describing the magnitude and direction of intervention effects. In these fields, Cohen’s \(d\) is so pervasive that its use is nearly automatic, and analysts rarely question its utility or consider alternatives (response ratios, anyone? POMP?). Despite this state of affairs, working with Cohen’s \(d\) is theoretically challenging because the standardized mean difference metric does not have a singular definition. Rather, its definition depends on the choice of the standardizing variance used in the denominator.">
<meta name="twitter:card" content="summary">
<meta name="citation_title" content="Cohen&amp;amp;#039;s $d_z$ makes me dizzy when considering measurement error">
<meta name="citation_author" content="James E. Pustejovsky">
<meta name="citation_publication_date" content="2023-02-17">
<meta name="citation_cover_date" content="2023-02-17">
<meta name="citation_year" content="2023">
<meta name="citation_online_date" content="2023-02-17">
<meta name="citation_fulltext_html_url" content="https://mellifluous-buttercream-e2edd2.netlify.app/posts/dizzy-for-d-z">
<meta name="citation_language" content="en">
</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top quarto-banner">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">James E. Pustejovsky</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../files/Pustejovsky-CV.pdf"> 
<span class="menu-text">CV</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../people/index.html"> 
<span class="menu-text">People</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../posts/index.html"> 
<span class="menu-text">Posts</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../working-papers.html"> 
<span class="menu-text">Working Papers</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../publication/index.html"> 
<span class="menu-text">Publications</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../presentations/index.html"> 
<span class="menu-text">Presentations</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../software/index.html"> 
<span class="menu-text">Software</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../teaching/index.html"> 
<span class="menu-text">Teaching</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../contact.html"> 
<span class="menu-text">Contact</span></a>
  </li>  
</ul>
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item compact">
    <a class="nav-link" href="../../index.xml"> <i class="bi bi-rss" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
          <div class="quarto-navbar-tools">
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <div class="quarto-title-block"><div><h1 class="title">Cohen’s <span class="math inline">\(d_z\)</span> makes me dizzy when considering measurement error</h1><button type="button" class="btn code-tools-button dropdown-toggle" id="quarto-code-tools-menu" data-bs-toggle="dropdown" aria-expanded="false"><i class="bi"></i> Code</button><ul class="dropdown-menu dropdown-menu-end" aria-labelelledby="quarto-code-tools-menu"><li><a id="quarto-show-all-code" class="dropdown-item" href="javascript:void(0)" role="button">Show All Code</a></li><li><a id="quarto-hide-all-code" class="dropdown-item" href="javascript:void(0)" role="button">Hide All Code</a></li><li><hr class="dropdown-divider"></li><li><a id="quarto-view-source" class="dropdown-item" href="javascript:void(0)" role="button">View Source</a></li></ul></div></div>
                  <div>
        <div class="description">
          Meta-analyses in education, psychology, and related fields rely heavily of Cohen’s <span class="math inline">\(d\)</span>, or the standardized mean difference effect size, for quantitatively describing the magnitude and direction of intervention effects. In these fields, Cohen’s <span class="math inline">\(d\)</span> is so pervasive that its use is nearly automatic, and analysts rarely question its utility or consider alternatives (response ratios, anyone? POMP?). Despite this state of affairs, working with Cohen’s <span class="math inline">\(d\)</span> is theoretically challenging because the standardized mean difference metric does not have a singular definition. Rather, its definition depends on the choice of the standardizing variance used in the denominator.
        </div>
      </div>
                          <div class="quarto-categories">
                <div class="quarto-category">effect size</div>
                <div class="quarto-category">standardized mean difference</div>
                <div class="quarto-category">design-comparable SMD</div>
                <div class="quarto-category">measurement-error</div>
              </div>
                  </div>
  </div>
    
  
  <div class="quarto-title-meta">

      <div>
      <div class="quarto-title-meta-heading">Author</div>
      <div class="quarto-title-meta-contents">
               <p>admin </p>
            </div>
    </div>
      
      <div>
      <div class="quarto-title-meta-heading">Published</div>
      <div class="quarto-title-meta-contents">
        <p class="date">February 17, 2023</p>
      </div>
    </div>
    
      
    </div>
    
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Contents</h2>
   
  <ul>
  <li><a href="#effect-size-definitions-in-within-group-designs" id="toc-effect-size-definitions-in-within-group-designs" class="nav-link active" data-scroll-target="#effect-size-definitions-in-within-group-designs">Effect size definitions in within-group designs</a></li>
  <li><a href="#a-within-group-design-with-measurement-error" id="toc-a-within-group-design-with-measurement-error" class="nav-link" data-scroll-target="#a-within-group-design-with-measurement-error">A within-group design with measurement error</a></li>
  <li><a href="#meta-analysis" id="toc-meta-analysis" class="nav-link" data-scroll-target="#meta-analysis">Meta-analysis</a>
  <ul class="collapse">
  <li><a href="#a-model-for-delta_av" id="toc-a-model-for-delta_av" class="nav-link" data-scroll-target="#a-model-for-delta_av">A model for <span class="math inline">\(\delta_{av}\)</span></a></li>
  <li><a href="#a-model-for-delta_z" id="toc-a-model-for-delta_z" class="nav-link" data-scroll-target="#a-model-for-delta_z">A model for <span class="math inline">\(\delta_z\)</span></a></li>
  <li><a href="#consequences-for-heterogeneity" id="toc-consequences-for-heterogeneity" class="nav-link" data-scroll-target="#consequences-for-heterogeneity">Consequences for heterogeneity</a></li>
  </ul></li>
  <li><a href="#so-whats-your-point" id="toc-so-whats-your-point" class="nav-link" data-scroll-target="#so-whats-your-point">So what’s your point?</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content quarto-banner-title-block" id="quarto-document-content">





<p><span class="math display">\[
\def\Pr{{\text{Pr}}}
\def\E{{\text{E}}}
\def\Var{{\text{Var}}}
\def\Cov{{\text{Cov}}}
\def\cor{{\text{cor}}}
\def\bm{\mathbf}
\def\bs{\boldsymbol}
\]</span></p>
<p>Meta-analyses in education, psychology, and related fields rely heavily of Cohen’s <span class="math inline">\(d\)</span>, or the standardized mean difference effect size, for quantitatively describing the magnitude and direction of intervention effects. In these fields, Cohen’s <span class="math inline">\(d\)</span> is so pervasive that its use is nearly automatic, and analysts rarely question its utility or consider alternatives (response ratios, anyone? POMP?). Despite this state of affairs, working with Cohen’s <span class="math inline">\(d\)</span> is theoretically challenging because the standardized mean difference metric does not have a singular definition. Rather, its definition depends on the choice of the standardizing variance used in the denominator.<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a></p>
<p>In this post, I’m going to mull over how measurement error and design decisions influence the metric definition of Cohen’s <span class="math inline">\(d\)</span> in basic within-group experimental designs. The distorting effects of measurement error has long been a source of concern within psychometric meta-analysis, a perspective associated with the work of <a href="https://methods.sagepub.com/book/methods-of-meta-analysis-3e">Frank Schmidt and Jack Hunter</a>, and measurement-error corrections are well developed and often applied in meta-analyses of correlations. Straight-forward measurement-error corrections have also been described for Cohen’s <span class="math inline">\(d\)</span> from between-group designs (see recent work by <a href="https://psyarxiv.com/9mpbn/">Brenton Wiernik and Jeff Dahlke</a>). However, I have literally never seen a meta-analytic application that applied these corrections and I have thus far been unable to locate work on such corrections specifically for effect sizes in within-group designs.<a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a> So, time to muck about…</p>
<section id="effect-size-definitions-in-within-group-designs" class="level2">
<h2 class="anchored" data-anchor-id="effect-size-definitions-in-within-group-designs">Effect size definitions in within-group designs</h2>
<p>In basic between-group designs, the only variances in the model are the within-group variances, so the choice of standardizing variance is limited to a) the singular population variance, assuming it is homogeneous across groups, b) the variance of one group, or c) the average of the variances in each group. In most applications, homogeneity is assumed (often without much reflection), version (a) of Cohen’s <span class="math inline">\(d\)</span> is estimated, and the meta-analyst can go along their merry way. For sake of succinctness, I’ll call this effect size <span class="math inline">\(d_{b}\)</span>, where the <span class="math inline">\(b\)</span> indicates the usual version for basic between-group designs.</p>
<p>For within-group or repeated measures designs, the set of choices is more involved and includes a) standardizing by the across-participant variance in one condition (or both conditions, assuming homogeneity) or b) standardizing by the variance of the difference scores. The former approach is sometimes called <span class="math inline">\(d_{av}\)</span><a href="#fn3" class="footnote-ref" id="fnref3" role="doc-noteref"><sup>3</sup></a>, the latter is called <span class="math inline">\(d_z\)</span><a href="#fn4" class="footnote-ref" id="fnref4" role="doc-noteref"><sup>4</sup></a>. The <span class="math inline">\(d_{av}\)</span> metric uses the same standardizing variance as the <span class="math inline">\(d\)</span> from a basic between-group design, and so results from both types of designs are, in principle, on the same scale.</p>
<p>In the context of meta-analysis, the comparability of <span class="math inline">\(d_b\)</span> and <span class="math inline">\(d_{av}\)</span> is useful when working with a set of studies that include both types of designs. On the other hand, in meta-analyses that consist solely of within-group or repeated measures designs, comparability with <span class="math inline">\(d_b\)</span> may be less of a priority and one could consider using <span class="math inline">\(d_z\)</span> for synthesis. Purely on a pragmatic level, using <span class="math inline">\(d_z\)</span> might be attractive because the only pieces of information needed to calculate it are the total sample size and the <span class="math inline">\(t\)</span> statistic (or <span class="math inline">\(p\)</span>-value) from the comparison between conditions. In contrast, calculating <span class="math inline">\(d_{av}\)</span> also requires the between-participant standard deviations from one or both groups, which primary studies might not always report.</p>
<p>Going in to this exercise, I had the notion that measurement error would affect <span class="math inline">\(d_z\)</span> to a greater degree than <span class="math inline">\(d_{av}\)</span> because <span class="math inline">\(d_z\)</span> involves difference scores and difference scores get hit by measurement error twice. Does this intuition hold up? Let me try to formalize things a bit.</p>
</section>
<section id="a-within-group-design-with-measurement-error" class="level2">
<h2 class="anchored" data-anchor-id="a-within-group-design-with-measurement-error">A within-group design with measurement error</h2>
<p>Suppose we have a within-group design involving two conditions, where participants are assessed on <span class="math inline">\(K\)</span> trials under each condition. Let <span class="math inline">\(Y_{ijk}\)</span> denote the outcome from trial <span class="math inline">\(k\)</span> for participant <span class="math inline">\(j\)</span> under condition <span class="math inline">\(i\)</span>, for <span class="math inline">\(i = 1,2\)</span>, <span class="math inline">\(j = 1,...,N\)</span>, and <span class="math inline">\(k = 1,...,K\)</span>. A basic model for this set-up is <span class="math display">\[
Y_{ijk} = \mu_i + u_{ij} + e_{ijk}
\]</span> where <span class="math inline">\(u_{1j}\)</span> and <span class="math inline">\(u_{2j}\)</span> are participant-specific errors in the true scores under each condition and the <span class="math inline">\(e_{ijk}\)</span>’s are measurement errors. For simplicity, I will assume that:</p>
<ul>
<li>the true-score variance is equal across conditions, with <span class="math inline">\(\Var(u_{ij}) = \sigma^2\)</span> for <span class="math inline">\(i = 1,2\)</span>,</li>
<li>the true scores are correlated across conditions, <span class="math inline">\(\cor(u_{1j}, u_{2j}) = \rho\)</span>, and</li>
<li>measurement errors are uncorrelated and have homogeneous variance across conditions, with <span class="math inline">\(\Var(e_{ijk}) = \psi^2\)</span>.</li>
</ul>
<p>Let <span class="math inline">\(\phi = \sigma^2 / (\sigma^2 + \psi^2)\)</span> denote the reliability (intra-class correlation) of a single observed score. Note that we can write <span class="math inline">\(\psi^2\)</span> in terms of the reliability and true-score variance as <span class="math inline">\(\psi^2 = \sigma^2 \times \frac{1 - \phi}{\phi}\)</span>.</p>
<p>Under this model, there are several different standardized mean difference metrics that we could consider. Since measurement reliability might vary from study to study, it would make sense to define the metric in terms of true score variances alone, as <span class="math display">\[
\delta_{av} = \frac{\mu_2 - \mu_1}{\sigma}
\]</span> or in terms of the variance of the difference in true scores, as <span class="math display">\[
\delta_z = \frac{\mu_2 - \mu_1}{\sigma \sqrt{2(1 - \rho)}}.
\]</span> However, we don’t directly observe the true scores, and we can’t estimate their variance unless we have information about score reliabilities. Thus, meta-analysts will usually need to calculate effect sizes in terms of <em>observed</em> scores that include measurement error.</p>
<p>Suppose that the analysis is conducted by taking the average of the <span class="math inline">\(K\)</span> trials for each participant under each condition, <span class="math inline">\(\bar{Y}_{ij} = \frac{1}{K} \sum_{k=1}^K Y_{ijk}\)</span>, and conducting the analysis using these mean scores. The variance of the mean scores is <span class="math display">\[
\Var(\bar{Y}_{ij}) = \sigma^2 \left(1 + \frac{1 - \phi}{\phi K}\right),
\]</span> so we can define the observed-score standardized mean difference using raw score standardization as <span class="math display">\[
\tilde\delta_{av} = \frac{\mu_2 - \mu_1}{\sigma \sqrt{1 + \frac{1 - \phi}{\phi K}}} = \frac{1}{\sqrt{1 + \frac{1 - \phi}{\phi K}}} \times \delta_{av}.
\]</span> From this expression, we can see that measurement error attenuates the true-score effect size because the multiplier term is always going to be less than one.</p>
<p>Similarly, the variance of the observed difference scores is <span class="math display">\[
\Var(\bar{Y}_{2j} - \bar{Y}_{1j}) =2 \sigma^2 \left(1 - \rho + \frac{1 - \phi}{\phi K}\right),
\]</span> so we can define the observed-score standardized mean difference using change score standardization as <span class="math display">\[
\tilde\delta_z = \frac{\mu_2 - \mu_1}{\sigma \sqrt{2 \left(1 - \rho + \frac{1 - \phi}{\phi K}\right)}} = \sqrt{\frac{1 - \rho}{1 - \rho + \frac{1 - \phi}{\phi K}}} \times \delta_z.
\]</span> Again, we can see that measurement error attenuates the true-score effect size because the multiplier term is always going to be less than one. However, unlike with <span class="math inline">\(d_{av}\)</span>, the attenuation factor here depends on <span class="math inline">\(\rho\)</span> in addition to <span class="math inline">\(\phi\)</span> and <span class="math inline">\(K\)</span>. This additional term in the correction factor is one indication that <span class="math inline">\(d_z\)</span> might be less desirable for meta-analysis. Correcting <span class="math inline">\(d_z\)</span> for the distortion from measurement error would require estimates of both the true-score correlation and the reliability of the scores, whereas correcting <span class="math inline">\(d_{av}\)</span> would require only the latter.</p>
</section>
<section id="meta-analysis" class="level2">
<h2 class="anchored" data-anchor-id="meta-analysis">Meta-analysis</h2>
<p>The relationships between the true-score effect sizes and the analogous observed score effect sizes starts to be a problem when we consider a meta-analysis of multiple primary studies. Primary studies will often use different instruments and procedures for measuring outcomes (necessitating the use of some standardized effect size), and those differences in instruments and procedures might come along with differences in score reliability as well as variation in the number of trials collected per condition (and plenty of other things, such as sample size, participant characteristics, etc.). Procedural heterogeneity like this creates two potential challenges for meta-analysis: bias in average effect sizes and extra heterogeneity in the distribution of effect sizes. Both could make findings from a meta-analysis more difficult to interpret, although I will argue that extra heterogeneity is more concerning than bias.</p>
<p>To illustrate, let’s now imagine that the parameters of the within-group study design, <span class="math inline">\(\delta_{av}\)</span> or <span class="math inline">\(\delta_z\)</span>, <span class="math inline">\(\rho\)</span>, <span class="math inline">\(\phi\)</span>, and <span class="math inline">\(K\)</span> are random variables, drawn from the distribution of parameters across a population of hypothetical studies.</p>
<section id="a-model-for-delta_av" class="level3">
<h3 class="anchored" data-anchor-id="a-model-for-delta_av">A model for <span class="math inline">\(\delta_{av}\)</span></h3>
<p>Let’s first consider <span class="math inline">\(\delta_{av}\)</span> and assume that it follows a random effects model, with <span class="math display">\[
\delta_{av} \sim N\left(\mu, \tau^2\right).
\]</span> Let’s also assume that the remaining parameters <span class="math inline">\(\rho\)</span>, <span class="math inline">\(\phi\)</span>, and <span class="math inline">\(K\)</span> are independent of <span class="math inline">\(\delta_{av}\)</span>. These parameters determine the attenuation factor <span class="math inline">\(A_{av} = \left(1 + \frac{1 - \phi}{\phi K}\right)^{-1/2}\)</span>, which relates the observed-score effect size parameter to the true score effect size parameter.</p>
<p>The bias of <span class="math inline">\(\tilde\delta_{av}\)</span> is therefore <span class="math display">\[
\E\left(\tilde\delta_{av}\right) = \E\left(A_{av}\delta_{av}\right) = \E(A_{av}) \times \mu.
\]</span> Thus, under my very simplistic assumptions, a meta-analysis of observed score Cohen’s <span class="math inline">\(d_{av}\)</span> estimates will be biased (downward) for the overall average effect in the true-score distribution.</p>
<p>You might find that the downward bias in <span class="math inline">\(\tilde\delta_{av}\)</span> is undesirable. On the other hand, bias might not be as a big a problem as it first seems. If all of the observed-score effect sizes are biased to a degree that is unrelated to the true effects, then bias just stretches or compresses the scale of measurement, but doesn’t necessarily lead to interpretive problems. Imagine you have a ruler that is half an inch too short, and you’re trying to compare the heights of different objects. As long as you use the same ruler, then you will still be able to determine which objects are bigger and which are smaller, and by how much, even if the measurements are off in an absolute sense.</p>
<p>Apart from bias, however, variability in <span class="math inline">\(A_{av}\)</span> will also induce <em>additional heterogeneity</em> in the distribution of observed score effect sizes. This is a clear problem because it creates additional uncertainty, making it harder to draw inferences about the distribution of effects, predict new effect sizes, or identify substantively interesting moderators. To measure this additional heterogeneity and keep its consequences separate from the consequences for bias, I will look at the coefficient of variation in <span class="math inline">\(\tilde\delta_{av}\)</span>. Under the assumption that <span class="math inline">\(A_{av}\)</span> is independent of <span class="math inline">\(\delta_{av}\)</span>, <span class="math display">\[
\frac{\sqrt{\Var(\tilde\delta_{av})}}{\E(\tilde\delta_{av})} = \frac{\sqrt{(\mu^2 + 2 \tau^2) \Var(A_{av}) + \tau^2 \left[\E(A_{av})\right]^2}}{\E\left(A_{av} \right) \times \mu } = \sqrt{\left[\left(1 + \frac{2 \tau^2}{\mu^2}\right) \frac{\Var(A_{av})}{\left[\E\left(A_{av} \right)\right]^2} + \frac{\tau^2}{\mu^2}\right]}.
\]</span> Thus, the coefficient of variation for <span class="math inline">\(\tilde\delta_{av}\)</span> is amplified by a factor that depends on the squared coefficient of variation of <span class="math inline">\(A_{av}\)</span>. Under the same model, <span class="math inline">\(\tilde\delta_z = A_z \times \delta_{av}\)</span>, where <span class="math inline">\(A_z = \left(1 - \rho + \frac{1 - \phi}{\phi K}\right)^{-1/2}\)</span>, and so <span class="math inline">\(\E(\tilde\delta_z) = \E(A_z) \times \mu\)</span> and <span class="math display">\[
\frac{\sqrt{\Var(\tilde\delta_z)}}{\E(\tilde\delta_z)} = \sqrt{\left[\left(1 + \frac{2 \tau^2}{\mu^2}\right) \frac{\Var(A_z)}{\left[\E\left(A_z \right)\right]^2} + \frac{\tau^2}{\mu^2}\right]}.
\]</span></p>
<p>To see what’s going on here, let’s consider some specific distributions for these measurement factors. First, let’s assume:</p>
<ul>
<li><span class="math inline">\(\rho \sim B(14, 6)\)</span>, so that <span class="math inline">\(\E(\rho) = .7\)</span> and <span class="math inline">\(\Var(\rho) = 0.1^2\)</span>;</li>
<li><span class="math inline">\(\phi \sim B(3, 5)\)</span>, so that <span class="math inline">\(\E(\rho) = .0.375\)</span> and <span class="math inline">\(\Var(\rho) = 0.161^2\)</span>;</li>
<li><span class="math inline">\(K \sim 1 + Pois(9)\)</span>, so <span class="math inline">\(\E(K) = 10\)</span> and <span class="math inline">\(\Var(K) = 3^2\)</span>; and</li>
<li><span class="math inline">\(\rho\)</span>, <span class="math inline">\(\phi\)</span>, and <span class="math inline">\(K\)</span> are mutually independent and independent of <span class="math inline">\(\delta_{av}\)</span>.</li>
</ul>
<p>Below I simulate 50000 samples from these distributions and calculate <span class="math inline">\(A_{av}\)</span> and <span class="math inline">\(A_z\)</span>.</p>
<div class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a>R <span class="ot">&lt;-</span> <span class="dv">50000</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a>rho <span class="ot">&lt;-</span> <span class="fu">rbeta</span>(R, <span class="dv">14</span>, <span class="dv">6</span>)</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a>phi <span class="ot">&lt;-</span> <span class="fu">rbeta</span>(R, <span class="dv">3</span>, <span class="dv">5</span>)</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a>K <span class="ot">&lt;-</span> <span class="dv">1</span> <span class="sc">+</span> <span class="fu">rpois</span>(R, <span class="dv">9</span>)</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a>A_av <span class="ot">&lt;-</span> <span class="dv">1</span> <span class="sc">/</span> <span class="fu">sqrt</span>(<span class="dv">1</span> <span class="sc">+</span> (<span class="dv">1</span> <span class="sc">-</span> phi) <span class="sc">/</span> (phi <span class="sc">*</span> K))</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a>A_z <span class="ot">&lt;-</span> <span class="dv">1</span> <span class="sc">/</span> <span class="fu">sqrt</span>(<span class="dv">2</span> <span class="sc">*</span> (<span class="dv">1</span> <span class="sc">-</span> rho <span class="sc">+</span> (<span class="dv">1</span> <span class="sc">-</span> phi) <span class="sc">/</span> (phi <span class="sc">*</span> K)))</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(ggplot2)</span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(patchwork)</span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a>density_plot <span class="ot">&lt;-</span> <span class="cf">function</span>(x, lab, col, limits) {</span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">data.frame</span>(x), <span class="fu">aes</span>(x)) <span class="sc">+</span> </span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a>    <span class="fu">xlim</span>(limits) <span class="sc">+</span></span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a>    <span class="fu">geom_density</span>(<span class="at">alpha =</span> <span class="fl">0.4</span>, <span class="at">fill =</span> col) <span class="sc">+</span> </span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a>    <span class="fu">scale_y_continuous</span>(<span class="at">labels =</span> <span class="cn">NULL</span>) <span class="sc">+</span></span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a>    <span class="fu">theme_minimal</span>() <span class="sc">+</span> </span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a>    <span class="fu">labs</span>(<span class="at">x =</span> lab, <span class="at">y =</span> <span class="cn">NULL</span>)</span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a>p_A_av <span class="ot">&lt;-</span> <span class="fu">density_plot</span>(A_av, <span class="fu">expression</span>(A[av]),<span class="st">"blue"</span>, <span class="fu">c</span>(<span class="fl">0.2</span>,<span class="dv">1</span>))</span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a>p_A_z <span class="ot">&lt;-</span> <span class="fu">density_plot</span>(A_z, <span class="fu">expression</span>(A[z]), <span class="st">"purple"</span>, <span class="fu">c</span>(<span class="dv">0</span>, <span class="dv">3</span>))</span>
<span id="cb1-22"><a href="#cb1-22" aria-hidden="true" tabindex="-1"></a>p_A_av <span class="sc">+</span> p_A_z</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="index_files/figure-html/unnamed-chunk-2-1.png" class="img-fluid figure-img" width="960"></p>
</figure>
</div>
</div>
</div>
<p>The distribution of <span class="math inline">\(A_{av}\)</span> is mostly concentrated around the mean of <span class="math inline">\(E(A_{av}) = 0.898\)</span>, with a coefficient of variation of <span class="math inline">\(CV(A_{av}) = 0.085\)</span>. In contrast, the distribution of <span class="math inline">\(A_{av}\)</span> has a mean very close to one, <span class="math inline">\(E(A_z) = 1.006\)</span> but a coefficient of variation of <span class="math inline">\(CV(A_z) = 0.21\)</span>, about 2.5 times larger. Thus, variation in measurement procedure induces more extra heterogeneity into the distribution of <span class="math inline">\(\tilde\delta_z\)</span> than into <span class="math inline">\(\tilde\delta_{av}\)</span>.</p>
<p>Now, one potential objection to this hypothetical scenario is that researchers do not choose the number of trials at random, without consideration for the other parameters of the study design. A more realistic assumption might be that researchers choose <span class="math inline">\(K\)</span> to ensure they achieve at least some threshold level of reliability for the observed scores. The reliability of the observed scores is <span class="math inline">\(A_{av}^2\)</span>, so ensuring some threshold of reliability is equivalent to ensuring the square root of the threshold for <span class="math inline">\(A_{av}\)</span>. Let’s suppose that researchers always ensure <span class="math inline">\(A_{av} \geq 0.8\)</span> so that reliability is always at least <span class="math inline">\(0.64\)</span>. This leads to the following distributions for <span class="math inline">\(A_{av}\)</span> and <span class="math inline">\(A_z\)</span>:</p>
<div class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a>A_av_trunc <span class="ot">&lt;-</span> A_av[A_av <span class="sc">&gt;=</span> <span class="fl">0.8</span>]</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a>A_z_trunc <span class="ot">&lt;-</span> A_z[A_av <span class="sc">&gt;=</span> <span class="fl">0.8</span>]</span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a>p_A_av_trunc <span class="ot">&lt;-</span> <span class="fu">density_plot</span>(A_av_trunc, <span class="fu">expression</span>(A[av]),<span class="st">"blue"</span>, <span class="fu">c</span>(<span class="fl">0.2</span>, <span class="dv">1</span>))</span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a>p_A_z_trunc <span class="ot">&lt;-</span> <span class="fu">density_plot</span>(A_z_trunc, <span class="fu">expression</span>(A[z]), <span class="st">"purple"</span>, <span class="fu">c</span>(<span class="dv">0</span>, <span class="dv">3</span>))</span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a>p_A_av_trunc <span class="sc">+</span> p_A_z_trunc</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="index_files/figure-html/unnamed-chunk-3-1.png" class="img-fluid figure-img" width="960"></p>
</figure>
</div>
</div>
</div>
<p>The distribution of <span class="math inline">\(A_{av}\)</span> loses its left tail, so that its mean is <span class="math inline">\(E(A_{av}|A_{av} \geq 0.8) = 0.917\)</span> and its coefficient of variation is reduced to <span class="math inline">\(CV(A_{av} | A_{av} \geq 0.8) = 0.049\)</span>. The distribution of <span class="math inline">\(A_{av}\)</span> now has a mean of <span class="math inline">\(E(A_z | A_{av} \geq 0.8) = 1.045\)</span> and a coefficient of variation of <span class="math inline">\(CV(A_z | A_{av} \geq 0.8) = 0.173\)</span>, about 3.5 times larger than the squared coefficient of variation of <span class="math inline">\(A_{av}\)</span>.</p>
<p>Under both of these scenarios, the observed-score <span class="math inline">\(\tilde\delta_z\)</span> is substantially more sensitive to procedural heterogeneity than is <span class="math inline">\(\tilde\delta_{av}\)</span>. Based on this model and hypothetical example, it seems clear <span class="math inline">\(\tilde\delta_{av}\)</span> should be preferred over <span class="math inline">\(\tilde\delta_z\)</span> as a metric for meta-analysis. However, these relationships are predicated on a certain model for the study-specific parameters. One might object to this model because there’s a sense that we have assumed that <span class="math inline">\(\delta_{av}\)</span> is the right answer. After all, the underlying effect size model is specified in terms of <span class="math inline">\(\delta_{av}\)</span>, and the design parameters—including <span class="math inline">\(\rho\)</span> in particular—are treated as noise, uncorrelated with <span class="math inline">\(\delta_{av}\)</span>. What happens to the observed-score metrics <span class="math inline">\(\tilde\delta_z\)</span> and <span class="math inline">\(\tilde\delta_{av}\)</span> if we instead start with a model specified in terms of <span class="math inline">\(\delta_z\)</span>?</p>
</section>
<section id="a-model-for-delta_z" class="level3">
<h3 class="anchored" data-anchor-id="a-model-for-delta_z">A model for <span class="math inline">\(\delta_z\)</span></h3>
<p>Let’s now see how this works if we treat <span class="math inline">\(\delta_z\)</span> as the correct metric and assume that the design parameters are independent of <span class="math inline">\(\delta_z\)</span>. Assume that <span class="math display">\[
\delta_z \sim N(\alpha, \omega^2)
\]</span> and that the remaining parameters <span class="math inline">\(\rho\)</span>, <span class="math inline">\(\phi\)</span>, and <span class="math inline">\(K\)</span> are independent of <span class="math inline">\(\delta_{av}\)</span>. Then the observed-score standardized mean difference using change score standardization can be written as <span class="math inline">\(\tilde\delta_z = B_z \times \delta_z\)</span>, where <span class="math display">\[
B_z = \sqrt{\frac{1 - \rho}{1 - \rho + \frac{1 - \phi}{\phi K}}}
\]</span> and the observed-score standardized mean difference using raw score standardization can be written as <span class="math inline">\(\tilde\delta_{av} = B_{av} \times \delta_z\)</span>, where <span class="math display">\[
B_{av} = \frac{\sqrt{2}(1 - \rho)}{\sqrt{1 - \rho + \frac{1 - \phi}{\phi K}}}.
\]</span> The plots below show the distribution of <span class="math inline">\(B_z\)</span> and <span class="math inline">\(B_{av}\)</span> under the same scenarios considered above. First, the scenario where observed-score reliability is not controlled:</p>
<div class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a>B_z <span class="ot">&lt;-</span> <span class="fu">sqrt</span>((<span class="dv">1</span> <span class="sc">-</span> rho) <span class="sc">/</span> (<span class="dv">1</span> <span class="sc">-</span> rho <span class="sc">+</span> (<span class="dv">1</span> <span class="sc">-</span> phi) <span class="sc">/</span> (phi <span class="sc">*</span> K)))</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>B_av <span class="ot">&lt;-</span> <span class="fu">sqrt</span>(<span class="dv">2</span>) <span class="sc">*</span> (<span class="dv">1</span> <span class="sc">-</span> rho) <span class="sc">/</span> <span class="fu">sqrt</span>(<span class="dv">1</span> <span class="sc">-</span> rho <span class="sc">+</span> (<span class="dv">1</span> <span class="sc">-</span> phi) <span class="sc">/</span> (phi <span class="sc">*</span> K))</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a>p_B_av <span class="ot">&lt;-</span> <span class="fu">density_plot</span>(B_av, <span class="fu">expression</span>(B[av]),<span class="st">"green"</span>, <span class="fu">c</span>(<span class="dv">0</span>,<span class="fl">1.5</span>))</span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a>p_B_z <span class="ot">&lt;-</span> <span class="fu">density_plot</span>(B_z, <span class="fu">expression</span>(B[z]), <span class="st">"yellow"</span>, <span class="fu">c</span>(<span class="dv">0</span>, <span class="dv">1</span>))</span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a>p_B_av <span class="sc">+</span> p_B_z</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="index_files/figure-html/unnamed-chunk-4-1.png" class="img-fluid figure-img" width="960"></p>
</figure>
</div>
</div>
</div>
<p>Second, the scenario where observed-score reliability is at least 0.64:</p>
<div class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a>B_z_trunc <span class="ot">&lt;-</span> B_z[A_av <span class="sc">&gt;=</span> <span class="fl">0.8</span>]</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a>B_av_trunc <span class="ot">&lt;-</span> B_av[A_av <span class="sc">&gt;=</span> <span class="fl">0.8</span>]</span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a>p_B_av_trunc <span class="ot">&lt;-</span> <span class="fu">density_plot</span>(B_av, <span class="fu">expression</span>(B[av]),<span class="st">"green"</span>, <span class="fu">c</span>(<span class="dv">0</span>,<span class="fl">1.5</span>))</span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a>p_B_z_trunc <span class="ot">&lt;-</span> <span class="fu">density_plot</span>(B_z, <span class="fu">expression</span>(B[z]), <span class="st">"yellow"</span>, <span class="fu">c</span>(<span class="dv">0</span>, <span class="dv">1</span>))</span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a>p_B_av_trunc <span class="sc">+</span> p_B_z_trunc</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="index_files/figure-html/unnamed-chunk-5-1.png" class="img-fluid figure-img" width="960"></p>
</figure>
</div>
</div>
</div>
<p>The table below reports the coefficients of variation for each of the multiplicative factors I have considered.</p>
<div class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(dplyr)</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a>dat <span class="ot">&lt;-</span> <span class="fu">tibble</span>(A_av, A_z, B_av, B_z)</span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a>random_rel <span class="ot">&lt;-</span> </span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a>  dat <span class="sc">|&gt;</span></span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a>  <span class="fu">summarise</span>(</span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a>    <span class="fu">across</span>(<span class="fu">everything</span>(), <span class="sc">~</span> <span class="fu">sd</span>(.) <span class="sc">/</span> <span class="fu">mean</span>(.)),</span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true" tabindex="-1"></a>    <span class="at">reliability =</span> <span class="st">"random"</span></span>
<span id="cb5-9"><a href="#cb5-9" aria-hidden="true" tabindex="-1"></a>  )</span>
<span id="cb5-10"><a href="#cb5-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-11"><a href="#cb5-11" aria-hidden="true" tabindex="-1"></a>controlled_rel <span class="ot">&lt;-</span></span>
<span id="cb5-12"><a href="#cb5-12" aria-hidden="true" tabindex="-1"></a>  dat <span class="sc">|&gt;</span></span>
<span id="cb5-13"><a href="#cb5-13" aria-hidden="true" tabindex="-1"></a>  <span class="fu">filter</span>(A_av <span class="sc">&gt;=</span> <span class="fl">0.8</span>) <span class="sc">|&gt;</span></span>
<span id="cb5-14"><a href="#cb5-14" aria-hidden="true" tabindex="-1"></a>  <span class="fu">summarise</span>(</span>
<span id="cb5-15"><a href="#cb5-15" aria-hidden="true" tabindex="-1"></a>    <span class="fu">across</span>(<span class="fu">everything</span>(), <span class="sc">~</span> <span class="fu">sd</span>(.) <span class="sc">/</span> <span class="fu">mean</span>(.)),</span>
<span id="cb5-16"><a href="#cb5-16" aria-hidden="true" tabindex="-1"></a>    <span class="at">reliability =</span> <span class="st">"at least 0.64"</span></span>
<span id="cb5-17"><a href="#cb5-17" aria-hidden="true" tabindex="-1"></a>  )</span>
<span id="cb5-18"><a href="#cb5-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-19"><a href="#cb5-19" aria-hidden="true" tabindex="-1"></a>CVs <span class="ot">&lt;-</span> </span>
<span id="cb5-20"><a href="#cb5-20" aria-hidden="true" tabindex="-1"></a>  <span class="fu">bind_rows</span>(random_rel, controlled_rel) <span class="sc">|&gt;</span></span>
<span id="cb5-21"><a href="#cb5-21" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(</span>
<span id="cb5-22"><a href="#cb5-22" aria-hidden="true" tabindex="-1"></a>    <span class="at">A_ratio =</span> A_z <span class="sc">/</span> A_av,</span>
<span id="cb5-23"><a href="#cb5-23" aria-hidden="true" tabindex="-1"></a>    <span class="at">B_ratio =</span> B_z <span class="sc">/</span> B_av</span>
<span id="cb5-24"><a href="#cb5-24" aria-hidden="true" tabindex="-1"></a>  ) <span class="sc">|&gt;</span></span>
<span id="cb5-25"><a href="#cb5-25" aria-hidden="true" tabindex="-1"></a>  <span class="fu">select</span>(reliability, A_av, A_z, A_ratio, B_av, B_z, B_ratio)</span>
<span id="cb5-26"><a href="#cb5-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-27"><a href="#cb5-27" aria-hidden="true" tabindex="-1"></a>knitr<span class="sc">::</span><span class="fu">kable</span>(</span>
<span id="cb5-28"><a href="#cb5-28" aria-hidden="true" tabindex="-1"></a>  CVs, </span>
<span id="cb5-29"><a href="#cb5-29" aria-hidden="true" tabindex="-1"></a>  <span class="at">digits =</span> <span class="fu">c</span>(<span class="dv">0</span>,<span class="dv">3</span>,<span class="dv">3</span>,<span class="dv">1</span>,<span class="dv">3</span>,<span class="dv">3</span>,<span class="dv">2</span>),</span>
<span id="cb5-30"><a href="#cb5-30" aria-hidden="true" tabindex="-1"></a>  <span class="at">caption =</span> <span class="st">"Coefficients of variation"</span>,</span>
<span id="cb5-31"><a href="#cb5-31" aria-hidden="true" tabindex="-1"></a>  <span class="at">col.names =</span> <span class="fu">c</span>(<span class="st">"Reliability"</span>,<span class="st">"$A_{av}$"</span>,<span class="st">"$A_z$"</span>,<span class="st">"$A_z /A_{av}$"</span>, <span class="st">"$B_{av}$"</span>,<span class="st">"$B_z$"</span>, <span class="st">"$B_{z} / B_{av}$"</span>),</span>
<span id="cb5-32"><a href="#cb5-32" aria-hidden="true" tabindex="-1"></a>  <span class="at">escape =</span> <span class="cn">TRUE</span></span>
<span id="cb5-33"><a href="#cb5-33" aria-hidden="true" tabindex="-1"></a>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output-display">
<table class="table table-sm table-striped small">
<caption>Coefficients of variation</caption>
<colgroup>
<col style="width: 18%">
<col style="width: 12%">
<col style="width: 8%">
<col style="width: 18%">
<col style="width: 12%">
<col style="width: 8%">
<col style="width: 22%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: left;">Reliability</th>
<th style="text-align: right;"><span class="math inline">\(A_{av}\)</span></th>
<th style="text-align: right;"><span class="math inline">\(A_z\)</span></th>
<th style="text-align: right;"><span class="math inline">\(A_z /A_{av}\)</span></th>
<th style="text-align: right;"><span class="math inline">\(B_{av}\)</span></th>
<th style="text-align: right;"><span class="math inline">\(B_z\)</span></th>
<th style="text-align: right;"><span class="math inline">\(B_{z} / B_{av}\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">random</td>
<td style="text-align: right;">0.085</td>
<td style="text-align: right;">0.210</td>
<td style="text-align: right;">2.5</td>
<td style="text-align: right;">0.285</td>
<td style="text-align: right;">0.182</td>
<td style="text-align: right;">0.64</td>
</tr>
<tr class="even">
<td style="text-align: left;">at least 0.64</td>
<td style="text-align: right;">0.049</td>
<td style="text-align: right;">0.173</td>
<td style="text-align: right;">3.5</td>
<td style="text-align: right;">0.257</td>
<td style="text-align: right;">0.137</td>
<td style="text-align: right;">0.53</td>
</tr>
</tbody>
</table>
</div>
</div>
<p>The tables are now more or less turned. Under both reliability scenarios, <span class="math inline">\(B_z\)</span> has a lower coefficient of variation than <span class="math inline">\(B_{av}\)</span>, indicating that <span class="math inline">\(\tilde\delta_z\)</span> is less affected by procedural heterogeneity than is <span class="math inline">\(\tilde\delta_{av}\)</span>. However, <span class="math inline">\(\tilde\delta_z\)</span> is still affected in absolute terms, considering that the coefficient of variation for <span class="math inline">\(B_z\)</span> is about 79% of the coefficient of variation for <span class="math inline">\(A_z\)</span>. Of course, <span class="math inline">\(\tilde\delta_{av}\)</span> is quite strongly affected under this model, with a coefficient of variation of 0.257.</p>
</section>
<section id="consequences-for-heterogeneity" class="level3">
<h3 class="anchored" data-anchor-id="consequences-for-heterogeneity">Consequences for heterogeneity</h3>
<p>To make these results a bit more concrete, it’s useful to consider think in terms of heterogeneity of the observed score effect sizes. The figure below plots the CVs of observed-score effect size parameters as a function of the CVs of the true effect size distribution.</p>
<div class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(tidyr)</span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(stringr)</span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a>CV_obs <span class="ot">&lt;-</span> </span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a>  CVs <span class="sc">|&gt;</span></span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a>  <span class="fu">select</span>(<span class="sc">-</span>A_ratio, <span class="sc">-</span>B_ratio) <span class="sc">|&gt;</span></span>
<span id="cb6-7"><a href="#cb6-7" aria-hidden="true" tabindex="-1"></a>  <span class="fu">pivot_longer</span>(<span class="sc">-</span>reliability, <span class="at">names_to =</span> <span class="st">"metric"</span>, <span class="at">values_to =</span> <span class="st">"het"</span>) <span class="sc">|&gt;</span></span>
<span id="cb6-8"><a href="#cb6-8" aria-hidden="true" tabindex="-1"></a>  <span class="fu">expand_grid</span>(<span class="at">tau_mu =</span> <span class="fu">seq</span>(<span class="dv">0</span>,<span class="dv">1</span>,<span class="fl">0.02</span>)) <span class="sc">|&gt;</span></span>
<span id="cb6-9"><a href="#cb6-9" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(</span>
<span id="cb6-10"><a href="#cb6-10" aria-hidden="true" tabindex="-1"></a>    <span class="at">reliability =</span> <span class="fu">recode</span>(reliability, <span class="st">'at least 0.64'</span> <span class="ot">=</span> <span class="st">"Reliability of at least 0.64"</span>, <span class="st">'random'</span> <span class="ot">=</span> <span class="st">"Random reliability"</span>),</span>
<span id="cb6-11"><a href="#cb6-11" aria-hidden="true" tabindex="-1"></a>    <span class="at">metric =</span> <span class="fu">paste0</span>(<span class="fu">str_replace</span>(metric, <span class="st">"</span><span class="sc">\\</span><span class="st">_"</span>,<span class="st">"</span><span class="sc">\\</span><span class="st">["</span>),<span class="st">"]"</span>),</span>
<span id="cb6-12"><a href="#cb6-12" aria-hidden="true" tabindex="-1"></a>    <span class="at">CV =</span> <span class="fu">sqrt</span>((<span class="dv">1</span> <span class="sc">+</span> <span class="dv">2</span> <span class="sc">*</span> tau_mu<span class="sc">^</span><span class="dv">2</span>) <span class="sc">*</span> het<span class="sc">^</span><span class="dv">2</span> <span class="sc">+</span> tau_mu<span class="sc">^</span><span class="dv">2</span>)</span>
<span id="cb6-13"><a href="#cb6-13" aria-hidden="true" tabindex="-1"></a>  )</span>
<span id="cb6-14"><a href="#cb6-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-15"><a href="#cb6-15" aria-hidden="true" tabindex="-1"></a>CV_ex <span class="ot">&lt;-</span> CV_obs <span class="sc">|&gt;</span></span>
<span id="cb6-16"><a href="#cb6-16" aria-hidden="true" tabindex="-1"></a>  <span class="fu">filter</span>(tau_mu <span class="sc">==</span> <span class="fl">0.5</span>, reliability <span class="sc">==</span> <span class="st">"Reliability of at least 0.64"</span>) <span class="sc">|&gt;</span></span>
<span id="cb6-17"><a href="#cb6-17" aria-hidden="true" tabindex="-1"></a>  <span class="fu">select</span>(metric, CV) <span class="sc">|&gt;</span></span>
<span id="cb6-18"><a href="#cb6-18" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(</span>
<span id="cb6-19"><a href="#cb6-19" aria-hidden="true" tabindex="-1"></a>    <span class="at">CV =</span> <span class="fu">round</span>(CV, <span class="dv">3</span>),</span>
<span id="cb6-20"><a href="#cb6-20" aria-hidden="true" tabindex="-1"></a>    <span class="at">metric =</span> <span class="fu">str_replace</span>(<span class="fu">str_sub</span>(metric, <span class="dv">1</span>, <span class="sc">-</span><span class="dv">2</span>), <span class="st">"</span><span class="sc">\\</span><span class="st">["</span>, <span class="st">"_"</span>)</span>
<span id="cb6-21"><a href="#cb6-21" aria-hidden="true" tabindex="-1"></a>  ) <span class="sc">|&gt;</span></span>
<span id="cb6-22"><a href="#cb6-22" aria-hidden="true" tabindex="-1"></a>  <span class="fu">pivot_wider</span>(<span class="at">names_from =</span> metric, <span class="at">values_from =</span> CV)</span>
<span id="cb6-23"><a href="#cb6-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-24"><a href="#cb6-24" aria-hidden="true" tabindex="-1"></a>CV_obs_labs <span class="ot">&lt;-</span></span>
<span id="cb6-25"><a href="#cb6-25" aria-hidden="true" tabindex="-1"></a>  CV_obs <span class="sc">%&gt;%</span></span>
<span id="cb6-26"><a href="#cb6-26" aria-hidden="true" tabindex="-1"></a>  <span class="fu">filter</span>(tau_mu <span class="sc">==</span> <span class="dv">0</span>)</span>
<span id="cb6-27"><a href="#cb6-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-28"><a href="#cb6-28" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>(CV_obs, <span class="fu">aes</span>(tau_mu, CV, <span class="at">color =</span> metric)) <span class="sc">+</span> </span>
<span id="cb6-29"><a href="#cb6-29" aria-hidden="true" tabindex="-1"></a>  <span class="fu">facet_wrap</span>(<span class="fu">vars</span>(reliability)) <span class="sc">+</span></span>
<span id="cb6-30"><a href="#cb6-30" aria-hidden="true" tabindex="-1"></a>  <span class="fu">scale_x_continuous</span>(<span class="at">expand =</span> <span class="fu">expansion</span>(<span class="fu">c</span>(<span class="fl">0.1</span>,<span class="dv">0</span>),<span class="dv">0</span>), <span class="at">breaks =</span> <span class="fu">seq</span>(<span class="fl">0.2</span>, <span class="dv">1</span>, <span class="fl">0.2</span>)) <span class="sc">+</span> </span>
<span id="cb6-31"><a href="#cb6-31" aria-hidden="true" tabindex="-1"></a>  <span class="fu">scale_y_continuous</span>(<span class="at">expand =</span> <span class="fu">expansion</span>(<span class="dv">0</span>,<span class="dv">0</span>), <span class="at">breaks =</span> <span class="fu">seq</span>(<span class="dv">0</span>, <span class="dv">1</span>, <span class="fl">0.2</span>)) <span class="sc">+</span> </span>
<span id="cb6-32"><a href="#cb6-32" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_vline</span>(<span class="at">xintercept =</span> <span class="dv">0</span>) <span class="sc">+</span> </span>
<span id="cb6-33"><a href="#cb6-33" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_hline</span>(<span class="at">yintercept =</span> <span class="dv">0</span>) <span class="sc">+</span> </span>
<span id="cb6-34"><a href="#cb6-34" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_abline</span>(<span class="at">slope =</span> <span class="dv">1</span>, <span class="at">linetype =</span> <span class="st">"dashed"</span>) <span class="sc">+</span> </span>
<span id="cb6-35"><a href="#cb6-35" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_text</span>(</span>
<span id="cb6-36"><a href="#cb6-36" aria-hidden="true" tabindex="-1"></a>    <span class="at">data =</span> CV_obs_labs, </span>
<span id="cb6-37"><a href="#cb6-37" aria-hidden="true" tabindex="-1"></a>    <span class="fu">aes</span>(<span class="at">x =</span> tau_mu, <span class="at">y =</span> CV, <span class="at">color =</span> metric, <span class="at">label =</span> metric),</span>
<span id="cb6-38"><a href="#cb6-38" aria-hidden="true" tabindex="-1"></a>    <span class="at">nudge_x =</span> <span class="sc">-</span><span class="fl">0.05</span>,</span>
<span id="cb6-39"><a href="#cb6-39" aria-hidden="true" tabindex="-1"></a>    <span class="at">parse =</span> <span class="cn">TRUE</span></span>
<span id="cb6-40"><a href="#cb6-40" aria-hidden="true" tabindex="-1"></a>  ) <span class="sc">+</span> </span>
<span id="cb6-41"><a href="#cb6-41" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_line</span>() <span class="sc">+</span> </span>
<span id="cb6-42"><a href="#cb6-42" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme_minimal</span>() <span class="sc">+</span> </span>
<span id="cb6-43"><a href="#cb6-43" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">x =</span> <span class="fu">expression</span>(tau <span class="sc">/</span> mu), <span class="at">y =</span> <span class="st">"Coefficient of variation for observed score ES"</span>) <span class="sc">+</span> </span>
<span id="cb6-44"><a href="#cb6-44" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme</span>(<span class="at">legend.position =</span> <span class="st">"none"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="index_files/figure-html/unnamed-chunk-7-1.png" class="img-fluid figure-img" width="672"></p>
</figure>
</div>
</div>
</div>
<p>Consider, for instance, a scenario where observed-score reliability is always at least 0.64 and <span class="math inline">\(\tau / \mu = 0.5\)</span>, which would be the case if effect sizes are normally distributed and about 97% of effect sizes are positive. Under the effect size model based on <span class="math inline">\(\delta_{av}\)</span>, the observed-score <span class="math inline">\(\tilde\delta_{av}\)</span> is hardly affected by measurement heterogeneity at all, with a CV of 0.504 but the CV of the observed-score <span class="math inline">\(\tilde\delta_z\)</span> is 0.543. Under the effect size model based on <span class="math inline">\(\delta_{av}\)</span>, the observed-score <span class="math inline">\(\tilde\delta_{av}\)</span> is strongly affected by measurement heterogeneity, with a CV of 0.591; in comparison, the CV of the observed-score <span class="math inline">\(\tilde\delta_z\)</span> of 0.528. Under both models, these increases in CV are effectively constant for larger values of <span class="math inline">\(\tau / \mu\)</span>.</p>
</section>
</section>
<section id="so-whats-your-point" class="level2">
<h2 class="anchored" data-anchor-id="so-whats-your-point">So what’s your point?</h2>
<p>Unfortunately, this particular trip down a rabbit hole doesn’t seem to yield many clear take-aways. For the scenario that I looked at here, the preferred choice of effect size metric is apparently driven by what assumptions we find more plausible. If we think the more plausible model is the one in which <span class="math inline">\(\rho\)</span> is independent of <span class="math inline">\(\delta_{av}\)</span>, then <span class="math inline">\(\tilde\delta_{av}\)</span> is less strongly affected by measurement variation and therefore preferred. Further, the attenuation in <span class="math inline">\(\tilde\delta_{av}\)</span> depends only on <span class="math inline">\(\phi\)</span>, which might mean that a correction for attenuation is more feasible. However, if we think the more plausible model is the one in which <span class="math inline">\(\rho\)</span> is independent of <span class="math inline">\(\delta_z\)</span>, then <span class="math inline">\(\tilde\delta_z\)</span> is less strongly affected by measurement variation and therefore preferred.<a href="#fn5" class="footnote-ref" id="fnref5" role="doc-noteref"><sup>5</sup></a> In the latter model, one caveat is that the measurement error attenuation in <span class="math inline">\(\tilde\delta_z\)</span> is still a complicated mess, depending on both the true score correlation <span class="math inline">\(\rho\)</span> and the reliability <span class="math inline">\(\phi\)</span>. This would make it pretty hard to implement some sort of correction for attenuation.</p>
<p>So, how could one decide which meta-analytic model is more plausible in a given application? On a conceptual level, I would argue that the model for <span class="math inline">\(\delta_{av}\)</span> would tend to be more plausible in meta-analyses where there is more operational variation in the interventions examined. I would venture that syntheses that include many different versions of an intervention would tend to have a wider range of correlations between true scores (i.e., more heterogeneous correlations between potential outcomes), even holding the outcome measurement procedures constant. This doesn’t necessarily justify the assumption that <span class="math inline">\(\rho\)</span> is independent of <span class="math inline">\(\delta_{av}\)</span>, but it does make it seem rather implausible that <span class="math inline">\(\delta_z\)</span> would be independent of <span class="math inline">\(\rho\)</span>.</p>
<p>On a more practical level, it seems like there are a few empirical things that a meta-analyst could do to inform a choice between a model for <span class="math inline">\(\delta_{av}\)</span> and one for <span class="math inline">\(\delta_z\)</span>. Pragmatically, one could calculate both effect size metrics and just see which one exhibits more heterogeneity. All else equal, it seems reasonable to prefer the metric that has less heterogeneity. One could also try to gather data on the correlation between observed scores, on the reliability of the observed scores, and on the number of trials used in each study. With this information, one could construct measurement-related predictors and use them in a meta-regression to explain variation in the observed effect size estimates. Alternately, one could use the formulas given above to implement attenuation corrections for the effect sizes and see if this leads to reduced heterogeneity. How well would any of these approaches actually work? Answering that question would take some further, more careful and systematic investigation.</p>


<!-- -->

</section>


<a onclick="window.scrollTo(0, 0); return false;" role="button" id="quarto-back-to-top"><i class="bi bi-arrow-up"></i> Back to top</a><div id="quarto-appendix" class="default"><section id="footnotes" class="footnotes footnotes-end-of-document" role="doc-endnotes"><h2 class="anchored quarto-appendix-heading">Footnotes</h2>

<ol>
<li id="fn1"><p>Perhaps because use of Cohen’s <span class="math inline">\(d\)</span> is so under-scrutinized in practice, methodologists have spent many an afternoon blogging about this problem. For general discussions about issues with how to define Cohen’s <span class="math inline">\(d\)</span>, see excellent posts from <a href="https://janhove.github.io/reporting/2015/02/05/standardised-vs-unstandardised-es">Jan Vanhove</a> (with <a href="https://janhove.github.io/design/2015/03/16/standardised-es-revisited">a sequel</a>), <a href="http://jakewestfall.org/blog/index.php/2016/03/25/five-different-cohens-d-statistics-for-within-subject-designs/">Jake Westfall</a>, <a href="http://datacolada.org/33">Uri Simonsohn</a>, and <a href="https://transparentstatistics.org/2018/07/05/meanings-effect-size/">Pierre Dragicevic</a>; a more formal discussion by <a href="https://www.floppybunny.org/robin/web/virtualclassroom/stats/basics/articles/effect_size/effect_size_baguley_2009.pdf">Thom Baguley</a>; and some very interesting work on alternative conceptualizations by <a href="https://doi.org/10.1002/jrsm.1130">Tony Ades and colleagues</a>. I can promise, dear reader, that the present blog post will not be nearly as cogent as these contributions—this is more about getting my own thoughts straight than making any recommendations—and so <em>caveat lector</em> applies.<a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn2"><p>I would <em>love</em> to be corrected on both of these points. Please drop a comment or email me with suggested reading.<a href="#fnref2" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn3"><p>When the standardizing variance is calculated using measurements from only one condition (i.e., the pre-test in a repeated measures design), this version of <span class="math inline">\(d\)</span> corresponds to <code>measure = "SMCR"</code>, the “standardized mean change using raw score standardization” in <code>metafor::escalc</code>.<a href="#fnref3" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn4"><p>This version of <span class="math inline">\(d\)</span> corresponds to <code>measure = "SMCC"</code>, the “standardized mean change using change score standardization” in <code>metafor::escalc</code>.<a href="#fnref4" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn5"><p>And of course, these two models are not the only alternatives—one could look at intermediate scenarios where <span class="math inline">\(\rho\)</span> and <span class="math inline">\(\phi\)</span> are more or less strongly correlated with the true score effect sizes.<a href="#fnref5" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</section><section class="quarto-appendix-contents" id="quarto-reuse"><h2 class="anchored quarto-appendix-heading">Reuse</h2><div class="quarto-appendix-contents"><div><a rel="license" href="https://creativecommons.org/licenses/by/4.0/">CC BY 4.0</a></div></div></section><section class="quarto-appendix-contents" id="quarto-citation"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@online{pustejovsky2023,
  author = {Pustejovsky, James E.},
  title = {Cohen’s \$d\_z\$ Makes Me Dizzy When Considering Measurement
    Error},
  date = {2023-02-17},
  url = {https://mellifluous-buttercream-e2edd2.netlify.app/posts/dizzy-for-d-z},
  langid = {en}
}
</code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-pustejovsky2023" class="csl-entry quarto-appendix-citeas" role="listitem">
Pustejovsky, James E. 2023. <span>“Cohen’s $d_z$ Makes Me Dizzy When
Considering Measurement Error.”</span> February 17, 2023. <a href="https://mellifluous-buttercream-e2edd2.netlify.app/posts/dizzy-for-d-z">https://mellifluous-buttercream-e2edd2.netlify.app/posts/dizzy-for-d-z</a>.
</div></div></section></div></main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  const viewSource = window.document.getElementById('quarto-view-source') ||
                     window.document.getElementById('quarto-code-tools-source');
  if (viewSource) {
    const sourceUrl = viewSource.getAttribute("data-quarto-source-url");
    viewSource.addEventListener("click", function(e) {
      if (sourceUrl) {
        // rstudio viewer pane
        if (/\bcapabilities=\b/.test(window.location)) {
          window.open(sourceUrl);
        } else {
          window.location.href = sourceUrl;
        }
      } else {
        const modal = new bootstrap.Modal(document.getElementById('quarto-embedded-source-code-modal'));
        modal.show();
      }
      return false;
    });
  }
  function toggleCodeHandler(show) {
    return function(e) {
      const detailsSrc = window.document.querySelectorAll(".cell > details > .sourceCode");
      for (let i=0; i<detailsSrc.length; i++) {
        const details = detailsSrc[i].parentElement;
        if (show) {
          details.open = true;
        } else {
          details.removeAttribute("open");
        }
      }
      const cellCodeDivs = window.document.querySelectorAll(".cell > .sourceCode");
      const fromCls = show ? "hidden" : "unhidden";
      const toCls = show ? "unhidden" : "hidden";
      for (let i=0; i<cellCodeDivs.length; i++) {
        const codeDiv = cellCodeDivs[i];
        if (codeDiv.classList.contains(fromCls)) {
          codeDiv.classList.remove(fromCls);
          codeDiv.classList.add(toCls);
        } 
      }
      return false;
    }
  }
  const hideAllCode = window.document.getElementById("quarto-hide-all-code");
  if (hideAllCode) {
    hideAllCode.addEventListener("click", toggleCodeHandler(false));
  }
  const showAllCode = window.document.getElementById("quarto-show-all-code");
  if (showAllCode) {
    showAllCode.addEventListener("click", toggleCodeHandler(true));
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp("https:\/\/mellifluous-buttercream-e2edd2\.netlify\.app");
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<script src="https://utteranc.es/client.js" repo="jepusto/jepusto-quarto" issue-term="pathname" theme="github-light" crossorigin="anonymous" async="">
</script><div class="modal fade" id="quarto-embedded-source-code-modal" tabindex="-1" aria-labelledby="quarto-embedded-source-code-modal-label" aria-hidden="true"><div class="modal-dialog modal-dialog-scrollable"><div class="modal-content"><div class="modal-header"><h5 class="modal-title" id="quarto-embedded-source-code-modal-label">Source Code</h5><button class="btn-close" data-bs-dismiss="modal"></button></div><div class="modal-body"><div class="">
<div class="sourceCode" id="cb7" data-shortcodes="false"><pre class="sourceCode markdown code-with-copy"><code class="sourceCode markdown"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a><span class="an">title:</span><span class="co"> Cohen's $d_z$ makes me dizzy when considering measurement error</span></span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a><span class="an">date:</span><span class="co"> '2023-02-17'</span></span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a><span class="an">categories:</span></span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a><span class="co">- effect size</span></span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a><span class="co">- standardized mean difference</span></span>
<span id="cb7-7"><a href="#cb7-7" aria-hidden="true" tabindex="-1"></a><span class="co">- design-comparable SMD</span></span>
<span id="cb7-8"><a href="#cb7-8" aria-hidden="true" tabindex="-1"></a><span class="co">- measurement-error</span></span>
<span id="cb7-9"><a href="#cb7-9" aria-hidden="true" tabindex="-1"></a><span class="an">description:</span><span class="co"> Meta-analyses in education, psychology, and related fields rely heavily of Cohen's $d$, or the standardized mean difference effect size, for quantitatively describing the magnitude and direction of intervention effects. In these fields, Cohen's $d$ is so pervasive that its use is nearly automatic, and analysts rarely question its utility or consider alternatives (response ratios, anyone? POMP?). Despite this state of affairs, working with Cohen's $d$ is theoretically challenging because the standardized mean difference metric does not have a singular definition. Rather, its definition depends on the choice of the standardizing variance used in the denominator.</span></span>
<span id="cb7-10"><a href="#cb7-10" aria-hidden="true" tabindex="-1"></a><span class="an">code-fold:</span><span class="co"> true</span></span>
<span id="cb7-11"><a href="#cb7-11" aria-hidden="true" tabindex="-1"></a><span class="an">code-tools:</span><span class="co"> true</span></span>
<span id="cb7-12"><a href="#cb7-12" aria-hidden="true" tabindex="-1"></a><span class="an">toc:</span><span class="co"> true</span></span>
<span id="cb7-13"><a href="#cb7-13" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb7-14"><a href="#cb7-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-15"><a href="#cb7-15" aria-hidden="true" tabindex="-1"></a><span class="in">```{r, echo = FALSE}</span></span>
<span id="cb7-16"><a href="#cb7-16" aria-hidden="true" tabindex="-1"></a><span class="in">knitr::opts_chunk$set(warning = FALSE, message = FALSE)</span></span>
<span id="cb7-17"><a href="#cb7-17" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb7-18"><a href="#cb7-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-19"><a href="#cb7-19" aria-hidden="true" tabindex="-1"></a>Meta-analyses in education, psychology, and related fields rely heavily of Cohen's $d$, or the standardized mean difference effect size, for quantitatively describing the magnitude and direction of intervention effects. In these fields, Cohen's $d$ is so pervasive that its use is nearly automatic, and analysts rarely question its utility or consider alternatives (response ratios, anyone? POMP?). Despite this state of affairs, working with Cohen's $d$ is theoretically challenging because the standardized mean difference metric does not have a singular definition. Rather, its definition depends on the choice of the standardizing variance used in the denominator.<span class="ot">[^blogroll]</span> </span>
<span id="cb7-20"><a href="#cb7-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-21"><a href="#cb7-21" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb7-22"><a href="#cb7-22" aria-hidden="true" tabindex="-1"></a>\def\Pr{{\text{Pr}}}</span>
<span id="cb7-23"><a href="#cb7-23" aria-hidden="true" tabindex="-1"></a>\def\E{{\text{E}}}</span>
<span id="cb7-24"><a href="#cb7-24" aria-hidden="true" tabindex="-1"></a>\def\Var{{\text{Var}}}</span>
<span id="cb7-25"><a href="#cb7-25" aria-hidden="true" tabindex="-1"></a>\def\Cov{{\text{Cov}}}</span>
<span id="cb7-26"><a href="#cb7-26" aria-hidden="true" tabindex="-1"></a>\def\cor{{\text{cor}}}</span>
<span id="cb7-27"><a href="#cb7-27" aria-hidden="true" tabindex="-1"></a>\def\bm{\mathbf}</span>
<span id="cb7-28"><a href="#cb7-28" aria-hidden="true" tabindex="-1"></a>\def\bs{\boldsymbol}</span>
<span id="cb7-29"><a href="#cb7-29" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb7-30"><a href="#cb7-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-31"><a href="#cb7-31" aria-hidden="true" tabindex="-1"></a><span class="ot">[^blogroll]: </span>Perhaps because use of Cohen's $d$ is so under-scrutinized in practice, methodologists have spent many an afternoon blogging about this problem. For general discussions about issues with how to define Cohen's $d$, see excellent posts from <span class="co">[</span><span class="ot">Jan Vanhove</span><span class="co">](https://janhove.github.io/reporting/2015/02/05/standardised-vs-unstandardised-es)</span> (with <span class="co">[</span><span class="ot">a sequel</span><span class="co">](https://janhove.github.io/design/2015/03/16/standardised-es-revisited)</span>), <span class="co">[</span><span class="ot">Jake Westfall</span><span class="co">](http://jakewestfall.org/blog/index.php/2016/03/25/five-different-cohens-d-statistics-for-within-subject-designs/)</span>, <span class="co">[</span><span class="ot">Uri Simonsohn</span><span class="co">](http://datacolada.org/33)</span>, and <span class="co">[</span><span class="ot">Pierre Dragicevic</span><span class="co">](https://transparentstatistics.org/2018/07/05/meanings-effect-size/)</span>; a more formal discussion by <span class="co">[</span><span class="ot">Thom Baguley</span><span class="co">](https://www.floppybunny.org/robin/web/virtualclassroom/stats/basics/articles/effect_size/effect_size_baguley_2009.pdf)</span>; and some very interesting work on alternative conceptualizations by <span class="co">[</span><span class="ot">Tony Ades and colleagues</span><span class="co">](https://doi.org/10.1002/jrsm.1130)</span>. </span>
<span id="cb7-32"><a href="#cb7-32" aria-hidden="true" tabindex="-1"></a>I can promise, dear reader, that the present blog post will not be nearly as cogent as these contributions---this is more about getting my own thoughts straight than making any recommendations---and so _caveat lector_ applies. </span>
<span id="cb7-33"><a href="#cb7-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-34"><a href="#cb7-34" aria-hidden="true" tabindex="-1"></a>In this post, I'm going to mull over how measurement error and design decisions influence the metric definition of Cohen's $d$ in basic within-group experimental designs. The distorting effects of measurement error has long been a source of concern within psychometric meta-analysis, a perspective associated with the work of <span class="co">[</span><span class="ot">Frank Schmidt and Jack Hunter</span><span class="co">](https://methods.sagepub.com/book/methods-of-meta-analysis-3e)</span>, and measurement-error corrections are well developed and often applied in meta-analyses of correlations. Straight-forward measurement-error corrections have also been described for Cohen's $d$ from between-group designs (see recent work by <span class="co">[</span><span class="ot">Brenton Wiernik and Jeff Dahlke</span><span class="co">](https://psyarxiv.com/9mpbn/)</span>). However, I have literally never seen a meta-analytic application that applied these corrections and I have thus far been unable to locate work on such corrections specifically for effect sizes in within-group designs.<span class="ot">[^comment]</span> So, time to muck about...</span>
<span id="cb7-35"><a href="#cb7-35" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-36"><a href="#cb7-36" aria-hidden="true" tabindex="-1"></a><span class="ot">[^comment]: </span>I would _love_ to be corrected on both of these points. Please drop a comment or email me with suggested reading.</span>
<span id="cb7-37"><a href="#cb7-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-38"><a href="#cb7-38" aria-hidden="true" tabindex="-1"></a><span class="fu">## Effect size definitions in within-group designs</span></span>
<span id="cb7-39"><a href="#cb7-39" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-40"><a href="#cb7-40" aria-hidden="true" tabindex="-1"></a>In basic between-group designs, the only variances in the model are the within-group variances, so the choice of standardizing variance is limited to a) the singular population variance, assuming it is homogeneous across groups, b) the variance of one group, or c) the average of the variances in each group. In most applications, homogeneity is assumed (often without much reflection), version (a) of Cohen's $d$ is estimated, and the meta-analyst can go along their merry way. For sake of succinctness, I'll call this effect size $d_{b}$, where the $b$ indicates the usual version for basic between-group designs.</span>
<span id="cb7-41"><a href="#cb7-41" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-42"><a href="#cb7-42" aria-hidden="true" tabindex="-1"></a>For within-group or repeated measures designs, the set of choices is more involved and includes a) standardizing by the across-participant variance in one condition (or both conditions, assuming homogeneity) or b) standardizing by the variance of the difference scores. The former approach is sometimes called $d_{av}$<span class="ot">[^SMCR]</span>, the latter is called $d_z$<span class="ot">[^SMCC]</span>. The $d_{av}$ metric uses the same standardizing variance as the $d$ from a basic between-group design, and so results from both types of designs are, in principle, on the same scale. </span>
<span id="cb7-43"><a href="#cb7-43" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-44"><a href="#cb7-44" aria-hidden="true" tabindex="-1"></a><span class="ot">[^SMCR]: </span>When the standardizing variance is calculated using measurements from only one condition (i.e., the pre-test in a repeated measures design), this version of $d$ corresponds to <span class="in">`measure = "SMCR"`</span>, the "standardized mean change using raw score standardization" in <span class="in">`metafor::escalc`</span>.</span>
<span id="cb7-45"><a href="#cb7-45" aria-hidden="true" tabindex="-1"></a> </span>
<span id="cb7-46"><a href="#cb7-46" aria-hidden="true" tabindex="-1"></a><span class="ot">[^SMCC]: </span>This version of $d$ corresponds to <span class="in">`measure = "SMCC"`</span>, the "standardized mean change using change score standardization" in <span class="in">`metafor::escalc`</span>.</span>
<span id="cb7-47"><a href="#cb7-47" aria-hidden="true" tabindex="-1"></a> </span>
<span id="cb7-48"><a href="#cb7-48" aria-hidden="true" tabindex="-1"></a>In the context of meta-analysis, the comparability of $d_b$ and $d_{av}$ is useful when working with a set of studies that include both types of designs. On the other hand, in meta-analyses that consist solely of within-group or repeated measures designs, comparability with $d_b$ may be less of a priority and one could consider using $d_z$ for synthesis. Purely on a pragmatic level, using $d_z$ might be attractive because the only pieces of information needed to calculate it are the total sample size and the $t$ statistic (or $p$-value) from the comparison between conditions. In contrast, calculating $d_{av}$ also requires the between-participant standard deviations from one or both groups, which primary studies might not always report.</span>
<span id="cb7-49"><a href="#cb7-49" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-50"><a href="#cb7-50" aria-hidden="true" tabindex="-1"></a>Going in to this exercise, I had the notion that measurement error would affect $d_z$ to a greater degree than $d_{av}$ because $d_z$ involves difference scores and difference scores get hit by measurement error twice. Does this intuition hold up? Let me try to formalize things a bit.</span>
<span id="cb7-51"><a href="#cb7-51" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-52"><a href="#cb7-52" aria-hidden="true" tabindex="-1"></a><span class="fu">## A within-group design with measurement error</span></span>
<span id="cb7-53"><a href="#cb7-53" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-54"><a href="#cb7-54" aria-hidden="true" tabindex="-1"></a>Suppose we have a within-group design involving two conditions, where participants are assessed on $K$ trials under each condition. Let $Y_{ijk}$ denote the outcome from trial $k$ for participant $j$ under condition $i$, for $i = 1,2$, $j = 1,...,N$, and $k = 1,...,K$. A basic model for this set-up is </span>
<span id="cb7-55"><a href="#cb7-55" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb7-56"><a href="#cb7-56" aria-hidden="true" tabindex="-1"></a>Y_{ijk} = \mu_i + u_{ij} + e_{ijk}</span>
<span id="cb7-57"><a href="#cb7-57" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb7-58"><a href="#cb7-58" aria-hidden="true" tabindex="-1"></a>where $u_{1j}$ and $u_{2j}$ are participant-specific errors in the true scores under each condition and the $e_{ijk}$'s are measurement errors. For simplicity, I will assume that: </span>
<span id="cb7-59"><a href="#cb7-59" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-60"><a href="#cb7-60" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>the true-score variance is equal across conditions, with $\Var(u_{ij}) = \sigma^2$ for $i = 1,2$, </span>
<span id="cb7-61"><a href="#cb7-61" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>the true scores are correlated across conditions, $\cor(u_{1j}, u_{2j}) = \rho$, and </span>
<span id="cb7-62"><a href="#cb7-62" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>measurement errors are uncorrelated and have homogeneous variance across conditions, with $\Var(e_{ijk}) = \psi^2$. </span>
<span id="cb7-63"><a href="#cb7-63" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-64"><a href="#cb7-64" aria-hidden="true" tabindex="-1"></a>Let $\phi = \sigma^2 / (\sigma^2 + \psi^2)$ denote the reliability (intra-class correlation) of a single observed score. Note that we can write $\psi^2$ in terms of the reliability and true-score variance as $\psi^2 = \sigma^2 \times \frac{1 - \phi}{\phi}$.</span>
<span id="cb7-65"><a href="#cb7-65" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-66"><a href="#cb7-66" aria-hidden="true" tabindex="-1"></a>Under this model, there are several different standardized mean difference metrics that we could consider. Since measurement reliability might vary from study to study, it would make sense to define the metric in terms of true score variances alone, as</span>
<span id="cb7-67"><a href="#cb7-67" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb7-68"><a href="#cb7-68" aria-hidden="true" tabindex="-1"></a>\delta_{av} = \frac{\mu_2 - \mu_1}{\sigma}</span>
<span id="cb7-69"><a href="#cb7-69" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb7-70"><a href="#cb7-70" aria-hidden="true" tabindex="-1"></a>or in terms of the variance of the difference in true scores, as </span>
<span id="cb7-71"><a href="#cb7-71" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb7-72"><a href="#cb7-72" aria-hidden="true" tabindex="-1"></a>\delta_z = \frac{\mu_2 - \mu_1}{\sigma \sqrt{2(1 - \rho)}}.</span>
<span id="cb7-73"><a href="#cb7-73" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb7-74"><a href="#cb7-74" aria-hidden="true" tabindex="-1"></a>However, we don't directly observe the true scores, and we can't estimate their variance unless we have information about score reliabilities. Thus, meta-analysts will usually need to calculate effect sizes in terms of _observed_ scores that include measurement error. </span>
<span id="cb7-75"><a href="#cb7-75" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-76"><a href="#cb7-76" aria-hidden="true" tabindex="-1"></a>Suppose that the analysis is conducted by taking the average of the $K$ trials for each participant under each condition, $\bar{Y}_{ij} = \frac{1}{K} \sum_{k=1}^K Y_{ijk}$, and conducting the analysis using these mean scores. The variance of the mean scores is </span>
<span id="cb7-77"><a href="#cb7-77" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb7-78"><a href="#cb7-78" aria-hidden="true" tabindex="-1"></a>\Var(\bar{Y}_{ij}) = \sigma^2 \left(1 + \frac{1 - \phi}{\phi K}\right), </span>
<span id="cb7-79"><a href="#cb7-79" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb7-80"><a href="#cb7-80" aria-hidden="true" tabindex="-1"></a>so we can define the observed-score standardized mean difference using raw score standardization  as</span>
<span id="cb7-81"><a href="#cb7-81" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb7-82"><a href="#cb7-82" aria-hidden="true" tabindex="-1"></a>\tilde\delta_{av} = \frac{\mu_2 - \mu_1}{\sigma \sqrt{1 + \frac{1 - \phi}{\phi K}}} = \frac{1}{\sqrt{1 + \frac{1 - \phi}{\phi K}}} \times \delta_{av}.</span>
<span id="cb7-83"><a href="#cb7-83" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb7-84"><a href="#cb7-84" aria-hidden="true" tabindex="-1"></a>From this expression, we can see that measurement error attenuates the true-score effect size because the multiplier term is always going to be less than one. </span>
<span id="cb7-85"><a href="#cb7-85" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-86"><a href="#cb7-86" aria-hidden="true" tabindex="-1"></a>Similarly, the variance of the observed difference scores is</span>
<span id="cb7-87"><a href="#cb7-87" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb7-88"><a href="#cb7-88" aria-hidden="true" tabindex="-1"></a>\Var(\bar{Y}_{2j} - \bar{Y}_{1j}) =2 \sigma^2 \left(1 - \rho + \frac{1 - \phi}{\phi K}\right), </span>
<span id="cb7-89"><a href="#cb7-89" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb7-90"><a href="#cb7-90" aria-hidden="true" tabindex="-1"></a>so we can define the observed-score standardized mean difference using change score standardization as</span>
<span id="cb7-91"><a href="#cb7-91" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb7-92"><a href="#cb7-92" aria-hidden="true" tabindex="-1"></a>\tilde\delta_z = \frac{\mu_2 - \mu_1}{\sigma \sqrt{2 \left(1 - \rho + \frac{1 - \phi}{\phi K}\right)}} = \sqrt{\frac{1 - \rho}{1 - \rho + \frac{1 - \phi}{\phi K}}} \times \delta_z.</span>
<span id="cb7-93"><a href="#cb7-93" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb7-94"><a href="#cb7-94" aria-hidden="true" tabindex="-1"></a>Again, we can see that measurement error attenuates the true-score effect size because the multiplier term is always going to be less than one. However, unlike with $d_{av}$, the attenuation factor here depends on $\rho$ in addition to $\phi$ and $K$. This additional term in the correction factor is one indication that $d_z$ might be less desirable for meta-analysis.  Correcting $d_z$ for the distortion from measurement error would require estimates of both the true-score correlation and the reliability of the scores, whereas correcting $d_{av}$ would require only the latter.</span>
<span id="cb7-95"><a href="#cb7-95" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-96"><a href="#cb7-96" aria-hidden="true" tabindex="-1"></a><span class="fu">## Meta-analysis</span></span>
<span id="cb7-97"><a href="#cb7-97" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-98"><a href="#cb7-98" aria-hidden="true" tabindex="-1"></a>The relationships between the true-score effect sizes and the analogous observed score effect sizes starts to be a problem when we consider a meta-analysis of multiple primary studies. Primary studies will often use different instruments and procedures for measuring outcomes (necessitating the use of some standardized effect size), and those differences in instruments and procedures might come along with differences in score reliability as well as variation in the number of trials collected per condition (and plenty of other things, such as sample size, participant characteristics, etc.). Procedural heterogeneity like this creates two potential challenges for meta-analysis: bias in average effect sizes and extra heterogeneity in the distribution of effect sizes. Both could make findings from a meta-analysis more difficult to interpret, although I will argue that extra heterogeneity is more concerning than bias.</span>
<span id="cb7-99"><a href="#cb7-99" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-100"><a href="#cb7-100" aria-hidden="true" tabindex="-1"></a>To illustrate, let's now imagine that the parameters of the within-group study design, $\delta_{av}$ or $\delta_z$, $\rho$, $\phi$, and $K$ are random variables, drawn from the distribution of parameters across a population of hypothetical studies. </span>
<span id="cb7-101"><a href="#cb7-101" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-102"><a href="#cb7-102" aria-hidden="true" tabindex="-1"></a><span class="fu">### A model for $\delta_{av}$</span></span>
<span id="cb7-103"><a href="#cb7-103" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-104"><a href="#cb7-104" aria-hidden="true" tabindex="-1"></a>Let's first consider $\delta_{av}$ and assume that it follows a random effects model, with</span>
<span id="cb7-105"><a href="#cb7-105" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb7-106"><a href="#cb7-106" aria-hidden="true" tabindex="-1"></a>\delta_{av} \sim N\left(\mu, \tau^2\right).</span>
<span id="cb7-107"><a href="#cb7-107" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb7-108"><a href="#cb7-108" aria-hidden="true" tabindex="-1"></a>Let's also assume that the remaining parameters $\rho$, $\phi$, and $K$ are independent of $\delta_{av}$. These parameters determine the attenuation factor $A_{av} = \left(1 + \frac{1 - \phi}{\phi K}\right)^{-1/2}$, which relates the observed-score effect size parameter to the true score effect size parameter. </span>
<span id="cb7-109"><a href="#cb7-109" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-110"><a href="#cb7-110" aria-hidden="true" tabindex="-1"></a>The bias of $\tilde\delta_{av}$ is therefore</span>
<span id="cb7-111"><a href="#cb7-111" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb7-112"><a href="#cb7-112" aria-hidden="true" tabindex="-1"></a>\E\left(\tilde\delta_{av}\right) = \E\left(A_{av}\delta_{av}\right) = \E(A_{av}) \times \mu.</span>
<span id="cb7-113"><a href="#cb7-113" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb7-114"><a href="#cb7-114" aria-hidden="true" tabindex="-1"></a>Thus, under my very simplistic assumptions, a meta-analysis of observed score Cohen's $d_{av}$ estimates will be biased (downward) for the overall average effect in the true-score distribution. </span>
<span id="cb7-115"><a href="#cb7-115" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-116"><a href="#cb7-116" aria-hidden="true" tabindex="-1"></a>You might find that the downward bias in $\tilde\delta_{av}$ is undesirable. On the other hand, bias might not be as a big a problem as it first seems. If all of the observed-score effect sizes are biased to a degree that is unrelated to the true effects, then bias just stretches or compresses the scale of measurement, but doesn't necessarily lead to interpretive problems. Imagine you have a ruler that is half an inch too short, and you're trying to compare the heights of different objects. As long as you use the same ruler, then you will still be able to determine which objects are bigger and which are smaller, and by how much, even if the measurements are off in an absolute sense.</span>
<span id="cb7-117"><a href="#cb7-117" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-118"><a href="#cb7-118" aria-hidden="true" tabindex="-1"></a>Apart from bias, however, variability in $A_{av}$ will also induce _additional heterogeneity_ in the distribution of observed score effect sizes. This is a clear problem because it creates additional uncertainty, making it harder to draw inferences about the distribution of effects, predict new effect sizes, or identify substantively interesting moderators. To measure this additional heterogeneity and keep its consequences separate from the consequences for bias, I will look at the coefficient of variation in $\tilde\delta_{av}$. Under the assumption that $A_{av}$ is independent of $\delta_{av}$, </span>
<span id="cb7-119"><a href="#cb7-119" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb7-120"><a href="#cb7-120" aria-hidden="true" tabindex="-1"></a>\frac{\sqrt{\Var(\tilde\delta_{av})}}{\E(\tilde\delta_{av})} = \frac{\sqrt{(\mu^2 + 2 \tau^2) \Var(A_{av}) + \tau^2 \left<span class="co">[</span><span class="ot">\E(A_{av})\right</span><span class="co">]</span>^2}}{\E\left(A_{av} \right) \times \mu } = \sqrt{\left<span class="co">[</span><span class="ot">\left(1 + \frac{2 \tau^2}{\mu^2}\right) \frac{\Var(A_{av})}{\left[\E\left(A_{av} \right)\right]^2} + \frac{\tau^2}{\mu^2}\right</span><span class="co">]</span>}.</span>
<span id="cb7-121"><a href="#cb7-121" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb7-122"><a href="#cb7-122" aria-hidden="true" tabindex="-1"></a>Thus, the coefficient of variation for $\tilde\delta_{av}$ is amplified by a factor that depends on the squared coefficient of variation of $A_{av}$. Under the same model, </span>
<span id="cb7-123"><a href="#cb7-123" aria-hidden="true" tabindex="-1"></a>$\tilde\delta_z = A_z \times \delta_{av}$, where $A_z = \left(1 - \rho + \frac{1 - \phi}{\phi K}\right)^{-1/2}$, and so $\E(\tilde\delta_z) = \E(A_z) \times \mu$ and </span>
<span id="cb7-124"><a href="#cb7-124" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb7-125"><a href="#cb7-125" aria-hidden="true" tabindex="-1"></a>\frac{\sqrt{\Var(\tilde\delta_z)}}{\E(\tilde\delta_z)} = \sqrt{\left<span class="co">[</span><span class="ot">\left(1 + \frac{2 \tau^2}{\mu^2}\right) \frac{\Var(A_z)}{\left[\E\left(A_z \right)\right]^2} + \frac{\tau^2}{\mu^2}\right</span><span class="co">]</span>}.</span>
<span id="cb7-126"><a href="#cb7-126" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb7-127"><a href="#cb7-127" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-128"><a href="#cb7-128" aria-hidden="true" tabindex="-1"></a>To see what's going on here, let's consider some specific distributions for these measurement factors. First, let's assume: </span>
<span id="cb7-129"><a href="#cb7-129" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-130"><a href="#cb7-130" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>$\rho \sim B(14, 6)$, so that $\E(\rho) = .7$ and $\Var(\rho) = <span class="in">`{r} round(sqrt(14 * 6 / (20^2 * 21)), 3)`</span>^2$;</span>
<span id="cb7-131"><a href="#cb7-131" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>$\phi \sim B(3, 5)$, so that $\E(\rho) = .<span class="in">`{r} 3 / 8`</span>$ and $\Var(\rho) = <span class="in">`{r} round(sqrt(3 * 5 / (8^2 * 9)), 3)`</span>^2$; </span>
<span id="cb7-132"><a href="#cb7-132" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>$K \sim 1 + Pois(9)$, so $\E(K) = 10$ and $\Var(K) = 3^2$; and</span>
<span id="cb7-133"><a href="#cb7-133" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>$\rho$, $\phi$, and $K$ are mutually independent and independent of $\delta_{av}$. </span>
<span id="cb7-134"><a href="#cb7-134" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-135"><a href="#cb7-135" aria-hidden="true" tabindex="-1"></a>Below I simulate 50000 samples from these distributions and calculate $A_{av}$ and $A_z$. </span>
<span id="cb7-136"><a href="#cb7-136" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-137"><a href="#cb7-137" aria-hidden="true" tabindex="-1"></a><span class="in">```{r, fig.width = 10, fig.height = 3}</span></span>
<span id="cb7-138"><a href="#cb7-138" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-139"><a href="#cb7-139" aria-hidden="true" tabindex="-1"></a><span class="in">R &lt;- 50000</span></span>
<span id="cb7-140"><a href="#cb7-140" aria-hidden="true" tabindex="-1"></a><span class="in">rho &lt;- rbeta(R, 14, 6)</span></span>
<span id="cb7-141"><a href="#cb7-141" aria-hidden="true" tabindex="-1"></a><span class="in">phi &lt;- rbeta(R, 3, 5)</span></span>
<span id="cb7-142"><a href="#cb7-142" aria-hidden="true" tabindex="-1"></a><span class="in">K &lt;- 1 + rpois(R, 9)</span></span>
<span id="cb7-143"><a href="#cb7-143" aria-hidden="true" tabindex="-1"></a><span class="in">A_av &lt;- 1 / sqrt(1 + (1 - phi) / (phi * K))</span></span>
<span id="cb7-144"><a href="#cb7-144" aria-hidden="true" tabindex="-1"></a><span class="in">A_z &lt;- 1 / sqrt(2 * (1 - rho + (1 - phi) / (phi * K)))</span></span>
<span id="cb7-145"><a href="#cb7-145" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-146"><a href="#cb7-146" aria-hidden="true" tabindex="-1"></a><span class="in">library(ggplot2)</span></span>
<span id="cb7-147"><a href="#cb7-147" aria-hidden="true" tabindex="-1"></a><span class="in">library(patchwork)</span></span>
<span id="cb7-148"><a href="#cb7-148" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-149"><a href="#cb7-149" aria-hidden="true" tabindex="-1"></a><span class="in">density_plot &lt;- function(x, lab, col, limits) {</span></span>
<span id="cb7-150"><a href="#cb7-150" aria-hidden="true" tabindex="-1"></a><span class="in">  ggplot(data.frame(x), aes(x)) + </span></span>
<span id="cb7-151"><a href="#cb7-151" aria-hidden="true" tabindex="-1"></a><span class="in">    xlim(limits) +</span></span>
<span id="cb7-152"><a href="#cb7-152" aria-hidden="true" tabindex="-1"></a><span class="in">    geom_density(alpha = 0.4, fill = col) + </span></span>
<span id="cb7-153"><a href="#cb7-153" aria-hidden="true" tabindex="-1"></a><span class="in">    scale_y_continuous(labels = NULL) +</span></span>
<span id="cb7-154"><a href="#cb7-154" aria-hidden="true" tabindex="-1"></a><span class="in">    theme_minimal() + </span></span>
<span id="cb7-155"><a href="#cb7-155" aria-hidden="true" tabindex="-1"></a><span class="in">    labs(x = lab, y = NULL)</span></span>
<span id="cb7-156"><a href="#cb7-156" aria-hidden="true" tabindex="-1"></a><span class="in">}</span></span>
<span id="cb7-157"><a href="#cb7-157" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-158"><a href="#cb7-158" aria-hidden="true" tabindex="-1"></a><span class="in">p_A_av &lt;- density_plot(A_av, expression(A[av]),"blue", c(0.2,1))</span></span>
<span id="cb7-159"><a href="#cb7-159" aria-hidden="true" tabindex="-1"></a><span class="in">p_A_z &lt;- density_plot(A_z, expression(A[z]), "purple", c(0, 3))</span></span>
<span id="cb7-160"><a href="#cb7-160" aria-hidden="true" tabindex="-1"></a><span class="in">p_A_av + p_A_z</span></span>
<span id="cb7-161"><a href="#cb7-161" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb7-162"><a href="#cb7-162" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-163"><a href="#cb7-163" aria-hidden="true" tabindex="-1"></a>The distribution of $A_{av}$ is mostly concentrated around the mean of $E(A_{av}) = <span class="in">`{r} round(mean(A_av), 3)`</span>$, with a coefficient of variation of $CV(A_{av}) = <span class="in">`{r} round(CV_A_av &lt;- sd(A_av) / mean(A_av), 3)`</span>$. In contrast, the distribution of $A_{av}$ has a mean very close to one, $E(A_z) = <span class="in">`{r} round(mean(A_z), 3)`</span>$ but a coefficient of variation of $CV(A_z) = <span class="in">`{r} round(CV_A_z &lt;- sd(A_z) / mean(A_z), 3)`</span>$, about <span class="in">`{r} round(CV_A_z / CV_A_av, 1)`</span> times larger. Thus, variation in measurement procedure induces more extra heterogeneity into the distribution of $\tilde\delta_z$ than into $\tilde\delta_{av}$. </span>
<span id="cb7-164"><a href="#cb7-164" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-165"><a href="#cb7-165" aria-hidden="true" tabindex="-1"></a>Now, one potential objection to this hypothetical scenario is that researchers do not choose the number of trials at random, without consideration for the other parameters of the study design. A more realistic assumption might be that researchers choose $K$ to ensure they achieve at least some threshold level of reliability for the observed scores. The reliability of the observed scores is $A_{av}^2$, so ensuring some threshold of reliability is equivalent to ensuring the square root of the threshold for $A_{av}$. Let's suppose that researchers always ensure $A_{av} \geq 0.8$ so that reliability is always at least $0.64$. This leads to the following distributions for $A_{av}$ and $A_z$:</span>
<span id="cb7-166"><a href="#cb7-166" aria-hidden="true" tabindex="-1"></a><span class="in">```{r, fig.width = 10, fig.height = 3}</span></span>
<span id="cb7-167"><a href="#cb7-167" aria-hidden="true" tabindex="-1"></a><span class="in">A_av_trunc &lt;- A_av[A_av &gt;= 0.8]</span></span>
<span id="cb7-168"><a href="#cb7-168" aria-hidden="true" tabindex="-1"></a><span class="in">A_z_trunc &lt;- A_z[A_av &gt;= 0.8]</span></span>
<span id="cb7-169"><a href="#cb7-169" aria-hidden="true" tabindex="-1"></a><span class="in">p_A_av_trunc &lt;- density_plot(A_av_trunc, expression(A[av]),"blue", c(0.2, 1))</span></span>
<span id="cb7-170"><a href="#cb7-170" aria-hidden="true" tabindex="-1"></a><span class="in">p_A_z_trunc &lt;- density_plot(A_z_trunc, expression(A[z]), "purple", c(0, 3))</span></span>
<span id="cb7-171"><a href="#cb7-171" aria-hidden="true" tabindex="-1"></a><span class="in">p_A_av_trunc + p_A_z_trunc</span></span>
<span id="cb7-172"><a href="#cb7-172" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb7-173"><a href="#cb7-173" aria-hidden="true" tabindex="-1"></a>The distribution of $A_{av}$ loses its left tail, so that its mean is $E(A_{av}|A_{av} \geq 0.8) = <span class="in">`{r} round(mean(A_av_trunc), 3)`</span>$ and its coefficient of variation is reduced to $CV(A_{av} | A_{av} \geq 0.8) = <span class="in">`{r} round(CV_A_av_trunc &lt;- sd(A_av_trunc) / mean(A_av_trunc), 3)`</span>$. The distribution of $A_{av}$ now has a mean of $E(A_z | A_{av} \geq 0.8) = <span class="in">`{r} round(mean(A_z_trunc), 3)`</span>$ and a coefficient of variation of $CV(A_z | A_{av} \geq 0.8) = <span class="in">`{r} round(CV_A_z_trunc &lt;- sd(A_z_trunc) / mean(A_z_trunc), 3)`</span>$, about <span class="in">`{r} round(CV_A_z_trunc / CV_A_av_trunc, 1)`</span> times larger than the squared coefficient of variation of $A_{av}$. </span>
<span id="cb7-174"><a href="#cb7-174" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-175"><a href="#cb7-175" aria-hidden="true" tabindex="-1"></a>Under both of these scenarios, the observed-score $\tilde\delta_z$ is substantially more sensitive to procedural heterogeneity than is $\tilde\delta_{av}$. Based on this model and hypothetical example, it seems clear $\tilde\delta_{av}$ should be preferred over $\tilde\delta_z$ as a metric for meta-analysis. However, these relationships are predicated on a certain model for the study-specific parameters. One might object to this model because there's a sense that we have assumed that $\delta_{av}$ is the right answer. After all, the underlying effect size model is specified in terms of $\delta_{av}$, and the design parameters---including $\rho$ in particular---are treated as noise, uncorrelated with $\delta_{av}$. </span>
<span id="cb7-176"><a href="#cb7-176" aria-hidden="true" tabindex="-1"></a>What happens to the observed-score metrics $\tilde\delta_z$ and $\tilde\delta_{av}$ if we instead start with a model specified in terms of $\delta_z$? </span>
<span id="cb7-177"><a href="#cb7-177" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-178"><a href="#cb7-178" aria-hidden="true" tabindex="-1"></a><span class="fu">### A model for $\delta_z$</span></span>
<span id="cb7-179"><a href="#cb7-179" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-180"><a href="#cb7-180" aria-hidden="true" tabindex="-1"></a>Let's now see how this works if we treat $\delta_z$ as the correct metric and assume that the design parameters are independent of $\delta_z$. Assume that </span>
<span id="cb7-181"><a href="#cb7-181" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb7-182"><a href="#cb7-182" aria-hidden="true" tabindex="-1"></a>\delta_z \sim N(\alpha, \omega^2)</span>
<span id="cb7-183"><a href="#cb7-183" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb7-184"><a href="#cb7-184" aria-hidden="true" tabindex="-1"></a>and that the remaining parameters $\rho$, $\phi$, and $K$ are independent of $\delta_{av}$. Then the observed-score standardized mean difference using change score standardization can  be written as $\tilde\delta_z = B_z \times \delta_z$, where</span>
<span id="cb7-185"><a href="#cb7-185" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb7-186"><a href="#cb7-186" aria-hidden="true" tabindex="-1"></a>B_z = \sqrt{\frac{1 - \rho}{1 - \rho + \frac{1 - \phi}{\phi K}}}</span>
<span id="cb7-187"><a href="#cb7-187" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb7-188"><a href="#cb7-188" aria-hidden="true" tabindex="-1"></a>and the observed-score standardized mean difference using raw score standardization can be written as $\tilde\delta_{av} = B_{av} \times \delta_z$, where</span>
<span id="cb7-189"><a href="#cb7-189" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb7-190"><a href="#cb7-190" aria-hidden="true" tabindex="-1"></a>B_{av} = \frac{\sqrt{2}(1 - \rho)}{\sqrt{1 - \rho + \frac{1 - \phi}{\phi K}}}.</span>
<span id="cb7-191"><a href="#cb7-191" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb7-192"><a href="#cb7-192" aria-hidden="true" tabindex="-1"></a>The plots below show the distribution of $B_z$ and $B_{av}$ under the same scenarios considered above. First, the scenario where observed-score reliability is not controlled:</span>
<span id="cb7-193"><a href="#cb7-193" aria-hidden="true" tabindex="-1"></a><span class="in">```{r, fig.width = 10, fig.height = 3}</span></span>
<span id="cb7-194"><a href="#cb7-194" aria-hidden="true" tabindex="-1"></a><span class="in">B_z &lt;- sqrt((1 - rho) / (1 - rho + (1 - phi) / (phi * K)))</span></span>
<span id="cb7-195"><a href="#cb7-195" aria-hidden="true" tabindex="-1"></a><span class="in">B_av &lt;- sqrt(2) * (1 - rho) / sqrt(1 - rho + (1 - phi) / (phi * K))</span></span>
<span id="cb7-196"><a href="#cb7-196" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-197"><a href="#cb7-197" aria-hidden="true" tabindex="-1"></a><span class="in">p_B_av &lt;- density_plot(B_av, expression(B[av]),"green", c(0,1.5))</span></span>
<span id="cb7-198"><a href="#cb7-198" aria-hidden="true" tabindex="-1"></a><span class="in">p_B_z &lt;- density_plot(B_z, expression(B[z]), "yellow", c(0, 1))</span></span>
<span id="cb7-199"><a href="#cb7-199" aria-hidden="true" tabindex="-1"></a><span class="in">p_B_av + p_B_z</span></span>
<span id="cb7-200"><a href="#cb7-200" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb7-201"><a href="#cb7-201" aria-hidden="true" tabindex="-1"></a>Second, the scenario where observed-score reliability is at least 0.64:</span>
<span id="cb7-202"><a href="#cb7-202" aria-hidden="true" tabindex="-1"></a><span class="in">```{r, fig.width = 10, fig.height = 3}</span></span>
<span id="cb7-203"><a href="#cb7-203" aria-hidden="true" tabindex="-1"></a><span class="in">B_z_trunc &lt;- B_z[A_av &gt;= 0.8]</span></span>
<span id="cb7-204"><a href="#cb7-204" aria-hidden="true" tabindex="-1"></a><span class="in">B_av_trunc &lt;- B_av[A_av &gt;= 0.8]</span></span>
<span id="cb7-205"><a href="#cb7-205" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-206"><a href="#cb7-206" aria-hidden="true" tabindex="-1"></a><span class="in">p_B_av_trunc &lt;- density_plot(B_av, expression(B[av]),"green", c(0,1.5))</span></span>
<span id="cb7-207"><a href="#cb7-207" aria-hidden="true" tabindex="-1"></a><span class="in">p_B_z_trunc &lt;- density_plot(B_z, expression(B[z]), "yellow", c(0, 1))</span></span>
<span id="cb7-208"><a href="#cb7-208" aria-hidden="true" tabindex="-1"></a><span class="in">p_B_av_trunc + p_B_z_trunc</span></span>
<span id="cb7-209"><a href="#cb7-209" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb7-210"><a href="#cb7-210" aria-hidden="true" tabindex="-1"></a>The table below reports the coefficients of variation for each of the multiplicative factors I have considered.</span>
<span id="cb7-213"><a href="#cb7-213" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb7-214"><a href="#cb7-214" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(dplyr)</span>
<span id="cb7-215"><a href="#cb7-215" aria-hidden="true" tabindex="-1"></a>dat <span class="ot">&lt;-</span> <span class="fu">tibble</span>(A_av, A_z, B_av, B_z)</span>
<span id="cb7-216"><a href="#cb7-216" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-217"><a href="#cb7-217" aria-hidden="true" tabindex="-1"></a>random_rel <span class="ot">&lt;-</span> </span>
<span id="cb7-218"><a href="#cb7-218" aria-hidden="true" tabindex="-1"></a>  dat <span class="sc">|&gt;</span></span>
<span id="cb7-219"><a href="#cb7-219" aria-hidden="true" tabindex="-1"></a>  <span class="fu">summarise</span>(</span>
<span id="cb7-220"><a href="#cb7-220" aria-hidden="true" tabindex="-1"></a>    <span class="fu">across</span>(<span class="fu">everything</span>(), <span class="sc">~</span> <span class="fu">sd</span>(.) <span class="sc">/</span> <span class="fu">mean</span>(.)),</span>
<span id="cb7-221"><a href="#cb7-221" aria-hidden="true" tabindex="-1"></a>    <span class="at">reliability =</span> <span class="st">"random"</span></span>
<span id="cb7-222"><a href="#cb7-222" aria-hidden="true" tabindex="-1"></a>  )</span>
<span id="cb7-223"><a href="#cb7-223" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-224"><a href="#cb7-224" aria-hidden="true" tabindex="-1"></a>controlled_rel <span class="ot">&lt;-</span></span>
<span id="cb7-225"><a href="#cb7-225" aria-hidden="true" tabindex="-1"></a>  dat <span class="sc">|&gt;</span></span>
<span id="cb7-226"><a href="#cb7-226" aria-hidden="true" tabindex="-1"></a>  <span class="fu">filter</span>(A_av <span class="sc">&gt;=</span> <span class="fl">0.8</span>) <span class="sc">|&gt;</span></span>
<span id="cb7-227"><a href="#cb7-227" aria-hidden="true" tabindex="-1"></a>  <span class="fu">summarise</span>(</span>
<span id="cb7-228"><a href="#cb7-228" aria-hidden="true" tabindex="-1"></a>    <span class="fu">across</span>(<span class="fu">everything</span>(), <span class="sc">~</span> <span class="fu">sd</span>(.) <span class="sc">/</span> <span class="fu">mean</span>(.)),</span>
<span id="cb7-229"><a href="#cb7-229" aria-hidden="true" tabindex="-1"></a>    <span class="at">reliability =</span> <span class="st">"at least 0.64"</span></span>
<span id="cb7-230"><a href="#cb7-230" aria-hidden="true" tabindex="-1"></a>  )</span>
<span id="cb7-231"><a href="#cb7-231" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-232"><a href="#cb7-232" aria-hidden="true" tabindex="-1"></a>CVs <span class="ot">&lt;-</span> </span>
<span id="cb7-233"><a href="#cb7-233" aria-hidden="true" tabindex="-1"></a>  <span class="fu">bind_rows</span>(random_rel, controlled_rel) <span class="sc">|&gt;</span></span>
<span id="cb7-234"><a href="#cb7-234" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(</span>
<span id="cb7-235"><a href="#cb7-235" aria-hidden="true" tabindex="-1"></a>    <span class="at">A_ratio =</span> A_z <span class="sc">/</span> A_av,</span>
<span id="cb7-236"><a href="#cb7-236" aria-hidden="true" tabindex="-1"></a>    <span class="at">B_ratio =</span> B_z <span class="sc">/</span> B_av</span>
<span id="cb7-237"><a href="#cb7-237" aria-hidden="true" tabindex="-1"></a>  ) <span class="sc">|&gt;</span></span>
<span id="cb7-238"><a href="#cb7-238" aria-hidden="true" tabindex="-1"></a>  <span class="fu">select</span>(reliability, A_av, A_z, A_ratio, B_av, B_z, B_ratio)</span>
<span id="cb7-239"><a href="#cb7-239" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-240"><a href="#cb7-240" aria-hidden="true" tabindex="-1"></a>knitr<span class="sc">::</span><span class="fu">kable</span>(</span>
<span id="cb7-241"><a href="#cb7-241" aria-hidden="true" tabindex="-1"></a>  CVs, </span>
<span id="cb7-242"><a href="#cb7-242" aria-hidden="true" tabindex="-1"></a>  <span class="at">digits =</span> <span class="fu">c</span>(<span class="dv">0</span>,<span class="dv">3</span>,<span class="dv">3</span>,<span class="dv">1</span>,<span class="dv">3</span>,<span class="dv">3</span>,<span class="dv">2</span>),</span>
<span id="cb7-243"><a href="#cb7-243" aria-hidden="true" tabindex="-1"></a>  <span class="at">caption =</span> <span class="st">"Coefficients of variation"</span>,</span>
<span id="cb7-244"><a href="#cb7-244" aria-hidden="true" tabindex="-1"></a>  <span class="at">col.names =</span> <span class="fu">c</span>(<span class="st">"Reliability"</span>,<span class="st">"$A_{av}$"</span>,<span class="st">"$A_z$"</span>,<span class="st">"$A_z /A_{av}$"</span>, <span class="st">"$B_{av}$"</span>,<span class="st">"$B_z$"</span>, <span class="st">"$B_{z} / B_{av}$"</span>),</span>
<span id="cb7-245"><a href="#cb7-245" aria-hidden="true" tabindex="-1"></a>  <span class="at">escape =</span> <span class="cn">TRUE</span></span>
<span id="cb7-246"><a href="#cb7-246" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb7-247"><a href="#cb7-247" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb7-248"><a href="#cb7-248" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-249"><a href="#cb7-249" aria-hidden="true" tabindex="-1"></a>The tables are now more or less turned. Under both reliability scenarios, $B_z$ has a lower coefficient of variation than $B_{av}$, indicating that $\tilde\delta_z$ is less affected by procedural heterogeneity than is $\tilde\delta_{av}$. However, $\tilde\delta_z$ is still affected in absolute terms, considering that the coefficient of variation for $B_z$ is about <span class="in">`{r} round(100 * controlled_rel$B_z / controlled_rel$A_z)`</span>% of the coefficient of variation for $A_z$. Of course, $\tilde\delta_{av}$ is quite strongly affected under this model, with a coefficient of variation of <span class="in">`{r} round(controlled_rel$B_av, 3)`</span>.</span>
<span id="cb7-250"><a href="#cb7-250" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-251"><a href="#cb7-251" aria-hidden="true" tabindex="-1"></a><span class="fu">### Consequences for heterogeneity</span></span>
<span id="cb7-252"><a href="#cb7-252" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-253"><a href="#cb7-253" aria-hidden="true" tabindex="-1"></a>To make these results a bit more concrete, it's useful to consider think in terms of heterogeneity of the observed score effect sizes. The figure below plots the CVs of observed-score effect size parameters as a function of the CVs of the true effect size distribution. </span>
<span id="cb7-254"><a href="#cb7-254" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-255"><a href="#cb7-255" aria-hidden="true" tabindex="-1"></a><span class="in">```{r, fig.width = 7, fig.height = 3.5}</span></span>
<span id="cb7-256"><a href="#cb7-256" aria-hidden="true" tabindex="-1"></a><span class="in">library(tidyr)</span></span>
<span id="cb7-257"><a href="#cb7-257" aria-hidden="true" tabindex="-1"></a><span class="in">library(stringr)</span></span>
<span id="cb7-258"><a href="#cb7-258" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-259"><a href="#cb7-259" aria-hidden="true" tabindex="-1"></a><span class="in">CV_obs &lt;- </span></span>
<span id="cb7-260"><a href="#cb7-260" aria-hidden="true" tabindex="-1"></a><span class="in">  CVs |&gt;</span></span>
<span id="cb7-261"><a href="#cb7-261" aria-hidden="true" tabindex="-1"></a><span class="in">  select(-A_ratio, -B_ratio) |&gt;</span></span>
<span id="cb7-262"><a href="#cb7-262" aria-hidden="true" tabindex="-1"></a><span class="in">  pivot_longer(-reliability, names_to = "metric", values_to = "het") |&gt;</span></span>
<span id="cb7-263"><a href="#cb7-263" aria-hidden="true" tabindex="-1"></a><span class="in">  expand_grid(tau_mu = seq(0,1,0.02)) |&gt;</span></span>
<span id="cb7-264"><a href="#cb7-264" aria-hidden="true" tabindex="-1"></a><span class="in">  mutate(</span></span>
<span id="cb7-265"><a href="#cb7-265" aria-hidden="true" tabindex="-1"></a><span class="in">    reliability = recode(reliability, 'at least 0.64' = "Reliability of at least 0.64", 'random' = "Random reliability"),</span></span>
<span id="cb7-266"><a href="#cb7-266" aria-hidden="true" tabindex="-1"></a><span class="in">    metric = paste0(str_replace(metric, "\\_","\\["),"]"),</span></span>
<span id="cb7-267"><a href="#cb7-267" aria-hidden="true" tabindex="-1"></a><span class="in">    CV = sqrt((1 + 2 * tau_mu^2) * het^2 + tau_mu^2)</span></span>
<span id="cb7-268"><a href="#cb7-268" aria-hidden="true" tabindex="-1"></a><span class="in">  )</span></span>
<span id="cb7-269"><a href="#cb7-269" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-270"><a href="#cb7-270" aria-hidden="true" tabindex="-1"></a><span class="in">CV_ex &lt;- CV_obs |&gt;</span></span>
<span id="cb7-271"><a href="#cb7-271" aria-hidden="true" tabindex="-1"></a><span class="in">  filter(tau_mu == 0.5, reliability == "Reliability of at least 0.64") |&gt;</span></span>
<span id="cb7-272"><a href="#cb7-272" aria-hidden="true" tabindex="-1"></a><span class="in">  select(metric, CV) |&gt;</span></span>
<span id="cb7-273"><a href="#cb7-273" aria-hidden="true" tabindex="-1"></a><span class="in">  mutate(</span></span>
<span id="cb7-274"><a href="#cb7-274" aria-hidden="true" tabindex="-1"></a><span class="in">    CV = round(CV, 3),</span></span>
<span id="cb7-275"><a href="#cb7-275" aria-hidden="true" tabindex="-1"></a><span class="in">    metric = str_replace(str_sub(metric, 1, -2), "\\[", "_")</span></span>
<span id="cb7-276"><a href="#cb7-276" aria-hidden="true" tabindex="-1"></a><span class="in">  ) |&gt;</span></span>
<span id="cb7-277"><a href="#cb7-277" aria-hidden="true" tabindex="-1"></a><span class="in">  pivot_wider(names_from = metric, values_from = CV)</span></span>
<span id="cb7-278"><a href="#cb7-278" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-279"><a href="#cb7-279" aria-hidden="true" tabindex="-1"></a><span class="in">CV_obs_labs &lt;-</span></span>
<span id="cb7-280"><a href="#cb7-280" aria-hidden="true" tabindex="-1"></a><span class="in">  CV_obs %&gt;%</span></span>
<span id="cb7-281"><a href="#cb7-281" aria-hidden="true" tabindex="-1"></a><span class="in">  filter(tau_mu == 0)</span></span>
<span id="cb7-282"><a href="#cb7-282" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-283"><a href="#cb7-283" aria-hidden="true" tabindex="-1"></a><span class="in">ggplot(CV_obs, aes(tau_mu, CV, color = metric)) + </span></span>
<span id="cb7-284"><a href="#cb7-284" aria-hidden="true" tabindex="-1"></a><span class="in">  facet_wrap(vars(reliability)) +</span></span>
<span id="cb7-285"><a href="#cb7-285" aria-hidden="true" tabindex="-1"></a><span class="in">  scale_x_continuous(expand = expansion(c(0.1,0),0), breaks = seq(0.2, 1, 0.2)) + </span></span>
<span id="cb7-286"><a href="#cb7-286" aria-hidden="true" tabindex="-1"></a><span class="in">  scale_y_continuous(expand = expansion(0,0), breaks = seq(0, 1, 0.2)) + </span></span>
<span id="cb7-287"><a href="#cb7-287" aria-hidden="true" tabindex="-1"></a><span class="in">  geom_vline(xintercept = 0) + </span></span>
<span id="cb7-288"><a href="#cb7-288" aria-hidden="true" tabindex="-1"></a><span class="in">  geom_hline(yintercept = 0) + </span></span>
<span id="cb7-289"><a href="#cb7-289" aria-hidden="true" tabindex="-1"></a><span class="in">  geom_abline(slope = 1, linetype = "dashed") + </span></span>
<span id="cb7-290"><a href="#cb7-290" aria-hidden="true" tabindex="-1"></a><span class="in">  geom_text(</span></span>
<span id="cb7-291"><a href="#cb7-291" aria-hidden="true" tabindex="-1"></a><span class="in">    data = CV_obs_labs, </span></span>
<span id="cb7-292"><a href="#cb7-292" aria-hidden="true" tabindex="-1"></a><span class="in">    aes(x = tau_mu, y = CV, color = metric, label = metric),</span></span>
<span id="cb7-293"><a href="#cb7-293" aria-hidden="true" tabindex="-1"></a><span class="in">    nudge_x = -0.05,</span></span>
<span id="cb7-294"><a href="#cb7-294" aria-hidden="true" tabindex="-1"></a><span class="in">    parse = TRUE</span></span>
<span id="cb7-295"><a href="#cb7-295" aria-hidden="true" tabindex="-1"></a><span class="in">  ) + </span></span>
<span id="cb7-296"><a href="#cb7-296" aria-hidden="true" tabindex="-1"></a><span class="in">  geom_line() + </span></span>
<span id="cb7-297"><a href="#cb7-297" aria-hidden="true" tabindex="-1"></a><span class="in">  theme_minimal() + </span></span>
<span id="cb7-298"><a href="#cb7-298" aria-hidden="true" tabindex="-1"></a><span class="in">  labs(x = expression(tau / mu), y = "Coefficient of variation for observed score ES") + </span></span>
<span id="cb7-299"><a href="#cb7-299" aria-hidden="true" tabindex="-1"></a><span class="in">  theme(legend.position = "none")</span></span>
<span id="cb7-300"><a href="#cb7-300" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-301"><a href="#cb7-301" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb7-302"><a href="#cb7-302" aria-hidden="true" tabindex="-1"></a>Consider, for instance, a scenario where observed-score reliability is always at least 0.64 and $\tau / \mu = 0.5$, which would be the case if effect sizes are normally distributed and about 97% of effect sizes are positive. Under the effect size model based on $\delta_{av}$, the observed-score $\tilde\delta_{av}$ is hardly affected by measurement heterogeneity at all, with a CV of <span class="in">`{r} CV_ex$A_av`</span> but the CV of the observed-score $\tilde\delta_z$ is <span class="in">`{r} CV_ex$A_z`</span>. Under the effect size model based on $\delta_{av}$, the observed-score $\tilde\delta_{av}$ is strongly affected by measurement heterogeneity, with a CV of <span class="in">`{r} CV_ex$B_av`</span>; in comparison, the CV of the observed-score $\tilde\delta_z$ of <span class="in">`{r} CV_ex$B_z`</span>. Under both models, these increases in CV are effectively constant for larger values of $\tau / \mu$. </span>
<span id="cb7-303"><a href="#cb7-303" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-304"><a href="#cb7-304" aria-hidden="true" tabindex="-1"></a><span class="fu">## So what's your point?</span></span>
<span id="cb7-305"><a href="#cb7-305" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-306"><a href="#cb7-306" aria-hidden="true" tabindex="-1"></a>Unfortunately, this particular trip down a rabbit hole doesn't seem to yield many clear take-aways. For the scenario that I looked at here, the preferred choice of effect size metric is apparently driven by what assumptions we find more plausible. If we think the more plausible model is the one in which $\rho$ is independent of $\delta_{av}$, then $\tilde\delta_{av}$ is less strongly affected by measurement variation and therefore preferred. Further, the attenuation in $\tilde\delta_{av}$ depends only on $\phi$, which might mean that a correction for attenuation is more feasible. However, if we think the more plausible model is the one in which $\rho$ is independent of $\delta_z$, then $\tilde\delta_z$ is less strongly affected by measurement variation and therefore preferred.<span class="ot">[^other-models]</span> In the latter model, one caveat is that the measurement error attenuation in $\tilde\delta_z$ is still a complicated mess, depending on both the true score correlation $\rho$ and the reliability $\phi$. This would make it pretty hard to implement some sort of correction for attenuation.</span>
<span id="cb7-307"><a href="#cb7-307" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-308"><a href="#cb7-308" aria-hidden="true" tabindex="-1"></a><span class="ot">[^other-models]: </span>And of course, these two models are not the only alternatives---one could look at intermediate scenarios where $\rho$ and $\phi$ are more or less strongly correlated with the true score effect sizes. </span>
<span id="cb7-309"><a href="#cb7-309" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-310"><a href="#cb7-310" aria-hidden="true" tabindex="-1"></a>So, how could one decide which meta-analytic model is more plausible in a given application? On a conceptual level, I would argue that the model for $\delta_{av}$ would tend to be more plausible in meta-analyses where there is more operational variation in the interventions examined. I would venture that syntheses that include many different versions of an intervention would tend to have a wider range of correlations between true scores (i.e., more heterogeneous correlations between potential outcomes), even holding the outcome measurement procedures constant. This doesn't necessarily justify the assumption that $\rho$ is independent of $\delta_{av}$, but it does make it seem rather implausible that $\delta_z$ would be independent of $\rho$. </span>
<span id="cb7-311"><a href="#cb7-311" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-312"><a href="#cb7-312" aria-hidden="true" tabindex="-1"></a>On a more practical level, it seems like there are a few empirical things that a meta-analyst could do to inform a choice between a model for $\delta_{av}$ and one for $\delta_z$. Pragmatically, one could calculate both effect size metrics and just see which one exhibits more heterogeneity. All else equal, it seems reasonable to prefer the metric that has less heterogeneity. One could also try to gather data on the correlation between observed scores, on the reliability of the observed scores, and on the number of trials used in each study. With this information, one could construct measurement-related predictors and use them in a meta-regression to explain variation in the observed effect size estimates. Alternately, one could use the formulas given above to implement attenuation corrections for the effect sizes and see if this leads to reduced heterogeneity. How well would any of these approaches actually work? Answering that question would take some further, more careful and systematic investigation.</span>
</code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div></div></div></div></div>
</div> <!-- /content -->




<script src="../../site_libs/quarto-html/zenscroll-min.js"></script>
</body></html>