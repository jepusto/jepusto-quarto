<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.4.552">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="James E. Pustejovsky">
<meta name="dcterms.date" content="2016-06-03">

<title>James E. Pustejovsky - Alternative formulas for the standardized mean difference</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<style>html{ scroll-behavior: smooth; }</style>

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<meta property="og:title" content="James E. Pustejovsky - Alternative formulas for the standardized mean difference">
<meta property="og:description" content="Education Statistics and Meta-Analysis">
<meta property="og:site_name" content="James E. Pustejovsky">
<meta name="twitter:title" content="James E. Pustejovsky - Alternative formulas for the standardized mean difference">
<meta name="twitter:description" content="Education Statistics and Meta-Analysis">
<meta name="twitter:card" content="summary">
<meta name="citation_title" content="Alternative formulas for the standardized mean difference">
<meta name="citation_author" content="James E. Pustejovsky">
<meta name="citation_publication_date" content="2016-06-03">
<meta name="citation_cover_date" content="2016-06-03">
<meta name="citation_year" content="2016">
<meta name="citation_online_date" content="2016-06-03">
<meta name="citation_fulltext_html_url" content="https://mellifluous-buttercream-e2edd2.netlify.app/posts/alternative-formulas-for-the-SMD">
<meta name="citation_language" content="en">
</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top quarto-banner">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">James E. Pustejovsky</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../files/Pustejovsky-CV.pdf"> 
<span class="menu-text">CV</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../people/index.html"> 
<span class="menu-text">People</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../posts/index.html"> 
<span class="menu-text">Posts</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../working-papers.html"> 
<span class="menu-text">Working Papers</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../publication/index.html"> 
<span class="menu-text">Publications</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../presentations/index.html"> 
<span class="menu-text">Presentations</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../software/index.html"> 
<span class="menu-text">Software</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../teaching/index.html"> 
<span class="menu-text">Teaching</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../contact.html"> 
<span class="menu-text">Contact</span></a>
  </li>  
</ul>
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item compact">
    <a class="nav-link" href="../../index.xml"> <i class="bi bi-rss" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
          <div class="quarto-navbar-tools">
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <div class="quarto-title-block"><div><h1 class="title">Alternative formulas for the standardized mean difference</h1><button type="button" class="btn code-tools-button" id="quarto-code-tools-source"><i class="bi"></i> Code</button></div></div>
                                <div class="quarto-categories">
                <div class="quarto-category">meta-analysis</div>
                <div class="quarto-category">effect size</div>
                <div class="quarto-category">distribution theory</div>
                <div class="quarto-category">standardized mean difference</div>
              </div>
                  </div>
  </div>
    
  
  <div class="quarto-title-meta">

      <div>
      <div class="quarto-title-meta-heading">Author</div>
      <div class="quarto-title-meta-contents">
               <p>James E. Pustejovsky </p>
            </div>
    </div>
      
      <div>
      <div class="quarto-title-meta-heading">Published</div>
      <div class="quarto-title-meta-contents">
        <p class="date">June 3, 2016</p>
      </div>
    </div>
    
      
    </div>
    
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Contents</h2>
   
  <ul>
  <li><a href="#smd-from-a-simple-independent-groups-design" id="toc-smd-from-a-simple-independent-groups-design" class="nav-link active" data-scroll-target="#smd-from-a-simple-independent-groups-design">SMD from a simple, independent groups design</a></li>
  <li><a href="#a-general-formula-for-g-and-its-sampling-variance" id="toc-a-general-formula-for-g-and-its-sampling-variance" class="nav-link" data-scroll-target="#a-general-formula-for-g-and-its-sampling-variance">A general formula for <span class="math inline">\(g\)</span> and its sampling variance</a></li>
  <li><a href="#non-standard-estimators-of-d" id="toc-non-standard-estimators-of-d" class="nav-link" data-scroll-target="#non-standard-estimators-of-d">Non-standard estimators of <span class="math inline">\(d\)</span></a>
  <ul>
  <li><a href="#independent-groups-with-different-variances" id="toc-independent-groups-with-different-variances" class="nav-link" data-scroll-target="#independent-groups-with-different-variances">Independent groups with different variances</a></li>
  <li><a href="#multiple-independent-groups" id="toc-multiple-independent-groups" class="nav-link" data-scroll-target="#multiple-independent-groups">Multiple independent groups</a></li>
  <li><a href="#single-group-pre-test-post-test-design" id="toc-single-group-pre-test-post-test-design" class="nav-link" data-scroll-target="#single-group-pre-test-post-test-design">Single group, pre-test post-test design</a></li>
  <li><a href="#two-group-pre-test-post-test-design-ancova-estimation" id="toc-two-group-pre-test-post-test-design-ancova-estimation" class="nav-link" data-scroll-target="#two-group-pre-test-post-test-design-ancova-estimation">Two group, pre-test post-test design: ANCOVA estimation</a></li>
  <li><a href="#two-group-pre-test-post-test-design-repeated-measures-estimation" id="toc-two-group-pre-test-post-test-design-repeated-measures-estimation" class="nav-link" data-scroll-target="#two-group-pre-test-post-test-design-repeated-measures-estimation">Two group, pre-test post-test design: repeated measures estimation</a></li>
  <li><a href="#randomized-trial-with-longitudinal-follow-up" id="toc-randomized-trial-with-longitudinal-follow-up" class="nav-link" data-scroll-target="#randomized-trial-with-longitudinal-follow-up">Randomized trial with longitudinal follow-up</a></li>
  <li><a href="#cluster-randomized-trials" id="toc-cluster-randomized-trials" class="nav-link" data-scroll-target="#cluster-randomized-trials">Cluster-randomized trials</a></li>
  </ul></li>
  <li><a href="#closing-thoughts" id="toc-closing-thoughts" class="nav-link" data-scroll-target="#closing-thoughts">Closing thoughts</a></li>
  <li><a href="#references" id="toc-references" class="nav-link" data-scroll-target="#references">References</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content quarto-banner-title-block" id="quarto-document-content">





<p>The standardized mean difference (SMD) is surely one of the best known and most widely used effect size metrics used in meta-analysis. In generic terms, the SMD parameter is defined as the difference in population means between two groups (often this difference represents the effect of some intervention), scaled by the population standard deviation of the outcome metric. Estimates of the SMD can be obtained from a wide variety of experimental designs, ranging from simple, completely randomized designs, to repeated measures designs, to cluster-randomized trials.</p>
<p>There’s some nuance involved in figuring out how to calculate estimates of the SMD from each design, mostly to do with exactly what sort of standard deviation to use in the denominator of the effect size. I’ll leave that discussion for another day. Here, I’d like to look at the question of how to estimate the sampling variance of the SMD. An estimate of the sampling variance is needed in order to meta-analyze a collection of effect sizes, and so getting the variance calculations right is an important (and sometimes time consuming) part of any meta-analysis project. However, the standard textbook treatments of effect size calculations cover this question only for a limited number of simple cases. I’d like to suggest a different, more general way of thinking about it, which provides a way to estimate the SMD and its variance in some non-standard cases (and also leads to slight differences from conventional formulas for the standard ones). All of this will be old hat for seasoned synthesists, but I hope it might be useful for students and researchers just getting started with meta-analysis.</p>
<p>To start, let me review (regurgitate?) the standard presentation.</p>
<section id="smd-from-a-simple-independent-groups-design" class="level3">
<h3 class="anchored" data-anchor-id="smd-from-a-simple-independent-groups-design">SMD from a simple, independent groups design</h3>
<p>Textbook presentations of the SMD estimator almost always start by introducing the estimator in the context of a <strong>simple, independent groups design</strong>. Call the groups T and C, the sample sizes <span class="math inline">\(n_T\)</span> and <span class="math inline">\(n_C\)</span>, the sample means <span class="math inline">\(\bar{y}_T\)</span> and <span class="math inline">\(\bar{y}_C\)</span>, and the sample variances <span class="math inline">\(s_T^2\)</span> and <span class="math inline">\(s_C^2\)</span>. A basic moment estimator of the SMD is then</p>
<p><span class="math display">\[
d = \frac{\bar{y}_T - \bar{y}_C}{s_p}
\]</span></p>
<p>where <span class="math inline">\(s_p^2 = \frac{\left(n_T - 1\right)s_T^2 + \left(n_C - 1\right) s_C^2}{n_T + n_C - 2}\)</span> is a pooled estimator of the population variance. The standard estimator for the sampling variance of <span class="math inline">\(d\)</span> is</p>
<p><span class="math display">\[
V_d = \frac{n_T + n_C}{n_T n_C} + \frac{d^2}{2\left(n_T + n_C - 2\right)},
\]</span></p>
<p>or some slight variant thereof. This estimator is based on a delta-method approximation for the asymptotic variance of <span class="math inline">\(d\)</span>.</p>
<p>It is well known that <span class="math inline">\(d\)</span> has a small sample bias that depends on sample sizes. Letting</p>
<p><span class="math display">\[
J(x) = 1 - \frac{3}{4x - 1},
\]</span></p>
<p>the bias-corrected estimator is</p>
<p><span class="math display">\[
g = J\left(n_T + n_C - 2\right) \times d,
\]</span></p>
<p>and is often referred to as Hedges’ <span class="math inline">\(g\)</span> because it was proposed in <a href="http://doi.org/10.3102/10769986006002107">Hedges (1981)</a>. Some meta-analysts use <span class="math inline">\(V_d\)</span>, but with <span class="math inline">\(d^2\)</span> replaced by <span class="math inline">\(g^2\)</span>, as an estimator of the large-sample variance of <span class="math inline">\(g\)</span>; others use</p>
<p><span class="math display">\[
V_g = J^2\left(n_T + n_C - 2\right) \left(\frac{n_T + n_C}{n_T n_C} + \frac{g^2}{2\left(n_T + n_C - 2\right)}\right).
\]</span></p>
<p><a href="http://doi.org/10.3102/1076998606298034">Viechtbauer (2007)</a> provides further details on variance estimation and confidence intervals for the SMD in this case.</p>
</section>
<section id="a-general-formula-for-g-and-its-sampling-variance" class="level3">
<h3 class="anchored" data-anchor-id="a-general-formula-for-g-and-its-sampling-variance">A general formula for <span class="math inline">\(g\)</span> and its sampling variance</h3>
<p>The above formulas are certainly useful, but in practice meta-analyses often include studies that use other, more complex designs. Good textbook presentations also cover computation of <span class="math inline">\(g\)</span> and its variance for some other cases (e.g., Borenstein, 2009, also covers one-group pre/post designs and analysis of covariance). Less careful presentations only cover the simple, independent groups design and thus may inadvertently leave the impression that the variance estimator <span class="math inline">\(V_d\)</span> given above applies in general. With other types of studies, <span class="math inline">\(V_d\)</span> can be a wildly biased estimator of the actual sampling variance of <span class="math inline">\(d\)</span>, because it is derived under the assumption that the numerator of <span class="math inline">\(d\)</span> is estimated as the difference in means of two simple random samples. In some designs (e.g., ANCOVA designs, randomized block designs, repeated measures designs), the treatment effect estimate will be much more precise than this; in other designs (e.g., cluster-randomized trials), it will be less precise.</p>
<p>Here’s what I think is a more useful way to think about the sampling variance of <span class="math inline">\(d\)</span>. Let’s suppose that we have an unbiased estimator for the difference in means that goes into the numerator of the SMD. Call this estimator <span class="math inline">\(b\)</span>, its sampling variance <span class="math inline">\(\text{Var}(b)\)</span>, and its standard error <span class="math inline">\(se_{b}\)</span>. Also suppose that we have an unbiased (or reasonably close-to-unbiased) estimator of the population variance of the outcome, the square root of which goes into the denominator of the SMD. Call this estimator <span class="math inline">\(S^2\)</span>, with expectation <span class="math inline">\(\text{E}\left(S^2\right) = \sigma^2\)</span> and sampling variance <span class="math inline">\(\text{Var}(S^2)\)</span>. Finally, suppose that <span class="math inline">\(b\)</span> and <span class="math inline">\(S^2\)</span> are independent (which will often be a pretty reasonable assumption). A delta-method approximation for the sampling variance of <span class="math inline">\(d = b / S\)</span> is then</p>
<p><span class="math display">\[
\text{Var}\left(d\right) \approx \frac{\text{Var}(b)}{\sigma^2} + \frac{\delta^2}{2 \nu},
\]</span></p>
<p>where <span class="math inline">\(\nu = 2 \left[\text{E}\left(S^2\right)\right]^2 / \text{Var}\left(S^2\right)\)</span>. Plugging in sample estimates of the relevant parameters provides a reasonable estimator for the sampling variance of <span class="math inline">\(d\)</span>:</p>
<p><span class="math display">\[
V_d = \left(\frac{se_b}{S}\right)^2 + \frac{d^2}{2 \nu}.
\]</span></p>
<p>This estimator has two parts. The first part involves <span class="math inline">\(se_b / S\)</span>, which is just the standard error of <span class="math inline">\(b\)</span>, but re-scaled into standard deviation units; this part captures the variability in <span class="math inline">\(d\)</span> from its numerator. This scaled standard error can be calculated directly if an article reports <span class="math inline">\(se_b\)</span>.</p>
<p>The second part of <span class="math inline">\(V_d\)</span> is <span class="math inline">\(d^2 / (2 \nu)\)</span>, which captures the variability in <span class="math inline">\(d\)</span> due to its denominator. More precise estimates of <span class="math inline">\(\sigma\)</span> will have larger degrees of freedom, so that the second part will be smaller. For some designs, the degrees of freedom <span class="math inline">\(\nu\)</span> depend only on sample sizes, and thus can be calculated exactly. For some other designs, <span class="math inline">\(\nu\)</span> must be estimated.</p>
<p>The same degrees of freedom can also be used in the small-sample correction for the bias of <span class="math inline">\(d\)</span>, as given by</p>
<p><span class="math display">\[
g = J(\nu) \times d.
\]</span></p>
<p>This small-sample correction is based on a Satterthwaite-type approximation to the distribution of <span class="math inline">\(d\)</span>.</p>
<p>Here’s another way to express the variance estimator for <span class="math inline">\(d\)</span>:</p>
<p><span class="math display">\[
V_d = d^2 \left(\frac{1}{t^2} + \frac{1}{2 \nu}\right),
\]</span></p>
<p>where <span class="math inline">\(t\)</span> is the test statistic corresponding to the hypothesis test for no difference between groups. I’ve never seen that formula in print before, but it could be convenient if an article reports the <span class="math inline">\(t\)</span> statistic (or <span class="math inline">\(F = t^2\)</span> statistic).</p>
</section>
<section id="non-standard-estimators-of-d" class="level3">
<h3 class="anchored" data-anchor-id="non-standard-estimators-of-d">Non-standard estimators of <span class="math inline">\(d\)</span></h3>
<p>The advantage of this formulation of <span class="math inline">\(d\)</span>, <span class="math inline">\(g\)</span>, and <span class="math inline">\(V_d\)</span> is that it can be applied in quite a wide variety of circumstances, including cases that aren’t usually covered in textbook treatments. Rather than having to use separate formulas for every combination of design and analytic approach under the sun, the same formulas apply throughout. What changes are the components of the formulas: the scaled standard error <span class="math inline">\(se_b / S\)</span> and the degrees of freedom <span class="math inline">\(\nu\)</span>. The general formulation also makes it easier to swap in different estimates of <span class="math inline">\(b\)</span> or <span class="math inline">\(S\)</span>—i.e., if you estimate the numerator a different way but keep the denominator the same, you’ll need a new scaled standard error but can still use the same degrees of freedom. A bunch of examples:</p>
<section id="independent-groups-with-different-variances" class="level4">
<h4 class="anchored" data-anchor-id="independent-groups-with-different-variances">Independent groups with different variances</h4>
<p>Suppose that we’re looking at two independent groups but do not want to assume that their variances are the same. In this case, it would make sense to standardize the difference in means by the control group standard deviation (without pooling), so that <span class="math inline">\(d = \left(\bar{y}_T - \bar{y}_C\right) / s_C\)</span>. Since <span class="math inline">\(s_C^2\)</span> has <span class="math inline">\(\nu = n_C - 1\)</span> degrees of freedom, the small-sample bias correction will then need to be <span class="math inline">\(J(n_C - 1)\)</span>. The scaled standard error will be</p>
<p><span class="math display">\[
\frac{se_b}{s_C} = \sqrt{\frac{s_T^2}{s_C^2 n_T} + \frac{1}{n_C}}.
\]</span></p>
<p>This is then everything that we need to calculate <span class="math inline">\(V_d\)</span>, <span class="math inline">\(g\)</span>, <span class="math inline">\(V_g\)</span>, etc.</p>
</section>
<section id="multiple-independent-groups" class="level4">
<h4 class="anchored" data-anchor-id="multiple-independent-groups">Multiple independent groups</h4>
<p>Suppose that the study involves <span class="math inline">\(K - 1\)</span> treatment groups, 1 control group, and <span class="math inline">\(N\)</span> total participants. If the meta-analysis will include SMDs comparing <em>each</em> treatment group to the control group, it would make sense to pool the sample variance across all <span class="math inline">\(K\)</span> groups rather than just the pair of groups, so that a common estimate of scale is used across all the effect sizes. The pooled standard deviation is then calculated as</p>
<p><span class="math display">\[
s_p^2 = \frac{1}{N - K} \sum_{k=0}^K (n_k - 1) s_k^2.
\]</span></p>
<p>For a comparison between treatment group <span class="math inline">\(k\)</span> and the control group, we would then use</p>
<p><span class="math display">\[
d = \frac{\bar{y}_k - \bar{y}_C}{s_p}, \qquad \nu = N - K, \qquad \frac{se_b}{s_p} = \sqrt{\frac{1}{n_C} + \frac{1}{n_k}},
\]</span></p>
<p>where <span class="math inline">\(n_k\)</span> is the sample size for treatment group <span class="math inline">\(k\)</span> (cf.&nbsp;Gleser &amp; Olkin, 2009).</p>
</section>
<section id="single-group-pre-test-post-test-design" class="level4">
<h4 class="anchored" data-anchor-id="single-group-pre-test-post-test-design">Single group, pre-test post-test design</h4>
<p>Suppose that a study involves taking pre-test and post-test measurements on a single group of <span class="math inline">\(n\)</span> participants. Borenstein (2009) recommends calculating the standardized mean difference for this study as the difference in means between the post-test and pre-test, scaled by the pooled (across pre- and post-test measurements) standard deviation. With obvious notation:</p>
<p><span class="math display">\[
d = \frac{\bar{y}_{post} - \bar{y}_{pre}}{s_p}, \qquad \text{where} \qquad s_p^2 = \frac{1}{2}\left(s_{pre}^2 + s_{post}^2\right).
\]</span></p>
<p>In this design,</p>
<p><span class="math display">\[
\frac{se_b}{s_p} = \sqrt{\frac{2(1 - r)}{n}},
\]</span></p>
<p>where <span class="math inline">\(r\)</span> is the sample correlation between the pre- and post-tests. The remaining question is what to use for <span class="math inline">\(\nu\)</span>. Borenstein (2009) uses <span class="math inline">\(\nu = n - 1\)</span>. My previous post <a href="../..\posts/distribution-of-sample-variances/">on the sampling covariance of sample variances</a> gave the result that <span class="math inline">\(\text{Var}(s_p^2) = \sigma^4 (1 + \rho^2) / (n - 1)\)</span>, which would instead suggest using</p>
<p><span class="math display">\[
\nu = \frac{2 (n - 1)}{1 + r^2}.
\]</span></p>
<p>This formula will tend to give slightly larger degrees of freedom, but probably won’t be that discrepant from Borenstein’s approach except in quite small samples. It would be interesting to investigate which approach is better in small samples (i.e., leading to less biased estimates of the SMD and more accurate estimates of sampling variance, and by how much), although its possible than neither is all that good because the variance estimator itself is based on a large-sample approximation.</p>
</section>
<section id="two-group-pre-test-post-test-design-ancova-estimation" class="level4">
<h4 class="anchored" data-anchor-id="two-group-pre-test-post-test-design-ancova-estimation">Two group, pre-test post-test design: ANCOVA estimation</h4>
<p>Suppose that a study involves taking pre-test and post-test measurements on two groups of participants, with sample sizes <span class="math inline">\(n_T\)</span> and <span class="math inline">\(n_C\)</span> respectively. One way to analyze this design is via ANCOVA using the pre-test measure as the covariate, so that the treatment effect estimate is the difference in adjusted post-test means. In this design, the scaled standard error will be approximately</p>
<p><span class="math display">\[
\frac{se_b}{S} = \sqrt{ \frac{(n_C + n_T)(1 - r^2)}{n_C n_T} },
\]</span></p>
<p>where <span class="math inline">\(r\)</span> is the pooled, within-group sample correlation between the pre-test and the post-test measures (this approximation assumes that the pre-test SMD between groups is relatively small). Alternately, if <span class="math inline">\(se_b\)</span> is provided then the scaled standard error could be calculated directly.</p>
<p>Borenstein (2009) suggests calculating <span class="math inline">\(d\)</span> as the difference in adjusted means, scaled by the pooled sample variances on the post-test measures. The post-test pooled sample variance will have the same degrees of freedom as in the two-sample t-test case: <span class="math inline">\(\nu = n_C + n_T - 2\)</span>. (Borenstein instead uses <span class="math inline">\(\nu = n_C + n_T - 2 - q\)</span>, where <span class="math inline">\(q\)</span> is the number of covariates in the analysis, but this won’t usually make much difference unless the total sample size is quite small.)</p>
<p>Scaling by the pooled post-test sample variance isn’t the only reasonable way to estimate the SMD though. If the covariate is a true pre-test, then why not scale by the pooled pre-test sample variance instead? To do so, you would need to calculate <span class="math inline">\(se_b / S\)</span> directly and use <span class="math inline">\(\nu = n_C + n_T - 2\)</span>. If it is reasonable to assume that the pre- and post-test population variances are equal, then another alternative would be to pool across the pre-test <em>and</em> post-test sample variances in each group. Using this approach, you would again need to calculate <span class="math inline">\(se_b / S\)</span> directly and then use <span class="math inline">\(\nu = 2(n_C + n_T - 2) / (1 + r^2)\)</span>.</p>
</section>
<section id="two-group-pre-test-post-test-design-repeated-measures-estimation" class="level4">
<h4 class="anchored" data-anchor-id="two-group-pre-test-post-test-design-repeated-measures-estimation">Two group, pre-test post-test design: repeated measures estimation</h4>
<p>Another way to analyze the data from the same type of study design is to use repeated measures ANOVA. I’ve recently encountered a number of studies that use this approach (here’s a recent example from <a href="http://dx.doi.org/10.1371/journal.pone.0154075">a highly publicized study in PLOS ONE</a>—see Table 2). The studies I’ve seen typically report the sample means and variances in each group and at each time point, from which the difference in change scores can be calculated. Let <span class="math inline">\(\bar{y}_{gt}\)</span> and <span class="math inline">\(s_{gt}^2\)</span> denote the sample mean and sample variance in group <span class="math inline">\(g = T, C\)</span> at time <span class="math inline">\(t = 0, 1\)</span>. The numerator of <span class="math inline">\(d\)</span> would then be calculated as</p>
<p><span class="math display">\[
b = \left(\bar{y}_{T1} - \bar{y}_{T0}\right) - \left(\bar{y}_{C1} - \bar{y}_{C0}\right),
\]</span></p>
<p>which has sampling variance <span class="math inline">\(\text{Var}(b) = 2(1 - \rho)\sigma^2\left(n_C + n_T \right) / (n_C n_T)\)</span>, where <span class="math inline">\(\rho\)</span> is the correlation between the pre-test and the post-test measures. Thus, the scaled standard error is</p>
<p><span class="math display">\[
\frac{se_b}{S} = \sqrt{\frac{2(1 - r)(n_C + n_T)}{n_C n_T}}.
\]</span></p>
<p>As with ANCOVA, there are several potential options for calculating the denominator of <span class="math inline">\(d\)</span>:</p>
<ul>
<li><p>Using the pooled sample variances on the post-test measures, with <span class="math inline">\(\nu = n_C + n_T - 2\)</span>;</p></li>
<li><p>Using the pooled sample variances on the pre-test measures, with <span class="math inline">\(\nu = n_C + n_T - 2\)</span>; or</p></li>
<li><p>Using the pooled sample variances at both time points and in both groups, i.e.,</p>
<p><span class="math display">\[
  S^2 = \frac{(n_C - 1)(s_{C0}^2 + s_{C1}^2) + (n_T - 1)(s_{T0}^2 + s_{T1}^2)}{2(n_C + n_T - 2)},
  \]</span></p>
<p>with <span class="math inline">\(\nu = 2(n_C + n_T - 2) / (1 + r^2)\)</span>.</p></li>
</ul>
<p>The range of approaches to scaling is the same as for ANCOVA. This makes sense because both analyses are based on data from the same study design, so the parameter of interest should be the same (i.e., the target parameter should not change based on the analytic method). Note that all of these approaches are a bit different than the effect size estimator proposed by <a href="http://doi.org/10.1037//1082-989X.7.1.105">Morris and DeShon (2002)</a> for the two-group, pre-post design; their approach does not fit into my framework because it involves taking a difference between standardized effect sizes (and therefore involves two separate estimates of scale, rather than just one).</p>
</section>
<section id="randomized-trial-with-longitudinal-follow-up" class="level4">
<h4 class="anchored" data-anchor-id="randomized-trial-with-longitudinal-follow-up">Randomized trial with longitudinal follow-up</h4>
<p>Many independent-groups designs—especially randomized trials in field settings—involve repeated, longitudinal follow-up assessments. An increasingly common approach to analysis of such data is through hierarchical linear models, which can be used to account for the dependence structure among measurements taken on the same individual. In this setting, <a href="http://doi.org/10.1037/a0014699">Feingold (2009)</a> proposes that the SMD be calculated as the model-based estimate of the treatment effect at the final follow-up time, scaled by the within-groups variance of the outcome at that time point. Let <span class="math inline">\(\hat\beta_1\)</span> denote the estimated difference in slopes (change per unit time) between groups in a linear growth model, <span class="math inline">\(F\)</span> denote the duration of the study, and <span class="math inline">\(s_{pF}^2\)</span> denote the pooled sample variance of the outcome at the final time point. For this model, Feingold (2009) proposes to calculate the standardized mean difference as</p>
<p><span class="math display">\[
d = \frac{F \hat\beta_1}{s_{pF}}.
\]</span></p>
<p>In a later paper, <a href="http://doi.org/10.1037/a0037721">Feingold (2015)</a> proposes that the sampling variance of <span class="math inline">\(d\)</span> be estimated as <span class="math inline">\(F \times se_{\hat\beta_1} / s_{pF}\)</span>, where <span class="math inline">\(se_{\hat\beta_1}\)</span> is the standard error of the estimated slope. My framework suggests that a better estimate of the sampling variance, which accounts for the uncertainty of the scale estimate, would be to use</p>
<p><span class="math display">\[
V_d = \left(\frac{F \times se_{\hat\beta_1}}{s_{pF}}\right)^2 + \frac{d^2}{2 \nu},
\]</span></p>
<p>with <span class="math inline">\(\nu = n_T + n_C - 2\)</span>. The same <span class="math inline">\(\nu\)</span> could be used to bias-correct the effect size estimate.</p>
<p>If estimates of the variance components of the HLM are reported, one could use them to construct a model-based estimate of the scale parameter in the denominator of <span class="math inline">\(d\)</span>. I explored this approach in a paper that uses HLM to model single-case designs, which are a certain type of longitudinal experiment that typically involve a very small number of participants (<a href="http://doi.org/10.3102/1076998614547577">Pustejovsky, Hedges, &amp; Shadish, 2014</a>). Estimates of the scale parameter can usually be written as</p>
<p><span class="math display">\[
S_{model}^2 = \mathbf{r}'\boldsymbol\omega,
\]</span></p>
<p>where <span class="math inline">\(\boldsymbol\omega\)</span> is a vector of all the variance components in the model and <span class="math inline">\(\mathbf{r}\)</span> is a vector of weights that depend on the model specification and length of follow-up. This estimate of scale will usually be more precise than <span class="math inline">\(s_{pF}^2\)</span> because it makes use of all of the data (and modeling assumptions). However, it can be challenging to determine appropriate degrees of freedom for <span class="math inline">\(S_{model}^2\)</span>. For single-case designs, I used estimates of <span class="math inline">\(\text{Var}(\boldsymbol\omega)\)</span> based on the inverse of the expected information matrix—call the estimate <span class="math inline">\(\mathbf{V}_{\boldsymbol\omega}\)</span>—in which case</p>
<p><span class="math display">\[
\nu = \frac{2 S_{model}^4}{\mathbf{r}' \mathbf{V}_{\boldsymbol\omega} \mathbf{r}}.
\]</span></p>
<p>However, most published articles will not provide estimates of the sampling variances of the variance components—in fact, a lot of software for estimating HLMs does not even provide these. It would be useful to work out some reasonable approximations for the degrees of freedom in these models—approximations that can be calculated based on the information that’s typically available—and to investigate the extent to which there’s any practical benefit to using <span class="math inline">\(S_{model}^2\)</span> over <span class="math inline">\(s_{pF}^2\)</span>.</p>
</section>
<section id="cluster-randomized-trials" class="level4">
<h4 class="anchored" data-anchor-id="cluster-randomized-trials">Cluster-randomized trials</h4>
<p><a href="http://doi.org/10.3102/1076998606298043">Hedges (2007)</a> addresses estimation of standardized mean differences for cluster-randomized trials, in which the units of measurement are nested within higher-level clusters that comprise the units of randomization. Such designs involve two variance components (within- and between-cluster variance), and thus there are three potential approaches to scaling the treatment effect: standardize by the total variance (i.e., the sum of the within- and between-cluster components), standardize by the within-cluster variance, or standardize by the between-cluster variance. Furthermore, some of the effect sizes can be estimated in several different ways, each with a different sampling variance. <a href="http://doi.org/10.3102/1076998606298043">Hedges (2007)</a> gives sampling variance estimates for each estimator of each effect size, but they all follow the same general formula as given above. (The appendix of the article actually gives the same formula as above, but using a more abstract formulation.)</p>
<p>For example, suppose the target SMD parameter uses the total variance and that we have data from a two-level, two-arm cluster randomized trial with <span class="math inline">\(M\)</span> clusters, <span class="math inline">\(n\)</span> observations per cluster, and total sample sizes in each arm of <span class="math inline">\(N_T\)</span> and <span class="math inline">\(N_C\)</span>, respectively. Let <span class="math inline">\(\tau^2\)</span> be the between-cluster variance, <span class="math inline">\(\sigma^2\)</span> be the within-cluster variance, and <span class="math inline">\(\rho = \tau^2 / (\tau^2 + \sigma^2)\)</span>. The target parameter is <span class="math inline">\(\delta = \left(\mu_T - \mu_C\right) / \left(\tau^2 + \sigma^2\right)\)</span>. The article assumes that the treatment effect will be estimated by the difference in grand means, <span class="math inline">\(\bar{\bar{y}}_T - \bar{\bar{y}}_C\)</span>. Letting <span class="math inline">\(S_B^2\)</span> be the pooled sample variance of the cluster means within each arm and <span class="math inline">\(S_W^2\)</span> be the pooled within-cluster sample variance, the total variance is estimated as</p>
<p><span class="math display">\[
S_{total}^2 = S_B^2 + \frac{n - 1}{n} S_W^2.
\]</span></p>
<p>An estimate of the SMD is then</p>
<p><span class="math display">\[
d = \left(\bar{\bar{y}}_T - \bar{\bar{y}}_C \right) / \sqrt{S_{total}^2}.
\]</span></p>
<p>The scaled standard error of <span class="math inline">\(\bar{\bar{y}}_T - \bar{\bar{y}}_C\)</span> is</p>
<p><span class="math display">\[
se_b = \sqrt{\left(\frac{N_C + N_T}{N_C N_T}\right)\left[1 + (n - 1)\rho\right]}.
\]</span></p>
<p>The appendix of the article demonstrates that <span class="math inline">\(\text{E}\left(S_{total}^2\right) = \tau^2 + \sigma^2\)</span> and</p>
<p><span class="math display">\[
\text{Var}\left( S_{total}^2 \right) = \frac{2}{n^2}\left(\frac{(n \tau^2 + \sigma^2)^2}{M - 2} + \frac{(n - 1)^2 \sigma^4}{N_C + N_T - M}\right),
\]</span></p>
<p>by which it follows that</p>
<p><span class="math display">\[
\nu = \frac{n^2 M (M - 2)}{M[(n - 1)\rho + 1]^2 + (M - 2)(n - 1)(1 - \rho)^2}.
\]</span></p>
<p>Substituting <span class="math inline">\(se_b / S_{total}\)</span> and <span class="math inline">\(\nu\)</span> into the formula for <span class="math inline">\(V_d\)</span> gives the same as Expression (14) in the article.</p>
<p>A limitation of <a href="http://doi.org/10.3102/1076998606298043">Hedges (2007)</a> is that it only covers the case where the treatment effect is estimated by the difference in grand means (although it does cover the case of unequal cluster sizes, which gets quite messy). In practice, every cluster-randomized trial I’ve ever seen uses baseline covariates to adjust the mean difference (often based on a hierarchical linear model) and improve the precision of the treatment effect estimate. The SMD estimate should also be based on this covariate-adjustment estimate, scaled by the total variance <em>without adjusting for the covariate</em>. An advantage of the general formulation given above is that its clear how to estimate the sampling variance of <span class="math inline">\(d\)</span>. I would guess that it will often be possible to calculate the scaled standard error directly, given the standard error of the covariate-adjusted treatment effect estimate. And since <span class="math inline">\(S_{total}\)</span> would be estimated just as before, its degrees of freedom remain the same.</p>
<p><a href="http://doi.org/10.3102/1076998610376617">Hedges (2011)</a> discusses estimation of SMDs in three-level cluster-randomized trials—an even more complicated case. However, the general approach is the same; all that’s needed are the scaled standard error and the degrees of freedom <span class="math inline">\(\nu\)</span> of whatever combination of variance components go into the denominator of the effect size. In both the two-level and three-level cases, the degrees of freedom get quite complicated in unbalanced samples and are probably not calculable from the information that is usually provided in an article. Hedges (2007, 2011) comments on a couple of cases where more tractable approximations can be used, although it seems like there might be room for further investigation here.</p>
</section>
</section>
<section id="closing-thoughts" class="level3">
<h3 class="anchored" data-anchor-id="closing-thoughts">Closing thoughts</h3>
<p>I think this framework is useful in that it unifies a large number of cases that have been treated separately, and can also be applied (more-or-less immediately) to <span class="math inline">\(d\)</span> estimators that haven’t been widely considered before, such as the <span class="math inline">\(d\)</span> that involves scaling by the pooled pre-and-post, treatment-and-control sample variance. I hope it also illustrates that, while the point estimator <span class="math inline">\(d\)</span> can be applied across a large number of study designs, the sampling variance of <span class="math inline">\(d\)</span> depends on the details of the design and estimation methods. The same is true for other families of effect sizes as well. For example, in other work I’ve demonstrated that the sampling variance of the correlation coefficient depends on the design from which the correlations are estimated (<a href="http://doi.org/10.1037/a0033788">Pustejovsky, 2014</a>).</p>
<p>If you have read this far, I’d love to get your feedback about whether you think this is a useful way to organize the calculations of <span class="math inline">\(d\)</span> estimators. Is this helpful? Or nothing you didn’t already know? Or still more complicated than it should be? Leave a comment!</p>
</section>
<section id="references" class="level3">
<h3 class="anchored" data-anchor-id="references">References</h3>
<p>Borenstein, M. (2009). Effect sizes for continuous data. In H. M. Cooper, L. V Hedges, &amp; J. C. Valentine (Eds.), The Handbook of Research Synthesis and Meta-Analysis (pp.&nbsp;221–236). New York, NY: Russell Sage Foundation.</p>
<p>Feingold, A. (2009). Effect sizes for growth-modeling analysis for controlled clinical trials in the same metric as for classical analysis. Psychological Methods, 14(1), 43–53. doi:10.1037/a0014699</p>
<p>Feingold, A. (2015). Confidence interval estimation for standardized effect sizes in multilevel and latent growth modeling. Journal of Consulting and Clinical Psychology, 83(1), 157–168. doi:10.1037/a0037721</p>
<p>Gleser, L. J., &amp; Olkin, I. (2009). Stochastically dependent effect sizes. In H. Cooper, L. V. Hedges, &amp; J. C. Valentine (Eds.), The Handbook of Research Synthesis and Meta-Analysis (2nd ed., pp.&nbsp;357–376). New York, NY: Russell Sage Foundation.</p>
<p>Hedges, L. V. (2007). Effect sizes in cluster-randomized designs. Journal of Educational and Behavioral Statistics, 32(4), 341–370. doi:10.3102/1076998606298043</p>
<p>Hedges, L. V. (2011). Effect sizes in three-level cluster-randomized experiments. Journal of Educational and Behavioral Statistics, 36(3), 346–380. doi:10.3102/1076998610376617</p>
<p>Morris, S. B., &amp; DeShon, R. P. (2002). Combining effect size estimates in meta-analysis with repeated measures and independent-groups designs. Psychological Methods, 7(1), 105–125. doi:10.1037//1082-989X.7.1.105</p>
<p>Pustejovsky, J. E. (2014). Converting from d to r to z when the design uses extreme groups, dichotomization, or experimental control. Psychological Methods, 19(1), 92–112. doi:10.1037/a0033788</p>
<p>Pustejovsky, J. E., Hedges, L. V, &amp; Shadish, W. R. (2014). Design-comparable effect sizes in multiple baseline designs: A general modeling framework. Journal of Educational and Behavioral Statistics, 39(5), 368–393. doi:10.3102/1076998614547577</p>
<p>Viechtbauer, W. (2007). Approximate confidence intervals for standardized effect sizes in the two-independent and two-dependent samples design. Journal of Educational and Behavioral Statistics, 32(1), 39–60. doi:10.3102/1076998606298034</p>


<!-- -->

</section>

<a onclick="window.scrollTo(0, 0); return false;" role="button" id="quarto-back-to-top"><i class="bi bi-arrow-up"></i> Back to top</a><div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" id="quarto-reuse"><h2 class="anchored quarto-appendix-heading">Reuse</h2><div class="quarto-appendix-contents"><div><a rel="license" href="https://creativecommons.org/licenses/by/4.0/">CC BY 4.0</a></div></div></section><section class="quarto-appendix-contents" id="quarto-citation"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@online{pustejovsky2016,
  author = {Pustejovsky, James E.},
  title = {Alternative Formulas for the Standardized Mean Difference},
  date = {2016-06-03},
  url = {https://mellifluous-buttercream-e2edd2.netlify.app/posts/alternative-formulas-for-the-SMD},
  langid = {en}
}
</code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-pustejovsky2016" class="csl-entry quarto-appendix-citeas" role="listitem">
Pustejovsky, James E. 2016. <span>“Alternative Formulas for the
Standardized Mean Difference.”</span> June 3, 2016. <a href="https://mellifluous-buttercream-e2edd2.netlify.app/posts/alternative-formulas-for-the-SMD">https://mellifluous-buttercream-e2edd2.netlify.app/posts/alternative-formulas-for-the-SMD</a>.
</div></div></section></div></main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  const viewSource = window.document.getElementById('quarto-view-source') ||
                     window.document.getElementById('quarto-code-tools-source');
  if (viewSource) {
    const sourceUrl = viewSource.getAttribute("data-quarto-source-url");
    viewSource.addEventListener("click", function(e) {
      if (sourceUrl) {
        // rstudio viewer pane
        if (/\bcapabilities=\b/.test(window.location)) {
          window.open(sourceUrl);
        } else {
          window.location.href = sourceUrl;
        }
      } else {
        const modal = new bootstrap.Modal(document.getElementById('quarto-embedded-source-code-modal'));
        modal.show();
      }
      return false;
    });
  }
  function toggleCodeHandler(show) {
    return function(e) {
      const detailsSrc = window.document.querySelectorAll(".cell > details > .sourceCode");
      for (let i=0; i<detailsSrc.length; i++) {
        const details = detailsSrc[i].parentElement;
        if (show) {
          details.open = true;
        } else {
          details.removeAttribute("open");
        }
      }
      const cellCodeDivs = window.document.querySelectorAll(".cell > .sourceCode");
      const fromCls = show ? "hidden" : "unhidden";
      const toCls = show ? "unhidden" : "hidden";
      for (let i=0; i<cellCodeDivs.length; i++) {
        const codeDiv = cellCodeDivs[i];
        if (codeDiv.classList.contains(fromCls)) {
          codeDiv.classList.remove(fromCls);
          codeDiv.classList.add(toCls);
        } 
      }
      return false;
    }
  }
  const hideAllCode = window.document.getElementById("quarto-hide-all-code");
  if (hideAllCode) {
    hideAllCode.addEventListener("click", toggleCodeHandler(false));
  }
  const showAllCode = window.document.getElementById("quarto-show-all-code");
  if (showAllCode) {
    showAllCode.addEventListener("click", toggleCodeHandler(true));
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp("https:\/\/mellifluous-buttercream-e2edd2\.netlify\.app");
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<script src="https://utteranc.es/client.js" repo="jepusto/jepusto-quarto" issue-term="pathname" theme="github-light" crossorigin="anonymous" async="">
</script><div class="modal fade" id="quarto-embedded-source-code-modal" tabindex="-1" aria-labelledby="quarto-embedded-source-code-modal-label" aria-hidden="true"><div class="modal-dialog modal-dialog-scrollable"><div class="modal-content"><div class="modal-header"><h5 class="modal-title" id="quarto-embedded-source-code-modal-label">Source Code</h5><button class="btn-close" data-bs-dismiss="modal"></button></div><div class="modal-body"><div class="">
<div class="sourceCode" id="cb1" data-shortcodes="false"><pre class="sourceCode markdown code-with-copy"><code class="sourceCode markdown"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="an">title:</span><span class="co"> Alternative formulas for the standardized mean difference</span></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="an">date:</span><span class="co"> '2016-06-03'</span></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="an">categories:</span></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="co">- meta-analysis</span></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="co">- effect size</span></span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a><span class="co">- distribution theory</span></span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a><span class="co">- standardized mean difference</span></span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a><span class="an">code-tools:</span><span class="co"> true</span></span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a><span class="an">toc:</span><span class="co"> true</span></span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a><span class="an">toc-depth:</span><span class="co"> 4</span></span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a><span class="an">toc-title:</span><span class="co"> Contents</span></span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a>The standardized mean difference (SMD) is surely one of the best known and most widely used effect size metrics used in meta-analysis. In generic terms, the SMD parameter is defined as the difference in population means between two groups (often this difference represents the effect of some intervention), scaled by the population standard deviation of the outcome metric. Estimates of the SMD can be obtained from a wide variety of experimental designs, ranging from simple, completely randomized designs, to repeated measures designs, to cluster-randomized trials.</span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a>There's some nuance involved in figuring out how to calculate estimates of the SMD from each design, mostly to do with exactly what sort of standard deviation to use in the denominator of the effect size. I'll leave that discussion for another day. Here, I'd like to look at the question of how to estimate the sampling variance of the SMD. An estimate of the sampling variance is needed in order to meta-analyze a collection of effect sizes, and so getting the variance calculations right is an important (and sometimes time consuming) part of any meta-analysis project. However, the standard textbook treatments of effect size calculations cover this question only for a limited number of simple cases. I'd like to suggest a different, more general way of thinking about it, which provides a way to estimate the SMD and its variance in some non-standard cases (and also leads to slight differences from conventional formulas for the standard ones). All of this will be old hat for seasoned synthesists, but I hope it might be useful for students and researchers just getting started with meta-analysis.</span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a>To start, let me review (regurgitate?) the standard presentation.</span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a><span class="fu">### SMD from a simple, independent groups design</span></span>
<span id="cb1-22"><a href="#cb1-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-23"><a href="#cb1-23" aria-hidden="true" tabindex="-1"></a>Textbook presentations of the SMD estimator almost always start by introducing the estimator in the context of a __simple, independent groups design__. Call the groups T and C, the sample sizes $n_T$ and $n_C$, the sample means $\bar{y}_T$ and $\bar{y}_C$, and the sample variances $s_T^2$ and $s_C^2$. A basic moment estimator of the SMD is then </span>
<span id="cb1-24"><a href="#cb1-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-25"><a href="#cb1-25" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-26"><a href="#cb1-26" aria-hidden="true" tabindex="-1"></a>d = \frac{\bar{y}_T - \bar{y}_C}{s_p}</span>
<span id="cb1-27"><a href="#cb1-27" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-28"><a href="#cb1-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-29"><a href="#cb1-29" aria-hidden="true" tabindex="-1"></a>where $s_p^2 = \frac{\left(n_T - 1\right)s_T^2 + \left(n_C - 1\right) s_C^2}{n_T + n_C - 2}$ is a pooled estimator of the population variance. The standard estimator for the sampling variance of $d$ is </span>
<span id="cb1-30"><a href="#cb1-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-31"><a href="#cb1-31" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-32"><a href="#cb1-32" aria-hidden="true" tabindex="-1"></a>V_d = \frac{n_T + n_C}{n_T n_C} + \frac{d^2}{2\left(n_T + n_C - 2\right)},</span>
<span id="cb1-33"><a href="#cb1-33" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-34"><a href="#cb1-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-35"><a href="#cb1-35" aria-hidden="true" tabindex="-1"></a>or some slight variant thereof. This estimator is based on a delta-method approximation for the asymptotic variance of $d$. </span>
<span id="cb1-36"><a href="#cb1-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-37"><a href="#cb1-37" aria-hidden="true" tabindex="-1"></a>It is well known that $d$ has a small sample bias that depends on sample sizes. Letting</span>
<span id="cb1-38"><a href="#cb1-38" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-39"><a href="#cb1-39" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-40"><a href="#cb1-40" aria-hidden="true" tabindex="-1"></a>J(x) = 1 - \frac{3}{4x - 1},</span>
<span id="cb1-41"><a href="#cb1-41" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-42"><a href="#cb1-42" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-43"><a href="#cb1-43" aria-hidden="true" tabindex="-1"></a>the bias-corrected estimator is </span>
<span id="cb1-44"><a href="#cb1-44" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-45"><a href="#cb1-45" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-46"><a href="#cb1-46" aria-hidden="true" tabindex="-1"></a>g = J\left(n_T + n_C - 2\right) \times d,</span>
<span id="cb1-47"><a href="#cb1-47" aria-hidden="true" tabindex="-1"></a>$$ </span>
<span id="cb1-48"><a href="#cb1-48" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-49"><a href="#cb1-49" aria-hidden="true" tabindex="-1"></a>and is often referred to as Hedges' $g$ because it was proposed in <span class="co">[</span><span class="ot">Hedges (1981)</span><span class="co">](http://doi.org/10.3102/10769986006002107)</span>. Some meta-analysts use $V_d$, but with $d^2$ replaced by $g^2$, as an estimator of the large-sample variance of $g$; others use </span>
<span id="cb1-50"><a href="#cb1-50" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-51"><a href="#cb1-51" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-52"><a href="#cb1-52" aria-hidden="true" tabindex="-1"></a>V_g = J^2\left(n_T + n_C - 2\right) \left(\frac{n_T + n_C}{n_T n_C} + \frac{g^2}{2\left(n_T + n_C - 2\right)}\right).</span>
<span id="cb1-53"><a href="#cb1-53" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-54"><a href="#cb1-54" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-55"><a href="#cb1-55" aria-hidden="true" tabindex="-1"></a><span class="co">[</span><span class="ot">Viechtbauer (2007)</span><span class="co">](http://doi.org/10.3102/1076998606298034)</span> provides further details on variance estimation and confidence intervals for the SMD in this case.</span>
<span id="cb1-56"><a href="#cb1-56" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-57"><a href="#cb1-57" aria-hidden="true" tabindex="-1"></a><span class="fu">### A general formula for $g$ and its sampling variance</span></span>
<span id="cb1-58"><a href="#cb1-58" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-59"><a href="#cb1-59" aria-hidden="true" tabindex="-1"></a>The above formulas are certainly useful, but in practice meta-analyses often include studies that use other, more complex designs. </span>
<span id="cb1-60"><a href="#cb1-60" aria-hidden="true" tabindex="-1"></a>Good textbook presentations also cover computation of $g$ and its variance for some other cases (e.g., Borenstein, 2009, also covers one-group pre/post designs and analysis of covariance). Less careful presentations only cover the simple, independent groups design and thus may inadvertently leave the impression that the variance estimator $V_d$ given above applies in general. With other types of studies, $V_d$ can be a wildly biased estimator of the actual sampling variance of $d$, because it is derived under the assumption that the numerator of $d$ is estimated as the difference in means of two simple random samples. In some designs (e.g., ANCOVA designs, randomized block designs, repeated measures designs), the treatment effect estimate will be much more precise than this; in other designs (e.g., cluster-randomized trials), it will be less precise. </span>
<span id="cb1-61"><a href="#cb1-61" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-62"><a href="#cb1-62" aria-hidden="true" tabindex="-1"></a>Here's what I think is a more useful way to think about the sampling variance of $d$. Let's suppose that we have an unbiased estimator for the difference in means that goes into the numerator of the SMD. Call this estimator $b$, its sampling variance $\text{Var}(b)$, and its standard error $se_{b}$. Also suppose that we have an unbiased (or reasonably close-to-unbiased) estimator of the population variance of the outcome, the square root of which goes into the denominator of the SMD. Call this estimator $S^2$, with expectation $\text{E}\left(S^2\right) = \sigma^2$ and sampling variance $\text{Var}(S^2)$. Finally, suppose that $b$ and $S^2$ are independent (which will often be a pretty reasonable assumption). A delta-method approximation for the sampling variance of $d = b / S$ is then </span>
<span id="cb1-63"><a href="#cb1-63" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-64"><a href="#cb1-64" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-65"><a href="#cb1-65" aria-hidden="true" tabindex="-1"></a>\text{Var}\left(d\right) \approx \frac{\text{Var}(b)}{\sigma^2} + \frac{\delta^2}{2 \nu},</span>
<span id="cb1-66"><a href="#cb1-66" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-67"><a href="#cb1-67" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-68"><a href="#cb1-68" aria-hidden="true" tabindex="-1"></a>where $\nu = 2 \left<span class="co">[</span><span class="ot">\text{E}\left(S^2\right)\right</span><span class="co">]</span>^2 / \text{Var}\left(S^2\right)$. Plugging in sample estimates of the relevant parameters provides a reasonable estimator for the sampling variance of $d$:</span>
<span id="cb1-69"><a href="#cb1-69" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-70"><a href="#cb1-70" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-71"><a href="#cb1-71" aria-hidden="true" tabindex="-1"></a>V_d = \left(\frac{se_b}{S}\right)^2 + \frac{d^2}{2 \nu}.</span>
<span id="cb1-72"><a href="#cb1-72" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-73"><a href="#cb1-73" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-74"><a href="#cb1-74" aria-hidden="true" tabindex="-1"></a>This estimator has two parts. The first part involves $se_b / S$, which is just the standard error of $b$, but re-scaled into standard deviation units; this part captures the variability in $d$ from its numerator. This scaled standard error can be calculated directly if an article reports $se_b$. </span>
<span id="cb1-75"><a href="#cb1-75" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-76"><a href="#cb1-76" aria-hidden="true" tabindex="-1"></a>The second part of $V_d$ is $d^2 / (2 \nu)$, which captures the variability in $d$ due to its denominator. More precise estimates of $\sigma$ will have larger degrees of freedom, so that the second part will be smaller. For some designs, the degrees of freedom $\nu$ depend only on sample sizes, and thus can be calculated exactly. For some other designs, $\nu$ must be estimated. </span>
<span id="cb1-77"><a href="#cb1-77" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-78"><a href="#cb1-78" aria-hidden="true" tabindex="-1"></a>The same degrees of freedom can also be used in the small-sample correction for the bias of $d$, as given by </span>
<span id="cb1-79"><a href="#cb1-79" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-80"><a href="#cb1-80" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-81"><a href="#cb1-81" aria-hidden="true" tabindex="-1"></a>g = J(\nu) \times d.</span>
<span id="cb1-82"><a href="#cb1-82" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-83"><a href="#cb1-83" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-84"><a href="#cb1-84" aria-hidden="true" tabindex="-1"></a>This small-sample correction is based on a Satterthwaite-type approximation to the distribution of $d$. </span>
<span id="cb1-85"><a href="#cb1-85" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-86"><a href="#cb1-86" aria-hidden="true" tabindex="-1"></a>Here's another way to express the variance estimator for $d$: </span>
<span id="cb1-87"><a href="#cb1-87" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-88"><a href="#cb1-88" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-89"><a href="#cb1-89" aria-hidden="true" tabindex="-1"></a>V_d = d^2 \left(\frac{1}{t^2} + \frac{1}{2 \nu}\right),</span>
<span id="cb1-90"><a href="#cb1-90" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-91"><a href="#cb1-91" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-92"><a href="#cb1-92" aria-hidden="true" tabindex="-1"></a>where $t$ is the test statistic corresponding to the hypothesis test for no difference between groups. I've never seen that formula in print before, but it could be convenient if an article reports the $t$ statistic (or $F = t^2$ statistic).</span>
<span id="cb1-93"><a href="#cb1-93" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-94"><a href="#cb1-94" aria-hidden="true" tabindex="-1"></a><span class="fu">### Non-standard estimators of $d$</span></span>
<span id="cb1-95"><a href="#cb1-95" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-96"><a href="#cb1-96" aria-hidden="true" tabindex="-1"></a>The advantage of this formulation of $d$, $g$, and $V_d$ is that it can be applied in quite a wide variety of circumstances, including cases that aren't usually covered in textbook treatments. Rather than having to use separate formulas for every combination of design and analytic approach under the sun, the same formulas apply throughout. What changes are the components of the formulas: the scaled standard error $se_b / S$ and the degrees of freedom $\nu$. The general formulation also makes it easier to swap in different estimates of $b$ or $S$---i.e., if you estimate the numerator a different way but keep the denominator the same, you'll need a new scaled standard error but can still use the same degrees of freedom. A bunch of examples:</span>
<span id="cb1-97"><a href="#cb1-97" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-98"><a href="#cb1-98" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Independent groups with different variances</span></span>
<span id="cb1-99"><a href="#cb1-99" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-100"><a href="#cb1-100" aria-hidden="true" tabindex="-1"></a>Suppose that we're looking at two independent groups but do not want to assume that their variances are the same. In this case, it would make sense to standardize the difference in means by the control group standard deviation (without pooling), so that $d = \left(\bar{y}_T - \bar{y}_C\right) / s_C$. Since $s_C^2$ has $\nu = n_C - 1$ degrees of freedom, the small-sample bias correction will then need to be $J(n_C - 1)$. The scaled standard error will be</span>
<span id="cb1-101"><a href="#cb1-101" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-102"><a href="#cb1-102" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-103"><a href="#cb1-103" aria-hidden="true" tabindex="-1"></a>\frac{se_b}{s_C} = \sqrt{\frac{s_T^2}{s_C^2 n_T} + \frac{1}{n_C}}.</span>
<span id="cb1-104"><a href="#cb1-104" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-105"><a href="#cb1-105" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-106"><a href="#cb1-106" aria-hidden="true" tabindex="-1"></a>This is then everything that we need to calculate $V_d$, $g$, $V_g$, etc.</span>
<span id="cb1-107"><a href="#cb1-107" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-108"><a href="#cb1-108" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Multiple independent groups</span></span>
<span id="cb1-109"><a href="#cb1-109" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-110"><a href="#cb1-110" aria-hidden="true" tabindex="-1"></a>Suppose that the study involves $K - 1$ treatment groups, 1 control group, and $N$ total participants. If the meta-analysis will include SMDs comparing _each_ treatment group to the control group, it would make sense to pool the sample variance across all $K$ groups rather than just the pair of groups, so that a common estimate of scale is used across all the effect sizes. The pooled standard deviation is then calculated as </span>
<span id="cb1-111"><a href="#cb1-111" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-112"><a href="#cb1-112" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-113"><a href="#cb1-113" aria-hidden="true" tabindex="-1"></a>s_p^2 = \frac{1}{N - K} \sum_{k=0}^K (n_k - 1) s_k^2.</span>
<span id="cb1-114"><a href="#cb1-114" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-115"><a href="#cb1-115" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-116"><a href="#cb1-116" aria-hidden="true" tabindex="-1"></a>For a comparison between treatment group $k$ and the control group, we would then use </span>
<span id="cb1-117"><a href="#cb1-117" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-118"><a href="#cb1-118" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-119"><a href="#cb1-119" aria-hidden="true" tabindex="-1"></a>d = \frac{\bar{y}_k - \bar{y}_C}{s_p}, \qquad \nu = N - K, \qquad \frac{se_b}{s_p} = \sqrt{\frac{1}{n_C} + \frac{1}{n_k}},</span>
<span id="cb1-120"><a href="#cb1-120" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-121"><a href="#cb1-121" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-122"><a href="#cb1-122" aria-hidden="true" tabindex="-1"></a>where $n_k$ is the sample size for treatment group $k$ (cf. Gleser &amp; Olkin, 2009). </span>
<span id="cb1-123"><a href="#cb1-123" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-124"><a href="#cb1-124" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Single group, pre-test post-test design </span></span>
<span id="cb1-125"><a href="#cb1-125" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-126"><a href="#cb1-126" aria-hidden="true" tabindex="-1"></a>Suppose that a study involves taking pre-test and post-test measurements on a single group of $n$ participants. Borenstein (2009) recommends calculating the standardized mean difference for this study as the difference in means between the post-test and pre-test, scaled by the pooled (across pre- and post-test measurements) standard deviation. With obvious notation:</span>
<span id="cb1-127"><a href="#cb1-127" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-128"><a href="#cb1-128" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-129"><a href="#cb1-129" aria-hidden="true" tabindex="-1"></a>d = \frac{\bar{y}_{post} - \bar{y}_{pre}}{s_p}, \qquad \text{where} \qquad s_p^2 = \frac{1}{2}\left(s_{pre}^2 + s_{post}^2\right).</span>
<span id="cb1-130"><a href="#cb1-130" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-131"><a href="#cb1-131" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-132"><a href="#cb1-132" aria-hidden="true" tabindex="-1"></a>In this design,</span>
<span id="cb1-133"><a href="#cb1-133" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-134"><a href="#cb1-134" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-135"><a href="#cb1-135" aria-hidden="true" tabindex="-1"></a>\frac{se_b}{s_p} = \sqrt{\frac{2(1 - r)}{n}},</span>
<span id="cb1-136"><a href="#cb1-136" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-137"><a href="#cb1-137" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-138"><a href="#cb1-138" aria-hidden="true" tabindex="-1"></a>where $r$ is the sample correlation between the pre- and post-tests. The remaining question is what to use for $\nu$. Borenstein (2009) uses $\nu = n - 1$. My previous post <span class="co">[</span><span class="ot">on the sampling covariance of sample variances</span><span class="co">](/posts/distribution-of-sample-variances/)</span> gave the result that $\text{Var}(s_p^2) = \sigma^4 (1 + \rho^2) / (n - 1)$, which would instead suggest using </span>
<span id="cb1-139"><a href="#cb1-139" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-140"><a href="#cb1-140" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-141"><a href="#cb1-141" aria-hidden="true" tabindex="-1"></a>\nu = \frac{2 (n - 1)}{1 + r^2}. </span>
<span id="cb1-142"><a href="#cb1-142" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-143"><a href="#cb1-143" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-144"><a href="#cb1-144" aria-hidden="true" tabindex="-1"></a>This formula will tend to give slightly larger degrees of freedom, but probably won't be that discrepant from Borenstein's approach except in quite small samples. It would be interesting to investigate which approach is better in small samples (i.e., leading to less biased estimates of the SMD and more accurate estimates of sampling variance, and by how much), although its possible than neither is all that good because the variance estimator itself is based on a large-sample approximation.</span>
<span id="cb1-145"><a href="#cb1-145" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-146"><a href="#cb1-146" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Two group, pre-test post-test design: ANCOVA estimation</span></span>
<span id="cb1-147"><a href="#cb1-147" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-148"><a href="#cb1-148" aria-hidden="true" tabindex="-1"></a>Suppose that a study involves taking pre-test and post-test measurements on two groups of participants, with sample sizes $n_T$ and $n_C$ respectively. One way to analyze this design is via ANCOVA using the pre-test measure as the covariate, so that the treatment effect estimate is the difference in adjusted post-test means. In this design, the scaled standard error will be approximately</span>
<span id="cb1-149"><a href="#cb1-149" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-150"><a href="#cb1-150" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-151"><a href="#cb1-151" aria-hidden="true" tabindex="-1"></a>\frac{se_b}{S} = \sqrt{ \frac{(n_C + n_T)(1 - r^2)}{n_C n_T} },</span>
<span id="cb1-152"><a href="#cb1-152" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-153"><a href="#cb1-153" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-154"><a href="#cb1-154" aria-hidden="true" tabindex="-1"></a>where $r$ is the pooled, within-group sample correlation between the pre-test and the post-test measures (this approximation assumes that the pre-test SMD between groups is relatively small). Alternately, if $se_b$ is provided then the scaled standard error could be calculated directly.</span>
<span id="cb1-155"><a href="#cb1-155" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-156"><a href="#cb1-156" aria-hidden="true" tabindex="-1"></a>Borenstein (2009) suggests calculating $d$ as the difference in adjusted means, scaled by the pooled sample variances on the post-test measures. The post-test pooled sample variance will have the same degrees of freedom as in the two-sample t-test case: $\nu = n_C + n_T - 2$. (Borenstein instead uses $\nu = n_C + n_T - 2 - q$, where $q$ is the number of covariates in the analysis, but this won't usually make much difference unless the total sample size is quite small.)</span>
<span id="cb1-157"><a href="#cb1-157" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-158"><a href="#cb1-158" aria-hidden="true" tabindex="-1"></a>Scaling by the pooled post-test sample variance isn't the only reasonable way to estimate the SMD though. If the covariate is a true pre-test, then why not scale by the pooled pre-test sample variance instead? To do so, you would need to calculate $se_b / S$ directly and use $\nu = n_C + n_T - 2$. If it is reasonable to assume that the pre- and post-test population variances are equal, then another alternative would be to pool across the pre-test _and_ post-test sample variances in each group. Using this approach, you would again need to calculate $se_b / S$ directly and then use $\nu = 2(n_C + n_T - 2) / (1 + r^2)$.</span>
<span id="cb1-159"><a href="#cb1-159" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-160"><a href="#cb1-160" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Two group, pre-test post-test design: repeated measures estimation</span></span>
<span id="cb1-161"><a href="#cb1-161" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-162"><a href="#cb1-162" aria-hidden="true" tabindex="-1"></a>Another way to analyze the data from the same type of study design is to use repeated measures ANOVA. I've recently encountered a number of studies that use this approach (here's a recent example from <span class="co">[</span><span class="ot">a highly publicized study in PLOS ONE</span><span class="co">](http://dx.doi.org/10.1371/journal.pone.0154075)</span>---see Table 2). The studies I've seen typically report the sample means and variances in each group and at each time point, from which the difference in change scores can be calculated. Let $\bar{y}_{gt}$ and $s_{gt}^2$ denote the sample mean and sample variance in group $g = T, C$ at time $t = 0, 1$. The numerator of $d$ would then be calculated as </span>
<span id="cb1-163"><a href="#cb1-163" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-164"><a href="#cb1-164" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-165"><a href="#cb1-165" aria-hidden="true" tabindex="-1"></a>b = \left(\bar{y}_{T1} - \bar{y}_{T0}\right) - \left(\bar{y}_{C1} - \bar{y}_{C0}\right),</span>
<span id="cb1-166"><a href="#cb1-166" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-167"><a href="#cb1-167" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-168"><a href="#cb1-168" aria-hidden="true" tabindex="-1"></a>which has sampling variance $\text{Var}(b) = 2(1 - \rho)\sigma^2\left(n_C + n_T \right) / (n_C n_T)$, where $\rho$ is the correlation between the pre-test and the post-test measures. Thus, the scaled standard error is</span>
<span id="cb1-169"><a href="#cb1-169" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-170"><a href="#cb1-170" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-171"><a href="#cb1-171" aria-hidden="true" tabindex="-1"></a>\frac{se_b}{S} = \sqrt{\frac{2(1 - r)(n_C + n_T)}{n_C n_T}}.</span>
<span id="cb1-172"><a href="#cb1-172" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-173"><a href="#cb1-173" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-174"><a href="#cb1-174" aria-hidden="true" tabindex="-1"></a>As with ANCOVA, there are several potential options for calculating the denominator of $d$:</span>
<span id="cb1-175"><a href="#cb1-175" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-176"><a href="#cb1-176" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>Using the pooled sample variances on the post-test measures, with $\nu = n_C + n_T - 2$;</span>
<span id="cb1-177"><a href="#cb1-177" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>Using the pooled sample variances on the pre-test measures, with $\nu = n_C + n_T - 2$; or</span>
<span id="cb1-178"><a href="#cb1-178" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>Using the pooled sample variances at both time points and in both groups, i.e., </span>
<span id="cb1-179"><a href="#cb1-179" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-180"><a href="#cb1-180" aria-hidden="true" tabindex="-1"></a>    $$</span>
<span id="cb1-181"><a href="#cb1-181" aria-hidden="true" tabindex="-1"></a>    S^2 = \frac{(n_C - 1)(s_{C0}^2 + s_{C1}^2) + (n_T - 1)(s_{T0}^2 + s_{T1}^2)}{2(n_C + n_T - 2)},</span>
<span id="cb1-182"><a href="#cb1-182" aria-hidden="true" tabindex="-1"></a>    $$</span>
<span id="cb1-183"><a href="#cb1-183" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb1-184"><a href="#cb1-184" aria-hidden="true" tabindex="-1"></a>    with $\nu = 2(n_C + n_T - 2) / (1 + r^2)$.</span>
<span id="cb1-185"><a href="#cb1-185" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-186"><a href="#cb1-186" aria-hidden="true" tabindex="-1"></a>The range of approaches to scaling is the same as for ANCOVA. This makes sense because both analyses are based on data from the same study design, so the parameter of interest should be the same (i.e., the target parameter should not change based on the analytic method). Note that all of these approaches are a bit different than the effect size estimator proposed by <span class="co">[</span><span class="ot">Morris and DeShon (2002)</span><span class="co">](http://doi.org/10.1037//1082-989X.7.1.105)</span> for the two-group, pre-post design; their approach does not fit into my framework because it involves taking a difference between standardized effect sizes (and therefore involves two separate estimates of scale, rather than just one). </span>
<span id="cb1-187"><a href="#cb1-187" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-188"><a href="#cb1-188" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Randomized trial with longitudinal follow-up</span></span>
<span id="cb1-189"><a href="#cb1-189" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-190"><a href="#cb1-190" aria-hidden="true" tabindex="-1"></a>Many independent-groups designs---especially randomized trials in field settings---involve repeated, longitudinal follow-up assessments. An increasingly common approach to analysis of such data is through hierarchical linear models, which can be used to account for the dependence structure among measurements taken on the same individual. In this setting, <span class="co">[</span><span class="ot">Feingold (2009)</span><span class="co">](http://doi.org/10.1037/a0014699)</span> proposes that the SMD be calculated as the model-based estimate of the treatment effect at the final follow-up time, scaled by the within-groups variance of the outcome at that time point. Let $\hat\beta_1$ denote the estimated difference in slopes (change per unit time) between groups in a linear growth model, $F$ denote the duration of the study, and $s_{pF}^2$ denote the pooled sample variance of the outcome at the final time point. For this model, Feingold (2009) proposes to calculate the standardized mean difference as </span>
<span id="cb1-191"><a href="#cb1-191" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-192"><a href="#cb1-192" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-193"><a href="#cb1-193" aria-hidden="true" tabindex="-1"></a>d = \frac{F \hat\beta_1}{s_{pF}}.</span>
<span id="cb1-194"><a href="#cb1-194" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-195"><a href="#cb1-195" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-196"><a href="#cb1-196" aria-hidden="true" tabindex="-1"></a>In a later paper, <span class="co">[</span><span class="ot">Feingold (2015)</span><span class="co">](http://doi.org/10.1037/a0037721)</span> proposes that the sampling variance of $d$ be estimated as $F \times se_{\hat\beta_1} / s_{pF}$, where $se_{\hat\beta_1}$ is the standard error of the estimated slope. My framework suggests that a better estimate of the sampling variance, which accounts for the uncertainty of the scale estimate, would be to use </span>
<span id="cb1-197"><a href="#cb1-197" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-198"><a href="#cb1-198" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-199"><a href="#cb1-199" aria-hidden="true" tabindex="-1"></a>V_d = \left(\frac{F \times se_{\hat\beta_1}}{s_{pF}}\right)^2 + \frac{d^2}{2 \nu},</span>
<span id="cb1-200"><a href="#cb1-200" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-201"><a href="#cb1-201" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-202"><a href="#cb1-202" aria-hidden="true" tabindex="-1"></a>with $\nu = n_T + n_C - 2$. The same $\nu$ could be used to bias-correct the effect size estimate. </span>
<span id="cb1-203"><a href="#cb1-203" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-204"><a href="#cb1-204" aria-hidden="true" tabindex="-1"></a>If estimates of the variance components of the HLM are reported, one could use them to construct a model-based estimate of the scale parameter in the denominator of $d$. I explored this approach in a paper that uses HLM to model single-case designs, which are a certain type of longitudinal experiment that typically involve a very small number of participants (<span class="co">[</span><span class="ot">Pustejovsky, Hedges, &amp; Shadish, 2014</span><span class="co">](http://doi.org/10.3102/1076998614547577)</span>). Estimates of the scale parameter can usually be written as </span>
<span id="cb1-205"><a href="#cb1-205" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-206"><a href="#cb1-206" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-207"><a href="#cb1-207" aria-hidden="true" tabindex="-1"></a>S_{model}^2 = \mathbf{r}'\boldsymbol\omega,</span>
<span id="cb1-208"><a href="#cb1-208" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-209"><a href="#cb1-209" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-210"><a href="#cb1-210" aria-hidden="true" tabindex="-1"></a>where $\boldsymbol\omega$ is a vector of all the variance components in the model and $\mathbf{r}$ is a vector of weights that depend on the model specification and length of follow-up. This estimate of scale will usually be more precise than $s_{pF}^2$ because it makes use of all of the data (and modeling assumptions). However, it can be challenging to determine appropriate degrees of freedom for $S_{model}^2$. For single-case designs, I used estimates of $\text{Var}(\boldsymbol\omega)$ based on the inverse of the expected information matrix---call the estimate $\mathbf{V}_{\boldsymbol\omega}$---in which case</span>
<span id="cb1-211"><a href="#cb1-211" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-212"><a href="#cb1-212" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-213"><a href="#cb1-213" aria-hidden="true" tabindex="-1"></a>\nu = \frac{2 S_{model}^4}{\mathbf{r}' \mathbf{V}_{\boldsymbol\omega} \mathbf{r}}.</span>
<span id="cb1-214"><a href="#cb1-214" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-215"><a href="#cb1-215" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-216"><a href="#cb1-216" aria-hidden="true" tabindex="-1"></a>However, most published articles will not provide estimates of the sampling variances of the variance components---in fact, a lot of software for estimating HLMs does not even provide these. It would be useful to work out some reasonable approximations for the degrees of freedom in these models---approximations that can be calculated based on the information that's typically available---and to investigate the extent to which there's any practical benefit to using $S_{model}^2$ over $s_{pF}^2$. </span>
<span id="cb1-217"><a href="#cb1-217" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-218"><a href="#cb1-218" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Cluster-randomized trials</span></span>
<span id="cb1-219"><a href="#cb1-219" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-220"><a href="#cb1-220" aria-hidden="true" tabindex="-1"></a><span class="co">[</span><span class="ot">Hedges (2007)</span><span class="co">](http://doi.org/10.3102/1076998606298043)</span> addresses estimation of standardized mean differences for cluster-randomized trials, in which the units of measurement are nested within higher-level clusters that comprise the units of randomization. Such designs involve two variance components (within- and between-cluster variance), and thus there are three potential approaches to scaling the treatment effect: standardize by the total variance (i.e., the sum of the within- and between-cluster components), standardize by the within-cluster variance, or standardize by the between-cluster variance. Furthermore, some of the effect sizes can be estimated in several different ways, each with a different sampling variance. <span class="co">[</span><span class="ot">Hedges (2007)</span><span class="co">](http://doi.org/10.3102/1076998606298043)</span> gives sampling variance estimates for each estimator of each effect size, but they all follow the same general formula as given above. (The appendix of the article actually gives the same formula as above, but using a more abstract formulation.) </span>
<span id="cb1-221"><a href="#cb1-221" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-222"><a href="#cb1-222" aria-hidden="true" tabindex="-1"></a>For example, suppose the target SMD parameter uses the total variance and that we have data from a two-level, two-arm cluster randomized trial with $M$ clusters, $n$ observations per cluster, and total sample sizes in each arm of $N_T$ and $N_C$, respectively. Let $\tau^2$ be the between-cluster variance, $\sigma^2$ be the within-cluster variance, and $\rho = \tau^2 / (\tau^2 + \sigma^2)$. The target parameter is $\delta = \left(\mu_T - \mu_C\right) / \left(\tau^2 + \sigma^2\right)$. The article assumes that the treatment effect will be estimated by the difference in grand means, $\bar{\bar{y}}_T - \bar{\bar{y}}_C$. Letting $S_B^2$ be the pooled sample variance of the cluster means within each arm and $S_W^2$ be the pooled within-cluster sample variance, the total variance is estimated as </span>
<span id="cb1-223"><a href="#cb1-223" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-224"><a href="#cb1-224" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-225"><a href="#cb1-225" aria-hidden="true" tabindex="-1"></a>S_{total}^2 = S_B^2 + \frac{n - 1}{n} S_W^2. </span>
<span id="cb1-226"><a href="#cb1-226" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-227"><a href="#cb1-227" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-228"><a href="#cb1-228" aria-hidden="true" tabindex="-1"></a>An estimate of the SMD is then </span>
<span id="cb1-229"><a href="#cb1-229" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-230"><a href="#cb1-230" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-231"><a href="#cb1-231" aria-hidden="true" tabindex="-1"></a>d = \left(\bar{\bar{y}}_T - \bar{\bar{y}}_C \right) / \sqrt{S_{total}^2}. </span>
<span id="cb1-232"><a href="#cb1-232" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-233"><a href="#cb1-233" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-234"><a href="#cb1-234" aria-hidden="true" tabindex="-1"></a>The scaled standard error of $\bar{\bar{y}}_T - \bar{\bar{y}}_C$ is </span>
<span id="cb1-235"><a href="#cb1-235" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-236"><a href="#cb1-236" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-237"><a href="#cb1-237" aria-hidden="true" tabindex="-1"></a>se_b = \sqrt{\left(\frac{N_C + N_T}{N_C N_T}\right)\left<span class="co">[</span><span class="ot">1 + (n - 1)\rho\right</span><span class="co">]</span>}.</span>
<span id="cb1-238"><a href="#cb1-238" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-239"><a href="#cb1-239" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-240"><a href="#cb1-240" aria-hidden="true" tabindex="-1"></a>The appendix of the article demonstrates that $\text{E}\left(S_{total}^2\right) = \tau^2 + \sigma^2$ and </span>
<span id="cb1-241"><a href="#cb1-241" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-242"><a href="#cb1-242" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-243"><a href="#cb1-243" aria-hidden="true" tabindex="-1"></a>\text{Var}\left( S_{total}^2 \right) = \frac{2}{n^2}\left(\frac{(n \tau^2 + \sigma^2)^2}{M - 2} + \frac{(n - 1)^2 \sigma^4}{N_C + N_T - M}\right),</span>
<span id="cb1-244"><a href="#cb1-244" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-245"><a href="#cb1-245" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-246"><a href="#cb1-246" aria-hidden="true" tabindex="-1"></a>by which it follows that </span>
<span id="cb1-247"><a href="#cb1-247" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-248"><a href="#cb1-248" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-249"><a href="#cb1-249" aria-hidden="true" tabindex="-1"></a>\nu = \frac{n^2 M (M - 2)}{M<span class="co">[</span><span class="ot">(n - 1)\rho + 1</span><span class="co">]</span>^2 + (M - 2)(n - 1)(1 - \rho)^2}.</span>
<span id="cb1-250"><a href="#cb1-250" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-251"><a href="#cb1-251" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-252"><a href="#cb1-252" aria-hidden="true" tabindex="-1"></a>Substituting $se_b / S_{total}$ and $\nu$ into the formula for $V_d$ gives the same as Expression (14) in the article. </span>
<span id="cb1-253"><a href="#cb1-253" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-254"><a href="#cb1-254" aria-hidden="true" tabindex="-1"></a>A limitation of <span class="co">[</span><span class="ot">Hedges (2007)</span><span class="co">](http://doi.org/10.3102/1076998606298043)</span> is that it only covers the case where the treatment effect is estimated by the difference in grand means (although it does cover the case of unequal cluster sizes, which gets quite messy). In practice, every cluster-randomized trial I've ever seen uses baseline covariates to adjust the mean difference (often based on a hierarchical linear model) and improve the precision of the treatment effect estimate. The SMD estimate should also be based on this covariate-adjustment estimate, scaled by the total variance _without adjusting for the covariate_. An advantage of the general formulation given above is that its clear how to estimate the sampling variance of $d$. I would guess that it will often be possible to calculate the scaled standard error directly, given the standard error of the covariate-adjusted treatment effect estimate. And since $S_{total}$ would be estimated just as before, its degrees of freedom remain the same. </span>
<span id="cb1-255"><a href="#cb1-255" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-256"><a href="#cb1-256" aria-hidden="true" tabindex="-1"></a><span class="co">[</span><span class="ot">Hedges (2011)</span><span class="co">](http://doi.org/10.3102/1076998610376617)</span> discusses estimation of SMDs in three-level cluster-randomized trials---an even more complicated case. However, the general approach is the same; all that's needed are the scaled standard error and the degrees of freedom $\nu$ of whatever combination of variance components go into the denominator of the effect size. In both the two-level and three-level cases, the degrees of freedom get quite complicated in unbalanced samples and are probably not calculable from the information that is usually provided in an article. Hedges (2007, 2011) comments on a couple of cases where more tractable approximations can be used, although it seems like there might be room for further investigation here. </span>
<span id="cb1-257"><a href="#cb1-257" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-258"><a href="#cb1-258" aria-hidden="true" tabindex="-1"></a><span class="fu">### Closing thoughts</span></span>
<span id="cb1-259"><a href="#cb1-259" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-260"><a href="#cb1-260" aria-hidden="true" tabindex="-1"></a>I think this framework is useful in that it unifies a large number of cases that have been treated separately, and can also be applied (more-or-less immediately) to $d$ estimators that haven't been widely considered before, such as the $d$ that involves scaling by the pooled pre-and-post, treatment-and-control sample variance. I hope it also illustrates that, while the point estimator $d$ can be applied across a large number of study designs, the sampling variance of $d$ depends on the details of the design and estimation methods. The same is true for other families of effect sizes as well. For example, in other work I've demonstrated that the sampling variance of the correlation coefficient depends on the design from which the correlations are estimated (<span class="co">[</span><span class="ot">Pustejovsky, 2014</span><span class="co">](http://doi.org/10.1037/a0033788)</span>). </span>
<span id="cb1-261"><a href="#cb1-261" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-262"><a href="#cb1-262" aria-hidden="true" tabindex="-1"></a>If you have read this far, I'd love to get your feedback about whether you think this is a useful way to organize the calculations of $d$ estimators. Is this helpful? Or nothing you didn't already know? Or still more complicated than it should be? Leave a comment!</span>
<span id="cb1-263"><a href="#cb1-263" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-264"><a href="#cb1-264" aria-hidden="true" tabindex="-1"></a><span class="fu">### References</span></span>
<span id="cb1-265"><a href="#cb1-265" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-266"><a href="#cb1-266" aria-hidden="true" tabindex="-1"></a>Borenstein, M. (2009). Effect sizes for continuous data. In H. M. Cooper, L. V Hedges, &amp; J. C. Valentine (Eds.), The Handbook of Research Synthesis and Meta-Analysis (pp. 221–236). New York, NY: Russell Sage Foundation.</span>
<span id="cb1-267"><a href="#cb1-267" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-268"><a href="#cb1-268" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-269"><a href="#cb1-269" aria-hidden="true" tabindex="-1"></a>Feingold, A. (2009). Effect sizes for growth-modeling analysis for controlled clinical trials in the same metric as for classical analysis. Psychological Methods, 14(1), 43–53. doi:10.1037/a0014699</span>
<span id="cb1-270"><a href="#cb1-270" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-271"><a href="#cb1-271" aria-hidden="true" tabindex="-1"></a>Feingold, A. (2015). Confidence interval estimation for standardized effect sizes in multilevel and latent growth modeling. Journal of Consulting and Clinical Psychology, 83(1), 157–168. doi:10.1037/a0037721</span>
<span id="cb1-272"><a href="#cb1-272" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-273"><a href="#cb1-273" aria-hidden="true" tabindex="-1"></a>Gleser, L. J., &amp; Olkin, I. (2009). Stochastically dependent effect sizes. In H. Cooper, L. V. Hedges, &amp; J. C. Valentine (Eds.), The Handbook of Research Synthesis and Meta-Analysis (2nd ed., pp. 357–376). New York, NY: Russell Sage Foundation.</span>
<span id="cb1-274"><a href="#cb1-274" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-275"><a href="#cb1-275" aria-hidden="true" tabindex="-1"></a>Hedges, L. V. (2007). Effect sizes in cluster-randomized designs. Journal of Educational and Behavioral Statistics, 32(4), 341–370. doi:10.3102/1076998606298043</span>
<span id="cb1-276"><a href="#cb1-276" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-277"><a href="#cb1-277" aria-hidden="true" tabindex="-1"></a>Hedges, L. V. (2011). Effect sizes in three-level cluster-randomized experiments. Journal of Educational and Behavioral Statistics, 36(3), 346–380. doi:10.3102/1076998610376617</span>
<span id="cb1-278"><a href="#cb1-278" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-279"><a href="#cb1-279" aria-hidden="true" tabindex="-1"></a>Morris, S. B., &amp; DeShon, R. P. (2002). Combining effect size estimates in meta-analysis with repeated measures and independent-groups designs. Psychological Methods, 7(1), 105–125. doi:10.1037//1082-989X.7.1.105</span>
<span id="cb1-280"><a href="#cb1-280" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-281"><a href="#cb1-281" aria-hidden="true" tabindex="-1"></a>Pustejovsky, J. E. (2014). Converting from d to r to z when the design uses extreme groups, dichotomization, or experimental control. Psychological Methods, 19(1), 92–112. doi:10.1037/a0033788</span>
<span id="cb1-282"><a href="#cb1-282" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-283"><a href="#cb1-283" aria-hidden="true" tabindex="-1"></a>Pustejovsky, J. E., Hedges, L. V, &amp; Shadish, W. R. (2014). Design-comparable effect sizes in multiple baseline designs: A general modeling framework. Journal of Educational and Behavioral Statistics, 39(5), 368–393. doi:10.3102/1076998614547577</span>
<span id="cb1-284"><a href="#cb1-284" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-285"><a href="#cb1-285" aria-hidden="true" tabindex="-1"></a>Viechtbauer, W. (2007). Approximate confidence intervals for standardized effect sizes in the two-independent and two-dependent samples design. Journal of Educational and Behavioral Statistics, 32(1), 39–60. doi:10.3102/1076998606298034</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div></div></div></div></div>
</div> <!-- /content -->




<script src="../../site_libs/quarto-html/zenscroll-min.js"></script>
</body></html>