@article{Ades2015simultaneous,
  title = {Simultaneous Synthesis of Treatment Effects and Mapping to a Common Scale: An Alternative to Standardisation},
  shorttitle = {Simultaneous Synthesis of Treatment Effects and Mapping to a Common Scale},
  author = {Ades, A. E. and Lu, Guobing and Dias, Sofia and Mayo-Wilson, Evan and Kounali, Daphne},
  date = {2015},
  journaltitle = {Research Synthesis Methods},
  volume = {6},
  number = {1},
  pages = {96--107},
  issn = {1759-2887},
  doi = {10.1002/jrsm.1130},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/jrsm.1130},
  urldate = {2025-03-27},
  abstract = {Objective Trials often may report several similar outcomes measured on different test instruments. We explored a method for synthesising treatment effect information both within and between trials and for reporting treatment effects on a common scale as an alternative to standardisation Study design We applied a procedure that simultaneously estimates a pooled treatment effect and the “mapping” ratios between the treatment effects on test instruments in a connected network. Standardised and non-standardised treatment effects were compared. The methods were illustrated in a dataset of 22 trials of selective serotonin reuptake inhibitors against placebo for social anxiety disorder, each reporting treatment effects on between one and six of a total nine test instruments. Results Ratios of treatment effects on different test instruments varied from trial to trial, with a coefficient of variation of 18\% (95\% credible interval 11–29\%). Standardised effect models fitted the data less well, and standardised treatment effects were estimated with less relative precision than non-standardised effects and with greater relative heterogeneity. Conclusion Simultaneous synthesis of treatment effects and mapping to a common scale make fewer assumptions than standardising by dividing effects by the sample standard deviation, allow results to be reported on a common scale, and deliver estimates with superior relative precision. © 2015 The Authors. Research Synthesis Methods published by John Wiley \& Sons, Ltd.},
  langid = {english},
  keywords = {evidence synthesis,mapping,multiple outcomes,social anxiety}
}


@article{borenstein2017basics,
  title = {Basics of Meta-Analysis: {{{\mkbibemph{I}}}} {\textsuperscript{2}} Is Not an Absolute Measure of Heterogeneity: {{{\mkbibemph{I}}}} {\textsuperscript{2}} Is Not an Absolute Measure of Heterogeneity},
  shorttitle = {Basics of Meta-Analysis},
  author = {Borenstein, Michael and Higgins, Julian P. T. and Hedges, Larry V. and Rothstein, Hannah R.},
  date = {2017-03},
  journaltitle = {Research Synthesis Methods},
  volume = {8},
  number = {1},
  pages = {5--18},
  issn = {17592879},
  doi = {10.1002/jrsm.1230},
  url = {http://doi.wiley.com/10.1002/jrsm.1230},
  urldate = {2019-05-28},
  langid = {english},
  file = {C:\Users\jamespustejovsky\Zotero\storage\9X69SRTF\Borenstein et al. (2017).pdf}
}


@article{Breiman2001statistical,
  title = {Statistical {{Modeling}}: {{The Two Cultures}} (with Comments and a Rejoinder by the Author)},
  shorttitle = {Statistical {{Modeling}}},
  author = {Breiman, Leo},
  date = {2001-08-01},
  journaltitle = {Statistical Science},
  shortjournal = {Statist. Sci.},
  volume = {16},
  number = {3},
  issn = {0883-4237},
  doi = {10.1214/ss/1009213726},
  url = {https://projecteuclid.org/journals/statistical-science/volume-16/issue-3/Statistical-Modeling--The-Two-Cultures-with-comments-and-a/10.1214/ss/1009213726.full},
  urldate = {2025-10-06},
  file = {C:\Users\jamespustejovsky\Zotero\storage\VQPJ4B48\Breiman - Statistical Modeling The Two Cultures.pdf}
}

@incollection{Cooper2009research,
  title = {Research synthesis as a scientific enterprise},
  booktitle = {The {{Handbook}} of {{Research Synthesis}} and {{Meta-Analysis}}},
  author = {Cooper, H. and Hedges, Larry V.},
  editor = {Cooper, H. and Hedges, Larry V. and Valentine, Jeffrey C.},
  date = {2009},
  edition = {2},
  pages = {3--14},
  publisher = {Russell Sage Foundation},
  location = {New York, NY}
}

@article{Crede2010class,
  title = {Class Attendance in College: A Meta-Analytic Review of the Relationship of Class Attendance With Grades and Student Characteristics},
  shorttitle = {Class {{Attendance}} in {{College}}},
  author = {Credé, Marcus and Roch, Sylvia G. and Kieszczynka, Urszula M.},
  date = {2010-06},
  journaltitle = {Review of Educational Research},
  shortjournal = {Review of Educational Research},
  volume = {80},
  number = {2},
  pages = {272--295},
  issn = {0034-6543, 1935-1046},
  doi = {10.3102/0034654310362998},
  url = {https://journals.sagepub.com/doi/10.3102/0034654310362998},
  urldate = {2025-10-07},
  abstract = {A meta-analysis of the relationship between class attendance in college and college grades reveals that attendance has strong relationships with both class grades ( k = 69, N = 21,195, ρ = .44) and GPA ( k = 33, N = 9,243, ρ = .41). These relationships make class attendance a better predictor of college grades than any other known predictor of academic performance, including scores on standardized admissions tests such as the SAT, high school GPA, study habits, and study skills. Results also show that class attendance explains large amounts of unique variance in college grades because of its relative independence from SAT scores and high school GPA and weak relationship with student characteristics such as conscientiousness and motivation. Mandatory attendance policies appear to have a small positive impact on average grades ( k = 3, N = 1,421, d = .21). Implications for theoretical frameworks of student academic performance and educational policy are discussed.},
  langid = {english}
}

@article{Cummings2011arguments,
  title = {Arguments for and against Standardized Mean Differences (Effect Sizes)},
  author = {Cummings, Peter},
  date = {2011-07},
  journaltitle = {Archives of Pediatrics \& Adolescent Medicine},
  shortjournal = {Arch Pediatr Adolesc Med},
  volume = {165},
  number = {7},
  eprint = {21727271},
  eprinttype = {pubmed},
  pages = {592--596},
  issn = {1538-3628},
  doi = {10.1001/archpediatrics.2011.97},
  langid = {english},
  keywords = {Causality,Effect Modifier Epidemiologic,Humans,Meta-Analysis as Topic,Statistics as Topic}
}

@article{Davies2024mapping,
  title = {Mapping between Measurement Scales in Meta-Analysis, with Application to Measures of Body Mass Index in Children},
  author = {Davies, Annabel L. and Ades, A. E. and Higgins, Julian P. T.},
  date = {2024},
  journaltitle = {Research Synthesis Methods},
  volume = {15},
  number = {6},
  pages = {1072--1093},
  issn = {1759-2887},
  doi = {10.1002/jrsm.1758},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/jrsm.1758},
  urldate = {2025-03-27},
  abstract = {Quantitative evidence synthesis methods aim to combine data from multiple medical trials to infer relative effects of different interventions. A challenge arises when trials report continuous outcomes on different measurement scales. To include all evidence in one coherent analysis, we require methods to “map” the outcomes onto a single scale. This is particularly challenging when trials report aggregate rather than individual data. We are motivated by a meta-analysis of interventions to prevent obesity in children. Trials report aggregate measurements of body mass index (BMI) either expressed as raw values or standardized for age and sex. We develop three methods for mapping between aggregate BMI data using known or estimated relationships between measurements on different scales at the individual level. The first is an analytical method based on the mathematical definitions of z-scores and percentiles. The other two approaches involve sampling individual participant data on which to perform the conversions. One method is a straightforward sampling routine, while the other involves optimization with respect to the reported outcomes. In contrast to the analytical approach, these methods also have wider applicability for mapping between any pair of measurement scales with known or estimable individual-level relationships. We verify and contrast our methods using simulation studies and trials from our data set which report outcomes on multiple scales. We find that all methods recreate mean values with reasonable accuracy, but for standard deviations, optimization outperforms the other methods. However, the optimization method is more likely to underestimate standard deviations and is vulnerable to non-convergence.},
  langid = {english},
  keywords = {body mass index,mapping,meta-analysis,optimization,sampling,standardization}
}

@article{Donoho2017fifty,
  title = {50 Years of Data Science},
  author = {Donoho, David},
  date = {2017-10-02},
  journaltitle = {Journal of Computational and Graphical Statistics},
  shortjournal = {Journal of Computational and Graphical Statistics},
  volume = {26},
  number = {4},
  pages = {745--766},
  issn = {1061-8600, 1537-2715},
  doi = {10.1080/10618600.2017.1384734},
  url = {https://www.tandfonline.com/doi/full/10.1080/10618600.2017.1384734},
  urldate = {2025-10-06},
  abstract = {More than 50 years ago, John Tukey called for a reformation of academic statistics. In “The Future of Data Analysis,” he pointed to the existence of an as-yet unrecognized science, whose subject of interest was learning from data, or “data analysis.” Ten to 20 years ago, John Chambers, Jeff Wu, Bill Cleveland, and Leo Breiman independently once again urged academic statistics to expand its boundaries beyond the classical domain of theoretical statistics; Chambers called for more emphasis on data preparation and presentation rather than statistical modeling; and Breiman called for emphasis on prediction rather than inference. Cleveland and Wu even suggested the catchy name “data science” for this envisioned field. A recent and growing phenomenon has been the emergence of “data science”programs at major universities, including UC Berkeley, NYU, MIT, and most prominently, the University of Michigan, which in September 2015 announced a \$100M “Data Science Initiative” that aims to hire 35 new faculty. Teaching in these new programs has significant overlap in curricular subject matter with traditional statistics courses; yet many academic statisticians perceive the new programs as “cultural appropriation.” This article reviews some ingredients of the current “data science moment,” including recent commentary about data science in the popular media, and about how/whether data science is really different from statistics. The now-contemplated field of data science amounts to a superset of the fields of statistics and machine learning, which adds some technology for “scaling up” to “big data.” This chosen superset is motivated by commercial rather than intellectual developments. Choosing in this way is likely to miss out on the really important intellectual event of the next 50 years. Because all of science itself will soon become data that can be mined, the imminent revolution in data science is not about mere “scaling up,” but instead the emergence of scientific studies of data analysis science-wide. In the future, we will be able to predict how a proposal to change data analysis workflows would impact the validity of data analysis across all of science, even predicting the impacts fieldby-field. Drawing on work by Tukey, Cleveland, Chambers, and Breiman, I present a vision of data science based on the activities of people who are “learning from data,” and I describe an academic field dedicated to improving that activity in an evidence-based manner. This new field is a better academic enlargement of statistics and machine learning than today’s data science initiatives, while being able to accommodate the same short-term goals. Based on a presentation at the Tukey Centennial Workshop, Princeton, NJ, September 18, 2015.},
  langid = {english},
  file = {C:\Users\jamespustejovsky\Zotero\storage\J3GHBM8Y\Donoho - 2017 - 50 Years of Data Science.pdf}
}

@article{Engels2000heterogeneity,
  title = {Heterogeneity and Statistical Significance in Meta-Analysis: An Empirical Study of 125 Meta-Analyses},
  shorttitle = {Heterogeneity and Statistical Significance in Meta-Analysis},
  author = {Engels, Eric A. and Schmid, Christopher H. and Terrin, Norma and Olkin, Ingram and Lau, Joseph},
  date = {2000-07-15},
  journaltitle = {Statistics in Medicine},
  shortjournal = {Statist. Med.},
  volume = {19},
  number = {13},
  pages = {1707--1728},
  issn = {0277-6715, 1097-0258},
  doi = {10.1002/1097-0258(20000715)19:13<1707::AID-SIM491>3.0.CO;2-P},
  url = {https://onlinelibrary.wiley.com/doi/10.1002/1097-0258(20000715)19:13<1707::AID-SIM491>3.0.CO;2-P},
  urldate = {2025-10-06},
  langid = {english}
}

@article{Fitzgerald2025using,
  title = {Using {{Extant Data}} to {{Improve Estimation}} of the {{Standardized Mean Difference}}},
  author = {Fitzgerald, Kaitlyn G. and Tipton, Elizabeth},
  date = {2025-02-01},
  journaltitle = {Journal of Educational and Behavioral Statistics},
  volume = {50},
  number = {1},
  pages = {128--148},
  publisher = {American Educational Research Association},
  issn = {1076-9986},
  doi = {10.3102/10769986241238478},
  url = {https://doi.org/10.3102/10769986241238478},
  urldate = {2025-05-25},
  abstract = {This article presents methods for using extant data to improve the properties of estimators of the standardized mean difference (SMD) effect size. Because samples recruited into education research studies are often more homogeneous than the populations of policy interest, the variation in educational outcomes can be smaller in these samples than is reflective of the true variation in the population. This affects effect size estimation since the sample standard deviation is used in the denominator of the SMD. We propose leveraging extant data on sample variance estimates from multiple studies, made available via clearinghouse databases such as the What Works Clearinghouse, to standardize a mean difference. This allows effect sizes to be benchmarked across a common and broad population, thus enabling better comparability across studies and interventions. We derive the new estimators of the population variance and the corresponding SMD, which pool sample variances from multiple studies using both an analysis of variance and a meta-analytic framework. We demonstrate the properties of these estimators via analytic and simulation results and offer recommendations for when these estimators are appropriate in practice.},
  langid = {english}
}

@article{Friedrich2011ratio,
  title = {Ratio of Means for Analyzing Continuous Outcomes in Meta-Analysis Performed as Well as Mean Difference Methods},
  author = {Friedrich, Jan O. and Adhikari, Neill K. J. and Beyene, Joseph},
  date = {2011-05},
  journaltitle = {Journal of Clinical Epidemiology},
  shortjournal = {J Clin Epidemiol},
  volume = {64},
  number = {5},
  eprint = {21447428},
  eprinttype = {pubmed},
  pages = {556--564},
  issn = {1878-5921},
  doi = {10.1016/j.jclinepi.2010.09.016},
  abstract = {OBJECTIVE: Meta-analyses of continuous outcomes typically use mean differences (MDs) or standardized mean differences (SMDs) (MD in pooled standard deviation units). Ratio of means (RoM) is an alternative effect measure that performs comparably in simulation. We compared treatment effects and heterogeneity for RoM, MD, and SMD using empiric data. STUDY DESIGN AND SETTING: From the Cochrane Database (2008, issue 1), we included systematic reviews reporting continuous outcomes, selected the meta-analysis with the most (and ≥five) trials, and calculated MD (where possible), SMD, and RoM. For each pair of effect measures, we compared P-values separately for treatment effect and heterogeneity and assessed asymmetry of discordant pairs (statistically significant result for only one of two measures). RESULTS: Two hundred thirty-two of 5,053 reviews were included. Measures demonstrated similar treatment effects, with ≤6\% discordant pairs and no asymmetry. A 0.5 SMD increase corresponded to 22 (95\% confidence interval: 19, 24)\% increase using RoM. There was less heterogeneity in RoM vs. MD (n=143, P=0.007), SMD vs. RoM (n=232, P=0.005), and SMD vs. MD (n=143, P=0.004). Comparing discordant pairs, fewer meta-analyses showed significant heterogeneity with SMD vs. RoM (P=0.04), consistent with the known bias of SMD. CONCLUSION: Empiric data from diverse meta-analyses demonstrate similar treatment effects and no large differences in heterogeneity of RoM compared with difference-based methods.},
  langid = {english},
  keywords = {Bias,Data Interpretation Statistical,Female,Humans,Male,Meta-Analysis as Topic,Outcome Assessment Health Care,Randomized Controlled Trials as Topic,Research Design,Review Literature as Topic}
}

@incollection{Hedges2019statistical,
  title = {Statistical Considerations},
  booktitle = {The {{Handbook}} of {{Research Synthesis}} and {{Meta-Analysis}}},
  author = {Hedges, Larry V.},
  editor = {Cooper, H. and Hedges, Larry V. and Valentine, Jeffrey C.},
  date = {2019},
  edition = {3},
  pages = {37--48},
  publisher = {Russell Sage Foundation},
  location = {New York, NY}
}

@article{Higgins2002quantifying,
  title = {Quantifying Heterogeneity in a Meta-Analysis.},
  author = {Higgins, Julian P. T. and Thompson, Simon G.},
  date = {2002},
  journaltitle = {Statistics in Medicine},
  volume = {21},
  number = {11},
  eprint = {12111919},
  eprinttype = {pubmed},
  pages = {1539--58},
  issn = {0277-6715},
  doi = {10.1002/sim.1186},
  abstract = {The extent of heterogeneity in a meta-analysis partly determines the difficulty in drawing overall conclusions. This extent may be measured by estimating a between-study variance, but interpretation is then specific to a particular treatment effect metric. A test for the existence of heterogeneity exists, but depends on the number of studies in the meta-analysis. We develop measures of the impact of heterogeneity on a meta-analysis, from mathematical criteria, that are independent of the number of studies and the treatment effect metric. We derive and propose three suitable statistics: H is the square root of the chi2 heterogeneity statistic divided by its degrees of freedom; R is the ratio of the standard error of the underlying mean from a random effects meta-analysis to the standard error of a fixed effect meta-analytic estimate, and I2 is a transformation of (H) that describes the proportion of total variation in study estimates that is due to heterogeneity. We discuss interpretation, interval estimates and other properties of these measures and examine them in five example data sets showing different amounts of heterogeneity. We conclude that H and I2, which can usually be calculated for published meta-analyses, are particularly useful summaries of the impact of heterogeneity. One or both should be presented in published meta-analyses in preference to the test for heterogeneity.},
  keywords = {Adjuvant,Adjuvant: methods,Albumins,Albumins: therapeutic use,Chemotherapy,Clinical Trials as Topic,Clinical Trials as Topic: methods,Cognition Disorders,Cognition Disorders: drug therapy,Cytidine Diphosphate Choline,Cytidine Diphosphate Choline: therapeutic use,Fibrosis,Fibrosis: therapy,Fracture Fixation,Fracture Fixation: methods,Hip Fractures,Hip Fractures: surgery,Humans,Meta-Analysis as Topic,Resuscitation,Resuscitation: methods,Sarcoma,Sarcoma: drug therapy,Sclerotherapy,Statistics as Topic,Statistics as Topic: methods}
}

@article{Higgins2009reevaluation,
  title = {A Re-Evaluation of Random-Effects Meta-Analysis},
  author = {Higgins, Julian P. T. and Thompson, Simon G. and Spiegelhalter, David J.},
  date = {2009-01},
  journaltitle = {Journal of the Royal Statistical Society: Series A (Statistics in Society)},
  volume = {172},
  number = {1},
  pages = {137--159},
  issn = {09641998, 1467985X},
  doi = {10.1111/j.1467-985X.2008.00552.x},
  url = {https://onlinelibrary.wiley.com/doi/10.1111/j.1467-985X.2008.00552.x},
  urldate = {2022-02-25},
  abstract = {Meta-analysis in the presence of unexplained heterogeneity is frequently undertaken by using a random-effects model, in which the effects underlying different studies are assumed to be drawn from a normal distribution. Here we discuss the justification and interpretation of such models, by addressing in turn the aims of estimation, prediction and hypothesis testing. A particular issue that we consider is the distinction between inference on the mean of the random-effects distribution and inference on the whole distribution. We suggest that random-effects meta-analyses as currently conducted often fail to provide the key results, and we investigate the extent to which distribution-free, classical and Bayesian approaches can provide satisfactory methods. We conclude that the Bayesian approach has the advantage of naturally allowing for full uncertainty, especially for prediction. However, it is not without problems, including computational intensity and sensitivity to a priori judgements. We propose a simple prediction interval for classical meta-analysis and offer extensions to standard practice of Bayesian meta-analysis, making use of an example of studies of ‘set shifting’ ability in people with eating disorders.},
  langid = {english},
  file = {C:\Users\jamespustejovsky\Zotero\storage\EEVJM9GI\Higgins et al. - 2009 - A re-evaluation of random-effects meta-analysis.pdf}
}

@article{Hopkins2024standardization,
  title = {Standardization and Other Approaches to Meta-Analyze Differences in Means},
  author = {Hopkins, Will G. and Rowlands, David S.},
  date = {2024},
  journaltitle = {Statistics in Medicine},
  volume = {43},
  number = {16},
  pages = {3092--3108},
  issn = {1097-0258},
  doi = {10.1002/sim.10114},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/sim.10114},
  urldate = {2025-04-24},
  abstract = {Meta-analysts often use standardized mean differences (SMD) to combine mean effects from studies in which the dependent variable has been measured with different instruments or scales. In this tutorial we show how the SMD is properly calculated as the difference in means divided by a between-subject reference-group, control-group, or pooled pre-intervention SD, usually free of measurement error. When combining mean effects from controlled trials and crossovers, most meta-analysts have divided by either the pooled SD of change scores, the pooled SD of post-intervention scores, or the pooled SD of pre- and post-intervention scores, resulting in SMDs that are biased and difficult to interpret. The frequent use of such inappropriate standardizing SDs by meta-analysts in three medical journals we surveyed is due to misleading advice in peer-reviewed publications and meta-analysis packages. Even with an appropriate standardizing SD, meta-analysis of SMDs increases heterogeneity artifactually via differences in the standardizing SD between settings. Furthermore, the usual magnitude thresholds for standardized mean effects are not thresholds for clinically important differences. We therefore explain how to use other approaches to combining mean effects of disparate measures: log transformation of factor effects (response ratios) and of percent effects converted to factors; rescaling of psychometrics to percent of maximum range; and rescaling with minimum clinically important differences. In the absence of clinically important differences, we explain how standardization after meta-analysis with appropriately transformed or rescaled pre-intervention SDs can be used to assess magnitudes of a meta-analyzed mean effect in different settings.},
  langid = {english},
  keywords = {bias,change-score SD,Cochrane,factor effect,meta-analysis,standardized mean difference}
}

@article{Lu2014simultaneous,
  title = {Simultaneous Multioutcome Synthesis and Mapping of Treatment Effects to a Common Scale},
  author = {Lu, Guobing and Kounali, Daphne and Ades, A. E.},
  date = {2014-03},
  journaltitle = {Value in Health: The Journal of the International Society for Pharmacoeconomics and Outcomes Research},
  shortjournal = {Value Health},
  volume = {17},
  number = {2},
  eprint = {24636388},
  eprinttype = {pubmed},
  pages = {280--287},
  issn = {1524-4733},
  doi = {10.1016/j.jval.2013.12.006},
  abstract = {OBJECTIVES: A new method is presented for both synthesizing treatment effects on multiple outcomes subject to measurement error and estimating coherent mapping coefficients between all outcomes. It can be applied to sets of trials reporting different combinations of patient- or clinician-reported outcomes, including both disease-specific measures and generic health-related quality-of-life measures. It is underpinned by a structural equation model that includes measurement error and latent common treatment effect factor. Treatment effects can be expressed on any of the test instruments that have been used. METHODS: This is illustrated in a synthesis of eight placebo-controlled trials of TNF-α inhibitors in ankylosing spondylitis, each reporting treatment effects on between two and five of a total six test instruments. RESULTS: The method has advantages over other methods for synthesis of multiple outcome data, including standardization and multivariate normal synthesis. Unlike standardization, it allows synthesis of treatment effect information from test instruments sensitive to different underlying constructs. It represents a special case of previously proposed multivariate normal models for evidence synthesis, but unlike the former, it also estimates mappings. Combining synthesis and mapping as a single operation makes more efficient use of available data than do current mapping methods and generates treatment effects that are consistent with the mappings. A limitation, however, is that it can only generate mappings to and from those instruments on which some trial data exist. CONCLUSIONS: The method should be assessed in a wide range of data sets on different clinical conditions, before it can be used routinely in health technology assessment.},
  langid = {english},
  pmcid = {PMC3991420},
  keywords = {Antirheumatic Agents,congeneric tests,cross-walking,Humans,mapping,Models Statistical,multioutcome synthesis,Multivariate Analysis,Outcome Assessment Health Care,Quality of Life,Randomized Controlled Trials as Topic,Spondylitis Ankylosing,Technology Assessment Biomedical,Tumor Necrosis Factor-alpha}
}

@article{Panagiotou2015commentary,
  title = {Commentary: On Effect Measures, Heterogeneity, and the Laws of Nature},
  shorttitle = {Commentary},
  author = {Panagiotou, Orestis A. and Trikalinos, Thomas A.},
  date = {2015-09},
  journaltitle = {Epidemiology},
  volume = {26},
  number = {5},
  pages = {710},
  issn = {1044-3983},
  doi = {10.1097/EDE.0000000000000359},
  url = {https://journals.lww.com/epidem/fulltext/2015/09000/commentary__on_effect_measures,_heterogeneity,_and.13.aspx},
  urldate = {2025-07-17},
  abstract = {An abstract is unavailable.},
  langid = {american},
  keywords = {Causality,Humans,Meta-Analysis as Topic,Models Statistical}
}

@article{Poole2015risk,
  title = {Is the Risk Difference Really a More Heterogeneous Measure?},
  author = {Poole, Charlie and Shrier, Ian and VanderWeele, Tyler J.},
  date = {2015-09},
  journaltitle = {Epidemiology},
  volume = {26},
  number = {5},
  pages = {714},
  issn = {1044-3983},
  doi = {10.1097/EDE.0000000000000354},
  url = {https://journals.lww.com/epidem/fulltext/2015/09000/is_the_risk_difference_really_a_more_heterogeneous.14.aspx},
  urldate = {2025-08-06},
  abstract = {There are claims in the literature that the risk difference is a more heterogeneous measure than the odds ratio or risk ratio. These claims are based on surveys of meta-analyses showing that tests reject the null hypothesis of homogeneity more often for the risk difference than for the ratio measures. Discussions of this point have neglected the fact that homogeneity tests can have different levels of statistical power (i.e., different probabilities of rejecting the null when it is false) across different scales. We give hypothetical examples in which there is arguably equal heterogeneity across risk difference and odds ratio measures but in which the risk difference homogeneity test rejects more often, and therefore has higher power, than the odds ratio homogeneity test. These examples suggest that current empirical evidence for the claim that the risk difference is more heterogeneous is not at present satisfactory. Further research could consider other approaches to empirical comparisons of the heterogeneity of the three measures.},
  langid = {american}
}

@online{Yang2024bivariate,
  title = {Bivariate Multilevel Meta-Analysis of Log Response Ratio and Standardized Mean Difference for Robust and Reproducible Environmental and Biological Sciences},
  author = {Yang, Yefeng and Williams, Coralie and Senior, Alistair M. and Morrison, Kyle and Ricolfi, Lorenzo and Pan, Jinming and Lagisz, Malgorzata and Nakagawa, Shinichi},
  date = {2024-05-16},
  eprinttype = {bioRxiv},
  eprintclass = {New Results},
  pages = {2024.05.13.594019},
  doi = {10.1101/2024.05.13.594019},
  url = {https://www.biorxiv.org/content/10.1101/2024.05.13.594019v1},
  urldate = {2025-04-24},
  abstract = {Meta-analytic modelling plays a pivotal role in synthesizing research and informing relevant policies. Yet researchers face many analytical challenges. In environmental and biological sciences, one of the most common yet unrecognised issues is the selection between two common effect size metrics, log response ratio (lnRR) and standardized mean difference (SMD); these two are the most popular and alternative effect sizes. Having to choose between them creates room for analytical flexibility, which is susceptible to researcher degrees of freedom. Another common issue is failure to deal with statistical dependence between effect sizes, resulting in invalid inferences on evidence. We propose addressing these two issues through the joint synthesis (dual use) of lnRR and SMD. Using 75 meta-analyses, including 3,887 environmental/biological primary studies (∼20,000 effect sizes), we show a high false positive rate (40\%) in conventional meta-analytic practices (random-effects model) compared to the proposed bivariate multilevel meta-analysis of lnRR and SMD along with robust variance estimation. Relying solely on either lnRR or SMD results in non-trivial discrepancies in detecting statistically significant effects (18\%) and occasional inconsistencies in sign (9\%). Discrepancies in interpreting effect size, heterogeneity, and publication bias are prevalent between models using lnRR and SMD (e.g., 52\% for publication bias). In contrast, bivariate synthesis of lnRR and SMD yields substantial information gain, reducing standard error in effect size estimates by 29\%, equivalent to adding 40 additional effect sizes. We present a user-friendly website with a step-by-step implementation guide. Our proposed robust approach aspires to improve meta-analytic modelling using lnRR and SMD in environmental and biological evidence synthesis, amplifying their reproducibility and credibility.},
  langid = {english},
  pubstate = {prepublished}
}

@article{Zhao2022empirical,
  title = {Empirical Comparisons of Heterogeneity Magnitudes of the Risk Difference, Relative Risk, and Odds Ratio},
  author = {Zhao, Yuxi and Slate, Elizabeth H. and Xu, Chang and Chu, Haitao and Lin, Lifeng},
  date = {2022-02-12},
  journaltitle = {Systematic Reviews},
  shortjournal = {Syst Rev},
  volume = {11},
  number = {1},
  eprint = {35151340},
  eprinttype = {pubmed},
  pages = {26},
  issn = {2046-4053},
  doi = {10.1186/s13643-022-01895-7},
  langid = {english},
  pmcid = {PMC8840324},
  keywords = {Humans,Odds Ratio,Risk}
}
